Benchmark,Website,Paper,Description,Cognitive Functions
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro evaluates real-world software engineering by asking a model/agent to generate patches in real repositories and pass the project’s test suite. Compared with easier software-fix benchmarks, it emphasizes harder, more diverse issues and stronger contamination resistance, requiring multi-file edits and correct integration with existing code.","Planning, Adaptive Error Correction, Logical Reasoning, Working Memory, Language Comprehension (minor), Language Production (minor)"
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld measures multimodal “computer use” in a full operating-system environment, where the agent must complete tasks by interacting with GUIs and applications across multiple steps. Success requires perceiving the screen, choosing actions (click/type/navigation), recovering from mistakes, and coordinating long action sequences under constraints.","Visual Perception, Scene Understanding & Visual Reasoning, Planning, Decision-making, Sensorimotor Coordination, Attention (minor), Working Memory (minor)"
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI tests fluid, abstract reasoning with small grid-based input–output examples, requiring inference of hidden transformation rules from only a few demonstrations. It is designed to emphasize generalization to novel tasks rather than memorized knowledge, pushing compositional and pattern-based reasoning.","Logical Reasoning, Spatial Representation & Mapping, Working Memory, Cognitive Flexibility, Attention (minor)"
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon autonomy by having an agent run a simulated vending-machine business over an extended period, optimizing decisions like purchasing, pricing, supplier negotiation, and inventory. The score reflects sustained coherence and strategy across many interdependent steps, not just single-turn correctness.","Planning, Decision-making, Working Memory, Reward Mechanisms, Semantic Understanding & Context Recognition (minor), Adaptive Error Correction (minor)"
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas evaluates tool-use via the Model Context Protocol by testing whether a model can discover, select, and correctly invoke tools across multi-step workflows. Tasks require handling real API-like interfaces, tool errors, and result synthesis into a final response.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Language Comprehension (minor), Language Production (minor)"
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity agent capability on real-world software vulnerability tasks, including identifying known vulnerabilities from high-level descriptions and discovering new issues. It emphasizes reasoning about code behavior, attack surfaces, and producing actionable findings or fixes under realistic constraints.","Logical Reasoning, Planning, Adaptive Error Correction, Working Memory, Language Comprehension (minor), Inhibitory Control (minor)"
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a large, frontier-difficulty benchmark spanning many academic and professional domains, including multimodal questions, intended to probe the limits of expert-level knowledge and reasoning. It often rewards careful multi-step inference, synthesis, and avoidance of confident errors on hard items.","Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory (minor), Visual Perception (minor), Planning (minor)"
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a curated subset of very difficult, graduate-level multiple-choice science questions designed to be resistant to shallow pattern matching. It probes deep scientific reasoning and understanding under tight answer options where plausible distractors are common.","Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory (minor), Language Comprehension (minor)"
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro evaluates multimodal, multi-discipline understanding and reasoning, using images paired with questions across many expert domains. It tests whether a model can integrate visual evidence with textual context to perform higher-level reasoning rather than simple recognition.","Visual Perception, Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Language Comprehension (minor), Working Memory (minor)"
OmniDocBench 1.5,https://github.com/opendatalab/OmniDocBench,https://arxiv.org/abs/2412.07626,"OmniDocBench 1.5 evaluates document understanding/OCR across heterogeneous document elements such as text, formulas, tables, and reading order. It stresses accurate extraction and structural interpretation under varied layouts where small visual mistakes can cascade into large semantic errors.","Visual Perception, Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Language Comprehension, Working Memory (minor)"
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU extends multimodal understanding to video, requiring models to answer questions that depend on temporal events, actions, and context across frames. It emphasizes integrating visual evidence over time and maintaining coherence about what changed and when.","Visual Perception, Cognitive Timing & Predictive Modeling, Working Memory, Scene Understanding & Visual Reasoning, Attention (minor), Multisensory Integration (minor)"
LiveCodeBench Pro,https://livecodebenchpro.com/projects/livecodebench-pro/leaderboard,https://arxiv.org/abs/2506.11928,"LiveCodeBench Pro evaluates competitive-programming-style coding with strong controls around freshness/contamination, focusing on the ability to write correct programs under problem constraints. It measures end-to-end algorithm selection, implementation, debugging, and adherence to input/output specifications.","Logical Reasoning, Planning, Adaptive Error Correction, Working Memory, Language Comprehension (minor), Language Production (minor)"
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"The FACTS Benchmark Suite systematically evaluates factuality-related behaviors, such as producing claims grounded in given sources and resisting unsupported assertions. It targets reliability failures like hallucinations and mis-grounded statements across a range of tasks and domains.","Semantic Understanding & Context Recognition, Inhibitory Control, Language Comprehension (minor), Working Memory (minor), Self-reflection (minor)"
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA evaluates physical commonsense reasoning across many languages, emphasizing whether models can select plausible actions or outcomes in everyday physical scenarios. It stresses robustness to linguistic variation and culturally broad coverage while still targeting grounded physical plausibility.","Semantic Understanding & Context Recognition, Logical Reasoning, Language Comprehension, Spatial Representation & Mapping (minor), Cognitive Flexibility (minor)"
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) tests long-context, multi-round co-reference resolution by embedding multiple similar “needle” requests inside long “haystacks” and asking the model to retrieve the correct associated response. It measures precise context tracking and resistance to interference from highly similar distractors.","Working Memory, Attention, Episodic Memory, Language Comprehension (minor), Inhibitory Control (minor)"
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval evaluates well-specified professional knowledge-work tasks spanning many occupations, where outputs are judged against human professional quality (often side-by-side). Tasks frequently require producing real artifacts (e.g., analyses, plans, spreadsheets/presentations) and following constraints to completion.","Planning, Decision-making, Language Production, Semantic Understanding & Context Recognition, Working Memory (minor), Self-reflection (minor), Social Reasoning & Theory of Mind (minor)"
SWE-Lancer,https://openai.com/index/swe-lancer/,https://arxiv.org/abs/2502.12115,"SWE-Lancer evaluates agentic software engineering in realistic settings where the model must understand a repository, decide what to change, implement patches, and validate behavior. It emphasizes longer-horizon project work and reliability across end-to-end workflows rather than isolated coding snippets.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Language Comprehension (minor), Language Production (minor)"
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath evaluates very challenging, expert-level mathematics problems intended to measure true frontier reasoning rather than routine computation. It emphasizes multi-step derivations, careful symbolic manipulation, and maintaining correctness over long solution chains.","Logical Reasoning, Working Memory, Planning, Cognitive Flexibility (minor), Adaptive Error Correction (minor)"
