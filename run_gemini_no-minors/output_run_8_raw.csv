Benchmark,Website,Paper,Description,Cognitive Functions
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates software engineering agents on real GitHub issues by asking the model to generate a code patch that makes repository tests pass, with tasks curated and verified by humans to be solvable. It stresses end-to-end debugging, codebase navigation, and iterative fixing under realistic tooling and dependency constraints.","Planning, Logical Reasoning, Working Memory, Adaptive Error Correction, Language Comprehension (minor), Language Production (minor)"
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 measures an agent’s ability to accomplish practical tasks in a command-line environment (e.g., install dependencies, run programs, manipulate files, diagnose failures) using terminal tools. It emphasizes procedural competence, error recovery, and maintaining task state across multi-step interactions.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Attention (minor), Language Comprehension (minor)"
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench evaluates interactive tool-using agents in multi-turn customer-support-like scenarios (e.g., retail, airline, telecom), where the model must call APIs, follow policies, and resolve user goals. It probes reliable tool invocation, policy adherence under pressure, and handling ambiguous or shifting user intents over dialogue.","Decision-making, Planning, Inhibitory Control, Social Reasoning & Theory of Mind, Language Comprehension, Language Production, Working Memory (minor)"
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI tests few-shot abstraction and reasoning on grid-based pattern transformation tasks, where a system must infer a hidden rule from a handful of input-output examples. It is designed to emphasize generalization to novel concepts rather than memorized domain knowledge.","Logical Reasoning, Cognitive Flexibility, Working Memory, Visual Perception, Spatial Representation & Mapping, Scene Understanding & Visual Reasoning (minor)"
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon agent coherence and strategy by simulating the operation of a vending-machine business across an extended timeline, requiring many sequential decisions. High scores require sustained planning, adapting to market dynamics, and avoiding compounding errors over thousands of steps.","Planning, Decision-making, Reward Mechanisms, Working Memory, Semantic Understanding & Context Recognition (minor), Adaptive Error Correction (minor)"
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a difficult, frontier-knowledge benchmark with expert-level questions spanning many domains, often requiring multi-step reasoning and (in some settings) multimodal understanding. It aims to stress breadth of knowledge, careful reasoning, and robustness to tricky problem formulations.","Language Comprehension, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Visual Perception (minor), Scene Understanding & Visual Reasoning (minor)"
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 evaluates competition-style mathematics problem solving, typically requiring creative algebraic manipulation, counting/probability reasoning, and multi-step derivations. It primarily measures deductive reasoning and symbolic consistency rather than tool-use or domain knowledge breadth.","Logical Reasoning, Working Memory, Planning (minor), Language Comprehension (minor)"
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a subset of GPQA containing especially high-quality, graduate-level science multiple-choice questions designed to be resistant to shallow pattern matching. It emphasizes deep scientific understanding and multi-hop reasoning across physics, chemistry, and biology concepts.","Semantic Understanding & Context Recognition, Logical Reasoning, Language Comprehension, Working Memory (minor)"
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends the MMLU-style broad academic testing to multiple languages, assessing knowledge and reasoning across subjects in non-English settings. It probes how well models transfer conceptual understanding and instruction-following across linguistic contexts.","Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning, Cognitive Flexibility (minor), Working Memory (minor)"
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro is a challenging multimodal benchmark that evaluates expert-level understanding and reasoning over images paired with text, covering many disciplines and problem types. It stresses integrating visual evidence with language to answer questions that often require multi-step inference.","Visual Perception, Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Multisensory Integration, Language Comprehension, Logical Reasoning, Working Memory (minor)"
MathArena Apex,https://matharena.ai/?view=problem&comp=apex--apex_2025,https://arxiv.org/abs/2505.23281,MathArena Apex aggregates difficult mathematics problems (including competition and research-adjacent styles) and typically evaluates models under standardized solution protocols. It targets deeper mathematical reasoning and robustness on harder distributions than standard contest sets.,"Logical Reasoning, Working Memory, Planning, Adaptive Error Correction (minor), Language Comprehension (minor)"
ScreenShot-Pro,https://gui-agent.github.io/grounding-leaderboard/,https://arxiv.org/abs/2504.07981,"ScreenShot-Pro evaluates GUI understanding and grounding from high-resolution screenshots, requiring models to interpret interface layout and identify the correct targets/actions. It emphasizes spatial localization, reading UI text, and mapping visual elements to intended operations.","Visual Perception, Visual Attention & Eye Movements, Spatial Representation & Mapping, Scene Understanding & Visual Reasoning, Sensorimotor Coordination (minor), Decision-making (minor)"
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,CharXiv Reasoning tests scientific figure understanding by asking questions that require interpreting charts/figures from research papers and reasoning about the depicted evidence. Strong performance depends on extracting quantitative/structural information from visuals and integrating it with scientific context.,"Visual Perception, Scene Understanding & Visual Reasoning, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory (minor), Multisensory Integration (minor)"
OmniDocBench 1.5,https://github.com/opendatalab/OmniDocBench,https://arxiv.org/abs/2412.07626,"OmniDocBench 1.5 evaluates document understanding/OCR across diverse layouts (text, formulas, tables, and reading order), often using edit-distance-style metrics against ground truth. It stresses robust visual-text extraction and structural parsing under varied formatting and noise.","Visual Perception, Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Language Comprehension (minor), Spatial Representation & Mapping (minor)"
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU evaluates multimodal reasoning over video, where the model must integrate information across frames and time to answer questions. It probes temporal integration, event understanding, and maintaining coherence over longer audiovisual contexts.","Visual Perception, Attention, Working Memory, Cognitive Timing & Predictive Modeling, Scene Understanding & Visual Reasoning, Multisensory Integration (minor)"
LiveCodeBench Pro,https://livecodebenchpro.com/projects/livecodebench-pro/leaderboard,https://arxiv.org/abs/2506.11928,"LiveCodeBench Pro evaluates coding ability on problems designed to reflect contemporary programming tasks, often scored via pass@k or ELO-style competitive ratings and aimed at reducing contamination. It emphasizes producing correct, runnable solutions and iterating when initial attempts fail.","Logical Reasoning, Planning, Adaptive Error Correction, Working Memory, Language Comprehension (minor), Language Production (minor)"
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"FACTS Benchmark Suite systematically evaluates factuality, including whether models make unsupported claims, contradict evidence, or fail to attribute uncertainty appropriately. It targets truthful response generation across diverse settings, often separating retrieval/grounding failures from reasoning errors.","Semantic Understanding & Context Recognition, Language Production, Language Comprehension (minor), Inhibitory Control (minor), Self-reflection (minor)"
SimpleQA Verified,https://www.kaggle.com/benchmarks/deepmind/simpleqa-verified,https://arxiv.org/abs/2509.07968,SimpleQA Verified is a short-form factual question answering benchmark with verification-oriented curation to reduce ambiguity and improve label reliability. It focuses on precise recall and avoiding hallucinated specifics when the answer is unknown or not supported.,"Semantic Understanding & Context Recognition, Language Comprehension, Language Production, Inhibitory Control (minor), Episodic Memory (minor)"
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA evaluates physical commonsense and pragmatic reasoning across languages and cultural contexts, aiming to test whether models generalize everyday physics-intuitive judgments beyond English-centric datasets. Items often require selecting the more plausible action or outcome in real-world scenarios.","Logical Reasoning, Semantic Understanding & Context Recognition, Cognitive Flexibility, Spatial Representation & Mapping (minor), Language Comprehension (minor)"
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) measures long-context retrieval and multi-round coreference by embedding repeated, similar “needle” requests inside long “haystacks,” then asking the model to reproduce the correct response for a specified needle. It stresses maintaining and selecting the correct referent among many confusable instances over very long contexts.","Working Memory, Attention, Episodic Memory, Language Comprehension, Semantic Understanding & Context Recognition (minor), Inhibitory Control (minor)"
