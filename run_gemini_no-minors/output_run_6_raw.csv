Benchmark,Website,Paper,Description,Cognitive Functions
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates software engineering agents on real GitHub issues by asking them to generate a patch that makes the project’s tests pass, with tasks filtered to be reliably solvable and automatically verifiable. It stresses end-to-end bug fixing and codebase navigation under realistic repo constraints rather than isolated coding puzzles.","Planning, Logical Reasoning, Adaptive Error Correction, Working Memory, Language Comprehension (minor), Language Production (minor)"
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 measures how well an agent can accomplish practical tasks inside a command-line environment (e.g., installing dependencies, editing files, running programs, diagnosing failures). Success depends on executing multi-step tool interactions with feedback loops under time/step limits.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Inhibitory Control (minor), Language Comprehension (minor)"
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench evaluates interactive customer-support agents that must follow domain policies while using tools/APIs in multi-turn conversations with a simulated user. It emphasizes policy adherence, state tracking across turns, and correct tool invocation to resolve cases in retail/airline/telecom settings.","Social Reasoning & Theory of Mind, Decision-making, Planning, Working Memory, Language Comprehension, Language Production, Inhibitory Control (minor), Empathy (minor)"
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,ARC-AGI tests few-shot abstract reasoning by giving a handful of input–output grid examples and asking the model to infer the hidden transformation rule for a new grid. It targets fluid intelligence via novel pattern induction with minimal training-set overlap and strong emphasis on generalization.,"Logical Reasoning, Cognitive Flexibility, Spatial Representation & Mapping, Visual Perception, Working Memory, Planning (minor)"
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon autonomous agency by having a model run a simulated vending-machine business over many in-environment days, managing inventory, suppliers, pricing, and cash flow. Scores reflect sustained coherence, strategic adaptation to a changing market, and effective multi-step action selection.","Planning, Decision-making, Reward Mechanisms, Working Memory, Episodic Memory (minor), Semantic Understanding & Context Recognition (minor), Social Reasoning & Theory of Mind (minor)"
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a frontier, often multimodal, academic benchmark spanning difficult questions across many disciplines, designed to probe advanced reasoning and knowledge at the edge of typical model competence. It is commonly reported both with and without tool access (e.g., search/code), reflecting differences between pure reasoning and tool-augmented performance.","Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory (minor), Visual Perception (minor), Planning (minor)"
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,AIME 2025 evaluates competition-style mathematics with short-answer problems requiring multi-step symbolic and quantitative reasoning under tight output constraints. Performance reflects the ability to reliably execute complex derivations without external tools (or with tools in some reporting variants).,"Logical Reasoning, Working Memory, Adaptive Error Correction (minor), Planning (minor)"
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,GPQA Diamond is a high-difficulty multiple-choice science QA benchmark curated so that non-experts tend to fail while subject-matter experts succeed. It probes rigorous scientific reasoning and knowledge while reducing the usefulness of superficial pattern matching or quick web lookups.,"Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory (minor), Language Comprehension (minor)"
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends MMLU-style broad knowledge and reasoning evaluation to multiple languages, testing whether a model can generalize subject knowledge beyond English. It emphasizes multilingual comprehension, cross-lingual transfer, and consistent reasoning across varied domains.","Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning (minor), Working Memory (minor)"
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro is a high-difficulty multimodal benchmark spanning many subjects where questions require integrating visual inputs (charts, diagrams, photos) with text to answer. It targets expert-level multimodal reasoning rather than simple image captioning or recognition.","Multisensory Integration, Scene Understanding & Visual Reasoning, Visual Perception, Language Comprehension, Visual Attention & Eye Movements (minor), Logical Reasoning (minor)"
MathArena Apex,https://matharena.ai/?view=problem&comp=apex--apex_2025,https://arxiv.org/abs/2505.23281,MathArena Apex aggregates very challenging math problems (often contest or research-adjacent) and is used to compare top models on difficult multi-step derivations. It is designed to stress depth of reasoning and robustness across diverse mathematical topics.,"Logical Reasoning, Working Memory, Planning (minor), Adaptive Error Correction (minor)"
ScreenShot-Pro,https://gui-agent.github.io/grounding-leaderboard/,https://arxiv.org/abs/2504.07981,"ScreenShot-Pro evaluates GUI understanding from screenshots, requiring a model/agent to interpret interface elements and answer questions or identify targets based on layout and visual cues. It probes spatial grounding and reliable perception-to-action mapping that is central to computer-use agents.","Visual Perception, Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Spatial Representation & Mapping, Decision-making (minor), Sensorimotor Coordination (minor)"
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning focuses on scientific figure understanding, asking models to answer reasoning questions grounded in figures from research papers (often benefiting from tools like Python for quantitative checks). It emphasizes extracting structured information from visuals and combining it with domain knowledge to justify conclusions.","Scene Understanding & Visual Reasoning, Visual Perception, Multisensory Integration, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory (minor)"
OmniDocBench 1.5,https://github.com/opendatalab/OmniDocBench,https://arxiv.org/abs/2412.07626,"OmniDocBench 1.5 evaluates document understanding/OCR across diverse layouts (text, formulas, tables, and reading order), measuring how accurately systems extract and structure information. It targets robustness to real-world document variability rather than clean, single-format inputs.","Visual Perception, Visual Attention & Eye Movements, Language Comprehension, Semantic Understanding & Context Recognition (minor), Working Memory (minor)"
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU extends multimodal understanding to videos, requiring models to integrate information over time to answer questions about events, actions, and causal relations. It stresses temporal comprehension and maintaining context across multiple frames and scenes.","Visual Perception, Cognitive Timing & Predictive Modeling, Working Memory, Scene Understanding & Visual Reasoning, Attention (minor), Multisensory Integration (minor)"
LiveCodeBench Pro,https://livecodebenchpro.com/projects/livecodebench-pro/leaderboard,https://arxiv.org/abs/2506.11928,"LiveCodeBench Pro evaluates coding ability on curated programming tasks with strong attention to contamination resistance and real-world coding skills. It is often scored with strict pass@k-style correctness checks, emphasizing functional accuracy over stylistic code generation.","Logical Reasoning, Planning, Adaptive Error Correction, Working Memory, Language Production (minor)"
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,FACTS Benchmark Suite systematically evaluates factuality by measuring whether model outputs remain consistent with reliable evidence and avoid unsupported claims across a variety of factuality-oriented tasks. It is designed to provide broader coverage than single QA sets and to expose different failure modes of hallucination.,"Semantic Understanding & Context Recognition, Language Comprehension, Inhibitory Control (minor), Self-reflection (minor), Logical Reasoning (minor)"
SimpleQA Verified,https://www.kaggle.com/benchmarks/deepmind/simpleqa-verified,https://arxiv.org/abs/2509.07968,"SimpleQA Verified is a short-form factual question answering benchmark with verified answers, intended to measure basic factual recall and precision. It penalizes confident hallucinations on straightforward queries where correctness is clearly checkable.","Semantic Understanding & Context Recognition, Language Comprehension, Inhibitory Control (minor)"
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA evaluates physical commonsense reasoning across many languages, using the PIQA-style format where models choose the more plausible solution to a physical interaction scenario. It targets grounded, everyday reasoning about objects and actions while testing cross-lingual robustness.","Semantic Understanding & Context Recognition, Logical Reasoning, Spatial Representation & Mapping (minor), Language Comprehension, Motor Coordination (minor)"
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,MRCR v2 (8-needle) tests long-context retrieval and multi-round coreference by embedding multiple similar “needle” interactions within a very long “haystack” and asking the model to reproduce the correct response for a specified needle. It stresses robustness to distractors and the ability to maintain and query relevant context over long token spans.,"Working Memory, Attention, Language Comprehension, Episodic Memory (minor), Planning (minor)"
