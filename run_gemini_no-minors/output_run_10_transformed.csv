Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro evaluates software engineering agents on realistic issues in real codebases, requiring them to understand repository context, implement changes, and produce patches that pass tests. Compared to SWE-bench Verified, it is larger and more difficult, includes multiple programming languages, and is designed to be more contamination-resistant and industry-relevant.","L1: Language Comprehension (minor), Language Production (minor)
L2: Planning, Logical Reasoning, Adaptive Error Correction, Working Memory
L3: ",L2
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld benchmarks multimodal computer-use agents in operating system environments, where models must interpret GUI screens and execute multi-step actions to complete tasks. Success depends on robust perception of UI state, sequential action selection, and recovery from mistakes under step limits.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Planning, Decision-making, Attention, Sensorimotor Coordination (minor), Adaptive Error Correction (minor)
L3: ",L2
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,ARC-AGI (Abstraction and Reasoning Corpus) measures few-shot “fluid” reasoning over small grid-world puzzles where the rule must be inferred from a handful of input–output examples. It emphasizes out-of-distribution generalization to new concepts and transformations rather than memorized domain knowledge.,"L1: Visual Perception (minor)
L2: Logical Reasoning, Working Memory, Spatial Representation & Mapping, Planning (minor)
L3: Cognitive Flexibility",L3
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon agent coherence by simulating a year of running a vending-machine business, including procurement, pricing, negotiation, and inventory management. The score is typically the final balance (or profitability), requiring sustained strategy and adaptation across many decisions.","L1: Language Production (minor)
L2: Planning, Decision-making, Reward Mechanisms, Working Memory, Adaptive Error Correction (minor)
L3: Self-reflection (minor)",L2
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas measures real-world tool use through the Model Context Protocol, requiring models to discover, call, and chain tools across multi-step workflows in production-like API environments. It emphasizes correct tool selection, parameterization, error handling, and synthesis of results into final answers.","L1: Language Comprehension (minor), Language Production (minor)
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory
L3: ",L2
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity agent capabilities on real-world vulnerability tasks, including finding known vulnerabilities from high-level weakness descriptions and discovering previously unknown issues in open-source projects. Performance depends on understanding code, reasoning about exploit conditions, and producing correct findings under a pass@1 setting.","L1: 
L2: Logical Reasoning, Planning, Adaptive Error Correction, Attention (minor), Working Memory (minor), Semantic Understanding & Context Recognition
L3: ",L2
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a large, frontier-level benchmark spanning many academic and professional domains, intended to probe reasoning and knowledge at or beyond expert human level. It often includes multimodal questions and is used to compare model performance with and without tools such as search or code execution.","L1: Language Comprehension, Visual Perception (minor)
L2: Logical Reasoning, Working Memory, Semantic Understanding & Context Recognition, Scene Understanding & Visual Reasoning (minor)
L3: ",L2
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a curated subset of very difficult, graduate-level multiple-choice science questions designed to be “Google-proof” and resistant to shallow pattern matching. It targets deep conceptual understanding and multi-step reasoning in physics, chemistry, and biology.","L1: Language Comprehension
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory (minor)
L3: ",L2
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro evaluates multimodal understanding and expert-level reasoning across disciplines using problems that combine images (e.g., diagrams, charts, figures) with text questions. It stresses extracting relevant visual evidence, integrating it with domain knowledge, and performing multi-step reasoning under varied question formats.","L1: Visual Perception, Language Comprehension (minor)
L2: Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Attention (minor), Working Memory (minor)
L3: ",L2
OmniDocBench 1.5,https://github.com/opendatalab/OmniDocBench,https://arxiv.org/abs/2412.07626,"OmniDocBench 1.5 evaluates document AI by testing end-to-end understanding of complex documents containing text, tables, formulas, and reading order structure. Metrics such as edit distance reflect how accurately a model can reconstruct and interpret document content from visual or mixed inputs.","L1: Visual Perception, Language Comprehension, Language Production
L2: Scene Understanding & Visual Reasoning, Attention (minor), Working Memory (minor)
L3: ",L2
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU extends multimodal understanding to the video setting, requiring models to answer questions that depend on events and visual details across time. It probes whether a model can integrate information from multiple frames/clips and maintain coherence over temporally extended evidence.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Working Memory, Attention (minor), Multisensory Integration (minor)
L3: Cognitive Timing & Predictive Modeling",L3
LiveCodeBench Pro,https://livecodebenchpro.com/projects/livecodebench-pro/leaderboard,https://arxiv.org/abs/2506.11928,"LiveCodeBench Pro measures competitive coding ability on recent, evolving programming problems, aiming to reduce training contamination by using time-based curation and fresh tasks. It tests algorithmic reasoning, implementation correctness, and iterative debugging under realistic constraints.","L1: Language Production (minor), Language Comprehension (minor)
L2: Logical Reasoning, Planning, Adaptive Error Correction, Working Memory
L3: ",L2
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"The FACTS Benchmark Suite systematically evaluates factuality in LLM outputs across multiple subtests that stress grounding, consistency, and resistance to hallucination. It measures whether a model can maintain truthfulness under prompting and retrieval-like settings while producing fluent natural language answers.","L1: Language Comprehension, Language Production
L2: Semantic Understanding & Context Recognition, Working Memory (minor)
L3: Inhibitory Control, Self-reflection (minor)",L3
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA is a multilingual, non-parallel extension of physical commonsense reasoning tasks where models choose the more plausible solution to everyday physical interaction scenarios across many languages and cultures. It probes whether commonsense physical reasoning generalizes beyond English and across linguistic variation.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Spatial Representation & Mapping (minor)
L3: Cognitive Flexibility (minor)",L2
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,MRCR v2 (multi-round coreference resolution) tests long-context integration by inserting multiple similar “needle” requests into long “haystacks” and asking the model to reproduce the correct response corresponding to a specific needle. The 8-needle variant stresses sustained retrieval and disambiguation over long documents with many confusable entries.,"L1: Language Comprehension
L2: Working Memory, Attention, Episodic Memory, Semantic Understanding & Context Recognition
L3: ",L2
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval evaluates well-specified professional knowledge-work tasks spanning many occupations, where models produce real work artifacts (e.g., spreadsheets, presentations, plans) judged against industry professionals. It emphasizes end-to-end task execution quality, including correctness, structure, and usefulness under realistic constraints.","L1: Language Production
L2: Planning, Decision-making, Semantic Understanding & Context Recognition, Working Memory (minor)
L3: Self-reflection (minor), Social Reasoning & Theory of Mind (minor)",L2
SWE-Lancer,https://openai.com/index/swe-lancer/,https://arxiv.org/abs/2502.12115,"SWE-Lancer evaluates agentic software engineering in a more autonomous “contractor” setting, where the model must complete programming tasks end-to-end with minimal hand-holding. It focuses on sustained execution, correct patch generation, and reliable completion of work-like engineering deliverables.","L1: Language Production (minor)
L2: Planning, Decision-making, Logical Reasoning, Adaptive Error Correction, Working Memory (minor)
L3: ",L2
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath is a curated benchmark of advanced mathematics problems intended to measure progress toward expert-level mathematical reasoning, with tiers covering increasing difficulty. It emphasizes rigorous multi-step derivations, abstraction, and error-sensitive reasoning, often benefiting from tool-assisted computation but requiring correct mathematical structure.","L1: 
L2: Logical Reasoning, Planning, Working Memory, Adaptive Error Correction
L3: Cognitive Flexibility (minor)",L2
