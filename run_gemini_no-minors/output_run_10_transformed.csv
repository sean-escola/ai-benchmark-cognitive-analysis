Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates an agent’s ability to solve real GitHub software engineering issues by producing code patches that make a project’s tests pass. The “Verified” subset focuses on problems that have been human-validated as solvable and aims to measure end-to-end debugging, code understanding, and patch generation.","L1: Language Comprehension (minor), Language Production (minor)
L2: Planning, Logical Reasoning, Adaptive Error Correction, Working Memory
L3: ",L2
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 measures how well agentic systems can accomplish real tasks in a command-line environment (e.g., installing dependencies, running tools, manipulating files, and debugging failures). It emphasizes iterative tool use, recovery from errors, and maintaining a coherent action sequence over many steps.","L1: Language Comprehension (minor)
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory, Attention (minor)
L3: ",L2
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench evaluates interactive agent performance in multi-turn customer-support-style settings where the model must use tools/APIs while following domain policies (e.g., retail, airline, telecom). It stresses policy compliance, robust dialogue, and correct execution of tool-mediated actions under user pressure and ambiguity.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making
L3: Social Reasoning & Theory of Mind, Inhibitory Control",L3
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,ARC-AGI tests “fluid” abstract reasoning by asking models to infer rules that map input grids to output grids from only a few examples. Success requires discovering latent pattern transformations and generalizing them to novel instances rather than relying on memorized domain knowledge.,"L1: Visual Perception (minor)
L2: Logical Reasoning, Spatial Representation & Mapping, Working Memory, Scene Understanding & Visual Reasoning (minor)
L3: Cognitive Flexibility",L3
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon agent coherence and strategy in a simulated vending-machine business over an extended time period. Agents must plan inventory, pricing, supplier interactions, and adapt to changing conditions to maximize final balance.","L1: 
L2: Planning, Decision-making, Reward Mechanisms, Working Memory, Adaptive Error Correction (minor), Semantic Understanding & Context Recognition (minor)
L3: ",L2
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam (HLE) is a frontier, multimodal benchmark spanning difficult academic and real-world questions intended to stress advanced reasoning and broad knowledge. It is often evaluated both without tools and with tools (e.g., search/code), measuring how well models integrate information and produce correct final answers.","L1: Language Comprehension, Visual Perception (minor)
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Scene Understanding & Visual Reasoning (minor)
L3: ",L2
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,AIME 2025 consists of competition-style math problems requiring multi-step derivations and careful symbolic manipulation. It emphasizes rigorous reasoning and intermediate-state tracking rather than reliance on external facts.,"L1: Language Comprehension (minor)
L2: Logical Reasoning, Working Memory, Planning (minor)
L3: ",L2
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,GPQA Diamond is a high-quality subset of graduate-level multiple-choice science questions designed to be difficult to answer via shallow pattern matching. It probes deep scientific reasoning and the ability to discriminate between plausible distractors.,"L1: Language Comprehension (minor)
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory (minor)
L3: ",L2
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,MMMLU extends broad academic knowledge and reasoning evaluation across many subjects and multiple non-English languages. It measures whether models can transfer understanding and reasoning skills across linguistic contexts.,"L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning (minor), Working Memory (minor)
L3: ",L2
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,MMMU-Pro evaluates expert-level multimodal understanding and reasoning using images paired with questions across many disciplines. It tests whether models can fuse visual evidence with textual instructions to choose or generate correct answers.,"L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Multisensory Integration, Working Memory (minor)
L3: ",L2
MathArena Apex,https://matharena.ai/?view=problem&comp=apex--apex_2025,https://arxiv.org/abs/2505.23281,"MathArena Apex is a challenging mathematics benchmark intended to separate top models via harder, longer-horizon problems than standard contest sets. It emphasizes robust multi-step reasoning, solution planning, and precision under complexity.","L1: 
L2: Logical Reasoning, Working Memory, Planning (minor)
L3: ",L2
ScreenShot-Pro,https://gui-agent.github.io/grounding-leaderboard/,https://arxiv.org/abs/2504.07981,"ScreenShot-Pro evaluates screenshot-based GUI understanding, where models must interpret high-resolution interfaces and ground actions/answers in specific on-screen elements. It probes visual grounding, spatial localization, and reliable mapping from intent to interface-relevant outputs.","L1: Visual Perception
L2: Visual Attention & Eye Movements, Spatial Representation & Mapping, Scene Understanding & Visual Reasoning, Sensorimotor Coordination (minor)
L3: ",L2
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning evaluates scientific figure and chart reasoning drawn from papers, requiring models to interpret plots, legends, axes, and quantitative relationships. Many setups allow computation tools, testing whether models can combine visual extraction with structured analysis to answer questions correctly.","L1: Visual Perception, Language Comprehension (minor)
L2: Scene Understanding & Visual Reasoning, Logical Reasoning, Working Memory (minor)
L3: ",L2
OmniDocBench 1.5,https://github.com/opendatalab/OmniDocBench,https://arxiv.org/abs/2412.07626,"OmniDocBench 1.5 evaluates document AI systems on faithful extraction and reconstruction from complex documents containing text, tables, formulas, and reading order constraints. It primarily measures robust visual-document parsing and producing accurate structured outputs from heterogeneous layouts.","L1: Visual Perception, Language Comprehension
L2: Spatial Representation & Mapping, Semantic Understanding & Context Recognition (minor), Working Memory (minor)
L3: ",L2
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU extends multimodal understanding to video, requiring models to answer questions that depend on events unfolding over time and sometimes on fine-grained visual details. It probes temporal integration and maintaining coherent interpretations across multiple frames/clips.","L1: Visual Perception, Language Comprehension (minor)
L2: Working Memory, Scene Understanding & Visual Reasoning
L3: Cognitive Timing & Predictive Modeling",L3
LiveCodeBench Pro,https://livecodebenchpro.com/projects/livecodebench-pro/leaderboard,https://arxiv.org/abs/2506.11928,"LiveCodeBench Pro evaluates coding ability on problems that reflect contemporary programming tasks, often emphasizing correctness under realistic constraints and up-to-date problem sets. It measures a model’s ability to reason about code, implement solutions, and iteratively correct mistakes to pass tests.","L1: Language Production (minor)
L2: Logical Reasoning, Planning, Adaptive Error Correction, Working Memory
L3: ",L2
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"The FACTS Benchmark Suite systematically evaluates factuality and faithfulness, including whether model outputs are supported by provided context or reliable sources and whether they avoid hallucinated details. It targets calibration-like behavior—sticking to evidence, refusing unsupported claims, and maintaining consistent factual statements.","L1: Language Comprehension (minor), Language Production (minor)
L2: Semantic Understanding & Context Recognition
L3: Inhibitory Control, Self-reflection (minor)",L3
SimpleQA Verified,https://www.kaggle.com/benchmarks/deepmind/simpleqa-verified,https://arxiv.org/abs/2509.07968,"SimpleQA Verified focuses on short, unambiguous factual questions with verified answers, aiming to measure precision and reduce grading noise. It emphasizes correctness on concise factual queries and the ability to avoid plausible-but-wrong completions.","L1: Language Comprehension (minor), Language Production (minor)
L2: Semantic Understanding & Context Recognition
L3: Inhibitory Control (minor)",L2
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA evaluates physical commonsense reasoning across many languages, testing whether models can choose the more plausible action/solution in everyday situations. It stresses robust, language-agnostic commonsense inference rather than narrow memorization of English-specific patterns.","L1: Language Comprehension
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Spatial Representation & Mapping (minor)
L3: ",L2
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) is a long-context co-reference retrieval evaluation where multiple similar “needle” interactions are embedded in a long “haystack,” and the model must reproduce the response to a specified needle. It primarily measures sustained attention and accurate retrieval across long sequences with high interference.","L1: Language Comprehension
L2: Attention, Working Memory, Episodic Memory, Adaptive Error Correction (minor)
L3: ",L2
