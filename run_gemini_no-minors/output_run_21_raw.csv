Benchmark,Website,Paper,Description,Cognitive Functions
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro evaluates models on realistic software engineering issues drawn from real repositories, where the model must produce correct code patches that satisfy hidden tests. Compared with earlier SWE-bench variants, it emphasizes harder, more diverse, and more contamination-resistant tasks across multiple programming languages and complex codebases.","Logical Reasoning, Planning, Adaptive Error Correction, Working Memory, Language Comprehension (minor), Decision-making (minor)"
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld is a multimodal benchmark for computer use in a full operating-system-like environment, where an agent must complete end-to-end tasks (e.g., browsing, file operations, app interactions) through GUI actions. It tests whether models can perceive screens, plan multi-step interaction sequences, recover from UI errors, and reliably execute actions under step limits.","Visual Perception, Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Sensorimotor Coordination, Planning, Decision-making, Adaptive Error Correction, Spatial Representation & Mapping (minor), Working Memory (minor)"
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI measures “fluid” reasoning by asking models to infer latent rules from a few input–output grid examples and produce the correct output for a new grid. The tasks are designed to reduce reliance on memorized knowledge and instead probe rapid abstraction, compositional generalization, and robust pattern induction under few-shot constraints.","Logical Reasoning, Working Memory, Cognitive Flexibility, Spatial Representation & Mapping, Scene Understanding & Visual Reasoning, Visual Perception (minor), Attention (minor)"
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon agent coherence by placing an agent in a year-long simulated vending-machine business with many sequential decisions (inventory, pricing, procurement, negotiation). Scores are based on final financial outcomes, incentivizing sustained planning, adaptation to changing conditions, and consistent execution over thousands of steps.","Planning, Decision-making, Reward Mechanisms, Working Memory, Episodic Memory (minor), Cognitive Timing & Predictive Modeling (minor), Adaptive Error Correction (minor), Language Production (minor)"
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas evaluates real-world tool use via the Model Context Protocol by requiring models to discover appropriate tools, call them with correct parameters, handle errors/retries, and synthesize outputs into a final answer. Tasks often require multi-step workflows across multiple services, stressing reliable orchestration rather than single-turn question answering.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Language Comprehension (minor), Language Production (minor), Semantic Understanding & Context Recognition (minor), Inhibitory Control (minor)"
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym measures cybersecurity agent competence on real-world vulnerability tasks at scale, including identifying known vulnerabilities from descriptions and discovering previously unknown issues in codebases. It emphasizes precise reasoning about program behavior, careful inspection of code and configs, and producing actionable findings or fixes under realistic constraints.","Logical Reasoning, Attention, Working Memory, Planning (minor), Adaptive Error Correction (minor), Decision-making (minor), Language Comprehension (minor), Inhibitory Control (minor)"
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a large, difficult benchmark of expert-level questions spanning many domains and modalities, intended to stress frontier knowledge and reasoning. It typically requires multi-step inference, careful reading of problem statements, and (for multimodal items) accurate interpretation of figures or diagrams.","Language Comprehension, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Attention (minor), Scene Understanding & Visual Reasoning (minor), Visual Perception (minor)"
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a high-quality subset of graduate-level, “Google-proof” multiple-choice science questions designed to be difficult for non-experts. The benchmark targets deep conceptual understanding and careful elimination of distractors rather than surface-level pattern matching.","Language Comprehension, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory (minor), Attention (minor)"
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro is a rigorous multimodal benchmark (images + text) spanning many disciplines, with tasks that require interpreting visual artifacts such as diagrams, charts, scientific figures, and UI-like images. It stresses cross-modal reasoning, spatial understanding, and the ability to ground language answers in visual evidence.","Visual Perception, Scene Understanding & Visual Reasoning, Multisensory Integration, Spatial Representation & Mapping, Language Comprehension, Logical Reasoning, Visual Attention & Eye Movements (minor), Working Memory (minor)"
OmniDocBench 1.5,https://github.com/opendatalab/OmniDocBench,https://arxiv.org/abs/2412.07626,"OmniDocBench 1.5 evaluates document AI systems on end-to-end document understanding, including OCR quality, layout understanding, tables, formulas, and reading order. It focuses on faithfully reconstructing and interpreting complex, real-world documents rather than only recognizing plain text.","Visual Perception, Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Spatial Representation & Mapping, Language Comprehension (minor), Working Memory (minor)"
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU extends multimodal understanding to video, requiring models to answer questions about events, actions, temporal relations, and visual details across time. It probes whether a model can integrate information across frames and maintain coherent representations of evolving scenes.","Visual Perception, Visual Attention & Eye Movements, Cognitive Timing & Predictive Modeling, Scene Understanding & Visual Reasoning, Working Memory, Multisensory Integration (minor), Language Comprehension (minor)"
LiveCodeBench Pro,https://livecodebenchpro.com/projects/livecodebench-pro/leaderboard,https://arxiv.org/abs/2506.11928,"LiveCodeBench Pro evaluates coding ability on time-split, execution-verified programming problems intended to reflect contemporary coding demands while reducing contamination. It typically scores models by whether their generated solutions pass tests in a single attempt, emphasizing correctness and robustness.","Logical Reasoning, Planning, Adaptive Error Correction, Working Memory, Language Production (minor), Language Comprehension (minor), Decision-making (minor)"
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"The FACTS Benchmark Suite systematically evaluates factuality-related behaviors, including whether models produce accurate claims, avoid unsupported hallucinations, and appropriately qualify uncertainty. It emphasizes truthfulness and grounding rather than creativity or open-ended plausibility.","Semantic Understanding & Context Recognition, Inhibitory Control, Self-reflection (minor), Language Production (minor), Language Comprehension (minor), Working Memory (minor), Logical Reasoning (minor)"
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA measures physical commonsense reasoning across languages and cultures, typically via scenario-based questions about everyday interactions with objects and environments. It aims to test whether models can generalize intuitive physics and practical affordances beyond English-centric datasets.","Semantic Understanding & Context Recognition, Logical Reasoning, Language Comprehension, Spatial Representation & Mapping (minor), Working Memory (minor)"
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 evaluates long-context multi-round coreference resolution by embedding multiple similar “needle” turns within long “haystack” dialogues and asking the model to retrieve the correct target response. The 8-needle setting stresses selective retrieval, interference resistance, and consistent tracking of entity/reference links across long contexts.","Working Memory, Attention, Episodic Memory, Language Comprehension, Semantic Understanding & Context Recognition, Inhibitory Control (minor), Cognitive Flexibility (minor)"
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval evaluates economically meaningful, well-specified professional knowledge work across many occupations by having models produce real work artifacts (e.g., presentations, spreadsheets, plans) judged against expert human outputs. It tests end-to-end task execution quality, including following specifications, producing polished deliverables, and maintaining internal consistency across multi-part work products.","Planning, Decision-making, Language Production, Language Comprehension, Semantic Understanding & Context Recognition, Working Memory, Self-reflection (minor), Social Reasoning & Theory of Mind (minor)"
SWE-Lancer,https://openai.com/index/swe-lancer/,https://arxiv.org/abs/2502.12115,"SWE-Lancer evaluates software engineering performance on practical tasks that resemble real development work, such as implementing features, fixing bugs, and integrating changes in existing repositories. It emphasizes producing correct patches under realistic constraints and often rewards reliable end-to-end execution over isolated algorithmic problem solving.","Planning, Logical Reasoning, Adaptive Error Correction, Working Memory, Decision-making (minor), Language Comprehension (minor), Language Production (minor)"
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath is a difficult mathematics benchmark intended to probe advanced mathematical problem solving beyond routine competition problems, with tiers reflecting increasing difficulty. It emphasizes multi-step derivations, rigorous reasoning, and error-sensitive symbolic manipulation, often with tool-assisted settings (e.g., Python) in some evaluations.","Logical Reasoning, Working Memory, Planning, Attention (minor), Cognitive Flexibility (minor), Adaptive Error Correction (minor), Language Comprehension (minor)"
