Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro is a large-scale software engineering benchmark where a model must understand a real repository, implement a correct patch, and pass the project’s tests under realistic constraints. Compared to SWE-bench Verified, it is designed to be more difficult and more contamination-resistant, spanning multiple languages and more diverse industrial tasks.","L1: Language Comprehension, Language Production (minor)
L2: Semantic Understanding & Context Recognition, Planning, Logical Reasoning, Working Memory, Adaptive Error Correction, Decision-making (minor)
L3: ",L2
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld evaluates multimodal “computer use” agents that must complete tasks inside a full operating-system desktop environment using screenshots and interactive actions (e.g., mouse/keyboard). Success requires navigating UI state, selecting correct tools/apps, and executing multi-step procedures robustly across many steps.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Attention, Planning, Decision-making, Sensorimotor Coordination, Working Memory (minor), Adaptive Error Correction (minor)
L3: ",L2
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI is a fluid intelligence benchmark based on abstract grid transformation puzzles, where models infer latent rules from a few input–output examples and generalize to a new query grid. It is intended to measure generalization and compositional reasoning on novel tasks rather than memorized knowledge.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Spatial Representation & Mapping, Logical Reasoning, Working Memory, Planning (minor)
L3: Cognitive Flexibility",L3
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon agent coherence in a simulated year-long vending-machine business, where the agent must manage inventory, suppliers, pricing, and cash flow over thousands of decisions. Scores reflect final business outcomes, requiring sustained strategy, adaptation to changing conditions, and avoidance of compounding mistakes.","L1: 
L2: Planning, Decision-making, Reward Mechanisms, Working Memory, Episodic Memory, Semantic Understanding & Context Recognition, Adaptive Error Correction (minor)
L3: Social Reasoning & Theory of Mind (minor)",L2
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas measures real-world tool-use via the Model Context Protocol by requiring models to discover, call, and chain authentic tools/APIs to complete multi-step tasks. It emphasizes correct tool selection, parameterization, error handling, and synthesis of tool results into a final answer.","L1: Language Comprehension, Language Production (minor)
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition
L3: ",L2
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity agent capabilities on large numbers of tasks drawn from real-world software vulnerabilities, including identifying known weaknesses and discovering new ones. Agents must interpret vulnerability descriptions, analyze codebases and execution traces, and produce correct exploit/patch-oriented reasoning under realistic constraints.","L1: Language Comprehension
L2: Logical Reasoning, Planning, Adaptive Error Correction, Working Memory, Decision-making, Semantic Understanding & Context Recognition (minor)
L3: ",L2
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a frontier, multi-domain benchmark of difficult questions intended to probe high-end reasoning and expert knowledge, with multimodal items and tool-enabled variants sometimes reported separately. It stresses solving novel, high-complexity problems where shallow pattern matching is insufficient and errors are costly.","L1: Language Comprehension
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Scene Understanding & Visual Reasoning, Multisensory Integration (minor), Decision-making (minor)
L3: ",L2
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a high-quality subset of extremely difficult graduate-level science multiple-choice questions designed to be “Google-proof,” meaning naive retrieval is unlikely to help. It targets deep scientific reasoning and precise understanding, with distractors that penalize shallow heuristics.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Decision-making (minor)
L3: ",L2
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro is an advanced multimodal benchmark where models answer expert-level questions that require integrating images (e.g., diagrams, charts, documents, UI screenshots) with text. It emphasizes higher difficulty and more realistic professional visual reasoning compared to earlier multimodal QA sets.","L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Visual Attention & Eye Movements (minor), Working Memory (minor)
L3: ",L2
OmniDocBench 1.5,https://github.com/opendatalab/OmniDocBench,https://arxiv.org/abs/2412.07626,"OmniDocBench 1.5 evaluates document understanding and OCR-centric extraction across complex layouts, including text, tables, formulas, and reading order. Metrics typically reflect how accurately a system reconstructs or edits structured document content from rendered pages.","L1: Visual Perception, Language Comprehension
L2: Visual Attention & Eye Movements, Semantic Understanding & Context Recognition, Working Memory (minor)
L3: ",L2
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU extends multimodal understanding to video, requiring models to answer questions that depend on temporal sequences, actions, and evolving scene context. It tests whether a model can integrate information across frames and maintain coherent understanding over time.","L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Attention, Working Memory, Multisensory Integration (minor)
L3: Cognitive Timing & Predictive Modeling",L3
LiveCodeBench Pro,https://livecodebenchpro.com/projects/livecodebench-pro/leaderboard,https://arxiv.org/abs/2506.11928,"LiveCodeBench Pro evaluates coding performance on fresh, competition-style programming tasks with strong contamination controls and time-aware splits. It measures a model’s ability to derive algorithms, implement correct solutions, and handle edge cases under single-attempt constraints.","L1: Language Comprehension, Language Production
L2: Logical Reasoning, Planning, Working Memory, Adaptive Error Correction, Decision-making (minor)
L3: ",L2
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"The FACTS Benchmark Suite systematically evaluates factuality, including whether models avoid fabricating unsupported claims, maintain grounding to provided sources, and correctly attribute uncertainty. It is designed to separate fluent generation from truthfulness across multiple factuality stressors.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Working Memory (minor)
L3: Inhibitory Control, Self-reflection (minor)",L3
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA is a multilingual, culturally broad variant of physical commonsense reasoning tasks, focusing on choosing or generating sensible actions for everyday physical interactions. It probes whether models generalize practical intuition across languages and regions rather than relying on English-centric priors.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Spatial Representation & Mapping, Scene Understanding & Visual Reasoning (minor), Multisensory Integration (minor)
L3: ",L2
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) is a long-context multi-round coreference/retrieval-style evaluation where multiple similar “needle” queries are embedded in a long “haystack,” and the model must reproduce the correct response corresponding to a specified needle. It stresses attention control and faithful recall across very long contexts with many confusable distractors.","L1: Language Comprehension
L2: Working Memory, Attention, Semantic Understanding & Context Recognition, Episodic Memory (minor)
L3: ",L2
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval evaluates economically valuable professional knowledge work across many occupations by judging models’ real work products (e.g., slides, spreadsheets, plans) against expert human outputs in head-to-head comparisons. It emphasizes end-to-end task execution quality, including following constraints, producing usable artifacts, and making sound professional decisions.","L1: Language Production, Language Comprehension
L2: Planning, Decision-making, Semantic Understanding & Context Recognition, Working Memory, Adaptive Error Correction (minor)
L3: Social Reasoning & Theory of Mind, Self-reflection (minor)",L3
SWE-Lancer,https://openai.com/index/swe-lancer/,https://arxiv.org/abs/2502.12115,"SWE-Lancer evaluates agentic software engineering in more realistic “contractor-style” settings, where models must handle ambiguous requirements, larger codebases, and iterative integration concerns. It is intended to better reflect real engineering work beyond isolated bug fixes, emphasizing robustness and practical delivery.","L1: Language Comprehension, Language Production (minor)
L2: Semantic Understanding & Context Recognition, Planning, Decision-making, Logical Reasoning, Working Memory, Adaptive Error Correction
L3: ",L2
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath is a set of very challenging, expert-level mathematics problems designed to measure frontier reasoning beyond routine competition math, with careful controls to reduce leakage and memorization. It emphasizes multi-step derivations, precise symbolic reasoning, and correctness under tight verification standards.","L1: 
L2: Logical Reasoning, Working Memory, Planning, Adaptive Error Correction (minor), Semantic Understanding & Context Recognition (minor)
L3: Cognitive Flexibility",L3
