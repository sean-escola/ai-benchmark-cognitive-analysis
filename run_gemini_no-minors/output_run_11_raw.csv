Benchmark,Website,Paper,Description,Cognitive Functions
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates LLM-based software engineering agents on real GitHub issues by requiring them to produce code patches that make a test suite pass. The “Verified” split emphasizes tasks that are confirmed solvable and uses strict, execution-based grading in repository environments.","Planning, Logical Reasoning, Adaptive Error Correction, Working Memory, Decision-making (minor), Language Comprehension (minor)"
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 measures agentic performance on real command-line tasks (e.g., debugging, data processing, environment configuration) executed in a terminal sandbox. Success depends on iteratively issuing commands, interpreting outputs/errors, and converging on a correct end state under realistic tooling constraints.","Planning, Adaptive Error Correction, Logical Reasoning, Working Memory, Decision-making (minor), Language Comprehension (minor)"
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench evaluates interactive tool-using agents in customer-support-like domains (e.g., retail, airline, telecom) where the model must follow policies while taking actions through APIs and multi-turn dialogue. It emphasizes robustness to conversation dynamics, correct tool invocation, and policy-consistent outcomes rather than one-shot answers.","Language Comprehension, Language Production, Planning, Decision-making, Inhibitory Control, Social Reasoning & Theory of Mind (minor)"
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,ARC-AGI tests fluid reasoning by presenting few input–output grid examples and asking the model to infer the hidden rule to transform a new input grid. The tasks are intentionally out-of-distribution and reward discovering novel abstractions rather than recalling learned patterns.,"Logical Reasoning, Cognitive Flexibility, Working Memory, Spatial Representation & Mapping, Visual Perception (minor), Scene Understanding & Visual Reasoning (minor)"
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon autonomy by having an agent run a simulated vending machine business over extended time, making thousands of decisions about inventory, suppliers, pricing, and operations. The score is based on cumulative business outcomes, incentivizing coherent strategy, adaptation to market dynamics, and sustained execution.","Planning, Decision-making, Reward Mechanisms, Working Memory, Semantic Understanding & Context Recognition (minor), Adaptive Error Correction (minor)"
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a difficult, broad benchmark designed to probe frontier knowledge and reasoning across many domains, including multimodal questions. It targets synthesis and problem-solving under uncertainty, often requiring integrating specialized concepts rather than relying on surface-level recall.","Language Comprehension, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory (minor), Visual Perception (minor), Scene Understanding & Visual Reasoning (minor)"
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 consists of competition-style mathematics problems that demand multi-step derivations and careful symbolic manipulation. Performance reflects the model’s ability to set up correct solution approaches, maintain intermediate results, and avoid arithmetic/algebraic slips.","Logical Reasoning, Working Memory, Planning, Adaptive Error Correction (minor), Language Comprehension (minor)"
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a high-quality subset of graduate-level multiple-choice science questions curated to be difficult for non-experts and resistant to simple web lookup. It emphasizes deep conceptual understanding and multi-step scientific reasoning across physics, chemistry, and biology.","Semantic Understanding & Context Recognition, Logical Reasoning, Language Comprehension, Working Memory (minor)"
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends MMLU-style knowledge and reasoning evaluation to many languages, testing whether models maintain competence across multilingual prompts and culturally varied phrasing. It stresses broad academic knowledge, comprehension in the target language, and consistent reasoning across translations.","Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning (minor), Working Memory (minor), Language Production (minor)"
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro evaluates expert-level multimodal understanding by asking questions that require interpreting images (e.g., diagrams, charts, scientific figures) together with text. It focuses on cross-domain reasoning where visual evidence must be combined with domain knowledge to reach the correct answer.","Visual Perception, Scene Understanding & Visual Reasoning, Multisensory Integration, Language Comprehension, Logical Reasoning (minor)"
MathArena Apex,https://matharena.ai/?view=problem&comp=apex--apex_2025,https://arxiv.org/abs/2505.23281,MathArena Apex aggregates challenging math problems and evaluates models in settings that often stress deliberate multi-step reasoning and solution reliability. It is intended to differentiate top models on difficult mathematical problem solving beyond standard contest items.,"Logical Reasoning, Planning, Working Memory, Adaptive Error Correction (minor), Language Comprehension (minor)"
ScreenShot-Pro,https://gui-agent.github.io/grounding-leaderboard/,https://arxiv.org/abs/2504.07981,"ScreenShot-Pro measures how well models can understand and act on high-resolution screenshots of real software interfaces, often requiring locating UI elements and interpreting visual layout to answer questions or execute intent. It emphasizes grounded visual understanding of GUIs rather than generic image captioning.","Visual Perception, Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Sensorimotor Coordination (minor), Planning (minor), Language Comprehension (minor)"
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning evaluates understanding of scientific figures and associated context from research papers, probing whether a model can extract quantitative/structural information from charts and diagrams. Questions often require multi-step reasoning over visual evidence plus domain text.","Scene Understanding & Visual Reasoning, Visual Perception, Multisensory Integration, Logical Reasoning, Language Comprehension (minor)"
OmniDocBench 1.5,https://github.com/opendatalab/OmniDocBench,https://arxiv.org/abs/2412.07626,"OmniDocBench 1.5 benchmarks document understanding and OCR across heterogeneous content such as text blocks, tables, formulas, and reading order. It evaluates whether models can accurately parse and reconstruct structured documents rather than merely recognizing isolated text.","Visual Perception, Visual Attention & Eye Movements, Language Comprehension, Semantic Understanding & Context Recognition (minor), Working Memory (minor)"
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU extends multimodal evaluation to videos, requiring models to reason about events over time, temporal dependencies, and visual details across frames. Tasks often demand integrating visual dynamics with textual questions to infer actions, causes, and outcomes.","Visual Perception, Scene Understanding & Visual Reasoning, Working Memory, Attention, Multisensory Integration (minor), Cognitive Timing & Predictive Modeling (minor)"
LiveCodeBench Pro,https://livecodebenchpro.com/projects/livecodebench-pro/leaderboard,https://arxiv.org/abs/2506.11928,"LiveCodeBench Pro evaluates coding ability on curated programming problems, emphasizing correctness under execution-based grading and recency-aware task design. It tests whether models can plan implementations, write coherent code, and debug failures when initial attempts are wrong.","Planning, Logical Reasoning, Adaptive Error Correction, Working Memory, Language Production (minor), Language Comprehension (minor)"
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"FACTS Benchmark Suite systematically evaluates LLM factuality across varied settings (e.g., grounded answering, attribution, resisting confabulation) to quantify how often outputs remain supported by evidence. It targets truthfulness under prompting pressure and measures failure modes like hallucinations and unsupported claims.","Semantic Understanding & Context Recognition, Inhibitory Control, Language Comprehension, Self-reflection (minor), Working Memory (minor)"
SimpleQA Verified,https://www.kaggle.com/benchmarks/deepmind/simpleqa-verified,https://arxiv.org/abs/2509.07968,"SimpleQA Verified is a short-form factual question answering benchmark with verified ground truth and tight grading, designed to measure whether models provide correct, non-hallucinated answers. It emphasizes precision on straightforward queries where over-generation and confident errors are common failure modes.","Semantic Understanding & Context Recognition, Episodic Memory, Language Comprehension, Inhibitory Control (minor), Language Production (minor)"
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA evaluates physical commonsense reasoning across many languages, aiming to reduce English-centric bias by using non-parallel multilingual data. It probes whether models can infer plausible actions and outcomes in everyday physical scenarios under diverse linguistic formulations.","Semantic Understanding & Context Recognition, Logical Reasoning, Language Comprehension, Spatial Representation & Mapping (minor), Multisensory Integration (minor)"
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,MRCR v2 (8-needle) tests long-context robustness by inserting multiple similar “needle” items into very long “haystack” conversations/documents and asking the model to retrieve the correct referenced response. It stresses accurate cross-reference resolution and retention under distractors as context length scales.,"Working Memory, Attention, Episodic Memory, Language Comprehension, Cognitive Timing & Predictive Modeling (minor)"
