Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates software engineering by giving a model a real GitHub repository, an issue description, and tests; the model must produce a patch that makes the test suite pass. The Verified subset adds human verification that each task is solvable and that passing tests correspond to a correct fix, making it a high-signal measure of end-to-end bug-fixing and implementation.","L1: Language Comprehension (minor)
L2: Planning, Logical Reasoning, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition, Decision-making (minor)
L3: ",L2
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 evaluates agentic performance in command-line environments, where models must complete practical tasks by issuing shell commands, inspecting files, and iterating based on tool outputs. It emphasizes robust autonomy under realistic constraints (dependencies, environment quirks, partial failures) rather than single-shot code generation.","L1: Language Comprehension (minor)
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition
L3: Inhibitory Control (minor)",L2
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench evaluates interactive tool-using agents in customer-support-style domains (e.g., retail, airline, telecom) where the model must follow policies while calling APIs over multi-turn conversations. It stresses reliable action selection, policy compliance, and recovery from user or tool-induced complications across long dialogues.","L1: Language Comprehension, Language Production
L2: Decision-making, Planning, Working Memory, Reward Mechanisms (minor)
L3: Social Reasoning & Theory of Mind, Inhibitory Control",L3
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI measures fluid intelligence via novel grid-transformation puzzles, where a model must infer latent rules from a handful of input-output examples and generalize to a new case. It is designed to reduce reliance on memorized facts and instead probe abstraction, compositional reasoning, and rapid adaptation to unfamiliar tasks.","L1: 
L2: Logical Reasoning, Spatial Representation & Mapping, Scene Understanding & Visual Reasoning, Working Memory, Attention (minor)
L3: Cognitive Flexibility",L3
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon autonomous agent behavior in a simulated year-long vending-machine business, scoring outcomes such as final balance and operational success. The agent must plan, negotiate, manage inventory/pricing, and adapt to changing conditions over many steps, stressing coherence and strategy over time.","L1: Language Production (minor)
L2: Planning, Decision-making, Working Memory, Reward Mechanisms
L3: Social Reasoning & Theory of Mind (minor), Self-reflection (minor)",L2
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a broad, frontier-level benchmark spanning many academic and professional topics, with an emphasis on difficult reasoning and (often) multimodal understanding rather than rote recall. It is intended to stress models’ ability to synthesize information, follow complex problem statements, and avoid confident errors on hard questions.","L1: Language Comprehension, Visual Perception (minor)
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Scene Understanding & Visual Reasoning (minor)
L3: ",L2
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 consists of competition-style mathematics problems requiring multi-step symbolic reasoning under tight problem specifications. It emphasizes exactness and structured derivations, where small logical slips typically lead to wrong final answers.","L1: 
L2: Logical Reasoning, Working Memory, Attention (minor)
L3: Cognitive Flexibility (minor)",L2
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a high-difficulty, “Google-proof” multiple-choice science benchmark curated so that non-experts tend to fail while experts succeed, reducing gains from shallow pattern matching. It probes deep scientific reasoning and precise understanding of domain concepts across biology, chemistry, and physics.","L1: Language Comprehension (minor)
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory (minor)
L3: ",L2
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends the MMLU-style academic knowledge and reasoning evaluation to multiple languages, testing whether models retain competence when prompts and answer options vary linguistically. It stresses multilingual generalization and consistent understanding across diverse subject areas.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning (minor)
L3: Cognitive Flexibility (minor)",L2
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro evaluates multimodal, multi-discipline understanding with harder questions and more realistic settings than earlier MMMU variants, requiring models to integrate text with images such as diagrams, plots, and documents. It targets higher-level visual reasoning and cross-domain expertise rather than simple recognition.","L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Multisensory Integration, Working Memory (minor), Spatial Representation & Mapping (minor)
L3: ",L2
MathArena Apex,https://matharena.ai/?view=problem&comp=apex--apex_2025,https://arxiv.org/abs/2505.23281,"MathArena Apex is a difficult math evaluation suite aimed at separating top-tier models on advanced problem solving, typically involving multi-step reasoning and careful manipulation of formal structures. It is commonly used to benchmark “ceiling” mathematical capability beyond standard contest sets.","L1: 
L2: Logical Reasoning, Working Memory, Planning (minor)
L3: Cognitive Flexibility (minor)",L2
ScreenShot-Pro,https://gui-agent.github.io/grounding-leaderboard/,https://arxiv.org/abs/2504.07981,"ScreenShot-Pro evaluates models on understanding high-resolution screenshots of software interfaces and performing grounded reasoning about what is displayed (often for UI understanding and agentic computer-use setups). Success requires locating relevant elements, interpreting layout and affordances, and mapping visual cues to precise answers or actions.","L1: Visual Perception
L2: Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Spatial Representation & Mapping, Decision-making (minor), Sensorimotor Coordination (minor)
L3: ",L2
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning focuses on interpreting and reasoning over scientific figures and charts (often from research papers), where answers depend on extracting quantitative/structural information from visuals and combining it with textual context. It stresses chart/figure comprehension and multi-step reasoning rather than generic image captioning.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Logical Reasoning, Multisensory Integration, Semantic Understanding & Context Recognition (minor), Working Memory (minor)
L3: ",L2
OmniDocBench 1.5,https://github.com/opendatalab/OmniDocBench,https://arxiv.org/abs/2412.07626,"OmniDocBench 1.5 evaluates document understanding and OCR-centric capabilities across heterogeneous layouts, including text, tables, formulas, and reading order. It emphasizes faithful extraction and structural reconstruction from complex documents rather than open-ended reasoning.","L1: Visual Perception
L2: Attention, Semantic Understanding & Context Recognition, Working Memory (minor), Scene Understanding & Visual Reasoning (minor)
L3: ",L2
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU evaluates multimodal understanding and reasoning over video, requiring models to integrate temporal visual information with text to answer questions about events, interactions, and outcomes. It probes whether models can maintain coherence across frames and use context that may be distributed over time.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Multisensory Integration, Working Memory, Attention (minor)
L3: Cognitive Timing & Predictive Modeling (minor)",L2
LiveCodeBench Pro,https://livecodebenchpro.com/projects/livecodebench-pro/leaderboard,https://arxiv.org/abs/2506.11928,"LiveCodeBench Pro evaluates coding ability using competitive-programming and practical coding tasks, often emphasizing correctness under hidden tests and realistic constraints. It is designed to reduce contamination and measure robust problem solving, debugging, and implementation skill.","L1: Language Production (minor)
L2: Logical Reasoning, Planning, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition (minor)
L3: ",L2
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"FACTS Benchmark Suite systematically evaluates factuality, including hallucination tendencies and faithfulness to sources or prompts across a collection of factuality-focused tasks. It aims to quantify when models produce unsupported claims and how reliably they maintain truthfulness under varied conditions.","L1: Language Production (minor)
L2: Semantic Understanding & Context Recognition, Working Memory (minor)
L3: Inhibitory Control, Self-reflection (minor)",L3
SimpleQA Verified,https://www.kaggle.com/benchmarks/deepmind/simpleqa-verified,https://arxiv.org/abs/2509.07968,SimpleQA Verified is a short-form factual question answering benchmark with verification procedures intended to improve label reliability and reduce ambiguity. It probes precise retrieval-like knowledge and restraint against guessing when uncertain.,"L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Working Memory (minor)
L3: Inhibitory Control (minor)",L2
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA extends physical commonsense reasoning (how actions affect objects and environments) across many languages, testing whether models can generalize grounded intuitions beyond English. It emphasizes choosing plausible action-outcome or solution descriptions in everyday physical scenarios.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Spatial Representation & Mapping (minor)
L3: Cognitive Flexibility (minor)",L2
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) is a long-context evaluation where multiple similar “needle” requests are embedded in a lengthy “haystack,” and the model must retrieve the correct referenced content (often involving multi-round co-reference resolution). It stresses robust information tracking across very long inputs, resisting interference from distractors and near-duplicates.","L1: Language Comprehension (minor)
L2: Working Memory, Episodic Memory, Attention, Semantic Understanding & Context Recognition
L3: ",L2
