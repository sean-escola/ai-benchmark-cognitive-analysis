Benchmark,Website,Paper,Description,Cognitive Functions
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates an agent’s ability to solve real GitHub software engineering issues by producing code patches that make a project’s tests pass. The “Verified” subset focuses on problems that have been human-validated as solvable and aims to measure end-to-end debugging, code understanding, and patch generation.","Planning, Logical Reasoning, Adaptive Error Correction, Working Memory, Language Comprehension (minor), Language Production (minor)"
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 measures how well agentic systems can accomplish real tasks in a command-line environment (e.g., installing dependencies, running tools, manipulating files, and debugging failures). It emphasizes iterative tool use, recovery from errors, and maintaining a coherent action sequence over many steps.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Attention (minor), Language Comprehension (minor)"
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench evaluates interactive agent performance in multi-turn customer-support-style settings where the model must use tools/APIs while following domain policies (e.g., retail, airline, telecom). It stresses policy compliance, robust dialogue, and correct execution of tool-mediated actions under user pressure and ambiguity.","Social Reasoning & Theory of Mind, Inhibitory Control, Planning, Decision-making, Language Comprehension, Language Production"
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,ARC-AGI tests “fluid” abstract reasoning by asking models to infer rules that map input grids to output grids from only a few examples. Success requires discovering latent pattern transformations and generalizing them to novel instances rather than relying on memorized domain knowledge.,"Logical Reasoning, Cognitive Flexibility, Spatial Representation & Mapping, Working Memory, Scene Understanding & Visual Reasoning (minor), Visual Perception (minor)"
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon agent coherence and strategy in a simulated vending-machine business over an extended time period. Agents must plan inventory, pricing, supplier interactions, and adapt to changing conditions to maximize final balance.","Planning, Decision-making, Reward Mechanisms, Working Memory, Adaptive Error Correction (minor), Semantic Understanding & Context Recognition (minor)"
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam (HLE) is a frontier, multimodal benchmark spanning difficult academic and real-world questions intended to stress advanced reasoning and broad knowledge. It is often evaluated both without tools and with tools (e.g., search/code), measuring how well models integrate information and produce correct final answers.","Language Comprehension, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Scene Understanding & Visual Reasoning (minor), Visual Perception (minor)"
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,AIME 2025 consists of competition-style math problems requiring multi-step derivations and careful symbolic manipulation. It emphasizes rigorous reasoning and intermediate-state tracking rather than reliance on external facts.,"Logical Reasoning, Working Memory, Language Comprehension (minor), Planning (minor)"
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,GPQA Diamond is a high-quality subset of graduate-level multiple-choice science questions designed to be difficult to answer via shallow pattern matching. It probes deep scientific reasoning and the ability to discriminate between plausible distractors.,"Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory (minor), Language Comprehension (minor)"
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,MMMLU extends broad academic knowledge and reasoning evaluation across many subjects and multiple non-English languages. It measures whether models can transfer understanding and reasoning skills across linguistic contexts.,"Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning (minor), Working Memory (minor)"
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,MMMU-Pro evaluates expert-level multimodal understanding and reasoning using images paired with questions across many disciplines. It tests whether models can fuse visual evidence with textual instructions to choose or generate correct answers.,"Scene Understanding & Visual Reasoning, Visual Perception, Multisensory Integration, Language Comprehension, Working Memory (minor)"
MathArena Apex,https://matharena.ai/?view=problem&comp=apex--apex_2025,https://arxiv.org/abs/2505.23281,"MathArena Apex is a challenging mathematics benchmark intended to separate top models via harder, longer-horizon problems than standard contest sets. It emphasizes robust multi-step reasoning, solution planning, and precision under complexity.","Logical Reasoning, Working Memory, Planning (minor)"
ScreenShot-Pro,https://gui-agent.github.io/grounding-leaderboard/,https://arxiv.org/abs/2504.07981,"ScreenShot-Pro evaluates screenshot-based GUI understanding, where models must interpret high-resolution interfaces and ground actions/answers in specific on-screen elements. It probes visual grounding, spatial localization, and reliable mapping from intent to interface-relevant outputs.","Visual Perception, Visual Attention & Eye Movements, Spatial Representation & Mapping, Scene Understanding & Visual Reasoning, Sensorimotor Coordination (minor)"
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning evaluates scientific figure and chart reasoning drawn from papers, requiring models to interpret plots, legends, axes, and quantitative relationships. Many setups allow computation tools, testing whether models can combine visual extraction with structured analysis to answer questions correctly.","Scene Understanding & Visual Reasoning, Visual Perception, Logical Reasoning, Language Comprehension (minor), Working Memory (minor)"
OmniDocBench 1.5,https://github.com/opendatalab/OmniDocBench,https://arxiv.org/abs/2412.07626,"OmniDocBench 1.5 evaluates document AI systems on faithful extraction and reconstruction from complex documents containing text, tables, formulas, and reading order constraints. It primarily measures robust visual-document parsing and producing accurate structured outputs from heterogeneous layouts.","Visual Perception, Spatial Representation & Mapping, Language Comprehension, Semantic Understanding & Context Recognition (minor), Working Memory (minor)"
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU extends multimodal understanding to video, requiring models to answer questions that depend on events unfolding over time and sometimes on fine-grained visual details. It probes temporal integration and maintaining coherent interpretations across multiple frames/clips.","Cognitive Timing & Predictive Modeling, Working Memory, Visual Perception, Scene Understanding & Visual Reasoning, Language Comprehension (minor)"
LiveCodeBench Pro,https://livecodebenchpro.com/projects/livecodebench-pro/leaderboard,https://arxiv.org/abs/2506.11928,"LiveCodeBench Pro evaluates coding ability on problems that reflect contemporary programming tasks, often emphasizing correctness under realistic constraints and up-to-date problem sets. It measures a model’s ability to reason about code, implement solutions, and iteratively correct mistakes to pass tests.","Logical Reasoning, Planning, Adaptive Error Correction, Working Memory, Language Production (minor)"
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"The FACTS Benchmark Suite systematically evaluates factuality and faithfulness, including whether model outputs are supported by provided context or reliable sources and whether they avoid hallucinated details. It targets calibration-like behavior—sticking to evidence, refusing unsupported claims, and maintaining consistent factual statements.","Semantic Understanding & Context Recognition, Inhibitory Control, Language Comprehension (minor), Language Production (minor), Self-reflection (minor)"
SimpleQA Verified,https://www.kaggle.com/benchmarks/deepmind/simpleqa-verified,https://arxiv.org/abs/2509.07968,"SimpleQA Verified focuses on short, unambiguous factual questions with verified answers, aiming to measure precision and reduce grading noise. It emphasizes correctness on concise factual queries and the ability to avoid plausible-but-wrong completions.","Semantic Understanding & Context Recognition, Language Comprehension (minor), Language Production (minor), Inhibitory Control (minor)"
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA evaluates physical commonsense reasoning across many languages, testing whether models can choose the more plausible action/solution in everyday situations. It stresses robust, language-agnostic commonsense inference rather than narrow memorization of English-specific patterns.","Logical Reasoning, Semantic Understanding & Context Recognition, Language Comprehension, Spatial Representation & Mapping (minor)"
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) is a long-context co-reference retrieval evaluation where multiple similar “needle” interactions are embedded in a long “haystack,” and the model must reproduce the response to a specified needle. It primarily measures sustained attention and accurate retrieval across long sequences with high interference.","Attention, Working Memory, Episodic Memory, Language Comprehension, Adaptive Error Correction (minor)"
