Benchmark,Website,Paper,Description,Cognitive Functions
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro evaluates software engineering agents on realistic issues in real codebases, requiring them to understand repository context, implement changes, and produce patches that pass tests. Compared to SWE-bench Verified, it is larger and more difficult, includes multiple programming languages, and is designed to be more contamination-resistant and industry-relevant.","Planning, Logical Reasoning, Adaptive Error Correction, Working Memory, Language Comprehension (minor), Language Production (minor)"
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld benchmarks multimodal computer-use agents in operating system environments, where models must interpret GUI screens and execute multi-step actions to complete tasks. Success depends on robust perception of UI state, sequential action selection, and recovery from mistakes under step limits.","Visual Perception, Scene Understanding & Visual Reasoning, Planning, Decision-making, Attention, Sensorimotor Coordination (minor), Adaptive Error Correction (minor)"
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,ARC-AGI (Abstraction and Reasoning Corpus) measures few-shot “fluid” reasoning over small grid-world puzzles where the rule must be inferred from a handful of input–output examples. It emphasizes out-of-distribution generalization to new concepts and transformations rather than memorized domain knowledge.,"Logical Reasoning, Cognitive Flexibility, Working Memory, Spatial Representation & Mapping, Visual Perception (minor), Planning (minor)"
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon agent coherence by simulating a year of running a vending-machine business, including procurement, pricing, negotiation, and inventory management. The score is typically the final balance (or profitability), requiring sustained strategy and adaptation across many decisions.","Planning, Decision-making, Reward Mechanisms, Working Memory, Adaptive Error Correction (minor), Self-reflection (minor), Language Production (minor)"
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas measures real-world tool use through the Model Context Protocol, requiring models to discover, call, and chain tools across multi-step workflows in production-like API environments. It emphasizes correct tool selection, parameterization, error handling, and synthesis of results into final answers.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Language Comprehension (minor), Language Production (minor)"
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity agent capabilities on real-world vulnerability tasks, including finding known vulnerabilities from high-level weakness descriptions and discovering previously unknown issues in open-source projects. Performance depends on understanding code, reasoning about exploit conditions, and producing correct findings under a pass@1 setting.","Logical Reasoning, Planning, Adaptive Error Correction, Attention (minor), Working Memory (minor), Semantic Understanding & Context Recognition"
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a large, frontier-level benchmark spanning many academic and professional domains, intended to probe reasoning and knowledge at or beyond expert human level. It often includes multimodal questions and is used to compare model performance with and without tools such as search or code execution.","Language Comprehension, Logical Reasoning, Working Memory, Semantic Understanding & Context Recognition, Visual Perception (minor), Scene Understanding & Visual Reasoning (minor)"
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a curated subset of very difficult, graduate-level multiple-choice science questions designed to be “Google-proof” and resistant to shallow pattern matching. It targets deep conceptual understanding and multi-step reasoning in physics, chemistry, and biology.","Logical Reasoning, Language Comprehension, Semantic Understanding & Context Recognition, Working Memory (minor)"
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro evaluates multimodal understanding and expert-level reasoning across disciplines using problems that combine images (e.g., diagrams, charts, figures) with text questions. It stresses extracting relevant visual evidence, integrating it with domain knowledge, and performing multi-step reasoning under varied question formats.","Visual Perception, Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Attention (minor), Working Memory (minor), Language Comprehension (minor)"
OmniDocBench 1.5,https://github.com/opendatalab/OmniDocBench,https://arxiv.org/abs/2412.07626,"OmniDocBench 1.5 evaluates document AI by testing end-to-end understanding of complex documents containing text, tables, formulas, and reading order structure. Metrics such as edit distance reflect how accurately a model can reconstruct and interpret document content from visual or mixed inputs.","Visual Perception, Scene Understanding & Visual Reasoning, Language Comprehension, Language Production, Attention (minor), Working Memory (minor)"
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU extends multimodal understanding to the video setting, requiring models to answer questions that depend on events and visual details across time. It probes whether a model can integrate information from multiple frames/clips and maintain coherence over temporally extended evidence.","Visual Perception, Scene Understanding & Visual Reasoning, Cognitive Timing & Predictive Modeling, Working Memory, Attention (minor), Multisensory Integration (minor)"
LiveCodeBench Pro,https://livecodebenchpro.com/projects/livecodebench-pro/leaderboard,https://arxiv.org/abs/2506.11928,"LiveCodeBench Pro measures competitive coding ability on recent, evolving programming problems, aiming to reduce training contamination by using time-based curation and fresh tasks. It tests algorithmic reasoning, implementation correctness, and iterative debugging under realistic constraints.","Logical Reasoning, Planning, Adaptive Error Correction, Working Memory, Language Production (minor), Language Comprehension (minor)"
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"The FACTS Benchmark Suite systematically evaluates factuality in LLM outputs across multiple subtests that stress grounding, consistency, and resistance to hallucination. It measures whether a model can maintain truthfulness under prompting and retrieval-like settings while producing fluent natural language answers.","Semantic Understanding & Context Recognition, Language Comprehension, Language Production, Inhibitory Control, Self-reflection (minor), Working Memory (minor)"
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA is a multilingual, non-parallel extension of physical commonsense reasoning tasks where models choose the more plausible solution to everyday physical interaction scenarios across many languages and cultures. It probes whether commonsense physical reasoning generalizes beyond English and across linguistic variation.","Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning, Cognitive Flexibility (minor), Spatial Representation & Mapping (minor)"
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,MRCR v2 (multi-round coreference resolution) tests long-context integration by inserting multiple similar “needle” requests into long “haystacks” and asking the model to reproduce the correct response corresponding to a specific needle. The 8-needle variant stresses sustained retrieval and disambiguation over long documents with many confusable entries.,"Working Memory, Attention, Episodic Memory, Language Comprehension, Semantic Understanding & Context Recognition"
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval evaluates well-specified professional knowledge-work tasks spanning many occupations, where models produce real work artifacts (e.g., spreadsheets, presentations, plans) judged against industry professionals. It emphasizes end-to-end task execution quality, including correctness, structure, and usefulness under realistic constraints.","Planning, Decision-making, Language Production, Semantic Understanding & Context Recognition, Working Memory (minor), Self-reflection (minor), Social Reasoning & Theory of Mind (minor)"
SWE-Lancer,https://openai.com/index/swe-lancer/,https://arxiv.org/abs/2502.12115,"SWE-Lancer evaluates agentic software engineering in a more autonomous “contractor” setting, where the model must complete programming tasks end-to-end with minimal hand-holding. It focuses on sustained execution, correct patch generation, and reliable completion of work-like engineering deliverables.","Planning, Decision-making, Logical Reasoning, Adaptive Error Correction, Working Memory (minor), Language Production (minor)"
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath is a curated benchmark of advanced mathematics problems intended to measure progress toward expert-level mathematical reasoning, with tiers covering increasing difficulty. It emphasizes rigorous multi-step derivations, abstraction, and error-sensitive reasoning, often benefiting from tool-assisted computation but requiring correct mathematical structure.","Logical Reasoning, Planning, Working Memory, Adaptive Error Correction, Cognitive Flexibility (minor)"
