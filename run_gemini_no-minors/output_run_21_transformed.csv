Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates an agent’s ability to solve real-world software engineering issues by generating patches that make a repository’s tests pass. The Verified split adds human validation that each task is solvable and that the evaluation reliably checks correctness, reducing false positives from brittle tests.","L1: Language Comprehension (minor), Language Production (minor)
L2: Planning, Logical Reasoning, Working Memory, Adaptive Error Correction
L3: ",L2
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 measures agent performance on practical command-line tasks (e.g., debugging, setup, data manipulation) executed in real terminal environments under an agent harness. Success requires correct sequences of shell actions with robust handling of tool output, environment state, and errors.","L1: Language Comprehension (minor), Language Production (minor)
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory
L3: ",L2
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench evaluates interactive tool-using agents in multi-turn customer-support style environments (e.g., retail, airline, telecom) with policies and APIs. The agent must follow domain rules, gather information, invoke tools appropriately, and maintain consistency across a dialogue while completing the user’s goal.","L1: Language Comprehension, Language Production
L2: Decision-making, Planning, Working Memory
L3: Inhibitory Control, Social Reasoning & Theory of Mind, Empathy (minor)",L3
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,ARC-AGI tests “fluid” abstract reasoning by asking models to infer transformation rules from a few input–output grid examples and apply them to a new grid. It is designed to reward systematic generalization to novel tasks rather than memorization of fixed problem types.,"L1: Visual Perception
L2: Logical Reasoning, Spatial Representation & Mapping, Working Memory, Planning (minor)
L3: Cognitive Flexibility",L3
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon agent coherence by simulating the operation of a vending-machine business over an extended period, scoring by final financial outcome. The agent must manage inventory, pricing, supplier interactions/negotiation, and adapt decisions as conditions change across many steps.","L1: 
L2: Planning, Decision-making, Reward Mechanisms, Working Memory
L3: Social Reasoning & Theory of Mind (minor), Cognitive Timing & Predictive Modeling (minor)",L2
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a broad, frontier-knowledge benchmark spanning many academic and professional domains, often including long-form and multimodal questions. It aims to probe difficult reasoning and synthesis under sparse cues, where shallow pattern matching is less reliable.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory (minor), Scene Understanding & Visual Reasoning (minor), Multisensory Integration (minor)
L3: ",L2
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,AIME 2025 consists of competition-style mathematics problems requiring multi-step derivations and exact numeric answers. It stresses precise symbolic manipulation and disciplined reasoning under constraints that punish small logical slips.,"L1: 
L2: Logical Reasoning, Working Memory, Adaptive Error Correction (minor), Planning (minor)
L3: ",L2
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a high-difficulty, “Google-proof” multiple-choice science QA benchmark curated to be challenging for non-experts. It emphasizes deep scientific reasoning and careful disambiguation over surface-level recall.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory (minor)
L3: ",L2
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends broad subject-matter testing to multiple languages, evaluating knowledge and reasoning across many academic topics in multilingual settings. It probes whether models preserve competence and nuance when questions are posed in different languages and cultural contexts.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning (minor), Working Memory (minor)
L3: ",L2
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro evaluates multimodal understanding and reasoning over images paired with text across many disciplines, using a more challenging and updated problem set than earlier MMMU variants. Tasks require integrating visual evidence (plots, diagrams, photos) with textual instructions to produce correct answers.","L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Multisensory Integration, Spatial Representation & Mapping (minor), Logical Reasoning, Working Memory (minor)
L3: ",L2
MathArena Apex,https://matharena.ai/?view=problem&comp=apex--apex_2025,https://arxiv.org/abs/2505.23281,MathArena Apex aggregates advanced math problems (often beyond standard competition difficulty) to compare frontier reasoning systems under consistent evaluation rules. It emphasizes long chains of correct deductions and robustness across diverse mathematical subareas.,"L1: 
L2: Logical Reasoning, Working Memory, Planning (minor), Adaptive Error Correction (minor)
L3: ",L2
ScreenShot-Pro,https://gui-agent.github.io/grounding-leaderboard/,https://arxiv.org/abs/2504.07981,"ScreenShot-Pro evaluates “GUI grounding”: answering questions about high-resolution screenshots from software interfaces and dashboards. It requires locating relevant UI elements and interpreting layout, labels, and visual structure to produce correct responses (often aided by precise visual parsing).","L1: Visual Perception, Language Comprehension (minor)
L2: Visual Attention & Eye Movements, Spatial Representation & Mapping, Scene Understanding & Visual Reasoning, Sensorimotor Coordination (minor)
L3: ",L2
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning tests scientific-figure understanding by asking questions grounded in charts and figures from research papers, often requiring quantitative or relational reasoning. Models must extract relevant visual evidence, map it to the question, and perform structured inference to answer.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory (minor), Attention (minor)
L3: ",L2
OmniDocBench 1.5,https://github.com/opendatalab/OmniDocBench,https://arxiv.org/abs/2412.07626,"OmniDocBench 1.5 evaluates document understanding and OCR-centric extraction across diverse layouts (text, tables, formulas, and reading order). It stresses robustness to complex formatting and requires models to preserve structure when converting documents into usable representations.","L1: Visual Perception
L2: Attention, Scene Understanding & Visual Reasoning, Semantic Understanding & Context Recognition (minor), Working Memory (minor)
L3: ",L2
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU evaluates multimodal reasoning over video, combining temporal visual information with text questions that may require integrating cues across frames. It targets understanding of events, temporal relationships, and context that cannot be inferred from a single image.","L1: Visual Perception
L2: Multisensory Integration, Scene Understanding & Visual Reasoning, Working Memory, Attention (minor)
L3: Cognitive Timing & Predictive Modeling",L3
LiveCodeBench Pro,https://livecodebenchpro.com/projects/livecodebench-pro/leaderboard,https://arxiv.org/abs/2506.11928,"LiveCodeBench Pro evaluates coding ability on up-to-date programming problems designed to reduce training-set leakage, often emphasizing practical implementation and correctness. It measures whether a model can reason through problem requirements and produce executable solutions reliably.","L1: Language Production
L2: Logical Reasoning, Planning, Working Memory, Adaptive Error Correction (minor)
L3: ",L2
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"FACTS Benchmark Suite systematically evaluates factuality by measuring whether model outputs remain grounded and correct across a variety of factuality-focused tasks and settings. It targets error modes like hallucination, unsupported claims, and failures to abstain when uncertain.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Working Memory (minor)
L3: Inhibitory Control, Self-reflection (minor)",L3
SimpleQA Verified,https://www.kaggle.com/benchmarks/deepmind/simpleqa-verified,https://arxiv.org/abs/2509.07968,"SimpleQA Verified is a benchmark of short-form factual questions with verification procedures intended to improve label reliability. It focuses on whether models provide correct, concise factual answers and avoid confidently stating incorrect information.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition
L3: Inhibitory Control (minor)",L2
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,Global PIQA evaluates pragmatic commonsense reasoning across languages by testing whether models can choose or generate plausible physical and everyday-action interpretations in multilingual contexts. It is designed to probe robustness of commonsense inference beyond English-only datasets.,"L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning
L3: Cognitive Flexibility (minor)",L2
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) is a long-context evaluation where multiple similar “needle” interactions are embedded in a long “haystack,” and the model must retrieve and reproduce the correct response for a specified needle. It stresses maintaining and accessing the right information under interference from many near-duplicate spans.","L1: Language Comprehension
L2: Working Memory, Attention, Episodic Memory
L3: Inhibitory Control (minor)",L2
