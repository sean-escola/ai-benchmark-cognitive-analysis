Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro evaluates models on realistic software engineering issues drawn from real repositories, where the model must produce correct code patches that satisfy hidden tests. Compared with earlier SWE-bench variants, it emphasizes harder, more diverse, and more contamination-resistant tasks across multiple programming languages and complex codebases.","L1: Language Comprehension (minor)
L2: Logical Reasoning, Planning, Adaptive Error Correction, Working Memory, Decision-making (minor)
L3: ",L2
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld is a multimodal benchmark for computer use in a full operating-system-like environment, where an agent must complete end-to-end tasks (e.g., browsing, file operations, app interactions) through GUI actions. It tests whether models can perceive screens, plan multi-step interaction sequences, recover from UI errors, and reliably execute actions under step limits.","L1: Visual Perception
L2: Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Sensorimotor Coordination, Planning, Decision-making, Adaptive Error Correction, Spatial Representation & Mapping (minor), Working Memory (minor)
L3: ",L2
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI measures “fluid” reasoning by asking models to infer latent rules from a few input–output grid examples and produce the correct output for a new grid. The tasks are designed to reduce reliance on memorized knowledge and instead probe rapid abstraction, compositional generalization, and robust pattern induction under few-shot constraints.","L1: Visual Perception (minor)
L2: Logical Reasoning, Working Memory, Spatial Representation & Mapping, Scene Understanding & Visual Reasoning, Attention (minor)
L3: Cognitive Flexibility",L3
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon agent coherence by placing an agent in a year-long simulated vending-machine business with many sequential decisions (inventory, pricing, procurement, negotiation). Scores are based on final financial outcomes, incentivizing sustained planning, adaptation to changing conditions, and consistent execution over thousands of steps.","L1: Language Production (minor)
L2: Planning, Decision-making, Reward Mechanisms, Working Memory, Episodic Memory (minor), Adaptive Error Correction (minor)
L3: Cognitive Timing & Predictive Modeling (minor)",L2
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas evaluates real-world tool use via the Model Context Protocol by requiring models to discover appropriate tools, call them with correct parameters, handle errors/retries, and synthesize outputs into a final answer. Tasks often require multi-step workflows across multiple services, stressing reliable orchestration rather than single-turn question answering.","L1: Language Comprehension (minor), Language Production (minor)
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition (minor)
L3: Inhibitory Control (minor)",L2
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym measures cybersecurity agent competence on real-world vulnerability tasks at scale, including identifying known vulnerabilities from descriptions and discovering previously unknown issues in codebases. It emphasizes precise reasoning about program behavior, careful inspection of code and configs, and producing actionable findings or fixes under realistic constraints.","L1: Language Comprehension (minor)
L2: Logical Reasoning, Attention, Working Memory, Planning (minor), Adaptive Error Correction (minor), Decision-making (minor)
L3: Inhibitory Control (minor)",L2
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a large, difficult benchmark of expert-level questions spanning many domains and modalities, intended to stress frontier knowledge and reasoning. It typically requires multi-step inference, careful reading of problem statements, and (for multimodal items) accurate interpretation of figures or diagrams.","L1: Language Comprehension, Visual Perception (minor)
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Attention (minor), Scene Understanding & Visual Reasoning (minor)
L3: ",L2
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a high-quality subset of graduate-level, “Google-proof” multiple-choice science questions designed to be difficult for non-experts. The benchmark targets deep conceptual understanding and careful elimination of distractors rather than surface-level pattern matching.","L1: Language Comprehension
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory (minor), Attention (minor)
L3: ",L2
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro is a rigorous multimodal benchmark (images + text) spanning many disciplines, with tasks that require interpreting visual artifacts such as diagrams, charts, scientific figures, and UI-like images. It stresses cross-modal reasoning, spatial understanding, and the ability to ground language answers in visual evidence.","L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Multisensory Integration, Spatial Representation & Mapping, Logical Reasoning, Visual Attention & Eye Movements (minor), Working Memory (minor)
L3: ",L2
OmniDocBench 1.5,https://github.com/opendatalab/OmniDocBench,https://arxiv.org/abs/2412.07626,"OmniDocBench 1.5 evaluates document AI systems on end-to-end document understanding, including OCR quality, layout understanding, tables, formulas, and reading order. It focuses on faithfully reconstructing and interpreting complex, real-world documents rather than only recognizing plain text.","L1: Visual Perception, Language Comprehension (minor)
L2: Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Spatial Representation & Mapping, Working Memory (minor)
L3: ",L2
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU extends multimodal understanding to video, requiring models to answer questions about events, actions, temporal relations, and visual details across time. It probes whether a model can integrate information across frames and maintain coherent representations of evolving scenes.","L1: Visual Perception, Language Comprehension (minor)
L2: Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Working Memory, Multisensory Integration (minor)
L3: Cognitive Timing & Predictive Modeling",L3
LiveCodeBench Pro,https://livecodebenchpro.com/projects/livecodebench-pro/leaderboard,https://arxiv.org/abs/2506.11928,"LiveCodeBench Pro evaluates coding ability on time-split, execution-verified programming problems intended to reflect contemporary coding demands while reducing contamination. It typically scores models by whether their generated solutions pass tests in a single attempt, emphasizing correctness and robustness.","L1: Language Production (minor), Language Comprehension (minor)
L2: Logical Reasoning, Planning, Adaptive Error Correction, Working Memory, Decision-making (minor)
L3: ",L2
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"The FACTS Benchmark Suite systematically evaluates factuality-related behaviors, including whether models produce accurate claims, avoid unsupported hallucinations, and appropriately qualify uncertainty. It emphasizes truthfulness and grounding rather than creativity or open-ended plausibility.","L1: Language Production (minor), Language Comprehension (minor)
L2: Semantic Understanding & Context Recognition, Working Memory (minor), Logical Reasoning (minor)
L3: Inhibitory Control, Self-reflection (minor)",L3
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA measures physical commonsense reasoning across languages and cultures, typically via scenario-based questions about everyday interactions with objects and environments. It aims to test whether models can generalize intuitive physics and practical affordances beyond English-centric datasets.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Spatial Representation & Mapping (minor), Working Memory (minor)
L3: ",L2
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 evaluates long-context multi-round coreference resolution by embedding multiple similar “needle” turns within long “haystack” dialogues and asking the model to retrieve the correct target response. The 8-needle setting stresses selective retrieval, interference resistance, and consistent tracking of entity/reference links across long contexts.","L1: Language Comprehension
L2: Working Memory, Attention, Episodic Memory, Semantic Understanding & Context Recognition
L3: Inhibitory Control (minor), Cognitive Flexibility (minor)",L2
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval evaluates economically meaningful, well-specified professional knowledge work across many occupations by having models produce real work artifacts (e.g., presentations, spreadsheets, plans) judged against expert human outputs. It tests end-to-end task execution quality, including following specifications, producing polished deliverables, and maintaining internal consistency across multi-part work products.","L1: Language Production, Language Comprehension
L2: Planning, Decision-making, Semantic Understanding & Context Recognition, Working Memory
L3: Self-reflection (minor), Social Reasoning & Theory of Mind (minor)",L2
SWE-Lancer,https://openai.com/index/swe-lancer/,https://arxiv.org/abs/2502.12115,"SWE-Lancer evaluates software engineering performance on practical tasks that resemble real development work, such as implementing features, fixing bugs, and integrating changes in existing repositories. It emphasizes producing correct patches under realistic constraints and often rewards reliable end-to-end execution over isolated algorithmic problem solving.","L1: Language Comprehension (minor), Language Production (minor)
L2: Planning, Logical Reasoning, Adaptive Error Correction, Working Memory, Decision-making (minor)
L3: ",L2
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath is a difficult mathematics benchmark intended to probe advanced mathematical problem solving beyond routine competition problems, with tiers reflecting increasing difficulty. It emphasizes multi-step derivations, rigorous reasoning, and error-sensitive symbolic manipulation, often with tool-assisted settings (e.g., Python) in some evaluations.","L1: Language Comprehension (minor)
L2: Logical Reasoning, Working Memory, Planning, Attention (minor), Adaptive Error Correction (minor)
L3: Cognitive Flexibility (minor)",L2
