Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates software engineering agents on real GitHub issues by asking them to produce a patch that makes a repository’s tests pass. The “Verified” subset uses human-validated tasks intended to reduce ambiguity and ensure solvability, emphasizing end-to-end debugging and code change execution under realistic constraints.","L1: Language Comprehension (minor), Language Production (minor)
L2: Planning, Logical Reasoning, Working Memory, Adaptive Error Correction, Decision-making (minor)
L3: ",L2
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 measures an agent’s ability to complete practical tasks in a command-line environment (e.g., inspecting files, running tools, fixing issues) using iterative interaction. It emphasizes tool-mediated problem solving, robustness to errors, and maintaining progress over multi-step sessions rather than single-turn coding.","L1: Language Comprehension (minor)
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory, Logical Reasoning
L3: Inhibitory Control (minor)",L2
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench evaluates interactive customer-support-style agents that must follow domain policies while using tools/APIs across multi-turn conversations with a simulated user. Success requires consistent policy adherence, correct tool invocation sequences, and recovering gracefully from misunderstandings or partial failures over the dialogue.","L1: Language Comprehension, Language Production
L2: Decision-making, Planning, Working Memory, Reward Mechanisms (minor)
L3: Social Reasoning & Theory of Mind, Inhibitory Control",L3
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI is a fluid-reasoning benchmark in which models infer a latent rule from a few input–output grid examples and apply it to a new grid. It is designed to stress generalization to novel tasks with minimal examples, relying on structured pattern induction rather than memorized domain knowledge.","L1: Visual Perception
L2: Logical Reasoning, Working Memory, Spatial Representation & Mapping, Attention (minor)
L3: Cognitive Flexibility",L3
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon agent coherence by placing the model in a simulated year-long vending-machine business management environment. The agent must make thousands of interdependent decisions (pricing, inventory, supplier negotiation, budgeting) to maximize final balance, stressing sustained strategy and adaptation.","L1: Language Comprehension (minor), Language Production (minor)
L2: Planning, Decision-making, Working Memory, Adaptive Error Correction, Reward Mechanisms
L3: Motivational Drives (minor)",L2
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a difficult frontier benchmark spanning many expert-level questions (often multimodal) across diverse domains. It targets broad reasoning and knowledge integration, including cases where tool use (search/code) can be enabled to test end-to-end problem solving in more realistic settings.","L1: Language Comprehension, Visual Perception (minor)
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory (minor), Scene Understanding & Visual Reasoning (minor), Planning (minor)
L3: ",L2
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 benchmarks mathematical problem solving using questions from the American Invitational Mathematics Examination, typically requiring multi-step derivations and careful symbolic manipulation. It primarily tests contest-style reasoning under short prompts, sometimes evaluated with or without computational tools.","L1: Language Comprehension (minor)
L2: Logical Reasoning, Working Memory, Attention (minor)
L3: ",L2
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a high-difficulty multiple-choice science QA benchmark constructed to be resistant to simple web lookup and to require genuine scientific reasoning. The “Diamond” subset is curated for quality and difficulty, emphasizing deep understanding over surface pattern matching.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory (minor)
L3: Inhibitory Control (minor)",L2
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends the MMLU-style multi-subject exam format to multiple languages, testing knowledge and reasoning across many academic areas under multilingual prompting. It stresses cross-lingual robustness: models must map non-English queries to the correct underlying concepts and select accurate answers.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning (minor), Working Memory (minor)
L3: ",L2
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro evaluates expert-level multimodal understanding by combining images (charts, diagrams, documents, photos) with questions that require reasoning and domain knowledge. The “Pro” setting is designed to be more challenging and to better separate strong multimodal reasoning from shallow captioning or OCR-only behavior.","L1: Visual Perception, Language Comprehension
L2: Multisensory Integration, Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements (minor), Logical Reasoning
L3: ",L2
MathArena Apex,https://matharena.ai/?view=problem&comp=apex--apex_2025,https://arxiv.org/abs/2505.23281,"MathArena Apex is a competitive math evaluation (with standardized prompts and grading) that aggregates challenging problems intended to separate frontier mathematical reasoning performance. It emphasizes multi-step solutions and robustness, often reporting results across tool/no-tool or reasoning settings depending on the track.","L1: Language Comprehension (minor)
L2: Logical Reasoning, Working Memory, Attention (minor)
L3: ",L2
ScreenShot-Pro,https://gui-agent.github.io/grounding-leaderboard/,https://arxiv.org/abs/2504.07981,"ScreenShot-Pro evaluates understanding of high-resolution GUI screenshots by asking models to answer questions that depend on correctly interpreting interface elements, layouts, and on-screen text. It targets real-world screen understanding needed for computer-use agents, where small visual details and spatial relationships are crucial.","L1: Visual Perception, Language Comprehension (minor)
L2: Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Spatial Representation & Mapping, Sensorimotor Coordination (minor)
L3: ",L2
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning evaluates reasoning over scientific paper figures (often chart- or plot-like), requiring interpretation of visual evidence alongside question text. It stresses diagram/chart reading, quantitative inference, and drawing conclusions that are not explicitly stated in the prompt.","L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Working Memory (minor)
L3: ",L2
OmniDocBench 1.5,https://github.com/opendatalab/OmniDocBench,https://arxiv.org/abs/2412.07626,"OmniDocBench 1.5 benchmarks document understanding and OCR-centric extraction over complex layouts, including text blocks, tables, formulas, and reading order. It focuses on reconstructing faithful structured content from documents, stressing layout-aware perception beyond plain text recognition.","L1: Visual Perception, Language Comprehension (minor)
L2: Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Spatial Representation & Mapping, Working Memory (minor)
L3: ",L2
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU extends multimodal understanding to the video domain by asking questions that require integrating information across frames and time. It probes temporal grounding (what happens when), causal/relational reasoning over events, and summarizing salient visual evidence into an answer.","L1: Visual Perception, Language Comprehension (minor)
L2: Multisensory Integration, Working Memory, Attention (minor), Scene Understanding & Visual Reasoning
L3: Cognitive Timing & Predictive Modeling",L3
LiveCodeBench Pro,https://livecodebenchpro.com/projects/livecodebench-pro/leaderboard,https://arxiv.org/abs/2506.11928,"LiveCodeBench Pro evaluates coding ability on programming tasks designed to be more resistant to contamination and to better reflect practical coding performance than older static sets. It typically emphasizes writing correct, executable code under time- and test-driven constraints, sometimes summarized via leaderboard-style ratings.","L1: Language Comprehension (minor), Language Production (minor)
L2: Logical Reasoning, Planning, Adaptive Error Correction, Working Memory
L3: ",L2
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"The FACTS Benchmark Suite evaluates factuality and grounding by testing whether model outputs remain consistent with provided sources and avoid unsupported claims across a set of targeted factuality tasks. It aims to measure not only raw knowledge, but also calibration, citation/grounding behavior, and resistance to hallucination.","L1: Language Comprehension, Language Production
L2: Semantic Understanding & Context Recognition, Working Memory (minor)
L3: Inhibitory Control, Self-reflection (minor)",L3
SimpleQA Verified,https://www.kaggle.com/benchmarks/deepmind/simpleqa-verified,https://arxiv.org/abs/2509.07968,SimpleQA Verified is a factual question answering benchmark with verified ground truth and evaluation protocols aimed at reliably measuring factual correctness. It focuses on whether models can provide accurate short answers (and avoid plausible-sounding errors) on straightforward information-seeking queries.,"L1: Language Comprehension, Language Production (minor)
L2: Semantic Understanding & Context Recognition
L3: Inhibitory Control (minor)",L2
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA evaluates physical commonsense reasoning across many languages using non-parallel (not direct translations) items, aiming to test robustness to linguistic and cultural variation. Questions typically involve selecting the more plausible action/solution in everyday physical scenarios, emphasizing grounded commonsense rather than memorized facts.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Spatial Representation & Mapping (minor), Sensorimotor Coordination (minor)
L3: ",L2
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,MRCR v2 (8-needle) tests long-context multi-round coreference resolution by placing repeated similar “needle” interactions inside a long “haystack” dialogue and asking the model to retrieve the correct referenced response. It emphasizes precise tracking of entities and events across very long contexts under distraction and interference.,"L1: Language Comprehension
L2: Working Memory, Attention, Semantic Understanding & Context Recognition, Episodic Memory (minor)
L3: Inhibitory Control (minor)",L2
