Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates software engineering agents on real GitHub issues by asking the model to produce a patch that fixes the problem and passes tests in a repository. The Verified split focuses on tasks that have been confirmed solvable and reliably testable, emphasizing end-to-end debugging, implementation, and regression prevention in realistic codebases.","L1: Language Comprehension (minor), Language Production (minor)
L2: Planning, Logical Reasoning, Working Memory, Adaptive Error Correction, Decision-making (minor)
L3: ",L2
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 measures an agent’s ability to complete tasks in a command-line environment, typically requiring iterative execution of shell commands, inspection of outputs, and corrective actions under resource and tooling constraints. It emphasizes interactive problem solving where progress depends on interpreting system feedback and adapting a multi-step workflow.","L1: Language Comprehension (minor), Language Production (minor)
L2: Planning, Adaptive Error Correction, Decision-making, Working Memory, Attention (minor)
L3: ",L2
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench evaluates tool-using customer-support agents interacting with simulated users and APIs in domains like retail, airline, and telecom. Success requires maintaining policy compliance, tracking conversational state across turns, and selecting correct tool calls to resolve cases end-to-end.","L1: Language Comprehension, Language Production (minor)
L2: Planning, Decision-making, Working Memory
L3: Social Reasoning & Theory of Mind, Inhibitory Control, Empathy (minor)",L3
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,ARC-AGI tests abstract reasoning and few-shot generalization on novel grid-based puzzles where models infer latent rules from a small set of examples and produce the correct transformed output. It is designed to reduce reliance on memorized patterns and highlight flexible problem solving under distribution shift.,"L1: 
L2: Logical Reasoning, Working Memory, Attention, Spatial Representation & Mapping, Scene Understanding & Visual Reasoning (minor)
L3: Cognitive Flexibility",L3
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon agent coherence and business decision-making in a simulated vending-machine company over an extended time period. The agent must manage inventory, pricing, supplier interactions, and budgeting, balancing short-term actions with long-term profitability.","L1: Language Comprehension (minor), Language Production (minor)
L2: Planning, Decision-making, Reward Mechanisms, Working Memory, Adaptive Error Correction (minor)
L3: ",L2
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam (HLE) is a frontier, multi-domain benchmark intended to probe difficult academic and professional questions, often requiring multi-step reasoning and (in some settings) tool use such as search and code. It stresses broad knowledge, synthesis across sources, and robustness to tricky or underspecified prompts.","L1: Language Comprehension, Visual Perception (minor)
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory (minor), Planning (minor), Multisensory Integration (minor)
L3: ",L2
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 evaluates competition-level mathematical problem solving drawn from the American Invitational Mathematics Examination. Questions typically require constructing nontrivial solution strategies, maintaining intermediate symbolic states, and avoiding arithmetic or logical slips.","L1: Language Comprehension (minor)
L2: Logical Reasoning, Working Memory, Planning (minor), Adaptive Error Correction (minor)
L3: ",L2
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,GPQA Diamond is a high-difficulty multiple-choice science benchmark curated to be challenging for non-experts and resistant to shallow pattern matching. It emphasizes precise scientific reasoning and the ability to discriminate among close distractors under tight information constraints.,"L1: Language Comprehension (minor)
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory (minor)
L3: Inhibitory Control (minor)",L2
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends broad academic knowledge evaluation to multiple languages, testing whether models can understand and answer subject-matter questions beyond English. It measures both multilingual comprehension and whether reasoning/knowledge transfer holds across linguistic and cultural contexts.","L1: Language Comprehension, Language Production (minor)
L2: Semantic Understanding & Context Recognition, Logical Reasoning (minor), Working Memory (minor)
L3: ",L2
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro is a challenging multimodal benchmark focused on expert-level understanding and reasoning over images plus text (e.g., diagrams, charts, technical figures) with harder question sets than earlier MMMU variants. It probes whether models can integrate visual evidence with domain knowledge to answer complex questions.","L1: Visual Perception, Language Comprehension (minor)
L2: Scene Understanding & Visual Reasoning, Multisensory Integration, Attention (minor), Logical Reasoning (minor)
L3: ",L2
MathArena Apex,https://matharena.ai/?view=problem&comp=apex--apex_2025,https://arxiv.org/abs/2505.23281,"MathArena Apex is an advanced mathematics benchmark suite intended to compare frontier models on difficult, often multi-step problems, sometimes with tool-assisted variants. It emphasizes strategy selection, sustained symbolic manipulation, and consistency across problem families.","L1: Language Comprehension (minor)
L2: Logical Reasoning, Working Memory, Planning (minor), Adaptive Error Correction (minor)
L3: ",L2
ScreenShot-Pro,https://gui-agent.github.io/grounding-leaderboard/,https://arxiv.org/abs/2504.07981,"ScreenShot-Pro evaluates vision-language agents on understanding high-resolution screenshots of software interfaces and answering questions or identifying UI elements accurately. It targets practical visual grounding needed for computer-use agents, where success depends on interpreting layout, text, icons, and widget affordances.","L1: Visual Perception, Language Comprehension (minor)
L2: Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Spatial Representation & Mapping, Multisensory Integration (minor)
L3: ",L2
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning tests reasoning over scientific figures and content from research papers, emphasizing extracting information from plots/diagrams and connecting it to the accompanying scientific context. It targets capabilities needed for technical literature understanding, including quantitative and causal interpretation from visual evidence.","L1: Visual Perception, Language Comprehension (minor)
L2: Scene Understanding & Visual Reasoning, Logical Reasoning, Multisensory Integration, Semantic Understanding & Context Recognition (minor)
L3: ",L2
OmniDocBench 1.5,https://github.com/opendatalab/OmniDocBench,https://arxiv.org/abs/2412.07626,"OmniDocBench 1.5 evaluates document understanding and conversion across varied layouts and modalities such as text blocks, tables, formulas, and reading order. It measures whether models can faithfully parse and represent structured documents, where small OCR/layout mistakes can cascade into major errors.","L1: Visual Perception, Language Comprehension (minor)
L2: Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Working Memory (minor), Semantic Understanding & Context Recognition (minor)
L3: ",L2
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU extends multimodal reasoning to the temporal domain by asking questions about videos that require integrating visual events across time with accompanying text prompts. It stresses tracking state changes, causality, and key moments rather than single-frame recognition.","L1: Visual Perception, Language Comprehension (minor)
L2: Scene Understanding & Visual Reasoning, Working Memory, Attention, Multisensory Integration (minor)
L3: Cognitive Timing & Predictive Modeling (minor)",L2
LiveCodeBench Pro,https://livecodebenchpro.com/projects/livecodebench-pro/leaderboard,https://arxiv.org/abs/2506.11928,"LiveCodeBench Pro evaluates coding ability on curated programming tasks with standardized execution-based grading and an emphasis on contemporary, contamination-aware problem selection. It measures whether models can produce correct, runnable solutions under realistic constraints and diverse task types.","L1: Language Production (minor), Language Comprehension (minor)
L2: Logical Reasoning, Planning, Adaptive Error Correction, Working Memory
L3: ",L2
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,FACTS Benchmark Suite evaluates factuality and grounding by testing whether model outputs remain consistent with provided evidence and whether they avoid unsupported or fabricated claims. It targets reliability under information-seeking and summarization-style prompts where hallucinations are a primary failure mode.,"L1: Language Comprehension (minor), Language Production (minor)
L2: Semantic Understanding & Context Recognition, Working Memory (minor)
L3: Inhibitory Control, Self-reflection (minor)",L3
SimpleQA Verified,https://www.kaggle.com/benchmarks/deepmind/simpleqa-verified,https://arxiv.org/abs/2509.07968,"SimpleQA Verified is a factual question answering benchmark designed to test whether models provide correct, verifiable answers to straightforward queries with high precision. The “Verified” framing emphasizes quality control of questions/answers and aims to reduce ambiguity in grading.","L1: Language Comprehension, Language Production (minor)
L2: Semantic Understanding & Context Recognition, Working Memory (minor)
L3: Inhibitory Control (minor)",L2
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA evaluates pragmatic and commonsense question answering across many languages and locales, focusing on whether models can infer plausible actions and intentions in everyday situations. It stresses robustness to linguistic variation and culturally diverse contexts in practical reasoning.","L1: Language Comprehension, Language Production (minor)
L2: Semantic Understanding & Context Recognition, Logical Reasoning (minor)
L3: Social Reasoning & Theory of Mind",L3
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) is a long-context evaluation where multiple similar “needle” requests and responses are embedded in large “haystack” transcripts, and the model must retrieve the correct referenced response. It primarily measures long-range dependency handling, attention allocation, and accurate coreference under heavy distractors.","L1: Language Comprehension (minor)
L2: Working Memory, Attention, Semantic Understanding & Context Recognition, Episodic Memory (minor)
L3: Inhibitory Control (minor)",L2
