Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro evaluates software engineering agents on real-world GitHub issues by asking them to generate patches that pass project tests and satisfy the issue requirements. Compared to earlier SWE-bench variants, it increases difficulty and contamination resistance and broadens coverage (including multiple languages and more complex repos), emphasizing end-to-end engineering execution rather than isolated code writing.","L1: Language Comprehension (minor), Language Production (minor)
L2: Planning, Adaptive Error Correction, Logical Reasoning, Working Memory
L3: ",L2
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld measures an agent’s ability to operate a real (or faithfully emulated) computer operating system through a GUI to complete user goals (e.g., navigating apps, editing files, configuring settings). It stresses multimodal perception-action loops, tool/control reliability, and robustness to long, multi-step tasks under partial observability.","L1: Visual Perception
L2: Planning, Decision-making, Sensorimotor Coordination, Visual Attention & Eye Movements, Spatial Representation & Mapping (minor), Adaptive Error Correction (minor)
L3: ",L2
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI is a fluid-intelligence-style benchmark where models infer the latent transformation rule from a few input-output grid examples and apply it to a new grid. It is designed to reward abstraction, compositional pattern discovery, and generalization under minimal training signal per task rather than memorization of domain knowledge.","L1: Visual Perception
L2: Logical Reasoning, Working Memory, Scene Understanding & Visual Reasoning, Planning (minor)
L3: Cognitive Flexibility",L3
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon autonomy by having an agent run a simulated vending-machine business over an extended period, optimizing inventory, pricing, supplier interactions, and cash flow. The score is tied to business outcomes (e.g., final balance), requiring persistent strategy, adaptation to changing conditions, and coherent decision-making across many steps.","L1: 
L2: Planning, Decision-making, Reward Mechanisms, Working Memory, Adaptive Error Correction (minor), Semantic Understanding & Context Recognition (minor)
L3: ",L2
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas evaluates real-world tool use via the Model Context Protocol by testing whether a model can discover tools, call them with correct schemas/arguments, handle errors, and synthesize multi-tool outputs into a final answer. It targets agentic reliability in workflow execution rather than pure language generation.","L1: Language Comprehension (minor)
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition (minor)
L3: ",L2
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity agents on tasks such as reproducing known vulnerabilities from descriptions and discovering new vulnerabilities in real open-source projects. Success requires navigating codebases, reasoning about program behavior and attack surfaces, and iteratively testing hypotheses in a tool-driven environment.","L1: Language Comprehension (minor)
L2: Logical Reasoning, Planning, Adaptive Error Correction, Working Memory
L3: Inhibitory Control (minor)",L2
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a frontier difficulty benchmark spanning many domains, often requiring multi-step reasoning and (in some settings) multimodal understanding and tool-assisted problem solving. It is designed to test generalization and depth on expert-level questions rather than narrow task formats.","L1: Language Comprehension, Visual Perception (minor)
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Scene Understanding & Visual Reasoning (minor)
L3: ",L2
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a high-quality subset of GPQA composed of difficult, graduate-level, “Google-proof” multiple-choice science questions where non-experts typically fail. It emphasizes deep scientific reasoning and careful reading under a constrained answer format.","L1: Language Comprehension
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory (minor)
L3: ",L2
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro is an expert-level multimodal benchmark that tests reasoning across many disciplines using text plus images (e.g., diagrams, charts, scientific figures). It requires integrating visual evidence with domain knowledge and performing structured inference under multiple-choice or short-answer constraints.","L1: Visual Perception, Language Comprehension (minor)
L2: Multisensory Integration, Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Logical Reasoning (minor)
L3: ",L2
OmniDocBench 1.5,https://github.com/opendatalab/OmniDocBench,https://arxiv.org/abs/2412.07626,"OmniDocBench 1.5 evaluates document understanding/OCR by measuring how accurately a system reconstructs text, formulas, tables, and reading order from complex documents. It stresses layout-aware perception and precise transcription, not just high-level semantic summarization.","L1: Visual Perception, Language Comprehension (minor)
L2: Spatial Representation & Mapping, Visual Attention & Eye Movements, Working Memory (minor)
L3: ",L2
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU extends multimodal evaluation to the temporal domain by asking questions that require understanding videos (often with accompanying text) and reasoning about events, interactions, and changes over time. It probes whether models can integrate across frames and maintain coherent event representations.","L1: Visual Perception
L2: Working Memory, Attention, Scene Understanding & Visual Reasoning (minor), Multisensory Integration (minor)
L3: Cognitive Timing & Predictive Modeling",L3
LiveCodeBench Pro,https://livecodebenchpro.com/projects/livecodebench-pro/leaderboard,https://arxiv.org/abs/2506.11928,"LiveCodeBench Pro evaluates coding ability on time-sliced, competition-style programming problems to reduce training contamination, often scored with pass@k or rating-style aggregates. It focuses on algorithm design, correctness, and iterative debugging under constrained I/O specifications.","L1: Language Comprehension (minor), Language Production (minor)
L2: Logical Reasoning, Planning, Adaptive Error Correction, Working Memory
L3: ",L2
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"FACTS Benchmark Suite systematically evaluates factuality-related behavior, including whether models make unsupported claims, maintain grounding to provided sources, and avoid confabulation across varied settings. It targets reliability of knowledge claims rather than raw task completion alone.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Working Memory (minor)
L3: Inhibitory Control, Self-reflection (minor)",L3
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA is a multilingual physical commonsense reasoning benchmark designed to test whether models can answer practical, everyday “what action makes sense” questions across diverse languages and cultures. Its non-parallel structure aims to reduce translation artifacts and better measure robustness of commonsense reasoning worldwide.","L1: Language Comprehension
L2: Logical Reasoning, Semantic Understanding & Context Recognition
L3: Cognitive Flexibility (minor)",L2
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,MRCR v2 (8-needle) tests long-context multi-round coreference resolution by inserting multiple similar “needle” interactions into a long “haystack” dialogue and asking the model to reproduce the correct response for a specified needle. It emphasizes precise retrieval and disambiguation under heavy distraction and long-range dependencies.,"L1: Language Comprehension
L2: Working Memory, Attention, Episodic Memory, Semantic Understanding & Context Recognition (minor)
L3: ",L2
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval evaluates well-specified professional “knowledge work” tasks across many occupations, with human judges comparing model outputs against industry professional baselines (often as win/tie/loss). Tasks require producing realistic work artifacts (e.g., spreadsheets, presentations, plans) and thus measure end-to-end task execution quality under practical constraints.","L1: Language Production
L2: Planning, Decision-making, Semantic Understanding & Context Recognition, Working Memory (minor)
L3: Social Reasoning & Theory of Mind (minor)",L2
SWE-Lancer,https://openai.com/index/swe-lancer/,https://arxiv.org/abs/2502.12115,"SWE-Lancer evaluates agentic software engineering in a more “freelancer-like” setting, where models must complete scoped tasks that resemble real development work with constraints, context, and iterative requirements. It aims to capture practical delivery quality beyond unit-test passing, including robustness and task-following in realistic workflows.","L1: Language Comprehension (minor), Language Production (minor)
L2: Planning, Adaptive Error Correction, Decision-making, Working Memory
L3: ",L2
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath is a dataset of expert-designed mathematics problems intended to measure advanced mathematical reasoning at or beyond typical graduate-level difficulty, with strong attention to contamination resistance. Many items require multi-step derivations and precise symbolic/quantitative reasoning, sometimes benefiting from tool-assisted computation depending on the evaluation protocol.","L1: 
L2: Logical Reasoning, Working Memory, Planning, Semantic Understanding & Context Recognition (minor)
L3: Cognitive Flexibility (minor)",L2
