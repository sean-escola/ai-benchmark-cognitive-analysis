Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates software engineering agents on real GitHub issues by asking them to generate a patch that makes the project’s tests pass, with tasks filtered to be reliably solvable and automatically verifiable. It stresses end-to-end bug fixing and codebase navigation under realistic repo constraints rather than isolated coding puzzles.","L1: Language Comprehension (minor), Language Production (minor)
L2: Planning, Logical Reasoning, Adaptive Error Correction, Working Memory
L3: ",L2
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 measures how well an agent can accomplish practical tasks inside a command-line environment (e.g., installing dependencies, editing files, running programs, diagnosing failures). Success depends on executing multi-step tool interactions with feedback loops under time/step limits.","L1: Language Comprehension (minor)
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory
L3: Inhibitory Control (minor)",L2
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench evaluates interactive customer-support agents that must follow domain policies while using tools/APIs in multi-turn conversations with a simulated user. It emphasizes policy adherence, state tracking across turns, and correct tool invocation to resolve cases in retail/airline/telecom settings.","L1: Language Comprehension, Language Production
L2: Decision-making, Planning, Working Memory
L3: Social Reasoning & Theory of Mind, Inhibitory Control (minor), Empathy (minor)",L3
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,ARC-AGI tests few-shot abstract reasoning by giving a handful of input–output grid examples and asking the model to infer the hidden transformation rule for a new grid. It targets fluid intelligence via novel pattern induction with minimal training-set overlap and strong emphasis on generalization.,"L1: Visual Perception
L2: Logical Reasoning, Spatial Representation & Mapping, Working Memory, Planning (minor)
L3: Cognitive Flexibility",L3
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon autonomous agency by having a model run a simulated vending-machine business over many in-environment days, managing inventory, suppliers, pricing, and cash flow. Scores reflect sustained coherence, strategic adaptation to a changing market, and effective multi-step action selection.","L1: 
L2: Planning, Decision-making, Reward Mechanisms, Working Memory, Episodic Memory (minor), Semantic Understanding & Context Recognition (minor)
L3: Social Reasoning & Theory of Mind (minor)",L2
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a frontier, often multimodal, academic benchmark spanning difficult questions across many disciplines, designed to probe advanced reasoning and knowledge at the edge of typical model competence. It is commonly reported both with and without tool access (e.g., search/code), reflecting differences between pure reasoning and tool-augmented performance.","L1: Language Comprehension, Visual Perception (minor)
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory (minor), Planning (minor)
L3: ",L2
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,AIME 2025 evaluates competition-style mathematics with short-answer problems requiring multi-step symbolic and quantitative reasoning under tight output constraints. Performance reflects the ability to reliably execute complex derivations without external tools (or with tools in some reporting variants).,"L1: 
L2: Logical Reasoning, Working Memory, Adaptive Error Correction (minor), Planning (minor)
L3: ",L2
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,GPQA Diamond is a high-difficulty multiple-choice science QA benchmark curated so that non-experts tend to fail while subject-matter experts succeed. It probes rigorous scientific reasoning and knowledge while reducing the usefulness of superficial pattern matching or quick web lookups.,"L1: Language Comprehension (minor)
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory (minor)
L3: ",L2
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends MMLU-style broad knowledge and reasoning evaluation to multiple languages, testing whether a model can generalize subject knowledge beyond English. It emphasizes multilingual comprehension, cross-lingual transfer, and consistent reasoning across varied domains.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning (minor), Working Memory (minor)
L3: ",L2
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro is a high-difficulty multimodal benchmark spanning many subjects where questions require integrating visual inputs (charts, diagrams, photos) with text to answer. It targets expert-level multimodal reasoning rather than simple image captioning or recognition.","L1: Visual Perception, Language Comprehension
L2: Multisensory Integration, Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements (minor), Logical Reasoning (minor)
L3: ",L2
MathArena Apex,https://matharena.ai/?view=problem&comp=apex--apex_2025,https://arxiv.org/abs/2505.23281,MathArena Apex aggregates very challenging math problems (often contest or research-adjacent) and is used to compare top models on difficult multi-step derivations. It is designed to stress depth of reasoning and robustness across diverse mathematical topics.,"L1: 
L2: Logical Reasoning, Working Memory, Planning (minor), Adaptive Error Correction (minor)
L3: ",L2
ScreenShot-Pro,https://gui-agent.github.io/grounding-leaderboard/,https://arxiv.org/abs/2504.07981,"ScreenShot-Pro evaluates GUI understanding from screenshots, requiring a model/agent to interpret interface elements and answer questions or identify targets based on layout and visual cues. It probes spatial grounding and reliable perception-to-action mapping that is central to computer-use agents.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Spatial Representation & Mapping, Decision-making (minor), Sensorimotor Coordination (minor)
L3: ",L2
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning focuses on scientific figure understanding, asking models to answer reasoning questions grounded in figures from research papers (often benefiting from tools like Python for quantitative checks). It emphasizes extracting structured information from visuals and combining it with domain knowledge to justify conclusions.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Multisensory Integration, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory (minor)
L3: ",L2
OmniDocBench 1.5,https://github.com/opendatalab/OmniDocBench,https://arxiv.org/abs/2412.07626,"OmniDocBench 1.5 evaluates document understanding/OCR across diverse layouts (text, formulas, tables, and reading order), measuring how accurately systems extract and structure information. It targets robustness to real-world document variability rather than clean, single-format inputs.","L1: Visual Perception, Language Comprehension
L2: Visual Attention & Eye Movements, Semantic Understanding & Context Recognition (minor), Working Memory (minor)
L3: ",L2
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU extends multimodal understanding to videos, requiring models to integrate information over time to answer questions about events, actions, and causal relations. It stresses temporal comprehension and maintaining context across multiple frames and scenes.","L1: Visual Perception
L2: Working Memory, Scene Understanding & Visual Reasoning, Attention (minor), Multisensory Integration (minor)
L3: Cognitive Timing & Predictive Modeling",L3
LiveCodeBench Pro,https://livecodebenchpro.com/projects/livecodebench-pro/leaderboard,https://arxiv.org/abs/2506.11928,"LiveCodeBench Pro evaluates coding ability on curated programming tasks with strong attention to contamination resistance and real-world coding skills. It is often scored with strict pass@k-style correctness checks, emphasizing functional accuracy over stylistic code generation.","L1: Language Production (minor)
L2: Logical Reasoning, Planning, Adaptive Error Correction, Working Memory
L3: ",L2
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,FACTS Benchmark Suite systematically evaluates factuality by measuring whether model outputs remain consistent with reliable evidence and avoid unsupported claims across a variety of factuality-oriented tasks. It is designed to provide broader coverage than single QA sets and to expose different failure modes of hallucination.,"L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning (minor)
L3: Inhibitory Control (minor), Self-reflection (minor)",L2
SimpleQA Verified,https://www.kaggle.com/benchmarks/deepmind/simpleqa-verified,https://arxiv.org/abs/2509.07968,"SimpleQA Verified is a short-form factual question answering benchmark with verified answers, intended to measure basic factual recall and precision. It penalizes confident hallucinations on straightforward queries where correctness is clearly checkable.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition
L3: Inhibitory Control (minor)",L2
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA evaluates physical commonsense reasoning across many languages, using the PIQA-style format where models choose the more plausible solution to a physical interaction scenario. It targets grounded, everyday reasoning about objects and actions while testing cross-lingual robustness.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Spatial Representation & Mapping (minor), Motor Coordination (minor)
L3: ",L2
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,MRCR v2 (8-needle) tests long-context retrieval and multi-round coreference by embedding multiple similar “needle” interactions within a very long “haystack” and asking the model to reproduce the correct response for a specified needle. It stresses robustness to distractors and the ability to maintain and query relevant context over long token spans.,"L1: Language Comprehension
L2: Working Memory, Attention, Episodic Memory (minor), Planning (minor)
L3: ",L2
