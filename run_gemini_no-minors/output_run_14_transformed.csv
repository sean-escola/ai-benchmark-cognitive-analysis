Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates software-engineering agents on real GitHub issues, requiring a model to generate a patch that makes the repository’s tests pass under a standardized harness. The “Verified” subset filters to tasks confirmed solvable and reduces noise from ambiguous or broken issues, emphasizing end-to-end repair rather than code trivia.","L1: Language Comprehension (minor)
L2: Planning, Logical Reasoning, Adaptive Error Correction, Working Memory
L3: ",L2
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 measures how well an agent can complete real command-line tasks (e.g., installing dependencies, manipulating files, debugging programs) inside a constrained terminal environment. It stresses tool-mediated action loops where the model must interpret system feedback and iteratively correct its approach.","L1: Language Comprehension (minor)
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory
L3: ",L2
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench evaluates interactive customer-support agents that must converse with a simulated user while calling domain APIs (e.g., retail, airline, telecom) and adhering to policies. It emphasizes multi-turn state tracking, correct tool invocation, and following constraints under pressure from user requests.","L1: Language Comprehension, Language Production
L2: Decision-making, Working Memory, Planning (minor)
L3: Social Reasoning & Theory of Mind, Inhibitory Control",L3
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,ARC-AGI tests fluid reasoning by asking models to infer latent rules from a few input–output grid examples and produce the correct transformation for a new grid. The tasks are designed to minimize reliance on memorized patterns and instead probe generalization to novel abstract concepts.,"L1: Visual Perception (minor)
L2: Logical Reasoning, Spatial Representation & Mapping, Working Memory, Scene Understanding & Visual Reasoning (minor)
L3: Cognitive Flexibility",L3
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon agent coherence by simulating operation of a vending-machine business over extended time, including procurement, pricing, inventory management, and communication with suppliers. Success requires sustained strategy under changing market conditions rather than short single-turn problem solving.","L1: 
L2: Planning, Decision-making, Reward Mechanisms, Working Memory, Adaptive Error Correction (minor), Semantic Understanding & Context Recognition (minor)
L3: ",L2
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a broad frontier benchmark spanning difficult questions across many fields, often requiring multi-step reasoning and (in some settings) tool use such as search or code. It is intended to stress models at the edge of coverage and reasoning, including multimodal items when enabled.","L1: Language Comprehension, Visual Perception (minor)
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Multisensory Integration (minor)
L3: ",L2
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 evaluates competition-style mathematical problem solving drawn from the 2025 AIME, typically requiring careful symbolic reasoning and multi-step derivations. The benchmark is commonly used to measure accuracy, robustness to traps, and consistency under strict answer formats.","L1: 
L2: Logical Reasoning, Working Memory, Planning (minor), Adaptive Error Correction (minor)
L3: ",L2
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,GPQA Diamond is a curated subset of very difficult graduate-level science multiple-choice questions designed to be resistant to simple web lookup and superficial pattern matching. It probes deep conceptual understanding and the ability to discriminate between closely related scientific alternatives.,"L1: Language Comprehension (minor)
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory (minor)
L3: ",L2
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends MMLU-style academic subject testing across many languages, evaluating breadth of knowledge and reasoning under multilingual prompts. It is used to quantify general knowledge, cross-lingual robustness, and subject coverage beyond English-only settings.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning (minor)
L3: Cognitive Flexibility (minor)",L2
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro is a more challenging variant of MMMU that tests expert-level multimodal understanding across disciplines using images paired with questions (often multiple-choice) that require interpretation, calculation, and grounded reasoning. It emphasizes visual grounding and cross-modal integration rather than pure text recall.","L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Multisensory Integration, Working Memory (minor)
L3: ",L2
MathArena Apex,https://matharena.ai/?view=problem&comp=apex--apex_2025,https://arxiv.org/abs/2505.23281,"MathArena Apex is a collection of challenging math problems designed to compare advanced mathematical reasoning and solution reliability across models, often under consistent evaluation protocols. It typically stresses multi-step proof-like reasoning, numerical accuracy, and resistance to shallow heuristics.","L1: 
L2: Logical Reasoning, Working Memory, Planning (minor), Adaptive Error Correction (minor)
L3: ",L2
ScreenShot-Pro,https://gui-agent.github.io/grounding-leaderboard/,https://arxiv.org/abs/2504.07981,"ScreenShot-Pro evaluates GUI understanding and interaction from high-resolution screenshots, requiring models to locate relevant UI elements and answer questions or take actions based on visual interface state. It targets “screen grounding” needed for computer-use agents that must align language instructions with pixel-space structure.","L1: Visual Perception
L2: Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Decision-making (minor), Sensorimotor Coordination (minor)
L3: ",L2
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning tests whether a model can answer questions about scientific figures and plots from arXiv-style papers, often requiring extracting quantitative relationships and interpreting chart conventions. The benchmark emphasizes grounded visual-to-text reasoning over long-form scientific content.","L1: Visual Perception, Language Comprehension (minor)
L2: Scene Understanding & Visual Reasoning, Logical Reasoning, Working Memory (minor)
L3: ",L2
OmniDocBench 1.5,https://github.com/opendatalab/OmniDocBench,https://arxiv.org/abs/2412.07626,"OmniDocBench 1.5 evaluates document understanding across heterogeneous layouts, including OCR text, formulas, tables, and reading order. It targets end-to-end robustness for processing real documents where correct structure and alignment matter as much as raw text recognition.","L1: Visual Perception, Language Comprehension (minor)
L2: Scene Understanding & Visual Reasoning, Attention, Working Memory (minor)
L3: ",L2
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU extends multimodal understanding to video, requiring temporal reasoning over sequences of frames plus text questions. Tasks probe whether models can integrate visual evidence over time, track entities/events, and answer questions that depend on temporal context.","L1: Visual Perception
L2: Working Memory, Multisensory Integration (minor), Scene Understanding & Visual Reasoning (minor)
L3: Cognitive Timing & Predictive Modeling",L3
LiveCodeBench Pro,https://livecodebenchpro.com/projects/livecodebench-pro/leaderboard,https://arxiv.org/abs/2506.11928,"LiveCodeBench Pro measures code generation and debugging on programming problems designed to better reflect contemporary coding tasks and reduce leakage, typically scored in pass@k or Elo-style aggregates. It emphasizes writing correct, executable code under specification and iterating when failures occur.","L1: Language Comprehension (minor)
L2: Logical Reasoning, Planning, Adaptive Error Correction, Working Memory
L3: ",L2
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"FACTS Benchmark Suite systematically evaluates factuality across diverse settings (e.g., grounded generation, attribution, consistency, and robustness to misleading prompts), aiming to characterize when models produce incorrect claims. It is used to measure reliability and the tendency to confabulate under uncertainty.","L1: Language Production
L2: Semantic Understanding & Context Recognition, Logical Reasoning (minor)
L3: Inhibitory Control, Self-reflection (minor)",L3
SimpleQA Verified,https://www.kaggle.com/benchmarks/deepmind/simpleqa-verified,https://arxiv.org/abs/2509.07968,"SimpleQA Verified is a factual QA benchmark with verified ground truth, focusing on whether models can answer short questions accurately without adding unsupported details. It is often used as a clean signal for factual recall and precision in brief responses.","L1: Language Comprehension, Language Production (minor)
L2: Semantic Understanding & Context Recognition, Working Memory (minor)
L3: ",L2
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA evaluates practical commonsense/physical reasoning across languages, probing whether models can choose plausible actions or explanations in everyday scenarios under multilingual variation. It targets robustness of grounded commonsense beyond English-centric datasets.","L1: 
L2: Logical Reasoning, Semantic Understanding & Context Recognition
L3: Cognitive Flexibility, Social Reasoning & Theory of Mind (minor)",L3
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) is a long-context coreference/retrieval evaluation where multiple similar “needle” interactions are embedded in a long “haystack,” and the model must reproduce the correct response for a specified needle. It stresses precision under distraction and the ability to maintain and retrieve the right conversational state over very long contexts.","L1: Language Comprehension
L2: Working Memory, Attention, Episodic Memory (minor)
L3: Inhibitory Control (minor)",L2
