Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro evaluates software engineering agents on realistic repo-level bug-fix and feature tasks that require producing correct patches that pass tests under an automated harness. Compared with simpler coding sets, it emphasizes harder, more diverse, and more contamination-resistant tasks across multiple programming languages and real project constraints.","L1: Language Comprehension (minor), Language Production (minor)
L2: Planning, Adaptive Error Correction, Logical Reasoning, Working Memory, Decision-making (minor)
L3: ",L2
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld measures “computer use” by having an agent complete user-like tasks in a real operating-system environment, such as navigating GUIs, manipulating files, and using applications. Success depends on interpreting screenshots and UI state, selecting actions over many steps, and recovering from tool/UI errors.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Planning, Decision-making, Attention, Sensorimotor Coordination, Working Memory (minor), Adaptive Error Correction (minor), Spatial Representation & Mapping (minor)
L3: ",L2
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,ARC-AGI evaluates abstraction and fluid reasoning using small grid-based input–output examples where a hidden rule must be inferred and applied to new instances. Tasks are designed to require systematic generalization from only a few demonstrations rather than relying on memorized patterns.,"L1: 
L2: Logical Reasoning, Working Memory, Spatial Representation & Mapping, Attention (minor), Scene Understanding & Visual Reasoning (minor)
L3: Cognitive Flexibility",L3
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon agent coherence by simulating a year of running a vending-machine business with many interdependent decisions (inventory, pricing, supplier negotiation, and adaptation to market conditions). The score is based on final outcomes (e.g., balance/profit), rewarding sustained strategy rather than single-step correctness.","L1: Language Production (minor)
L2: Planning, Decision-making, Reward Mechanisms, Working Memory, Adaptive Error Correction (minor), Semantic Understanding & Context Recognition (minor)
L3: ",L2
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas assesses real-world tool use via the Model Context Protocol, requiring models to discover tools, call them with correct schemas, handle errors/retries, and integrate results into final answers. It targets multi-step workflow execution over authentic API-like tool surfaces rather than pure text-only reasoning.","L1: Language Comprehension (minor), Language Production (minor)
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory
L3: ",L2
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity agent capability on tasks such as locating known vulnerabilities from descriptions and discovering new vulnerabilities in real open-source codebases at scale. It emphasizes end-to-end technical reasoning: understanding code, hypothesizing flaws, validating findings, and producing correct exploit/vulnerability reports under an evaluation harness.","L1: 
L2: Logical Reasoning, Planning, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition, Decision-making (minor)
L3: Inhibitory Control (minor)",L2
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a large, frontier-level benchmark intended to probe advanced academic and professional knowledge across many domains, with an emphasis on difficult reasoning rather than routine recall. It includes challenging questions (often multimodal) designed to separate top models and to test robustness under high uncertainty.","L1: Language Comprehension, Visual Perception (minor)
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory (minor), Scene Understanding & Visual Reasoning (minor)
L3: ",L2
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,GPQA Diamond is a high-quality subset of graduate-level multiple-choice science questions designed to be difficult for non-experts and resistant to simple web lookup. It tests whether a model can use scientific understanding and multi-step reasoning to select the correct option under tight distractors.,"L1: Language Comprehension
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory (minor), Decision-making (minor)
L3: ",L2
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro evaluates expert-level multimodal understanding across many disciplines using problems that combine images (e.g., diagrams, charts, scenes) and text. It targets multi-step reasoning grounded in visual evidence, often requiring careful interpretation of figures and domain knowledge.","L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Visual Attention & Eye Movements (minor), Working Memory (minor)
L3: ",L2
OmniDocBench 1.5,https://github.com/opendatalab/OmniDocBench,https://arxiv.org/abs/2412.07626,"OmniDocBench 1.5 measures document understanding and OCR robustness across heterogeneous layouts, including text, formulas, tables, and reading order. It evaluates whether a system can accurately perceive and reconstruct structured content from document images rather than only recognizing isolated text lines.","L1: Visual Perception, Language Comprehension (minor)
L2: Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Semantic Understanding & Context Recognition (minor), Working Memory (minor)
L3: ",L2
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU evaluates multimodal reasoning over video, requiring models to integrate information across time (events, actions, and scene changes) to answer questions. It emphasizes temporal grounding and cross-frame integration rather than single-image recognition.","L1: Visual Perception
L2: Working Memory, Attention, Scene Understanding & Visual Reasoning, Multisensory Integration (minor)
L3: Cognitive Timing & Predictive Modeling",L3
LiveCodeBench Pro,https://livecodebenchpro.com/projects/livecodebench-pro/leaderboard,https://arxiv.org/abs/2506.11928,"LiveCodeBench Pro evaluates code generation and debugging on time-separated, execution-checked programming tasks intended to reduce leakage and better reflect real developer workflows. It rewards producing correct, runnable solutions under realistic constraints rather than only writing plausible-looking code.","L1: Language Production, Language Comprehension (minor)
L2: Logical Reasoning, Planning, Adaptive Error Correction, Working Memory
L3: ",L2
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"The FACTS Benchmark Suite systematically evaluates LLM factuality across a collection of tasks that probe whether outputs are accurate, supported, and appropriately calibrated. It is designed to quantify different failure modes (e.g., confident but incorrect claims) and encourage methods that improve faithfulness to evidence.","L1: Language Production
L2: Semantic Understanding & Context Recognition, Logical Reasoning (minor), Working Memory (minor)
L3: Inhibitory Control, Self-reflection (minor)",L3
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA extends physical commonsense and everyday reasoning evaluation across many languages and cultural contexts using non-parallel items. It tests whether models can select or generate plausible actions/outcomes in grounded, physical scenarios beyond English-centric phrasing.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Spatial Representation & Mapping (minor)
L3: Cognitive Flexibility (minor)",L2
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) is a long-context evaluation where multiple similar “needle” interactions are embedded within a long “haystack” of text, and the model must retrieve and reproduce the correct referenced response. It stresses attention control and robust retrieval when many distractors are present and context is very long.","L1: Language Comprehension (minor)
L2: Working Memory, Attention, Episodic Memory
L3: Inhibitory Control (minor)",L2
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval evaluates well-specified professional knowledge-work tasks across dozens of occupations, with outputs judged against industry professionals on real deliverables like spreadsheets, presentations, and written analyses. It aims to measure economically meaningful competence rather than narrow academic QA.","L1: Language Production
L2: Planning, Decision-making, Semantic Understanding & Context Recognition, Working Memory (minor)
L3: Self-reflection (minor), Social Reasoning & Theory of Mind (minor)",L2
SWE-Lancer,https://openai.com/index/swe-lancer/,https://arxiv.org/abs/2502.12115,"SWE-Lancer evaluates agentic software engineering by having models work through realistic engineering tasks that resemble end-to-end issue resolution in codebases. It emphasizes correctness under an execution/test harness, along with the ability to navigate repositories, modify code safely, and converge on a working patch.","L1: Language Comprehension (minor), Language Production (minor)
L2: Planning, Adaptive Error Correction, Logical Reasoning, Working Memory, Decision-making (minor)
L3: ",L2
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath evaluates advanced mathematical problem solving intended to be difficult even for strong models, with tiers reflecting increasing depth and novelty. It emphasizes multi-step derivations and careful quantitative reasoning, often benefiting from structured solution planning and verification.","L1: 
L2: Logical Reasoning, Working Memory, Planning, Adaptive Error Correction (minor), Attention (minor)
L3: ",L2
