Benchmark,Website,Paper,Description,Cognitive Functions
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates software engineering agents by asking them to produce code patches that fix real issues in Python repositories, then running the project’s tests to verify correctness. The Verified subset uses tasks curated to be solvable and to reduce ambiguous problem statements and flaky evaluation, emphasizing end-to-end repo understanding, implementation, and debugging.","Planning, Logical Reasoning, Working Memory, Adaptive Error Correction, Decision-making (minor), Language Comprehension (minor)"
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 measures an agent’s ability to accomplish practical command-line tasks in sandboxed environments (e.g., installing tools, manipulating files, running programs, and debugging failures). Success typically requires iteratively issuing shell commands, interpreting tool outputs, and recovering from errors under resource and time constraints.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Attention (minor), Language Comprehension (minor)"
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench evaluates tool-using customer-support agents in multi-turn scenarios (e.g., retail, airline, telecom) where the model must follow domain policies while interacting with simulated users and backend APIs. It stresses reliable tool invocation, policy compliance across long dialogues, and robust handling of edge cases and user pressure.","Language Comprehension, Language Production, Planning, Decision-making, Inhibitory Control, Social Reasoning & Theory of Mind (minor)"
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI measures abstract, few-shot “fluid intelligence” by requiring models to infer transformation rules from a small number of input–output grid examples and apply them to a new grid. The tasks are designed to penalize rote memorization and reward flexible rule induction and compositional generalization.","Logical Reasoning, Cognitive Flexibility, Working Memory, Spatial Representation & Mapping, Scene Understanding & Visual Reasoning (minor)"
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon agent coherence and strategy in a simulated year-long vending-machine business, where the agent must manage inventory, pricing, supplier negotiation, and budgeting. The score is based on business outcomes (e.g., final balance), rewarding sustained planning and adaptation across many steps rather than single-turn correctness.","Planning, Decision-making, Reward Mechanisms, Working Memory, Adaptive Error Correction (minor), Semantic Understanding & Context Recognition (minor)"
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam (HLE) is a frontier, multidisciplinary benchmark with difficult questions intended to probe deep reasoning and broad knowledge, often across multimodal inputs. It is commonly used to compare models both with and without tools (e.g., search, code execution), emphasizing synthesis and error-avoidant problem solving on hard items.","Language Comprehension, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory (minor), Multisensory Integration (minor), Visual Perception (minor)"
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 evaluates competition-style mathematics problem solving using short-answer questions that typically require multi-step derivations rather than pattern matching. It stresses symbolic manipulation, precise quantitative reasoning, and maintaining intermediate results across a solution chain.","Logical Reasoning, Working Memory, Planning (minor), Cognitive Flexibility (minor)"
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,GPQA Diamond is a high-difficulty subset of graduate-level multiple-choice science questions designed to be resistant to shallow retrieval and to require careful reasoning. It probes whether models can integrate domain knowledge with multi-step inference under adversarially hard distractors.,"Semantic Understanding & Context Recognition, Logical Reasoning, Language Comprehension (minor), Working Memory (minor)"
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends MMLU to multiple languages, testing broad academic knowledge and reasoning across many subjects and non-English settings. It emphasizes cross-lingual understanding and robustness to linguistic variation while keeping the underlying task formats similar to standard knowledge exams.","Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning (minor), Working Memory (minor)"
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro is a multimodal expert-level benchmark that evaluates understanding and reasoning over images paired with text across many disciplines, with a focus on harder professional-grade items. It targets integrated visual–text reasoning (e.g., interpreting diagrams, tables, charts, and technical visuals) rather than pure OCR.","Visual Perception, Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Working Memory (minor)"
MathArena Apex,https://matharena.ai/?view=problem&comp=apex--apex_2025,https://arxiv.org/abs/2505.23281,"MathArena Apex aggregates challenging math problems to compare advanced mathematical reasoning, often emphasizing multi-step solutions and higher difficulty than standard contest sets. It is used to evaluate both correctness and robustness of solution strategies under complex algebraic, combinatorial, and geometric reasoning demands.","Logical Reasoning, Working Memory, Planning (minor), Cognitive Flexibility (minor)"
ScreenShot-Pro,https://gui-agent.github.io/grounding-leaderboard/,https://arxiv.org/abs/2504.07981,"ScreenShot-Pro evaluates models on understanding high-resolution screenshots of software interfaces and answering questions or producing grounded actions based on GUI elements. The benchmark stresses visual grounding (identifying the right UI targets), layout/spatial reasoning, and careful attention to small on-screen details typical of professional tools.","Visual Perception, Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Spatial Representation & Mapping, Sensorimotor Coordination (minor)"
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning evaluates reasoning over scientific figures and visual artifacts from research papers, requiring models to interpret plots, diagrams, and annotations to answer questions. It targets figure-grounded inference (not just recognition), including extracting relationships, comparing trends, and integrating figure context with scientific text.","Scene Understanding & Visual Reasoning, Visual Perception, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory (minor)"
OmniDocBench 1.5,https://github.com/opendatalab/OmniDocBench,https://arxiv.org/abs/2412.07626,"OmniDocBench 1.5 evaluates document understanding and OCR-centric capabilities across diverse layouts such as mixed text, formulas, tables, and reading order. It stresses faithfully transcribing and structuring content from complex documents where layout and formatting carry meaning.","Visual Perception, Scene Understanding & Visual Reasoning, Language Comprehension (minor), Working Memory (minor)"
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU is a multimodal benchmark for reasoning over video, requiring models to integrate information across frames and time to answer questions. It probes temporal understanding (events, causality, state changes) and the ability to retain and use visual evidence observed earlier in the clip.","Visual Perception, Episodic Memory, Scene Understanding & Visual Reasoning, Multisensory Integration (minor), Cognitive Timing & Predictive Modeling (minor), Working Memory (minor)"
LiveCodeBench Pro,https://livecodebenchpro.com/projects/livecodebench-pro/leaderboard,https://arxiv.org/abs/2506.11928,"LiveCodeBench Pro evaluates coding ability on competitively sourced, contamination-aware programming tasks, typically emphasizing algorithmic problem solving and correct executable solutions. It targets planning a solution approach, implementing reliably, and handling edge cases under realistic coding constraints.","Logical Reasoning, Planning, Working Memory, Adaptive Error Correction (minor)"
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"The FACTS Benchmark Suite systematically measures factuality and grounding, including whether a model’s outputs remain supported by provided sources and avoid introducing unsupported claims. It emphasizes reliability under information constraints and sensitivity to evidence, rather than creativity or open-ended generation.","Semantic Understanding & Context Recognition, Language Comprehension, Inhibitory Control, Self-reflection (minor), Working Memory (minor)"
SimpleQA Verified,https://www.kaggle.com/benchmarks/deepmind/simpleqa-verified,https://arxiv.org/abs/2509.07968,"SimpleQA Verified evaluates short-form factual question answering with verified ground truth, focusing on precision for straightforward queries rather than complex multi-hop reasoning. It is commonly used to measure hallucination propensity and basic factual reliability in a controlled setting.","Semantic Understanding & Context Recognition, Language Comprehension, Inhibitory Control (minor), Working Memory (minor)"
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA tests physical commonsense reasoning across many languages using non-parallel (not direct translations) question sets, aiming to measure whether models generalize “everyday physics” knowledge cross-lingually. The tasks emphasize selecting the more plausible action/outcome given constraints of the physical world.","Semantic Understanding & Context Recognition, Logical Reasoning, Spatial Representation & Mapping (minor), Cognitive Timing & Predictive Modeling (minor), Language Comprehension (minor)"
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,MRCR v2 (8-needle) evaluates long-context multi-round coreference resolution by embedding multiple similar “needle” interactions inside long “haystacks” and asking the model to reproduce the correct referenced response. It stresses maintaining and retrieving the right discourse state under heavy distractors across very long contexts.,"Working Memory, Attention, Language Comprehension, Episodic Memory (minor), Inhibitory Control (minor)"
