Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates an agent’s ability to fix real issues in open-source Python repositories by producing a patch that makes the project’s tests pass. The “Verified” subset emphasizes problem instances that have been validated as solvable and reliably graded, making it a common reference for agentic software engineering performance.","L1: Language Comprehension (minor), Language Production (minor)
L2: Planning, Logical Reasoning, Working Memory, Adaptive Error Correction
L3: ",L2
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 tests autonomous agent performance on real command-line tasks (e.g., inspecting files, running programs, installing dependencies, debugging failures) within constrained execution environments. It stresses iterative trial-and-error interaction with tools and the ability to recover from mistakes under resource and time limits.","L1: Language Comprehension (minor)
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory
L3: Inhibitory Control (minor)",L2
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench evaluates tool-using conversational agents in multi-turn customer-support style scenarios (e.g., retail, airline, telecom) that require following policies while interacting with simulated users and backend APIs. Success depends on maintaining dialogue state, choosing correct tool calls, and adhering to domain constraints over extended interactions.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Working Memory
L3: Social Reasoning & Theory of Mind, Inhibitory Control",L3
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI measures abstract pattern induction from a small number of examples, typically using grid-based input–output demonstrations that hide an underlying rule. It targets generalization to novel transformations with minimal task-specific priors, emphasizing reasoning over memorization.","L1: Visual Perception
L2: Logical Reasoning, Working Memory, Spatial Representation & Mapping, Attention (minor)
L3: Cognitive Flexibility, Cognitive Timing & Predictive Modeling (minor)",L3
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon autonomy by having an agent manage a simulated vending-machine business over an extended “year,” making many sequential decisions (inventory, pricing, negotiation, logistics). The score reflects the agent’s ability to plan, adapt to changing conditions, and maintain coherent strategy over long trajectories.","L1: 
L2: Planning, Decision-making, Reward Mechanisms, Working Memory, Episodic Memory (minor), Adaptive Error Correction (minor)
L3: ",L2
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a broad, frontier-oriented benchmark spanning challenging questions across many domains, often including multimodal inputs. It is designed to stress high-level reasoning and knowledge integration rather than narrow task skills, and is frequently used to compare overall frontier-model capability.","L1: Language Comprehension, Visual Perception (minor)
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory (minor), Scene Understanding & Visual Reasoning (minor)
L3: ",L2
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,AIME 2025 comprises competition mathematics problems (from the 2025 AIME) that require multi-step symbolic reasoning and careful handling of constraints. It is commonly used to probe precise mathematical problem-solving without relying on external tools.,"L1: 
L2: Logical Reasoning, Working Memory, Planning (minor), Adaptive Error Correction (minor)
L3: ",L2
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,GPQA Diamond is a curated subset of graduate-level multiple-choice science questions intended to be difficult to solve via superficial pattern matching or simple web lookup. It probes deep scientific reasoning and the ability to discriminate between closely competing answer choices.,"L1: 
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory (minor)
L3: Inhibitory Control (minor)",L2
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends broad academic knowledge testing to multiple languages, assessing whether models can answer subject-area questions across diverse linguistic contexts. It emphasizes cross-lingual robustness, domain knowledge retrieval, and consistent reasoning across translations and culturally varied phrasing.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning (minor), Working Memory (minor)
L3: ",L2
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro evaluates multimodal understanding and reasoning across many disciplines, requiring the model to integrate text with images such as diagrams, tables, charts, and scientific figures. Compared to simpler vision QA, it emphasizes expert-level interpretation and multi-step reasoning grounded in visual evidence.","L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Multisensory Integration, Working Memory (minor), Logical Reasoning (minor)
L3: ",L2
MathArena Apex,https://matharena.ai/?view=problem&comp=apex--apex_2025,https://arxiv.org/abs/2505.23281,MathArena Apex is a high-difficulty mathematics benchmark drawn from an aggregate of advanced problem sources and curated to stress strong mathematical reasoning. It is typically used to separate top models by requiring long chains of deduction and careful quantitative precision.,"L1: 
L2: Logical Reasoning, Working Memory, Planning (minor), Adaptive Error Correction (minor)
L3: ",L2
ScreenShot-Pro,https://gui-agent.github.io/grounding-leaderboard/,https://arxiv.org/abs/2504.07981,"ScreenShot-Pro evaluates visual grounding on realistic software screenshots, where models must interpret UI elements and their relationships to answer questions or identify targets. It stresses spatial understanding of interfaces and precise mapping from language instructions to on-screen evidence.","L1: Visual Perception, Language Comprehension (minor)
L2: Scene Understanding & Visual Reasoning, Spatial Representation & Mapping, Visual Attention & Eye Movements (minor)
L3: ",L2
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning focuses on reasoning over figures from scientific documents (e.g., plots, charts, and visual evidence accompanying papers), often requiring quantitative or relational interpretation. It probes whether a model can ground answers in figure content rather than relying on generic scientific priors.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Logical Reasoning, Semantic Understanding & Context Recognition (minor), Working Memory (minor)
L3: ",L2
OmniDocBench 1.5,https://github.com/opendatalab/OmniDocBench,https://arxiv.org/abs/2412.07626,"OmniDocBench 1.5 evaluates document understanding and OCR-centric capabilities across heterogeneous layouts, including text, tables, formulas, and reading order. It measures how well models can extract and structure information from visually complex documents while preserving layout-dependent semantics.","L1: Visual Perception, Language Comprehension (minor)
L2: Scene Understanding & Visual Reasoning, Attention, Spatial Representation & Mapping (minor)
L3: ",L2
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU extends multimodal reasoning to time-based inputs, requiring models to answer questions grounded in information distributed across video frames. It probes temporal integration, event understanding, and the ability to maintain coherence when evidence is sparse and spread over time.","L1: Visual Perception
L2: Working Memory, Scene Understanding & Visual Reasoning, Attention (minor)
L3: Cognitive Timing & Predictive Modeling",L3
LiveCodeBench Pro,https://livecodebenchpro.com/projects/livecodebench-pro/leaderboard,https://arxiv.org/abs/2506.11928,"LiveCodeBench Pro evaluates coding in conditions intended to better reflect modern development, emphasizing solving programming tasks with realistic constraints and minimizing leakage from static training data. It is often reported with metrics like Elo, reflecting relative strength across many coding problems.","L1: Language Production (minor)
L2: Logical Reasoning, Planning, Adaptive Error Correction, Working Memory
L3: ",L2
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"The FACTS Benchmark Suite systematically evaluates factuality by testing whether model outputs remain faithful to provided sources and whether unsupported claims are avoided across varied tasks. It targets hallucination-related failure modes, including subtle errors that can appear plausible but are not grounded.","L1: Language Comprehension (minor)
L2: Semantic Understanding & Context Recognition, Attention (minor)
L3: Inhibitory Control, Self-reflection (minor)",L3
SimpleQA Verified,https://www.kaggle.com/benchmarks/deepmind/simpleqa-verified,https://arxiv.org/abs/2509.07968,"SimpleQA Verified is a factual question answering benchmark with verification procedures intended to reduce ambiguous labeling and reward correct, supported answers. It is commonly used to measure precision on short, fact-based queries and sensitivity to false premises or misleading prompts.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition
L3: Inhibitory Control (minor), Self-reflection (minor)",L2
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA tests practical and physical commonsense reasoning across languages, focusing on whether models can select or generate plausible actions and outcomes in everyday situations. It aims to measure robustness of commonsense inference beyond English-centric distributions.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Spatial Representation & Mapping (minor)
L3: Social Reasoning & Theory of Mind (minor)",L2
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) evaluates long-context retrieval and multi-round coreference resolution by embedding multiple similar “needle” interactions within long “haystacks” of dialogue or documents. The model must reproduce the correct response associated with a specified needle, stressing robustness to interference and distractors at long context lengths.","L1: Language Comprehension
L2: Working Memory, Attention, Episodic Memory
L3: Inhibitory Control (minor)",L2
