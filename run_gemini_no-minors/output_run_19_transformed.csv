Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates software-engineering agents on real GitHub issues by requiring them to generate a patch that makes a target repository’s tests pass. The “Verified” subset filters tasks to those confirmed solvable and is typically run in a single-attempt, tool-driven repo-editing workflow (edit, run tests, iterate).","L1: Language Comprehension (minor), Language Production (minor)
L2: Planning, Logical Reasoning, Working Memory, Adaptive Error Correction
L3: ",L2
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 measures an agent’s ability to accomplish realistic command-line tasks in sandboxed environments (e.g., debugging, installing dependencies, running scripts, manipulating files). Success requires selecting and sequencing shell commands, interpreting outputs, and recovering from tool or environment errors under resource constraints.","L1: 
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory, Logical Reasoning (minor)
L3: Inhibitory Control (minor)",L2
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench evaluates interactive agentic tool use in customer-support-style simulations (e.g., retail, airline, telecom), where the model must follow policies while calling APIs over multiple turns. It stresses long-horizon dialogue control, correct tool invocation, and adherence to constraints despite ambiguous or adversarial user requests.","L1: Language Comprehension, Language Production (minor)
L2: Decision-making, Planning, Working Memory (minor)
L3: Social Reasoning & Theory of Mind, Inhibitory Control",L3
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,ARC-AGI (Abstraction and Reasoning Corpus) tests few-shot “fluid” reasoning by asking models to infer the hidden rule mapping between small input/output grids and apply it to a new grid. Tasks are designed to require novel pattern induction rather than memorization of domain facts.,"L1: Visual Perception
L2: Logical Reasoning, Working Memory, Spatial Representation & Mapping, Attention (minor)
L3: Cognitive Flexibility",L3
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon autonomous agent coherence by simulating a year of operating a vending machine business (inventory, pricing, supplier negotiation, budgeting). The score is typically the final balance, rewarding sustained strategy, adaptation to dynamics, and consistent execution across thousands of steps.","L1: 
L2: Planning, Decision-making, Reward Mechanisms, Working Memory, Adaptive Error Correction (minor)
L3: Motivational Drives (minor)",L2
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam (HLE) is a broad, frontier-style benchmark spanning difficult questions across many fields, often including multimodal items and tool-enabled variants (e.g., search or code). It aims to probe deep reasoning and knowledge integration rather than narrow skill execution.","L1: Language Comprehension, Visual Perception (minor)
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Multisensory Integration (minor), Working Memory (minor)
L3: ",L2
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 is a set of contest mathematics problems used to evaluate step-by-step quantitative reasoning and symbolic manipulation. Model performance is typically measured by accuracy across the problem set, sometimes with and without tool support (e.g., Python).","L1: 
L2: Logical Reasoning, Working Memory, Adaptive Error Correction (minor)
L3: Cognitive Timing & Predictive Modeling (minor)",L2
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a high-quality subset of extremely difficult, “Google-proof” multiple-choice science questions intended to require expert-level reasoning rather than retrieval. It emphasizes careful reading and elimination among close distractors across physics, chemistry, and biology.","L1: 
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory (minor)
L3: Inhibitory Control (minor)",L2
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends MMLU to multiple languages, testing broad academic knowledge and reasoning across many subjects and non-English settings. It probes whether models preserve competence under multilingual comprehension and culturally varied phrasing.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning (minor)
L3: Cognitive Flexibility (minor)",L2
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro is a multimodal benchmark of expert-level questions requiring joint understanding of images and text across many disciplines (e.g., diagrams, tables, charts). Compared to earlier variants, it is designed to be more challenging and to better separate perception from higher-level reasoning.","L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning (minor), Attention (minor)
L3: ",L2
MathArena Apex,https://matharena.ai/?view=problem&comp=apex--apex_2025,https://arxiv.org/abs/2505.23281,"MathArena Apex is a difficult mathematics benchmark curated to stress advanced problem solving beyond routine contest items, often emphasizing compositional reasoning and proof-like steps. Scores reflect accuracy on problems intended to discriminate top-tier reasoning models.","L1: 
L2: Logical Reasoning, Working Memory, Planning (minor)
L3: Cognitive Flexibility (minor)",L2
ScreenShot-Pro,https://gui-agent.github.io/grounding-leaderboard/,https://arxiv.org/abs/2504.07981,"ScreenShot-Pro evaluates GUI understanding from screenshots, requiring the model to interpret interface layout and answer questions or take grounded actions based on visual elements. It stresses robust visual grounding in real software UIs where small spatial or textual cues determine correctness.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Spatial Representation & Mapping (minor), Decision-making (minor)
L3: ",L2
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning evaluates reasoning over scientific figures from papers, emphasizing chart/plot interpretation, legend decoding, and quantitative inference from visuals. Tasks often require combining visual evidence with domain context to select or compute an answer.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Logical Reasoning, Semantic Understanding & Context Recognition (minor), Attention (minor)
L3: ",L2
OmniDocBench 1.5,https://github.com/opendatalab/OmniDocBench,https://arxiv.org/abs/2412.07626,"OmniDocBench 1.5 evaluates document understanding and OCR-like extraction across complex layouts, including text, formulas, tables, and reading order. Metrics (e.g., edit distance and structure-sensitive sub-scores) reflect how well models preserve content and layout semantics.","L1: Visual Perception, Language Production (minor)
L2: Attention, Scene Understanding & Visual Reasoning, Semantic Understanding & Context Recognition (minor)
L3: ",L2
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU extends multimodal understanding to time-based inputs, requiring models to answer questions about events, actions, and changes across video clips. It tests whether models can integrate information across frames rather than relying on single-image cues.","L1: Visual Perception
L2: Multisensory Integration, Episodic Memory, Attention, Scene Understanding & Visual Reasoning (minor)
L3: ",L2
LiveCodeBench Pro,https://livecodebenchpro.com/projects/livecodebench-pro/leaderboard,https://arxiv.org/abs/2506.11928,"LiveCodeBench Pro measures coding ability on continuously updated, contamination-resistant programming tasks, often emphasizing real-time problem solving rather than memorized solutions. Evaluation typically focuses on pass@1 correctness under realistic constraints and prompts.","L1: Language Comprehension (minor), Language Production (minor)
L2: Logical Reasoning, Planning, Adaptive Error Correction, Working Memory
L3: ",L2
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"FACTS Benchmark Suite evaluates factuality by testing whether models make correct, well-grounded statements and avoid unsupported claims across a systematic set of factuality tasks. It is intended to capture both knowledge accuracy and the tendency to hallucinate under realistic prompting conditions.","L1: Language Comprehension (minor)
L2: Semantic Understanding & Context Recognition
L3: Inhibitory Control, Self-reflection (minor)",L3
SimpleQA Verified,https://www.kaggle.com/benchmarks/deepmind/simpleqa-verified,https://arxiv.org/abs/2509.07968,SimpleQA Verified is a factual question-answering benchmark with verification procedures intended to ensure reliable ground truth and reduce ambiguity. It primarily measures precision on straightforward questions where a model should answer correctly or abstain rather than speculate.,"L1: Language Comprehension (minor)
L2: Semantic Understanding & Context Recognition
L3: Inhibitory Control",L3
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA evaluates physical commonsense reasoning with non-parallel multilingual coverage, probing whether models can choose plausible actions or explanations grounded in everyday physics. It emphasizes generalization across languages and culturally varied surface forms while keeping the underlying physical reasoning similar.","L1: Language Comprehension (minor)
L2: Spatial Representation & Mapping, Logical Reasoning, Semantic Understanding & Context Recognition
L3: Cognitive Flexibility (minor)",L2
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) is a long-context evaluation where multiple similar “needle” requests are embedded in long conversational “haystacks,” and the model must retrieve the correct response corresponding to a specific needle. It stresses robust cross-document co-reference and retrieval under heavy distractor similarity at long token lengths.","L1: Language Comprehension
L2: Working Memory, Attention, Episodic Memory
L3: Inhibitory Control (minor)",L2
