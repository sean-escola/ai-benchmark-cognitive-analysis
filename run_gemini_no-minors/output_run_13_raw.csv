Benchmark,Website,Paper,Description,Cognitive Functions
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro evaluates software engineering agents on realistic issues drawn from real repositories, where the model must produce code changes that satisfy tests and project constraints. It is designed to be harder and more contamination-resistant than SWE-bench Verified, and spans multiple languages and industrially relevant tasks.","Planning, Adaptive Error Correction, Logical Reasoning, Working Memory, Decision-making, Language Comprehension (minor), Language Production (minor)"
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld measures how well models can operate a computer-like environment by perceiving screens (GUI state) and executing actions over many steps to complete user goals. Tasks require robust interaction with real applications and interfaces, stressing end-to-end perception–cognition–action loops rather than single-turn Q&A.","Visual Perception, Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Planning, Decision-making, Sensorimotor Coordination, Working Memory, Adaptive Error Correction (minor)"
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,ARC-AGI is a few-shot abstract reasoning benchmark where systems infer a hidden transformation from a small set of input–output grid examples and apply it to a new grid. It targets “fluid intelligence” by emphasizing novel pattern induction and generalization rather than memorized knowledge.,"Logical Reasoning, Cognitive Flexibility, Spatial Representation & Mapping, Scene Understanding & Visual Reasoning, Working Memory, Attention (minor)"
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon autonomy by having an agent run a simulated vending machine business over an extended period, making thousands of interconnected decisions. High performance requires sustained coherence, strategic adaptation to market dynamics, and effective management of resources and risks.","Planning, Decision-making, Reward Mechanisms, Working Memory, Cognitive Timing & Predictive Modeling, Adaptive Error Correction (minor), Motivational Drives (minor)"
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas evaluates real-world tool use through the Model Context Protocol (MCP), focusing on discovering tools, calling them correctly, and composing multi-step workflows across services. It stresses robustness to tool errors, API schema constraints, and multi-hop execution needed for agentic task completion.","Planning, Decision-making, Working Memory, Adaptive Error Correction, Language Comprehension (minor), Language Production (minor)"
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity agent capabilities at scale, including finding known vulnerabilities in real open-source projects from high-level weakness descriptions and attempting discovery of new vulnerabilities. It emphasizes code understanding, structured troubleshooting, and iterative hypothesis testing under realistic constraints.","Logical Reasoning, Planning, Adaptive Error Correction, Working Memory, Language Comprehension (minor), Inhibitory Control (minor)"
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam (HLE) is a large multimodal benchmark intended to probe frontier-level academic knowledge and reasoning across many disciplines. Questions often require multi-step inference, careful interpretation of problem statements, and integrating text with visual or document-style evidence.","Language Comprehension, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Scene Understanding & Visual Reasoning (minor), Visual Perception (minor)"
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a challenging, “Google-proof” multiple-choice science benchmark drawn from graduate-level questions, with a high-quality subset curated to be difficult for non-experts. It stresses precise scientific reasoning and discrimination among plausible distractors rather than superficial pattern matching.","Logical Reasoning, Semantic Understanding & Context Recognition, Language Comprehension, Working Memory (minor)"
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro evaluates expert-level multimodal understanding and reasoning across many disciplines, combining text with images such as diagrams, charts, tables, and scientific figures. It tests whether models can ground answers in visual evidence and perform domain reasoning under structured question formats.","Visual Perception, Scene Understanding & Visual Reasoning, Multisensory Integration, Language Comprehension, Logical Reasoning, Working Memory (minor), Visual Attention & Eye Movements (minor)"
OmniDocBench 1.5,https://github.com/opendatalab/OmniDocBench,https://arxiv.org/abs/2412.07626,"OmniDocBench 1.5 evaluates document understanding, including OCR accuracy and structure preservation for elements like text blocks, formulas, tables, and reading order. It focuses on extracting and reconstructing content from complex, multi-layout documents where spatial organization carries meaning.","Visual Perception, Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Language Comprehension, Spatial Representation & Mapping (minor), Semantic Understanding & Context Recognition (minor)"
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU evaluates multimodal reasoning over videos, requiring models to integrate information across frames and time to answer questions about events, actions, and causal relations. It stresses temporal grounding, tracking entities through scenes, and summarizing dynamic visual evidence.","Visual Perception, Scene Understanding & Visual Reasoning, Working Memory, Attention, Cognitive Timing & Predictive Modeling (minor), Multisensory Integration (minor), Auditory Processing (minor)"
LiveCodeBench Pro,https://livecodebenchpro.com/projects/livecodebench-pro/leaderboard,https://arxiv.org/abs/2506.11928,"LiveCodeBench Pro measures coding ability on problems intended to reflect contemporary programming tasks, typically requiring correct algorithm design and executable solutions. It is often used to compare code generation under realistic constraints where small mistakes break correctness or tests.","Logical Reasoning, Planning, Working Memory, Adaptive Error Correction, Language Production (minor), Language Comprehension (minor)"
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"The FACTS Benchmark Suite systematically evaluates LLM factuality across multiple settings, including knowledge recall, grounding, and resistance to producing unsupported statements. It targets whether models can maintain truthful outputs under ambiguity, conflicting context, or misleading prompts.","Semantic Understanding & Context Recognition, Language Comprehension, Inhibitory Control, Working Memory (minor), Self-reflection (minor)"
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA evaluates commonsense physical reasoning across many languages and cultural contexts, using non-parallel multilingual data to reduce simple translation artifacts. It probes whether models can infer plausible actions, affordances, and everyday cause–effect relationships from language alone.","Language Comprehension, Logical Reasoning, Semantic Understanding & Context Recognition, Cognitive Flexibility (minor)"
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,MRCR v2 (8-needle) tests long-context multi-round coreference and retrieval by embedding multiple similar “needle” requests in a long “haystack” of interactions and asking the model to reproduce the correct referenced response. It stresses accurate retrieval under interference and maintaining coherence across very long contexts.,"Working Memory, Attention, Episodic Memory, Language Comprehension, Semantic Understanding & Context Recognition (minor)"
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval evaluates economically relevant, well-specified knowledge work across many occupations, where models produce professional artifacts (e.g., spreadsheets, presentations, plans) judged against expert human work. It emphasizes end-to-end task execution quality, adherence to constraints, and usefulness in real workflows.","Planning, Decision-making, Language Production, Semantic Understanding & Context Recognition, Working Memory (minor), Self-reflection (minor), Social Reasoning & Theory of Mind (minor)"
SWE-Lancer,https://openai.com/index/swe-lancer/,https://arxiv.org/abs/2502.12115,"SWE-Lancer evaluates agentic software engineering in settings closer to real contracting work, emphasizing the ability to navigate repositories, implement changes, and deliver patches that satisfy practical requirements. It stresses reliability over longer horizons than single-function coding questions, including iterative debugging and integration.","Planning, Adaptive Error Correction, Decision-making, Working Memory, Language Comprehension (minor), Self-reflection (minor)"
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath is an expert-level mathematics benchmark intended to measure reasoning on difficult problems that resist shallow pattern matching and require genuine mathematical insight. It emphasizes multi-step derivations, careful constraint handling, and error-sensitive symbolic reasoning.","Logical Reasoning, Working Memory, Planning, Adaptive Error Correction (minor), Cognitive Flexibility (minor)"
