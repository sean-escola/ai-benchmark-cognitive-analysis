Benchmark,Website,Paper,Description,Cognitive Functions
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates an agent’s ability to solve real software engineering issues in open-source repositories by producing patches that pass the project’s tests. The “Verified” subset focuses on tasks that have been validated as solvable, emphasizing end-to-end debugging, code changes, and adherence to project constraints.","Planning, Logical Reasoning, Adaptive Error Correction, Working Memory, Language Comprehension (minor), Language Production (minor)"
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 measures agent performance on real-world command-line tasks, where models must operate in a shell environment to inspect files, run tools, and execute multi-step workflows. It stresses procedural problem solving under execution feedback (errors, logs, test outputs) and resource constraints typical of developer tooling.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Language Comprehension (minor)"
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench evaluates interactive tool-using agents in simulated customer-support domains (e.g., retail, airline, telecom), requiring multi-turn dialog grounded in policy and external API actions. The benchmark emphasizes following domain rules, clarifying ambiguous user intent, and completing long workflows reliably.","Language Comprehension, Language Production, Decision-making, Planning, Inhibitory Control, Social Reasoning & Theory of Mind (minor), Working Memory (minor)"
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,ARC-AGI assesses “fluid” abstract reasoning by asking models to infer latent transformation rules from a few grid-based input–output examples and apply them to a new input. It is designed to minimize reliance on memorized domain knowledge and instead emphasize flexible generalization from sparse demonstrations.,"Logical Reasoning, Cognitive Flexibility, Working Memory, Spatial Representation & Mapping, Scene Understanding & Visual Reasoning (minor), Visual Perception (minor)"
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 measures long-horizon agent coherence and business decision-making in a simulated vending-machine enterprise over an extended time period. Agents must manage inventory, pricing, supplier interactions, and cash flow, where early choices affect later constraints and outcomes.","Planning, Decision-making, Reward Mechanisms, Working Memory, Semantic Understanding & Context Recognition (minor), Social Reasoning & Theory of Mind (minor)"
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam (HLE) is a broad, frontier-style benchmark spanning difficult questions across many domains and formats, often intended to be challenging even for strong models. It is used to probe deep reasoning, knowledge integration, and (when enabled) tool-augmented problem solving under realistic constraints.","Language Comprehension, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Visual Perception (minor), Scene Understanding & Visual Reasoning (minor)"
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 evaluates mathematical problem solving using questions from a high-level math competition, typically requiring multi-step derivations and careful handling of constraints. It emphasizes correctness under complex symbolic reasoning rather than retrieval of facts.","Logical Reasoning, Working Memory, Planning (minor), Adaptive Error Correction (minor)"
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,GPQA Diamond is a curated subset of graduate-level science multiple-choice questions designed to be hard to answer via shallow pattern matching. It aims to test scientific reasoning and precise understanding of domain concepts under adversarially difficult distractors.,"Logical Reasoning, Semantic Understanding & Context Recognition, Language Comprehension, Working Memory (minor)"
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends broad academic knowledge evaluation into multiple languages, testing whether models can maintain performance across linguistic variation. It probes general reasoning and subject-matter competence while stressing multilingual comprehension.","Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory (minor)"
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro evaluates expert-level multimodal reasoning, combining images (e.g., diagrams, charts, visuals) with text questions across many disciplines. It stresses grounding language in visual evidence and performing multi-step inference from mixed modalities.","Multisensory Integration, Visual Perception, Scene Understanding & Visual Reasoning, Language Comprehension, Working Memory (minor)"
MathArena Apex,https://matharena.ai/?view=problem&comp=apex--apex_2025,https://arxiv.org/abs/2505.23281,"MathArena Apex is a difficult mathematics benchmark suite intended to stress advanced reasoning beyond standard contest problems, often comparing models under tool/no-tool settings. It emphasizes long, brittle chains of quantitative inference where small mistakes can derail final answers.","Logical Reasoning, Working Memory, Adaptive Error Correction (minor), Planning (minor)"
ScreenShot-Pro,https://gui-agent.github.io/grounding-leaderboard/,https://arxiv.org/abs/2504.07981,"ScreenShot-Pro evaluates visual grounding on real software UI screenshots, requiring models to interpret interface elements and answer questions or identify targets based on spatial layout. It stresses accurate perception of text/icons and robust mapping from instructions to UI structure.","Visual Perception, Scene Understanding & Visual Reasoning, Spatial Representation & Mapping, Attention (minor), Visual Attention & Eye Movements (minor), Language Comprehension (minor)"
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning evaluates reasoning over scientific content derived from arXiv-style documents, often requiring connecting textual descriptions with technical figures/tables or formal notation. It is used to test whether models can sustain precise, evidence-grounded inference in research-like contexts.","Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning, Scene Understanding & Visual Reasoning (minor), Working Memory (minor)"
OmniDocBench 1.5,https://github.com/opendatalab/OmniDocBench,https://arxiv.org/abs/2412.07626,"OmniDocBench 1.5 evaluates document understanding and OCR quality across diverse layouts, including text, formulas, tables, and reading order. It targets robustness to complex formatting where correct output depends on both recognition and structural reconstruction.","Visual Perception, Scene Understanding & Visual Reasoning, Spatial Representation & Mapping, Language Production, Language Comprehension (minor), Working Memory (minor)"
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU extends multimodal understanding to videos, requiring models to integrate information across time to answer questions about events, objects, and actions. It stresses temporal evidence aggregation and maintaining coherence across multiple frames and scenes.","Visual Perception, Cognitive Timing & Predictive Modeling, Working Memory, Scene Understanding & Visual Reasoning, Attention (minor)"
LiveCodeBench Pro,https://livecodebenchpro.com/projects/livecodebench-pro/leaderboard,https://arxiv.org/abs/2506.11928,"LiveCodeBench Pro evaluates coding ability on competitive programming and practical coding tasks under controlled release and leakage constraints, often reported as an ELO-style score. It emphasizes generating correct, executable code and handling edge cases under time/feedback pressure.","Logical Reasoning, Planning, Adaptive Error Correction, Working Memory, Language Production (minor)"
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"The FACTS Benchmark Suite systematically evaluates factuality and faithfulness, testing whether model outputs remain grounded in provided sources and avoid unsupported claims. It targets failure modes like hallucination, incorrect attribution, and overconfident fabrication across varied prompting conditions.","Semantic Understanding & Context Recognition, Language Comprehension, Inhibitory Control, Self-reflection (minor), Working Memory (minor)"
SimpleQA Verified,https://www.kaggle.com/benchmarks/deepmind/simpleqa-verified,https://arxiv.org/abs/2509.07968,"SimpleQA Verified is a factual question-answering benchmark with verification procedures intended to ensure reliable ground truth and reduce ambiguity. It primarily measures whether models can return concise, correct factual answers rather than verbose reasoning traces.","Semantic Understanding & Context Recognition, Language Comprehension, Language Production (minor)"
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA evaluates multilingual commonsense reasoning for physical and everyday interactions across many languages, emphasizing robustness to linguistic and cultural variation. It probes whether models can infer plausible actions/outcomes in practical scenarios rather than relying on specialized knowledge.","Semantic Understanding & Context Recognition, Logical Reasoning, Language Comprehension, Social Reasoning & Theory of Mind (minor)"
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) tests long-context multi-round co-reference resolution by embedding multiple similar “needle” interactions within long “haystacks” and requiring retrieval of the correct referenced response. It is designed to stress long-range dependency tracking, interference resistance, and accurate context indexing.","Working Memory, Attention, Episodic Memory, Language Comprehension, Inhibitory Control (minor)"
