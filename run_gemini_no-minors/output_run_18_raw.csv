Benchmark,Website,Paper,Description,Cognitive Functions
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro evaluates software engineering agents on realistic, multi-file repository tasks that require producing patches which satisfy hidden tests and project constraints. It emphasizes end-to-end debugging and implementation under industrially representative conditions, including dependency management and adherence to existing code conventions.","Planning, Adaptive Error Correction, Logical Reasoning, Working Memory, Decision-making (minor), Language Comprehension (minor)"
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld measures an agent’s ability to operate a computer-like environment by perceiving GUI state (screenshots/DOM) and executing actions (clicks, typing, shortcuts) to complete user goals. Tasks stress robust tool use over long action sequences with partial observability and frequent need for recovery from mistakes.","Visual Perception, Scene Understanding & Visual Reasoning, Sensorimotor Coordination, Planning, Attention, Working Memory (minor), Adaptive Error Correction (minor)"
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI tests “fluid” abstract reasoning by requiring models to infer latent rules from a few input–output grid examples and generalize to a new grid. It is designed to minimize reliance on memorized knowledge and instead probe rapid induction, compositionality, and robust generalization.","Logical Reasoning, Cognitive Flexibility, Working Memory, Spatial Representation & Mapping, Attention (minor)"
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon autonomous agency by having a model manage a simulated vending-machine business over many decisions (inventory, pricing, supplier negotiation, budgeting) to maximize final profit. Success requires sustained coherence, strategy adaptation to changing conditions, and consistent execution across a lengthy trajectory.","Planning, Decision-making, Reward Mechanisms, Working Memory, Adaptive Error Correction (minor), Motivational Drives (minor)"
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas evaluates real-world tool use via the Model Context Protocol (MCP), where models must discover relevant tools, call them correctly, handle errors, and compose multi-step workflows. It targets practical agent reliability: selecting the right API, sequencing calls, and integrating tool outputs into a final response.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Language Comprehension (minor), Language Production (minor)"
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity agents on tasks such as identifying known vulnerabilities in real codebases from high-level descriptions and discovering previously unknown issues. It stresses reasoning about software behavior, navigating repositories, and producing correct, actionable findings under realistic constraints.","Logical Reasoning, Planning, Adaptive Error Correction, Working Memory, Decision-making (minor), Language Comprehension (minor)"
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a broad, frontier-knowledge benchmark spanning challenging questions across domains, often requiring multi-step reasoning and careful interpretation of problem statements. Its multimodal variant additionally requires extracting and integrating information from images alongside text.","Language Comprehension, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Visual Perception (minor), Scene Understanding & Visual Reasoning (minor)"
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a high-difficulty subset of graduate-level science multiple-choice questions intended to be resistant to shallow pattern matching and simple web lookup. It probes careful reading, domain reasoning, and selection among closely competing answer choices.","Language Comprehension, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory (minor), Decision-making (minor)"
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro evaluates expert-level multimodal understanding across disciplines, requiring models to answer questions that combine text with diagrams, charts, screenshots, and other visuals. It emphasizes grounded reasoning from visual evidence, not just caption-level recognition.","Multisensory Integration, Visual Perception, Scene Understanding & Visual Reasoning, Language Comprehension, Logical Reasoning, Attention (minor)"
OmniDocBench 1.5,https://github.com/opendatalab/OmniDocBench,https://arxiv.org/abs/2412.07626,"OmniDocBench 1.5 evaluates document AI systems on OCR and document understanding, including text recognition, formulas, tables, and reading order. It targets robust perception of complex layouts and faithful reconstruction/structuring of document content.","Visual Perception, Scene Understanding & Visual Reasoning, Spatial Representation & Mapping, Attention, Language Comprehension (minor), Working Memory (minor)"
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU measures multimodal reasoning over video, where the model must integrate information across frames and time to answer questions about events, actions, and causal relationships. It stresses temporal integration and maintaining coherent hypotheses as new visual evidence appears.","Visual Perception, Scene Understanding & Visual Reasoning, Cognitive Timing & Predictive Modeling, Working Memory, Attention, Multisensory Integration (minor)"
LiveCodeBench Pro,https://livecodebenchpro.com/projects/livecodebench-pro/leaderboard,https://arxiv.org/abs/2506.11928,"LiveCodeBench Pro evaluates coding capability on contemporary programming problems, emphasizing correct executable solutions under time-evolving, contamination-aware conditions. It captures the ability to understand task specs, implement algorithms, and debug until passing tests.","Logical Reasoning, Planning, Adaptive Error Correction, Working Memory, Language Comprehension (minor), Language Production (minor)"
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"The FACTS Benchmark Suite systematically evaluates factuality in LLM outputs, including whether models produce accurate statements, avoid unsupported claims, and properly use provided evidence. It targets robustness against hallucination and the ability to regulate generation when uncertain.","Semantic Understanding & Context Recognition, Inhibitory Control, Self-reflection (minor), Working Memory (minor), Logical Reasoning (minor), Episodic Memory (minor)"
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA evaluates commonsense physical reasoning across many languages using non-parallel (not direct translation) question sets, aiming to assess robustness beyond English-centric priors. It probes whether models can infer plausible physical interactions and outcomes from short descriptions.","Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning, Cognitive Flexibility (minor), Social Reasoning & Theory of Mind (minor)"
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) evaluates long-context multi-round coreference and retrieval by placing multiple similar “needle” requests inside a long “haystack” of distractors, then asking the model to reproduce the correct referenced response. It measures whether models can maintain and use precise references over very long sequences without drifting.","Working Memory, Attention, Episodic Memory, Language Comprehension, Semantic Understanding & Context Recognition (minor)"
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval evaluates economically meaningful, well-specified professional knowledge-work tasks (e.g., creating spreadsheets, presentations, and operational artifacts), judged against human professionals. It emphasizes producing usable work products that satisfy requirements, handle constraints, and maintain quality across multiple deliverables.","Planning, Decision-making, Language Production, Working Memory, Semantic Understanding & Context Recognition, Social Reasoning & Theory of Mind (minor), Self-reflection (minor)"
SWE-Lancer,https://openai.com/index/swe-lancer/,https://arxiv.org/abs/2502.12115,"SWE-Lancer evaluates software engineering agents on larger-scope, more realistic development tasks that may require multi-step implementation, iteration, and integration across a codebase rather than isolated edits. It is designed to better reflect professional engineering workflows, including maintaining correctness while making changes under constraints.","Planning, Adaptive Error Correction, Logical Reasoning, Working Memory, Decision-making (minor), Language Comprehension (minor)"
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath evaluates advanced mathematical problem solving intended to be challenging for frontier models, often requiring multi-step derivations and careful symbolic manipulation. It emphasizes depth of reasoning over rote recall, and can stress reliability under high difficulty and long solution chains.","Logical Reasoning, Working Memory, Planning, Cognitive Flexibility (minor), Inhibitory Control (minor)"
