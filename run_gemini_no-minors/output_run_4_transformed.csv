Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates an AI system’s ability to solve real, human-verified software engineering issues by generating patches that make a repository’s tests pass. Tasks require understanding issue context, navigating codebases, editing files, and validating fixes under an automated harness. It emphasizes reliability under single-attempt constraints and realistic repo/tool interactions.","L1: Language Comprehension (minor), Language Production (minor)
L2: Planning, Logical Reasoning, Working Memory, Adaptive Error Correction
L3: ",L2
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 measures autonomous performance on real command-line tasks (e.g., debugging, data wrangling, environment setup) in containerized terminal environments. Agents must decide what commands to run, interpret outputs, iterate on failures, and reach a goal state under resource constraints. It stresses tool-driven action loops, robustness, and iterative troubleshooting.","L1: Language Comprehension (minor)
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory
L3: ",L2
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench evaluates agentic tool use in multi-turn customer-support style environments (e.g., retail, airline, telecom) with simulated users and APIs plus policy constraints. Success requires maintaining dialogue context, choosing correct tool/API calls, following domain rules, and resolving tasks end-to-end. It probes instruction adherence and long-horizon interaction management rather than pure knowledge recall.","L1: Language Comprehension, Language Production
L2: Decision-making, Planning, Working Memory, Reward Mechanisms (minor)
L3: Social Reasoning & Theory of Mind, Inhibitory Control",L3
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI tests “fluid intelligence” by asking models to infer abstract transformation rules from a few input–output grid examples and apply them to a novel grid. The tasks are designed to be unfamiliar and to require generalization beyond memorized patterns. It emphasizes compositional pattern discovery, abstraction, and flexible reasoning under minimal supervision.","L1: 
L2: Logical Reasoning, Working Memory, Spatial Representation & Mapping, Scene Understanding & Visual Reasoning, Attention (minor)
L3: Cognitive Flexibility",L3
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon agent coherence and strategy in a simulated vending-machine business over an extended period, starting from limited capital. Agents must manage inventory, pricing, supplier negotiations (often via tool-mediated communication), and adapt to changing conditions to maximize final balance. It highlights sustained planning and the ability to operate consistently across many sequential decisions.","L1: 
L2: Planning, Decision-making, Working Memory, Reward Mechanisms, Adaptive Error Correction (minor)
L3: Cognitive Timing & Predictive Modeling, Social Reasoning & Theory of Mind (minor)",L3
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a broad, frontier-difficulty benchmark spanning many academic and real-world domains, often including multimodal questions. It targets deep reasoning, synthesis across contexts, and the ability to handle expert-level problems rather than shallow pattern matching. Variants may evaluate tool-augmented performance (e.g., search or code) versus no-tool reasoning.","L1: Language Comprehension, Visual Perception (minor)
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Multisensory Integration (minor), Planning (minor)
L3: ",L2
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 consists of competition-style mathematics problems requiring multi-step symbolic manipulation and careful constraint handling. It is typically scored by exact final answers, with tool-enabled variants allowing computation assistance. The benchmark emphasizes structured mathematical reasoning and error avoidance over natural language fluency.","L1: 
L2: Logical Reasoning, Working Memory, Adaptive Error Correction (minor), Planning (minor)
L3: ",L2
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,GPQA Diamond is a high-quality subset of extremely difficult graduate-level science multiple-choice questions designed to be resistant to shallow web-search strategies. Questions often require integrating domain knowledge with multi-step reasoning and careful elimination of distractors. It primarily measures scientific reasoning and knowledge application under tight answer formats.,"L1: Language Comprehension (minor)
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory (minor)
L3: Inhibitory Control (minor)",L2
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends broad academic knowledge evaluation across many subjects and multiple languages, stressing multilingual comprehension and reasoning. Models must answer domain questions while handling linguistic variation, translation ambiguity, and culturally grounded phrasing. It measures whether knowledge and reasoning transfer robustly across languages rather than only English-centric competence.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning (minor), Working Memory (minor)
L3: ",L2
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro evaluates multimodal understanding and expert-level reasoning over images paired with text questions across many disciplines. Tasks include interpreting diagrams, tables, plots, and domain-specific visuals while producing a grounded answer. It emphasizes visual reasoning and cross-modal integration more than pure linguistic recall.","L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Attention (minor)
L3: ",L2
MathArena Apex,https://matharena.ai/?view=problem&comp=apex--apex_2025,https://arxiv.org/abs/2505.23281,"MathArena Apex is a challenging mathematics benchmark intended to probe advanced reasoning under strong evaluation protocols, often aggregating across difficult problem sets and robust scoring. It focuses on solving complex, multi-step math problems reliably rather than producing convincing explanations. Compared to standard contest sets, it targets more frontier-level difficulty and consistency.","L1: 
L2: Logical Reasoning, Working Memory, Adaptive Error Correction (minor), Planning (minor)
L3: ",L2
ScreenShot-Pro,https://gui-agent.github.io/grounding-leaderboard/,https://arxiv.org/abs/2504.07981,"ScreenShot-Pro evaluates GUI understanding from high-resolution screenshots, typically requiring identification of relevant UI elements, states, and spatial relationships to answer questions or select actions. It stresses perceptual grounding in real interface layouts (menus, dialogs, dashboards) rather than generic vision classification. The benchmark is commonly used to assess computer-use readiness and UI-centric visual reasoning.","L1: Visual Perception, Language Comprehension (minor)
L2: Scene Understanding & Visual Reasoning, Spatial Representation & Mapping, Visual Attention & Eye Movements (minor), Sensorimotor Coordination (minor)
L3: ",L2
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning evaluates whether models can answer questions about scientific figures and visual evidence drawn from research-paper contexts, often requiring quantitative or structural interpretation. It tests grounded reasoning over charts, plots, and complex figure panels rather than only textual summarization. Tool-enabled variants (e.g., Python) can assess the ability to combine computation with visual interpretation.","L1: Visual Perception, Language Comprehension (minor)
L2: Scene Understanding & Visual Reasoning, Logical Reasoning, Multisensory Integration (minor), Working Memory (minor)
L3: ",L2
OmniDocBench 1.5,https://github.com/opendatalab/OmniDocBench,https://arxiv.org/abs/2412.07626,"OmniDocBench 1.5 evaluates document understanding and OCR-related capabilities across heterogeneous layouts such as text blocks, formulas, tables, and reading order. It measures how well systems extract, reconstruct, and structure content from visually rich documents. The benchmark emphasizes layout-aware perception and faithful transcription/structuring over open-ended generation.","L1: Visual Perception, Language Comprehension (minor)
L2: Scene Understanding & Visual Reasoning, Attention, Working Memory (minor), Spatial Representation & Mapping (minor)
L3: ",L2
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU evaluates multimodal understanding and reasoning over videos, requiring models to integrate information across frames and time to answer questions. Tasks often involve tracking events, interpreting visual cues, and using context from dialogue or captions when present. It stresses temporal coherence and long-range integration beyond single-image perception.","L1: Visual Perception, Language Comprehension (minor)
L2: Scene Understanding & Visual Reasoning, Working Memory, Multisensory Integration (minor)
L3: Cognitive Timing & Predictive Modeling",L3
LiveCodeBench Pro,https://livecodebenchpro.com/projects/livecodebench-pro/leaderboard,https://arxiv.org/abs/2506.11928,"LiveCodeBench Pro evaluates coding ability on fresh, competition-style or practical programming problems under controlled execution-based grading. Models must write correct programs (often with hidden tests), handle edge cases, and sometimes interact with an iterative coding setup. It focuses on algorithmic reasoning and reliable code synthesis rather than repository patching.","L1: Language Production (minor)
L2: Logical Reasoning, Planning, Working Memory, Adaptive Error Correction (minor)
L3: ",L2
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"FACTS Benchmark Suite systematically evaluates factuality and grounding by probing whether model outputs remain consistent with evidence and avoid unsupported claims across varied factuality tasks. It is designed to separate fluent generation from truthfulness, often stressing calibration and error detection. The suite is used to quantify hallucination-related failure modes and robustness of factual responses.","L1: 
L2: Semantic Understanding & Context Recognition, Logical Reasoning (minor), Working Memory (minor)
L3: Inhibitory Control, Self-reflection (minor)",L3
SimpleQA Verified,https://www.kaggle.com/benchmarks/deepmind/simpleqa-verified,https://arxiv.org/abs/2509.07968,"SimpleQA Verified is a short-form factual question answering benchmark with verified ground truth, intended to measure whether models can provide correct, concise answers without hallucinating. The questions are generally direct, but the evaluation penalizes incorrect or fabricated statements. It primarily probes factual recall/recognition and disciplined responding under uncertainty.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition
L3: Inhibitory Control, Self-reflection (minor)",L3
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA evaluates pragmatic commonsense reasoning across languages and cultures, typically focusing on selecting plausible actions or explanations in everyday situations. It aims to test whether models’ “physical and practical” intuitions transfer beyond English and beyond narrow Western-centric phrasing. The benchmark probes robustness to linguistic and contextual variation in commonsense judgments.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning
L3: Social Reasoning & Theory of Mind (minor), Cognitive Flexibility (minor)",L2
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,MRCR v2 (8-needle) evaluates long-context retrieval and multi-round co-reference resolution by embedding multiple similar “needle” interactions inside a long “haystack” and asking the model to reproduce the correct response for a specific needle. The 8-needle setting increases confusability and tests whether the model can maintain accurate associations across long documents. It emphasizes attention control and context-dependent recall under high interference.,"L1: Language Comprehension
L2: Working Memory, Attention, Episodic Memory
L3: Inhibitory Control (minor)",L2
