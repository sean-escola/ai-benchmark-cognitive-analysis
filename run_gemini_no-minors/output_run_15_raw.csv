Benchmark,Website,Paper,Description,Cognitive Functions
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro is a large-scale software engineering benchmark where a model must modify real-world repositories to satisfy a natural-language issue description and pass project-specific tests. It expands beyond Python-only settings and emphasizes more realistic engineering constraints, aiming to be harder and more contamination-resistant than earlier SWE-bench variants.","Language Comprehension, Language Production, Planning, Logical Reasoning, Working Memory, Adaptive Error Correction, Inhibitory Control (minor)"
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld evaluates autonomous computer-use agents in a desktop operating system environment, requiring them to complete tasks by perceiving screens and taking multi-step UI actions. Success depends on mapping high-level goals to low-level interactions (clicks/typing/navigation) while handling non-determinism and interface variability.","Visual Perception, Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Planning, Decision-making, Sensorimotor Coordination, Working Memory, Adaptive Error Correction"
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI measures fluid, abstract reasoning by asking models to infer hidden transformation rules from a few input–output grid examples and apply them to new grids. It is designed to reduce reliance on memorized knowledge and instead probe generalization to novel pattern-construction problems.","Logical Reasoning, Working Memory, Cognitive Flexibility, Spatial Representation & Mapping, Attention (minor)"
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon agent coherence and strategy by simulating the operation of a vending-machine business over an extended period. The agent must make repeated decisions (e.g., inventory, pricing, supplier interactions) to maximize the final balance under changing market conditions.","Planning, Decision-making, Reward Mechanisms, Working Memory, Semantic Understanding & Context Recognition, Social Reasoning & Theory of Mind (minor)"
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas evaluates practical tool use via the Model Context Protocol by requiring models to discover, call, and chain real tools to complete multi-step tasks. It stresses correct API selection and parameterization, recovery from tool errors, and synthesis of tool outputs into a final answer.","Planning, Decision-making, Language Comprehension, Semantic Understanding & Context Recognition, Working Memory, Adaptive Error Correction"
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity capabilities using tasks drawn from real-world vulnerability contexts, including identifying or reproducing known weaknesses and discovering new ones. Agents must reason over codebases and security signals to propose exploits or fixes, often under realistic constraints and ambiguity.","Logical Reasoning, Planning, Working Memory, Adaptive Error Correction, Language Comprehension, Language Production (minor), Inhibitory Control (minor)"
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a frontier, multi-domain benchmark of challenging questions intended to probe advanced academic reasoning and knowledge across modalities. Many items require synthesizing information, performing multi-step derivations, and interpreting diagrams or other visual inputs when present.","Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Visual Perception (minor), Scene Understanding & Visual Reasoning (minor)"
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a curated subset of extremely difficult, graduate-level multiple-choice science questions designed to be resistant to superficial pattern matching. It tests whether a model can perform deep scientific reasoning and select the correct option among strong distractors.","Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory (minor)"
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro is an expert-level multimodal benchmark covering many disciplines where questions require jointly reasoning over text and images (e.g., charts, figures, diagrams). It emphasizes integrated perception and reasoning rather than either pure language knowledge or pure visual recognition alone.","Multisensory Integration, Visual Perception, Scene Understanding & Visual Reasoning, Language Comprehension, Logical Reasoning, Working Memory (minor)"
OmniDocBench 1.5,https://github.com/opendatalab/OmniDocBench,https://arxiv.org/abs/2412.07626,"OmniDocBench 1.5 evaluates document understanding and OCR quality across complex layouts, including text, tables, formulas, and reading order. It focuses on robust extraction and structural interpretation of visually rich documents rather than only plain-text transcription.","Visual Perception, Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Spatial Representation & Mapping, Language Comprehension, Working Memory (minor)"
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU extends multimodal understanding to the temporal domain by asking questions that require reasoning over video content. It probes whether a model can track events over time, integrate cues across frames, and answer questions grounded in the evolving scene.","Visual Perception, Attention, Working Memory, Cognitive Timing & Predictive Modeling, Scene Understanding & Visual Reasoning, Multisensory Integration (minor), Language Comprehension (minor)"
LiveCodeBench Pro,https://livecodebenchpro.com/projects/livecodebench-pro/leaderboard,https://arxiv.org/abs/2506.11928,"LiveCodeBench Pro evaluates coding performance on recent, competition-style programming tasks, typically scored by executing submitted solutions against hidden tests. It is intended to reflect practical problem solving under time-evolving distributions and reduce leakage from older training corpora.","Logical Reasoning, Planning, Working Memory, Language Production, Adaptive Error Correction, Language Comprehension (minor)"
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"The FACTS Benchmark Suite systematically measures factuality and grounding-related behaviors, including whether models introduce unsupported claims and how they handle uncertain or missing information. It targets reliability in knowledge-intensive settings where hallucinations are costly.","Semantic Understanding & Context Recognition, Language Comprehension, Language Production, Inhibitory Control, Self-reflection (minor), Working Memory (minor)"
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA is a multilingual commonsense benchmark centered on physical interaction and everyday reasoning across languages, emphasizing non-parallel coverage rather than direct translation of the same items. It tests whether models maintain robust commonsense judgments under diverse linguistic and cultural contexts.","Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning, Social Reasoning & Theory of Mind (minor)"
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,MRCR v2 (8-needle) is a long-context evaluation that embeds multiple similar “needle” references within a long “haystack” of text and asks the model to retrieve or reproduce the correct referenced content. The 8-needle variant stresses robustness to interference and requires accurate multi-round coreference over long inputs.,"Working Memory, Attention, Language Comprehension, Episodic Memory (minor), Semantic Understanding & Context Recognition (minor)"
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval evaluates economically relevant, well-specified professional knowledge-work tasks spanning many occupations, judged by expert human raters on the quality of produced work artifacts. It is meant to capture end-to-end performance on realistic deliverables rather than single-answer questions.","Planning, Decision-making, Language Production, Semantic Understanding & Context Recognition, Social Reasoning & Theory of Mind (minor), Adaptive Error Correction (minor), Self-reflection (minor)"
SWE-Lancer,https://openai.com/index/swe-lancer/,https://arxiv.org/abs/2502.12115,"SWE-Lancer evaluates autonomous software engineering on real repositories with tasks that resemble professional development work, typically requiring repository understanding, patch creation, and verification against tests or graders. It emphasizes end-to-end completion quality and robustness across diverse codebases.","Language Comprehension, Language Production, Planning, Logical Reasoning, Working Memory, Adaptive Error Correction"
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath is a benchmark of difficult mathematics problems aimed at testing advanced quantitative reasoning beyond routine contest math. Problems often require multi-step derivations, careful abstraction, and error-sensitive symbolic manipulation.","Logical Reasoning, Working Memory, Planning, Cognitive Flexibility (minor), Adaptive Error Correction (minor)"
