Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates software engineering agents by asking them to produce code patches that fix real issues in Python repositories, then running the project’s tests to verify correctness. The Verified subset uses tasks curated to be solvable and to reduce ambiguous problem statements and flaky evaluation, emphasizing end-to-end repo understanding, implementation, and debugging.","L1: Language Comprehension (minor)
L2: Planning, Logical Reasoning, Working Memory, Adaptive Error Correction, Decision-making (minor)
L3: ",L2
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 measures an agent’s ability to accomplish practical command-line tasks in sandboxed environments (e.g., installing tools, manipulating files, running programs, and debugging failures). Success typically requires iteratively issuing shell commands, interpreting tool outputs, and recovering from errors under resource and time constraints.","L1: Language Comprehension (minor)
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory, Attention (minor)
L3: ",L2
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench evaluates tool-using customer-support agents in multi-turn scenarios (e.g., retail, airline, telecom) where the model must follow domain policies while interacting with simulated users and backend APIs. It stresses reliable tool invocation, policy compliance across long dialogues, and robust handling of edge cases and user pressure.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making
L3: Inhibitory Control, Social Reasoning & Theory of Mind (minor)",L3
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI measures abstract, few-shot “fluid intelligence” by requiring models to infer transformation rules from a small number of input–output grid examples and apply them to a new grid. The tasks are designed to penalize rote memorization and reward flexible rule induction and compositional generalization.","L1: 
L2: Logical Reasoning, Working Memory, Spatial Representation & Mapping, Scene Understanding & Visual Reasoning (minor)
L3: Cognitive Flexibility",L3
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon agent coherence and strategy in a simulated year-long vending-machine business, where the agent must manage inventory, pricing, supplier negotiation, and budgeting. The score is based on business outcomes (e.g., final balance), rewarding sustained planning and adaptation across many steps rather than single-turn correctness.","L1: 
L2: Planning, Decision-making, Reward Mechanisms, Working Memory, Adaptive Error Correction (minor), Semantic Understanding & Context Recognition (minor)
L3: ",L2
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam (HLE) is a frontier, multidisciplinary benchmark with difficult questions intended to probe deep reasoning and broad knowledge, often across multimodal inputs. It is commonly used to compare models both with and without tools (e.g., search, code execution), emphasizing synthesis and error-avoidant problem solving on hard items.","L1: Language Comprehension, Visual Perception (minor)
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory (minor), Multisensory Integration (minor)
L3: ",L2
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 evaluates competition-style mathematics problem solving using short-answer questions that typically require multi-step derivations rather than pattern matching. It stresses symbolic manipulation, precise quantitative reasoning, and maintaining intermediate results across a solution chain.","L1: 
L2: Logical Reasoning, Working Memory, Planning (minor)
L3: Cognitive Flexibility (minor)",L2
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,GPQA Diamond is a high-difficulty subset of graduate-level multiple-choice science questions designed to be resistant to shallow retrieval and to require careful reasoning. It probes whether models can integrate domain knowledge with multi-step inference under adversarially hard distractors.,"L1: Language Comprehension (minor)
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory (minor)
L3: ",L2
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends MMLU to multiple languages, testing broad academic knowledge and reasoning across many subjects and non-English settings. It emphasizes cross-lingual understanding and robustness to linguistic variation while keeping the underlying task formats similar to standard knowledge exams.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning (minor), Working Memory (minor)
L3: ",L2
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro is a multimodal expert-level benchmark that evaluates understanding and reasoning over images paired with text across many disciplines, with a focus on harder professional-grade items. It targets integrated visual–text reasoning (e.g., interpreting diagrams, tables, charts, and technical visuals) rather than pure OCR.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Working Memory (minor)
L3: ",L2
MathArena Apex,https://matharena.ai/?view=problem&comp=apex--apex_2025,https://arxiv.org/abs/2505.23281,"MathArena Apex aggregates challenging math problems to compare advanced mathematical reasoning, often emphasizing multi-step solutions and higher difficulty than standard contest sets. It is used to evaluate both correctness and robustness of solution strategies under complex algebraic, combinatorial, and geometric reasoning demands.","L1: 
L2: Logical Reasoning, Working Memory, Planning (minor)
L3: Cognitive Flexibility (minor)",L2
ScreenShot-Pro,https://gui-agent.github.io/grounding-leaderboard/,https://arxiv.org/abs/2504.07981,"ScreenShot-Pro evaluates models on understanding high-resolution screenshots of software interfaces and answering questions or producing grounded actions based on GUI elements. The benchmark stresses visual grounding (identifying the right UI targets), layout/spatial reasoning, and careful attention to small on-screen details typical of professional tools.","L1: Visual Perception
L2: Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Spatial Representation & Mapping, Sensorimotor Coordination (minor)
L3: ",L2
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning evaluates reasoning over scientific figures and visual artifacts from research papers, requiring models to interpret plots, diagrams, and annotations to answer questions. It targets figure-grounded inference (not just recognition), including extracting relationships, comparing trends, and integrating figure context with scientific text.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory (minor)
L3: ",L2
OmniDocBench 1.5,https://github.com/opendatalab/OmniDocBench,https://arxiv.org/abs/2412.07626,"OmniDocBench 1.5 evaluates document understanding and OCR-centric capabilities across diverse layouts such as mixed text, formulas, tables, and reading order. It stresses faithfully transcribing and structuring content from complex documents where layout and formatting carry meaning.","L1: Visual Perception, Language Comprehension (minor)
L2: Scene Understanding & Visual Reasoning, Working Memory (minor)
L3: ",L2
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU is a multimodal benchmark for reasoning over video, requiring models to integrate information across frames and time to answer questions. It probes temporal understanding (events, causality, state changes) and the ability to retain and use visual evidence observed earlier in the clip.","L1: Visual Perception
L2: Episodic Memory, Scene Understanding & Visual Reasoning, Multisensory Integration (minor), Working Memory (minor)
L3: Cognitive Timing & Predictive Modeling (minor)",L2
LiveCodeBench Pro,https://livecodebenchpro.com/projects/livecodebench-pro/leaderboard,https://arxiv.org/abs/2506.11928,"LiveCodeBench Pro evaluates coding ability on competitively sourced, contamination-aware programming tasks, typically emphasizing algorithmic problem solving and correct executable solutions. It targets planning a solution approach, implementing reliably, and handling edge cases under realistic coding constraints.","L1: 
L2: Logical Reasoning, Planning, Working Memory, Adaptive Error Correction (minor)
L3: ",L2
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"The FACTS Benchmark Suite systematically measures factuality and grounding, including whether a model’s outputs remain supported by provided sources and avoid introducing unsupported claims. It emphasizes reliability under information constraints and sensitivity to evidence, rather than creativity or open-ended generation.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Working Memory (minor)
L3: Inhibitory Control, Self-reflection (minor)",L3
SimpleQA Verified,https://www.kaggle.com/benchmarks/deepmind/simpleqa-verified,https://arxiv.org/abs/2509.07968,"SimpleQA Verified evaluates short-form factual question answering with verified ground truth, focusing on precision for straightforward queries rather than complex multi-hop reasoning. It is commonly used to measure hallucination propensity and basic factual reliability in a controlled setting.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Working Memory (minor)
L3: Inhibitory Control (minor)",L2
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA tests physical commonsense reasoning across many languages using non-parallel (not direct translations) question sets, aiming to measure whether models generalize “everyday physics” knowledge cross-lingually. The tasks emphasize selecting the more plausible action/outcome given constraints of the physical world.","L1: Language Comprehension (minor)
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Spatial Representation & Mapping (minor)
L3: Cognitive Timing & Predictive Modeling (minor)",L2
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,MRCR v2 (8-needle) evaluates long-context multi-round coreference resolution by embedding multiple similar “needle” interactions inside long “haystacks” and asking the model to reproduce the correct referenced response. It stresses maintaining and retrieving the right discourse state under heavy distractors across very long contexts.,"L1: Language Comprehension
L2: Working Memory, Attention, Episodic Memory (minor)
L3: Inhibitory Control (minor)",L2
