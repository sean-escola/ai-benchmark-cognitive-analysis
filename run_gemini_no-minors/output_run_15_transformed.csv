Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro is a large-scale software engineering benchmark where a model must modify real-world repositories to satisfy a natural-language issue description and pass project-specific tests. It expands beyond Python-only settings and emphasizes more realistic engineering constraints, aiming to be harder and more contamination-resistant than earlier SWE-bench variants.","L1: Language Comprehension, Language Production
L2: Planning, Logical Reasoning, Working Memory, Adaptive Error Correction
L3: Inhibitory Control (minor)",L2
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld evaluates autonomous computer-use agents in a desktop operating system environment, requiring them to complete tasks by perceiving screens and taking multi-step UI actions. Success depends on mapping high-level goals to low-level interactions (clicks/typing/navigation) while handling non-determinism and interface variability.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Planning, Decision-making, Sensorimotor Coordination, Working Memory, Adaptive Error Correction
L3: ",L2
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI measures fluid, abstract reasoning by asking models to infer hidden transformation rules from a few input–output grid examples and apply them to new grids. It is designed to reduce reliance on memorized knowledge and instead probe generalization to novel pattern-construction problems.","L1: 
L2: Logical Reasoning, Working Memory, Spatial Representation & Mapping, Attention (minor)
L3: Cognitive Flexibility",L3
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon agent coherence and strategy by simulating the operation of a vending-machine business over an extended period. The agent must make repeated decisions (e.g., inventory, pricing, supplier interactions) to maximize the final balance under changing market conditions.","L1: 
L2: Planning, Decision-making, Reward Mechanisms, Working Memory, Semantic Understanding & Context Recognition
L3: Social Reasoning & Theory of Mind (minor)",L2
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas evaluates practical tool use via the Model Context Protocol by requiring models to discover, call, and chain real tools to complete multi-step tasks. It stresses correct API selection and parameterization, recovery from tool errors, and synthesis of tool outputs into a final answer.","L1: Language Comprehension
L2: Planning, Decision-making, Semantic Understanding & Context Recognition, Working Memory, Adaptive Error Correction
L3: ",L2
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity capabilities using tasks drawn from real-world vulnerability contexts, including identifying or reproducing known weaknesses and discovering new ones. Agents must reason over codebases and security signals to propose exploits or fixes, often under realistic constraints and ambiguity.","L1: Language Comprehension, Language Production (minor)
L2: Logical Reasoning, Planning, Working Memory, Adaptive Error Correction
L3: Inhibitory Control (minor)",L2
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a frontier, multi-domain benchmark of challenging questions intended to probe advanced academic reasoning and knowledge across modalities. Many items require synthesizing information, performing multi-step derivations, and interpreting diagrams or other visual inputs when present.","L1: Language Comprehension, Visual Perception (minor)
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Scene Understanding & Visual Reasoning (minor)
L3: ",L2
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a curated subset of extremely difficult, graduate-level multiple-choice science questions designed to be resistant to superficial pattern matching. It tests whether a model can perform deep scientific reasoning and select the correct option among strong distractors.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory (minor)
L3: ",L2
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro is an expert-level multimodal benchmark covering many disciplines where questions require jointly reasoning over text and images (e.g., charts, figures, diagrams). It emphasizes integrated perception and reasoning rather than either pure language knowledge or pure visual recognition alone.","L1: Visual Perception, Language Comprehension
L2: Multisensory Integration, Scene Understanding & Visual Reasoning, Logical Reasoning, Working Memory (minor)
L3: ",L2
OmniDocBench 1.5,https://github.com/opendatalab/OmniDocBench,https://arxiv.org/abs/2412.07626,"OmniDocBench 1.5 evaluates document understanding and OCR quality across complex layouts, including text, tables, formulas, and reading order. It focuses on robust extraction and structural interpretation of visually rich documents rather than only plain-text transcription.","L1: Visual Perception, Language Comprehension
L2: Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Spatial Representation & Mapping, Working Memory (minor)
L3: ",L2
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU extends multimodal understanding to the temporal domain by asking questions that require reasoning over video content. It probes whether a model can track events over time, integrate cues across frames, and answer questions grounded in the evolving scene.","L1: Visual Perception, Language Comprehension (minor)
L2: Attention, Working Memory, Scene Understanding & Visual Reasoning, Multisensory Integration (minor)
L3: Cognitive Timing & Predictive Modeling",L3
LiveCodeBench Pro,https://livecodebenchpro.com/projects/livecodebench-pro/leaderboard,https://arxiv.org/abs/2506.11928,"LiveCodeBench Pro evaluates coding performance on recent, competition-style programming tasks, typically scored by executing submitted solutions against hidden tests. It is intended to reflect practical problem solving under time-evolving distributions and reduce leakage from older training corpora.","L1: Language Production, Language Comprehension (minor)
L2: Logical Reasoning, Planning, Working Memory, Adaptive Error Correction
L3: ",L2
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"The FACTS Benchmark Suite systematically measures factuality and grounding-related behaviors, including whether models introduce unsupported claims and how they handle uncertain or missing information. It targets reliability in knowledge-intensive settings where hallucinations are costly.","L1: Language Comprehension, Language Production
L2: Semantic Understanding & Context Recognition, Working Memory (minor)
L3: Inhibitory Control, Self-reflection (minor)",L3
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA is a multilingual commonsense benchmark centered on physical interaction and everyday reasoning across languages, emphasizing non-parallel coverage rather than direct translation of the same items. It tests whether models maintain robust commonsense judgments under diverse linguistic and cultural contexts.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning
L3: Social Reasoning & Theory of Mind (minor)",L2
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,MRCR v2 (8-needle) is a long-context evaluation that embeds multiple similar “needle” references within a long “haystack” of text and asks the model to retrieve or reproduce the correct referenced content. The 8-needle variant stresses robustness to interference and requires accurate multi-round coreference over long inputs.,"L1: Language Comprehension
L2: Working Memory, Attention, Episodic Memory (minor), Semantic Understanding & Context Recognition (minor)
L3: ",L2
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval evaluates economically relevant, well-specified professional knowledge-work tasks spanning many occupations, judged by expert human raters on the quality of produced work artifacts. It is meant to capture end-to-end performance on realistic deliverables rather than single-answer questions.","L1: Language Production
L2: Planning, Decision-making, Semantic Understanding & Context Recognition, Adaptive Error Correction (minor)
L3: Social Reasoning & Theory of Mind (minor), Self-reflection (minor)",L2
SWE-Lancer,https://openai.com/index/swe-lancer/,https://arxiv.org/abs/2502.12115,"SWE-Lancer evaluates autonomous software engineering on real repositories with tasks that resemble professional development work, typically requiring repository understanding, patch creation, and verification against tests or graders. It emphasizes end-to-end completion quality and robustness across diverse codebases.","L1: Language Comprehension, Language Production
L2: Planning, Logical Reasoning, Working Memory, Adaptive Error Correction
L3: ",L2
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath is a benchmark of difficult mathematics problems aimed at testing advanced quantitative reasoning beyond routine contest math. Problems often require multi-step derivations, careful abstraction, and error-sensitive symbolic manipulation.","L1: 
L2: Logical Reasoning, Working Memory, Planning, Adaptive Error Correction (minor)
L3: Cognitive Flexibility (minor)",L2
