Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates software-engineering agents on real GitHub issues by requiring them to produce a patch that makes a project’s tests pass. The “Verified” subset uses human validation to ensure the tasks are solvable and that evaluation is reliable, emphasizing end-to-end code understanding, modification, and debugging under realistic repo constraints.","L1: Language Comprehension (minor)
L2: Planning, Logical Reasoning, Working Memory, Adaptive Error Correction
L3: ",L2
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 measures agent performance on practical command-line tasks (e.g., environment setup, file manipulation, debugging, and scripting) in a sandboxed terminal. Success typically requires choosing and sequencing shell commands, interpreting noisy outputs, and iterating to fix errors under resource and time constraints.","L1: 
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory
L3: Inhibitory Control (minor)",L2
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench (Tau2-bench) evaluates tool-using customer-support agents in simulated domains (e.g., retail/airline/telecom) that involve multi-turn dialogue plus API interactions governed by policies. Agents must follow domain rules, resolve user requests, and maintain consistency across long interactions where policy conflicts and adversarial or ambiguous user behavior can occur.","L1: Language Comprehension, Language Production
L2: Decision-making, Planning (minor)
L3: Social Reasoning & Theory of Mind, Inhibitory Control",L3
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI tests abstract, novelty-driven reasoning using small sets of input–output grid examples that follow an unknown rule; models must infer and apply the rule to a new grid. The benchmark is designed to emphasize sample-efficient generalization to unfamiliar patterns rather than memorized knowledge.","L1: 
L2: Logical Reasoning, Working Memory, Spatial Representation & Mapping, Attention (minor)
L3: Cognitive Flexibility",L3
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon autonomous agent behavior in a simulated vending-machine business over many time steps, scoring by final financial performance. Agents must manage inventory, pricing, procurement/negotiation, and adapt to changing demand, requiring coherent strategy across extended interaction histories.","L1: 
L2: Planning, Decision-making, Reward Mechanisms, Working Memory, Semantic Understanding & Context Recognition (minor)
L3: ",L2
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a large, frontier-knowledge benchmark spanning difficult questions across many academic domains and often includes multimodal inputs. It targets robust reasoning and synthesis under uncertainty, and is frequently reported both with and without external tools such as search or code execution.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Scene Understanding & Visual Reasoning (minor)
L3: ",L2
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,AIME 2025 consists of competition-style math problems that require multi-step symbolic reasoning and precise final answers. It is commonly used to evaluate mathematical problem solving without relying on extensive external knowledge or tools.,"L1: 
L2: Logical Reasoning, Working Memory, Planning (minor), Adaptive Error Correction (minor)
L3: ",L2
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a high-quality subset of graduate-level, “Google-proof” multiple-choice science questions designed to resist shallow retrieval. It emphasizes deep conceptual understanding and careful reasoning in biology, chemistry, and physics, where distractors target common misconceptions.","L1: Language Comprehension (minor)
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory
L3: ",L2
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends MMLU-style subject tests to multiple languages, measuring broad academic knowledge and reasoning under multilingual prompts. It is typically evaluated as multiple-choice across many subjects and languages, probing generalization of knowledge and reasoning beyond English.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning (minor), Working Memory (minor)
L3: ",L2
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro is a challenging multimodal benchmark covering expert-level questions that require reasoning over images (e.g., charts, diagrams, scenes) paired with text. Compared to earlier MMMU settings, it emphasizes harder visual reasoning and robust multimodal grounding, often in multiple evaluation modes.","L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning (minor)
L3: ",L2
MathArena Apex,https://matharena.ai/?view=problem&comp=apex--apex_2025,https://arxiv.org/abs/2505.23281,MathArena Apex is an advanced mathematics evaluation that aggregates difficult problems (often beyond standard contest level) and supports comparative reporting across models and settings. It is intended to stress-test long-horizon mathematical reasoning and solution robustness rather than short factual recall.,"L1: 
L2: Logical Reasoning, Working Memory, Planning (minor)
L3: ",L2
ScreenShot-Pro,https://gui-agent.github.io/grounding-leaderboard/,https://arxiv.org/abs/2504.07981,"ScreenShot-Pro evaluates GUI understanding from screenshots, where models must ground language instructions or questions in interface elements and spatial layout. Tasks often require identifying relevant UI components, interpreting text and icons, and selecting actions or answers consistent with the screen state.","L1: Visual Perception
L2: Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Decision-making (minor), Sensorimotor Coordination (minor)
L3: ",L2
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning tests reasoning over scientific figures and related content drawn from arXiv-style papers, focusing on extracting and integrating information from charts/plots and technical diagrams. The benchmark emphasizes figure-grounded inference (often with quantitative or structural reasoning) rather than purely textual summarization.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Logical Reasoning, Semantic Understanding & Context Recognition, Multisensory Integration (minor)
L3: ",L2
OmniDocBench 1.5,https://github.com/opendatalab/OmniDocBench,https://arxiv.org/abs/2412.07626,"OmniDocBench 1.5 evaluates document intelligence (DocAI) across varied document types, measuring accurate extraction and reconstruction of content such as text blocks, tables, formulas, and reading order. It stresses robust layout understanding and fidelity of conversion from complex page imagery into structured outputs.","L1: Visual Perception, Language Comprehension
L2: Spatial Representation & Mapping, Attention, Semantic Understanding & Context Recognition (minor)
L3: ",L2
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU extends multimodal understanding to the temporal domain by asking questions about videos that require integrating information across multiple frames (and sometimes associated text). It targets event understanding, temporal consistency, and multi-step reasoning grounded in dynamic visual context.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Working Memory, Multisensory Integration (minor)
L3: Cognitive Timing & Predictive Modeling (minor)",L2
LiveCodeBench Pro,https://livecodebenchpro.com/projects/livecodebench-pro/leaderboard,https://arxiv.org/abs/2506.11928,"LiveCodeBench Pro evaluates coding ability on a curated, time-aware set of programming tasks, often scored in an ELO-style framework and designed to reduce contamination. It emphasizes producing correct, executable solutions under realistic constraints and can reflect interactive debugging and iterative refinement.","L1: Language Comprehension (minor)
L2: Logical Reasoning, Planning, Adaptive Error Correction, Working Memory
L3: ",L2
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"The FACTS Benchmark Suite systematically evaluates factuality-related behaviors, including whether models produce accurate statements, avoid unsupported claims, and appropriately handle uncertainty across different prompting conditions. It is designed to probe both correctness and calibration-like behavior in knowledge-intensive generation settings.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Working Memory (minor)
L3: Inhibitory Control, Self-reflection (minor)",L3
SimpleQA Verified,https://www.kaggle.com/benchmarks/deepmind/simpleqa-verified,https://arxiv.org/abs/2509.07968,"SimpleQA Verified is a factual question-answering benchmark focused on short, unambiguous queries with verified ground truth, enabling cleaner measurement of correctness. It is often used to quantify hallucination propensity and basic knowledge precision under straightforward prompting.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition
L3: Inhibitory Control (minor)",L2
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA is a multilingual, non-parallel extension of physical commonsense reasoning tasks, aiming to test whether models can choose plausible actions or explanations grounded in everyday physics across languages and locales. It stresses robust commonsense generalization rather than translation of a single shared item set.","L1: Language Comprehension (minor)
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Spatial Representation & Mapping (minor)
L3: Cognitive Timing & Predictive Modeling (minor)",L2
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) is a long-context evaluation that inserts multiple similar “needle” items into lengthy “haystack” conversations/documents and asks the model to retrieve or reproduce the correct associated content. It measures robustness of attention, retrieval, and interference handling as context length grows and distractors accumulate.","L1: Language Comprehension
L2: Working Memory, Attention, Episodic Memory (minor)
L3: Inhibitory Control (minor)",L2
