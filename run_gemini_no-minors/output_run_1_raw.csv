Benchmark,Website,Paper,Description,Cognitive Functions
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates software engineering by asking a model to generate a patch that fixes a real issue in an open-source repository, then running project tests to verify correctness. The Verified subset focuses on tasks that have been human-validated as solvable and aims to reduce noise from ambiguous or untestable issues.","Planning, Logical Reasoning, Adaptive Error Correction, Working Memory, Language Comprehension (minor), Language Production (minor)"
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 evaluates agentic ability to complete real command-line tasks in sandboxed environments (e.g., debugging, building, manipulating files, running tools) using a terminal interface. Success depends on executing correct sequences of commands, interpreting outputs, and recovering from errors under realistic constraints.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Language Comprehension (minor)"
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench measures how well an agent handles multi-turn customer-support style tasks while using tools/APIs and complying with domain policies (e.g., retail, airline, telecom). It stresses policy-following under conversational pressure, long-horizon tool coordination, and robustness to user behaviors that may tempt rule-bending.","Language Comprehension, Language Production, Decision-making, Planning, Inhibitory Control, Social Reasoning & Theory of Mind (minor)"
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI (Abstraction and Reasoning Corpus) tests few-shot fluid reasoning on novel grid-based pattern transformation problems, where the model must infer a hidden rule from a handful of examples. It emphasizes generalization to unseen concepts and compositional reasoning rather than memorized domain knowledge.","Logical Reasoning, Cognitive Flexibility, Working Memory, Spatial Representation & Mapping, Visual Perception (minor), Scene Understanding & Visual Reasoning (minor)"
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon autonomous agency in a simulated business setting where an agent runs a vending-machine operation over an extended period. The agent must manage inventory, pricing, supplier interactions, and strategy under changing conditions, and is scored by financial outcomes.","Planning, Decision-making, Reward Mechanisms, Working Memory, Cognitive Flexibility (minor), Self-reflection (minor)"
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a multimodal benchmark designed to probe frontier academic reasoning across diverse subjects with challenging questions that often require synthesis rather than recall. Some evaluation setups also test tool-augmented workflows (e.g., search or code) to measure end-to-end problem solving under realistic assistance settings.","Language Comprehension, Logical Reasoning, Semantic Understanding & Context Recognition, Scene Understanding & Visual Reasoning (minor), Visual Perception (minor), Working Memory (minor)"
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 tasks come from a high-level math competition and assess a model’s ability to solve multi-step problems that require algebraic manipulation, combinatorics, number theory, and careful case reasoning. Evaluation is typically strict on final numeric answers, stressing accuracy and coherence across steps.","Logical Reasoning, Working Memory, Planning (minor), Language Comprehension (minor)"
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a difficult multiple-choice science benchmark curated to be resistant to superficial pattern matching and to require graduate-level reasoning in physics, chemistry, and biology. The Diamond subset focuses on high-quality questions where experts succeed and non-experts commonly fail.","Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory (minor), Language Comprehension (minor)"
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends MMLU-style academic knowledge and reasoning evaluation across many subjects and multiple languages, probing whether competence transfers beyond English. It evaluates both factual/semantic knowledge and the ability to apply it in short reasoning contexts under multilingual prompts.","Language Comprehension, Semantic Understanding & Context Recognition, Cognitive Flexibility (minor), Working Memory (minor), Language Production (minor)"
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro is a multimodal understanding and reasoning benchmark spanning disciplines, where models answer questions grounded in images (e.g., diagrams, plots, tables) plus text. The Pro variant is designed to be more challenging and evaluation-focused for strong frontier multimodal models.","Visual Perception, Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Spatial Representation & Mapping (minor), Language Comprehension (minor)"
MathArena Apex,https://matharena.ai/?view=problem&comp=apex--apex_2025,https://arxiv.org/abs/2505.23281,"MathArena Apex is a high-difficulty mathematics evaluation intended to discriminate among top-tier models on complex competition-style or olympiad-adjacent problems. It stresses deep multi-step reasoning and precision, and is commonly used to compare frontier models’ mathematical ceilings.","Logical Reasoning, Working Memory, Planning (minor), Adaptive Error Correction (minor)"
ScreenShot-Pro,https://gui-agent.github.io/grounding-leaderboard/,https://arxiv.org/abs/2504.07981,"ScreenShot-Pro evaluates understanding of high-resolution screenshots of graphical user interfaces, requiring models to answer questions that depend on layout, icons, text snippets, and visual affordances. It is used to assess GUI grounding needed for “computer use” agents that navigate software via visual observations.","Visual Perception, Scene Understanding & Visual Reasoning, Spatial Representation & Mapping, Visual Attention & Eye Movements, Decision-making (minor), Sensorimotor Coordination (minor)"
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning evaluates whether models can interpret and reason over scientific figures from research papers, such as plots, charts, and composite panels, often requiring quantitative or relational inference. It targets figure-grounded scientific understanding beyond OCR, including reading trends, comparing conditions, and connecting captions/context to visuals.","Scene Understanding & Visual Reasoning, Visual Perception, Logical Reasoning, Semantic Understanding & Context Recognition, Spatial Representation & Mapping (minor), Working Memory (minor)"
OmniDocBench 1.5,https://github.com/opendatalab/OmniDocBench,https://arxiv.org/abs/2412.07626,"OmniDocBench 1.5 benchmarks document understanding and OCR across heterogeneous document elements such as text blocks, formulas, tables, and reading order. It emphasizes faithful extraction and structural reconstruction, which are critical for downstream reasoning over real-world documents.","Visual Perception, Visual Attention & Eye Movements, Language Comprehension, Semantic Understanding & Context Recognition (minor), Working Memory (minor)"
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU extends multimodal reasoning to videos, requiring models to integrate information across frames and time to answer questions about events, interactions, and visual details. It stresses temporal comprehension and maintaining consistency over longer perceptual sequences rather than single images.","Visual Perception, Scene Understanding & Visual Reasoning, Working Memory, Cognitive Timing & Predictive Modeling, Attention (minor), Multisensory Integration (minor)"
LiveCodeBench Pro,https://livecodebenchpro.com/projects/livecodebench-pro/leaderboard,https://arxiv.org/abs/2506.11928,"LiveCodeBench Pro evaluates code generation on competitive-programming-style tasks with emphasis on correctness, generalization, and up-to-date problem distributions. It typically requires producing runnable solutions, handling edge cases, and iterating based on failures in a way that resembles real coding workflows.","Logical Reasoning, Planning, Adaptive Error Correction, Working Memory, Language Production (minor), Language Comprehension (minor)"
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"The FACTS Benchmark Suite systematically evaluates factuality-related behaviors, such as truthfulness under ambiguity, resistance to fabrication, and consistency with provided evidence or context. It is intended to move beyond single QA datasets by covering multiple factuality failure modes and reporting robust aggregate indicators.","Semantic Understanding & Context Recognition, Inhibitory Control, Logical Reasoning (minor), Self-reflection (minor), Language Comprehension (minor)"
SimpleQA Verified,https://www.kaggle.com/benchmarks/deepmind/simpleqa-verified,https://arxiv.org/abs/2509.07968,"SimpleQA Verified evaluates short-form factual question answering with an emphasis on verifiable correctness and reduced evaluation noise via human/automatic verification pipelines. It targets whether models can provide precise, non-hallucinated answers to straightforward fact queries.","Semantic Understanding & Context Recognition, Inhibitory Control (minor), Language Comprehension (minor), Language Production (minor)"
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA is a multilingual benchmark for physical commonsense and practical reasoning, extending PIQA-style questions beyond English and across diverse linguistic contexts. It probes whether models can choose or generate plausible actions or explanations grounded in everyday physical constraints.","Logical Reasoning, Semantic Understanding & Context Recognition, Language Comprehension, Spatial Representation & Mapping (minor), Decision-making (minor)"
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) is a long-context evaluation where multiple similar “needle” interactions are embedded within a much larger “haystack,” and the model must retrieve the correct referenced content (e.g., the response corresponding to a specific needle). The 8-needle variant increases interference and tests whether models can sustain accurate retrieval and co-reference resolution over long inputs.","Working Memory, Attention, Language Comprehension, Episodic Memory (minor), Semantic Understanding & Context Recognition (minor)"
