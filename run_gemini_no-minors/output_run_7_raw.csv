Benchmark,Website,Paper,Description,Cognitive Functions
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates software engineering agents on real GitHub issues by asking them to produce a patch that makes a repository’s tests pass. The “Verified” subset uses human-validated tasks intended to reduce ambiguity and ensure solvability, emphasizing end-to-end debugging and code change execution under realistic constraints.","Planning, Logical Reasoning, Working Memory, Adaptive Error Correction, Decision-making (minor), Language Comprehension (minor), Language Production (minor)"
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 measures an agent’s ability to complete practical tasks in a command-line environment (e.g., inspecting files, running tools, fixing issues) using iterative interaction. It emphasizes tool-mediated problem solving, robustness to errors, and maintaining progress over multi-step sessions rather than single-turn coding.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Logical Reasoning, Language Comprehension (minor), Inhibitory Control (minor)"
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench evaluates interactive customer-support-style agents that must follow domain policies while using tools/APIs across multi-turn conversations with a simulated user. Success requires consistent policy adherence, correct tool invocation sequences, and recovering gracefully from misunderstandings or partial failures over the dialogue.","Social Reasoning & Theory of Mind, Decision-making, Planning, Inhibitory Control, Working Memory, Language Comprehension, Language Production, Reward Mechanisms (minor)"
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI is a fluid-reasoning benchmark in which models infer a latent rule from a few input–output grid examples and apply it to a new grid. It is designed to stress generalization to novel tasks with minimal examples, relying on structured pattern induction rather than memorized domain knowledge.","Cognitive Flexibility, Logical Reasoning, Working Memory, Spatial Representation & Mapping, Visual Perception, Attention (minor)"
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon agent coherence by placing the model in a simulated year-long vending-machine business management environment. The agent must make thousands of interdependent decisions (pricing, inventory, supplier negotiation, budgeting) to maximize final balance, stressing sustained strategy and adaptation.","Planning, Decision-making, Working Memory, Adaptive Error Correction, Reward Mechanisms, Language Comprehension (minor), Language Production (minor), Motivational Drives (minor)"
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a difficult frontier benchmark spanning many expert-level questions (often multimodal) across diverse domains. It targets broad reasoning and knowledge integration, including cases where tool use (search/code) can be enabled to test end-to-end problem solving in more realistic settings.","Language Comprehension, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory (minor), Visual Perception (minor), Scene Understanding & Visual Reasoning (minor), Planning (minor)"
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 benchmarks mathematical problem solving using questions from the American Invitational Mathematics Examination, typically requiring multi-step derivations and careful symbolic manipulation. It primarily tests contest-style reasoning under short prompts, sometimes evaluated with or without computational tools.","Logical Reasoning, Working Memory, Attention (minor), Language Comprehension (minor)"
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a high-difficulty multiple-choice science QA benchmark constructed to be resistant to simple web lookup and to require genuine scientific reasoning. The “Diamond” subset is curated for quality and difficulty, emphasizing deep understanding over surface pattern matching.","Semantic Understanding & Context Recognition, Logical Reasoning, Language Comprehension, Working Memory (minor), Inhibitory Control (minor)"
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends the MMLU-style multi-subject exam format to multiple languages, testing knowledge and reasoning across many academic areas under multilingual prompting. It stresses cross-lingual robustness: models must map non-English queries to the correct underlying concepts and select accurate answers.","Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning (minor), Working Memory (minor)"
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro evaluates expert-level multimodal understanding by combining images (charts, diagrams, documents, photos) with questions that require reasoning and domain knowledge. The “Pro” setting is designed to be more challenging and to better separate strong multimodal reasoning from shallow captioning or OCR-only behavior.","Multisensory Integration, Visual Perception, Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements (minor), Language Comprehension, Logical Reasoning"
MathArena Apex,https://matharena.ai/?view=problem&comp=apex--apex_2025,https://arxiv.org/abs/2505.23281,"MathArena Apex is a competitive math evaluation (with standardized prompts and grading) that aggregates challenging problems intended to separate frontier mathematical reasoning performance. It emphasizes multi-step solutions and robustness, often reporting results across tool/no-tool or reasoning settings depending on the track.","Logical Reasoning, Working Memory, Attention (minor), Language Comprehension (minor)"
ScreenShot-Pro,https://gui-agent.github.io/grounding-leaderboard/,https://arxiv.org/abs/2504.07981,"ScreenShot-Pro evaluates understanding of high-resolution GUI screenshots by asking models to answer questions that depend on correctly interpreting interface elements, layouts, and on-screen text. It targets real-world screen understanding needed for computer-use agents, where small visual details and spatial relationships are crucial.","Visual Perception, Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Spatial Representation & Mapping, Language Comprehension (minor), Sensorimotor Coordination (minor)"
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning evaluates reasoning over scientific paper figures (often chart- or plot-like), requiring interpretation of visual evidence alongside question text. It stresses diagram/chart reading, quantitative inference, and drawing conclusions that are not explicitly stated in the prompt.","Scene Understanding & Visual Reasoning, Visual Perception, Multisensory Integration, Logical Reasoning, Language Comprehension, Working Memory (minor)"
OmniDocBench 1.5,https://github.com/opendatalab/OmniDocBench,https://arxiv.org/abs/2412.07626,"OmniDocBench 1.5 benchmarks document understanding and OCR-centric extraction over complex layouts, including text blocks, tables, formulas, and reading order. It focuses on reconstructing faithful structured content from documents, stressing layout-aware perception beyond plain text recognition.","Visual Perception, Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Spatial Representation & Mapping, Language Comprehension (minor), Working Memory (minor)"
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU extends multimodal understanding to the video domain by asking questions that require integrating information across frames and time. It probes temporal grounding (what happens when), causal/relational reasoning over events, and summarizing salient visual evidence into an answer.","Multisensory Integration, Visual Perception, Cognitive Timing & Predictive Modeling, Working Memory, Attention (minor), Scene Understanding & Visual Reasoning, Language Comprehension (minor)"
LiveCodeBench Pro,https://livecodebenchpro.com/projects/livecodebench-pro/leaderboard,https://arxiv.org/abs/2506.11928,"LiveCodeBench Pro evaluates coding ability on programming tasks designed to be more resistant to contamination and to better reflect practical coding performance than older static sets. It typically emphasizes writing correct, executable code under time- and test-driven constraints, sometimes summarized via leaderboard-style ratings.","Logical Reasoning, Planning, Adaptive Error Correction, Working Memory, Language Comprehension (minor), Language Production (minor)"
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"The FACTS Benchmark Suite evaluates factuality and grounding by testing whether model outputs remain consistent with provided sources and avoid unsupported claims across a set of targeted factuality tasks. It aims to measure not only raw knowledge, but also calibration, citation/grounding behavior, and resistance to hallucination.","Semantic Understanding & Context Recognition, Inhibitory Control, Language Comprehension, Language Production, Working Memory (minor), Self-reflection (minor)"
SimpleQA Verified,https://www.kaggle.com/benchmarks/deepmind/simpleqa-verified,https://arxiv.org/abs/2509.07968,SimpleQA Verified is a factual question answering benchmark with verified ground truth and evaluation protocols aimed at reliably measuring factual correctness. It focuses on whether models can provide accurate short answers (and avoid plausible-sounding errors) on straightforward information-seeking queries.,"Semantic Understanding & Context Recognition, Language Comprehension, Language Production (minor), Inhibitory Control (minor)"
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA evaluates physical commonsense reasoning across many languages using non-parallel (not direct translations) items, aiming to test robustness to linguistic and cultural variation. Questions typically involve selecting the more plausible action/solution in everyday physical scenarios, emphasizing grounded commonsense rather than memorized facts.","Semantic Understanding & Context Recognition, Logical Reasoning, Language Comprehension, Spatial Representation & Mapping (minor), Sensorimotor Coordination (minor)"
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,MRCR v2 (8-needle) tests long-context multi-round coreference resolution by placing repeated similar “needle” interactions inside a long “haystack” dialogue and asking the model to retrieve the correct referenced response. It emphasizes precise tracking of entities and events across very long contexts under distraction and interference.,"Working Memory, Attention, Language Comprehension, Semantic Understanding & Context Recognition, Episodic Memory (minor), Inhibitory Control (minor)"
