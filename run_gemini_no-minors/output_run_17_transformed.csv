Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates software engineering agents by asking them to produce patches that fix real issues in open-source repositories under test-suite verification. The “Verified” subset emphasizes reliably solvable tasks with strong, human-validated evaluation, stressing end-to-end debugging, code navigation, and correct patch synthesis.","L1: Language Comprehension (minor), Language Production (minor)
L2: Logical Reasoning, Planning, Working Memory, Adaptive Error Correction
L3: ",L2
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 measures agent performance on real command-line tasks (e.g., environment setup, debugging, data wrangling) executed in terminal sandboxes. The benchmark stresses iterative tool use, interpreting system feedback, and recovering from errors under realistic operational constraints.","L1: Language Comprehension (minor)
L2: Planning, Decision-making, Working Memory, Adaptive Error Correction
L3: ",L2
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench evaluates interactive tool-using agents in multi-turn customer support domains (e.g., retail, airline, telecom), where the agent must use APIs/tools while adhering to domain policies. It probes consistency across turns, policy compliance under pressure, and goal-directed dialogue grounded in tool outputs.","L1: Language Comprehension, Language Production
L2: Decision-making, Planning (minor)
L3: Inhibitory Control, Social Reasoning & Theory of Mind",L3
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI tests abstract, few-shot reasoning on novel grid-based transformation puzzles where models infer latent rules from a handful of input-output examples. It is designed to emphasize fluid reasoning and generalization rather than memorized domain knowledge.","L1: 
L2: Logical Reasoning, Spatial Representation & Mapping, Scene Understanding & Visual Reasoning, Working Memory, Attention (minor)
L3: Cognitive Flexibility",L3
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon agentic performance by simulating management of a vending machine business over extended time, including inventory, pricing, supplier interaction, and adaptation to changing conditions. Scoring is based on sustained strategic coherence and resulting business outcomes (e.g., final balance).","L1: 
L2: Planning, Decision-making, Working Memory, Reward Mechanisms, Adaptive Error Correction
L3: Self-reflection (minor)",L2
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam (HLE) is a frontier, multi-domain benchmark intended to test advanced academic and expert-level reasoning across a broad range of subjects, often with challenging, research-like questions. It emphasizes synthesis and correctness under uncertainty, and is frequently used to compare “with tools” vs “no tools” settings.","L1: Language Comprehension, Visual Perception (minor)
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory (minor)
L3: ",L2
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 evaluates competition-style mathematical problem solving drawn from the American Invitational Mathematics Examination. Problems typically require multi-step derivations, algebraic manipulation, and careful constraint handling rather than rote computation.","L1: 
L2: Logical Reasoning, Working Memory, Planning (minor)
L3: Cognitive Flexibility (minor)",L2
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a high-difficulty, graduate-level science multiple-choice benchmark curated to be resistant to shallow pattern matching and casual web search. The Diamond subset emphasizes question quality and discriminates strongly between expert and non-expert performance.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory (minor)
L3: ",L2
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU is a multilingual extension of broad academic knowledge testing, covering many subjects across numerous languages. It is used to assess how well models maintain understanding and reasoning when the language of the prompt changes, not just when content changes.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning (minor), Working Memory (minor)
L3: ",L2
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro measures multimodal understanding and expert-level reasoning across disciplines using images plus text (e.g., diagrams, plots, tables, scenes) with more challenging evaluation protocols than earlier MMMU variants. It stresses joint perception-and-reasoning, requiring extraction of visual evidence and integrating it with domain knowledge.","L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Spatial Representation & Mapping (minor)
L3: ",L2
MathArena Apex,https://matharena.ai/?view=problem&comp=apex--apex_2025,https://arxiv.org/abs/2505.23281,"MathArena Apex is a high-difficulty mathematics benchmark suite used to compare frontier models on rigorous, multi-step problem solving beyond standard contest sets. It emphasizes robustness across problem types and discourages brittle shortcut strategies by using diverse tasks and evaluation procedures.","L1: 
L2: Logical Reasoning, Working Memory, Planning (minor)
L3: Cognitive Flexibility",L3
ScreenShot-Pro,https://gui-agent.github.io/grounding-leaderboard/,https://arxiv.org/abs/2504.07981,"ScreenShot-Pro evaluates visual grounding and reasoning over high-resolution screenshots of real software interfaces, requiring models to identify relevant UI elements and interpret layouts to answer questions. It targets practical GUI understanding where small visual details and spatial relationships determine correctness.","L1: Visual Perception
L2: Visual Attention & Eye Movements, Spatial Representation & Mapping, Scene Understanding & Visual Reasoning, Decision-making (minor), Sensorimotor Coordination (minor)
L3: ",L2
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning tests reasoning over scientific figures (often from arXiv-style papers), where the model must interpret charts/plots and connect them to textual context or quantitative questions. It emphasizes extracting the right visual evidence and performing structured inference, sometimes with optional tool assistance (e.g., Python).","L1: Visual Perception, Language Comprehension (minor)
L2: Scene Understanding & Visual Reasoning, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory (minor)
L3: ",L2
OmniDocBench 1.5,https://github.com/opendatalab/OmniDocBench,https://arxiv.org/abs/2412.07626,"OmniDocBench 1.5 evaluates document understanding and OCR-centric capabilities across heterogeneous layouts, including text, tables, formulas, and reading order. It stresses robust visual parsing of documents and faithful reconstruction of content structure, not just recognition of isolated text snippets.","L1: Visual Perception, Language Comprehension (minor)
L2: Scene Understanding & Visual Reasoning, Attention, Working Memory (minor)
L3: ",L2
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,Video-MMMU extends multimodal reasoning to the temporal domain by requiring understanding of events and evidence distributed across video frames (and associated text). It probes whether models can maintain coherent interpretations over time and answer questions that depend on tracking changes and causal sequences.,"L1: Visual Perception
L2: Attention, Working Memory, Scene Understanding & Visual Reasoning, Multisensory Integration (minor)
L3: Cognitive Timing & Predictive Modeling (minor)",L2
LiveCodeBench Pro,https://livecodebenchpro.com/projects/livecodebench-pro/leaderboard,https://arxiv.org/abs/2506.11928,"LiveCodeBench Pro measures coding ability on a curated set of programming tasks with strong evaluation, often emphasizing realistic developer workflows and correctness under execution-based checks. It targets reliable code generation and repair under constraints closer to professional programming than toy problems.","L1: Language Production (minor)
L2: Logical Reasoning, Planning, Working Memory, Adaptive Error Correction
L3: ",L2
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"The FACTS Benchmark Suite systematically evaluates factuality-related behaviors, including when models should answer, abstain, cite evidence, or avoid hallucinating unsupported details. It focuses on truthfulness under ambiguity and the model’s ability to maintain faithfulness to provided or retrievable evidence across varied settings.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Working Memory (minor)
L3: Inhibitory Control, Self-reflection (minor)",L3
SimpleQA Verified,https://www.kaggle.com/benchmarks/deepmind/simpleqa-verified,https://arxiv.org/abs/2509.07968,"SimpleQA Verified is a short-form factual question answering benchmark with verification-oriented evaluation designed to reduce grading ambiguity. It is used to measure whether models can provide accurate, concise answers and avoid confident fabrication on straightforward queries.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Working Memory (minor)
L3: Inhibitory Control (minor)",L2
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA evaluates physical commonsense reasoning across languages, focusing on everyday situations and practical plausibility rather than specialized academic knowledge. It probes whether models can generalize intuitive physical constraints and action outcomes in multilingual settings.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning
L3: Cognitive Flexibility (minor)",L2
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) measures long-context retrieval and multi-round coreference by placing multiple similar “needle” interactions within long conversational “haystacks,” then asking the model to reproduce the correct response associated with a specific needle. It stresses robust attention allocation and accurate retrieval of the right earlier context despite distractors and repetition.","L1: Language Comprehension
L2: Working Memory, Attention, Episodic Memory
L3: Inhibitory Control (minor)",L2
