Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates whether a model can solve real GitHub issue tasks by producing a patch that makes a repository’s tests pass. The Verified subset uses problems that have been manually checked to be solvable and to have reliable evaluation infrastructure, emphasizing end-to-end debugging and code changes under realistic constraints.","L1: Language Comprehension (minor)
L2: Planning, Logical Reasoning, Working Memory, Adaptive Error Correction
L3: ",L2
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 measures an agent’s ability to complete practical command-line tasks in sandboxed environments (e.g., installing dependencies, manipulating files, running tools, and validating outputs). It stresses iterative interaction with a system state, where success depends on choosing correct commands, interpreting errors, and recovering from failures.","L1: Language Comprehension (minor)
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory
L3: ",L2
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench evaluates tool-using customer-support agents in simulated domains (e.g., retail, airline, telecom) that require multi-turn dialogue plus programmatic API/tool calls while following policies. It tests whether an agent can coordinate conversation, comply with constraints, and execute correct actions over long interactions.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making
L3: Inhibitory Control (minor), Social Reasoning & Theory of Mind (minor)",L2
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI tests abstract, few-shot pattern induction from small sets of input–output grid examples, requiring models to infer the underlying rule and apply it to new inputs. The tasks are designed to emphasize novel reasoning over memorization, often involving compositional transformations and relational structure.","L1: Visual Perception (minor)
L2: Logical Reasoning, Working Memory, Spatial Representation & Mapping, Scene Understanding & Visual Reasoning
L3: Cognitive Flexibility",L3
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon agent coherence and business decision-making in a simulated vending-machine company over many steps (e.g., sourcing inventory, pricing, negotiating, and adapting to changing conditions). Performance is typically measured by final financial outcomes, rewarding sustained strategy rather than short-turn correctness.","L1: 
L2: Planning, Decision-making, Reward Mechanisms, Working Memory, Adaptive Error Correction (minor), Semantic Understanding & Context Recognition (minor)
L3: ",L2
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a difficult multimodal benchmark spanning frontier academic and professional knowledge, intended to probe complex reasoning and broad expertise. Questions may require integrating text with images (and sometimes tools in certain setups) and are designed to be challenging even for strong general models.","L1: Language Comprehension, Visual Perception (minor)
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Multisensory Integration (minor)
L3: ",L2
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 evaluates mathematical problem solving on the American Invitational Mathematics Examination, consisting of competition-style questions requiring multi-step derivations. It primarily measures symbolic reasoning accuracy and robustness under minimal scaffolding, sometimes with optional tool-augmented variants.","L1: 
L2: Logical Reasoning, Working Memory, Planning (minor)
L3: Cognitive Flexibility (minor)",L2
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a curated subset of graduate-level multiple-choice science questions intended to be difficult to answer via superficial recall or simple web search. The “Diamond” subset emphasizes high-quality items where experts succeed and non-experts often fail, probing deep scientific understanding and careful reasoning.","L1: Language Comprehension (minor)
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory (minor)
L3: ",L2
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends MMLU-style academic knowledge testing across many subjects and multiple languages, assessing whether models can answer standardized questions beyond English. It probes cross-lingual generalization and consistency of knowledge and reasoning across linguistic contexts.","L1: Language Comprehension, Language Production (minor)
L2: Semantic Understanding & Context Recognition, Logical Reasoning (minor)
L3: ",L2
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro is a high-difficulty multimodal benchmark spanning many disciplines, where models answer questions grounded in images (e.g., diagrams, charts, documents) along with text. It emphasizes expert-level multimodal reasoning and visual grounding under more challenging settings than earlier MMMU variants.","L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning (minor)
L3: ",L2
MathArena Apex,https://matharena.ai/?view=problem&comp=apex--apex_2025,https://arxiv.org/abs/2505.23281,"MathArena Apex aggregates difficult, competition- and research-adjacent math problems to compare frontier models under standardized evaluation. It targets complex multi-step reasoning and reliability, often including adversarially hard items that expose brittle heuristics.","L1: 
L2: Logical Reasoning, Working Memory, Planning (minor)
L3: Cognitive Flexibility (minor)",L2
ScreenShot-Pro,https://gui-agent.github.io/grounding-leaderboard/,https://arxiv.org/abs/2504.07981,"ScreenShot-Pro evaluates GUI understanding and grounding from screenshots, requiring models/agents to interpret interface elements and answer questions or select targets based on visual layout and semantics. It focuses on fine-grained screen understanding relevant to computer-use agents (e.g., locating buttons, reading UI text, and reasoning about spatial relationships).","L1: Visual Perception, Language Comprehension (minor)
L2: Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Spatial Representation & Mapping, Sensorimotor Coordination (minor)
L3: ",L2
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning tests reasoning over scientific paper figures and content (often charts/plots) from arXiv-style documents, asking questions that require extracting and manipulating information from the visual evidence. It aims to separate shallow caption reading from true figure-based inference and quantitative interpretation.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Logical Reasoning, Semantic Understanding & Context Recognition (minor), Working Memory (minor)
L3: ",L2
OmniDocBench 1.5,https://github.com/opendatalab/OmniDocBench,https://arxiv.org/abs/2412.07626,"OmniDocBench 1.5 evaluates document understanding and OCR-style extraction across diverse layouts, including text, tables, formulas, and reading order. Scores typically reflect how accurately a system reconstructs or edits structured document content, stressing layout-aware perception and precise transcription.","L1: Visual Perception, Language Production (minor)
L2: Attention, Scene Understanding & Visual Reasoning, Semantic Understanding & Context Recognition (minor)
L3: ",L2
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU extends multimodal understanding to video, requiring models to answer questions that depend on temporal events, actions, and visual details across frames. It stresses integrating information over time (rather than single images) and maintaining coherent representations of evolving scenes.","L1: Visual Perception
L2: Working Memory, Scene Understanding & Visual Reasoning, Multisensory Integration (minor)
L3: Cognitive Timing & Predictive Modeling",L3
LiveCodeBench Pro,https://livecodebenchpro.com/projects/livecodebench-pro/leaderboard,https://arxiv.org/abs/2506.11928,"LiveCodeBench Pro evaluates coding ability on competitive-programming and practical coding problems under controlled release and leakage-aware conditions, often reporting performance as an ELO-style rating. It emphasizes writing correct programs from specifications and handling hidden tests, rewarding robust algorithmic reasoning and debugging.","L1: Language Comprehension (minor)
L2: Logical Reasoning, Planning, Working Memory, Adaptive Error Correction
L3: ",L2
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"FACTS Benchmark Suite systematically measures factuality and grounding behavior across multiple factuality-focused tasks, targeting whether models make accurate claims and avoid unsupported statements. It is designed to diagnose different failure modes (e.g., hallucination versus misattribution) rather than only measuring general QA accuracy.","L1: Language Production
L2: Semantic Understanding & Context Recognition, Logical Reasoning (minor)
L3: Inhibitory Control (minor), Self-reflection (minor)",L2
SimpleQA Verified,https://www.kaggle.com/benchmarks/deepmind/simpleqa-verified,https://arxiv.org/abs/2509.07968,SimpleQA Verified is a factual QA benchmark emphasizing unambiguous questions with verified answers and careful evaluation to reduce labeling noise. It is used to measure whether models answer directly and correctly (and whether they refrain from fabricating when uncertain).,"L1: Language Comprehension, Language Production (minor)
L2: Semantic Understanding & Context Recognition
L3: Inhibitory Control (minor)",L2
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA evaluates physical commonsense and pragmatic reasoning across many languages, focusing on whether models can choose sensible actions or explanations in everyday scenarios beyond English. It emphasizes robustness of commonsense inference under multilingual variation and non-parallel translations.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning (minor)
L3: Social Reasoning & Theory of Mind (minor)",L2
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) tests long-context, multi-round coreference resolution by embedding multiple similar “needle” requests within long “haystacks” and asking the model to reproduce the correct response to a specified needle. It primarily measures attention control and retrieval of the correct referenced span under heavy distractors and long contexts.","L1: Language Comprehension
L2: Working Memory, Attention, Semantic Understanding & Context Recognition, Episodic Memory (minor)
L3: ",L2
