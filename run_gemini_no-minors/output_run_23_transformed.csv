Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro evaluates software engineering agents on a large set of real-world issues across multiple programming languages and repositories. Models must read project context, implement correct patches, and satisfy tests under realistic constraints, emphasizing robustness and contamination resistance compared to simpler coding sets.","L1: Language Comprehension, Language Production (minor)
L2: Logical Reasoning, Planning, Adaptive Error Correction, Working Memory
L3: Inhibitory Control (minor)",L2
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld benchmarks multimodal “computer use” agents in a full operating-system environment, where tasks require interacting with GUIs, applications, and files over many steps. Success depends on perceiving screens, choosing actions, and recovering from mistakes in a partially observable, tool-driven setting.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Attention, Planning, Decision-making, Sensorimotor Coordination, Working Memory, Adaptive Error Correction (minor), Spatial Representation & Mapping (minor)
L3: ",L2
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,ARC-AGI measures fluid reasoning on novel grid-based puzzles where models infer a transformation rule from a few input–output examples and apply it to a new input. It is designed to reward generalization and abstraction rather than memorized domain knowledge.,"L1: Visual Perception (minor)
L2: Scene Understanding & Visual Reasoning, Logical Reasoning, Working Memory, Attention (minor)
L3: Cognitive Flexibility",L3
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon agent coherence and business decision-making in a simulated vending-machine company over an extended time period. Agents must manage inventory, pricing, supplier negotiation, and strategy under budget constraints, where early choices compound into later outcomes.","L1: Language Comprehension (minor), Language Production (minor)
L2: Planning, Decision-making, Reward Mechanisms, Working Memory, Episodic Memory (minor)
L3: Social Reasoning & Theory of Mind (minor)",L2
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas measures real-world tool use via the Model Context Protocol, requiring models to discover, call, and compose tools across multi-step workflows. Tasks stress correct parameterization, error handling, and synthesis of tool outputs into a coherent final response.","L1: Language Comprehension
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition
L3: Inhibitory Control (minor)",L2
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity agents on large-scale tasks involving identifying known vulnerabilities from descriptions and discovering previously unknown issues in real open-source codebases. The benchmark emphasizes reasoning over code, reproducing weaknesses, and producing actionable findings under realistic constraints.","L1: Language Comprehension
L2: Logical Reasoning, Planning, Adaptive Error Correction, Working Memory, Decision-making (minor)
L3: Inhibitory Control (minor)",L2
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a difficult multimodal benchmark spanning many expert domains, designed to probe frontier knowledge, reasoning, and synthesis on problems near the limits of current models. It includes questions where tool use (e.g., search or code) can be important in practical evaluation settings, while still stressing robust reasoning and grounding.","L1: Language Comprehension, Visual Perception (minor)
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Scene Understanding & Visual Reasoning (minor)
L3: ",L2
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a curated subset of extremely challenging graduate-level multiple-choice science questions intended to be “Google-proof.” It tests whether models can apply deep scientific knowledge and reasoning, rather than relying on shallow pattern matching or retrieval shortcuts.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory (minor), Decision-making (minor)
L3: ",L2
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro is a high-difficulty multimodal benchmark covering many disciplines, where models answer questions requiring joint interpretation of images/diagrams and text. It targets expert-level multimodal understanding and reasoning beyond basic recognition, often involving charts, figures, and domain context.","L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Working Memory (minor), Visual Attention & Eye Movements (minor)
L3: ",L2
OmniDocBench 1.5,https://github.com/opendatalab/OmniDocBench,https://arxiv.org/abs/2412.07626,"OmniDocBench 1.5 evaluates document understanding and OCR-like capabilities across complex layouts, including text, formulas, tables, and reading order. It emphasizes faithful extraction and structural reconstruction from visually rich documents rather than simple plain-text transcription.","L1: Visual Perception, Language Comprehension
L2: Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Spatial Representation & Mapping, Working Memory (minor), Semantic Understanding & Context Recognition (minor)
L3: ",L2
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU benchmarks multimodal understanding and reasoning over video, requiring models to integrate information across frames and time to answer questions. It stresses temporal grounding, event understanding, and multi-step reasoning based on dynamic visual content.","L1: Visual Perception, Language Comprehension (minor)
L2: Attention, Working Memory, Scene Understanding & Visual Reasoning, Multisensory Integration (minor)
L3: Cognitive Timing & Predictive Modeling",L3
LiveCodeBench Pro,https://livecodebenchpro.com/projects/livecodebench-pro/leaderboard,https://arxiv.org/abs/2506.11928,"LiveCodeBench Pro evaluates coding models on competitive-style programming tasks with an emphasis on up-to-date, hard-to-memorize problems and standardized scoring (e.g., Elo). It tests end-to-end program synthesis and iterative debugging toward passing hidden test cases.","L1: Language Comprehension (minor), Language Production (minor)
L2: Logical Reasoning, Planning, Adaptive Error Correction, Working Memory
L3: ",L2
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"The FACTS Benchmark Suite systematically evaluates factuality, including whether model statements are supported by provided sources or general-world evidence under controlled settings. It targets hallucination resistance and calibrated, context-sensitive truthfulness across diverse tasks and domains.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning (minor), Working Memory (minor)
L3: Inhibitory Control, Self-reflection (minor)",L3
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,Global PIQA is a multilingual / cross-cultural physical commonsense benchmark that tests whether models can choose the more plausible action or outcome in everyday physical interaction scenarios. It emphasizes robust commonsense generalization across languages and culturally diverse contexts rather than English-only priors.,"L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Spatial Representation & Mapping (minor), Multisensory Integration (minor)
L3: ",L2
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) is a long-context retrieval and multi-round coreference benchmark where multiple similar “needle” requests are embedded in long “haystack” conversations. Models must locate and reproduce the correct referenced response, stressing attention control and interference resistance over long inputs.","L1: Language Comprehension
L2: Working Memory, Attention, Episodic Memory, Semantic Understanding & Context Recognition
L3: ",L2
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval evaluates economically meaningful professional work across many occupations by having models produce real deliverables (e.g., spreadsheets, presentations, schedules) judged by expert humans. It focuses on end-to-end task execution quality, including correctness, completeness, and professional standards under well-specified requirements.","L1: Language Production
L2: Planning, Decision-making, Semantic Understanding & Context Recognition, Working Memory (minor)
L3: Social Reasoning & Theory of Mind (minor), Self-reflection (minor)",L2
SWE-Lancer,https://openai.com/index/swe-lancer/,https://arxiv.org/abs/2502.12115,"SWE-Lancer evaluates agentic software engineering on realistic, higher-level development work where models must navigate repositories, implement changes, and produce patches that satisfy tests and project constraints. Compared to narrow coding problems, it stresses longer-horizon execution and engineering judgment under ambiguity.","L1: Language Comprehension (minor), Language Production (minor)
L2: Planning, Logical Reasoning, Adaptive Error Correction, Decision-making, Working Memory
L3: ",L2
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath is a benchmark of very challenging, expert-level mathematics problems intended to measure genuine mathematical reasoning at the research frontier. Many items require multi-step derivations and careful symbolic reasoning, and are designed to be difficult to solve via memorization alone.","L1: 
L2: Logical Reasoning, Working Memory, Planning, Semantic Understanding & Context Recognition (minor)
L3: Cognitive Flexibility (minor)",L2
