Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates software engineering by asking a model/agent to generate a code patch that resolves real GitHub issues in Python repositories, with correctness checked by running tests. The “Verified” split emphasizes tasks that have been validated as solvable and reduces noisy/ambiguous instances, making it a key benchmark for end-to-end code editing reliability.","L1: Language Comprehension (minor), Language Production (minor)
L2: Planning, Logical Reasoning, Working Memory, Adaptive Error Correction
L3: ",L2
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 measures an agent’s ability to accomplish realistic tasks in a command-line environment (e.g., inspecting files, running programs, installing dependencies, debugging, and producing artifacts). Success typically requires iterating between tool use and reasoning under resource and time constraints, making it a practical proxy for autonomous “ops-style” problem solving.","L1: Language Comprehension (minor)
L2: Planning, Decision-making, Working Memory, Adaptive Error Correction
L3: Inhibitory Control (minor)",L2
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench evaluates tool-using customer-support agents in simulated domains (e.g., retail, airline, telecom) where the model must follow policies, interact with APIs, and handle multi-turn dialogue with a user simulator. The benchmark stresses robustness to long interactions, policy compliance, and correct sequencing of tool calls to resolve user issues.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making
L3: Inhibitory Control, Social Reasoning & Theory of Mind (minor)",L3
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,ARC-AGI tests “fluid” abstract reasoning using small grid-based puzzles where a model must infer a hidden transformation rule from a handful of examples and apply it to a new input. It emphasizes out-of-distribution generalization and compositional pattern discovery rather than memorized knowledge.,"L1: Visual Perception (minor)
L2: Logical Reasoning, Spatial Representation & Mapping, Working Memory (minor), Scene Understanding & Visual Reasoning
L3: Cognitive Flexibility",L3
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon agent performance in a simulated year-long vending-machine business, where the agent must manage inventory, pricing, supplier communication, and finances over thousands of steps. The score is based on economic outcomes (e.g., final balance), emphasizing sustained coherence, strategic adaptation, and tool-mediated operations.","L1: 
L2: Planning, Decision-making, Working Memory, Reward Mechanisms, Adaptive Error Correction (minor)
L3: Social Reasoning & Theory of Mind (minor)",L2
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam (HLE) is a multimodal benchmark designed to probe frontier-level academic and professional knowledge with difficult questions that often require multi-step reasoning. It can be evaluated with or without tools (e.g., search, code execution), highlighting both raw reasoning and tool-augmented problem solving.","L1: Language Comprehension, Visual Perception (minor)
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Multisensory Integration (minor), Planning (minor)
L3: ",L2
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 consists of competition-style math problems requiring symbolic manipulation, proof-like reasoning, and careful case analysis, typically answered as integers. It is commonly used to test mathematical reasoning in a tool-free setting or with optional computation tools, separating conceptual skill from calculator-like execution.","L1: 
L2: Logical Reasoning, Working Memory, Planning (minor), Adaptive Error Correction (minor)
L3: ",L2
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is the most challenging subset of GPQA: graduate-level multiple-choice questions in physics, chemistry, and biology designed to be difficult to answer by web search alone. It tests deep scientific understanding and careful elimination of distractors under a standardized, closed-choice format.","L1: Language Comprehension (minor)
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory (minor)
L3: ",L2
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends MMLU to multiple languages, measuring broad academic knowledge and reasoning across many subjects with multilingual prompts and answer options. It probes whether models can transfer knowledge and reasoning across languages rather than relying on English-only patterns.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning (minor), Working Memory (minor)
L3: ",L2
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro is a stronger, more difficult variant of MMMU that evaluates multimodal understanding and reasoning across many disciplines using images paired with text questions. It targets high-fidelity visual reasoning (e.g., diagrams, charts, figures) alongside domain knowledge and structured inference.","L1: Visual Perception, Language Comprehension (minor)
L2: Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning (minor)
L3: ",L2
MathArena Apex,https://matharena.ai/?view=problem&comp=apex--apex_2025,https://arxiv.org/abs/2505.23281,"MathArena Apex is a curated set of difficult math problems used to compare frontier models under consistent evaluation protocols, often emphasizing rigorous multi-step reasoning. It is designed to reduce evaluation noise and provide a clearer signal on advanced mathematical problem-solving ability than easier contest subsets.","L1: 
L2: Logical Reasoning, Working Memory, Planning (minor), Adaptive Error Correction (minor)
L3: ",L2
ScreenShot-Pro,https://gui-agent.github.io/grounding-leaderboard/,https://arxiv.org/abs/2504.07981,"ScreenShot-Pro evaluates GUI grounding and screen understanding from high-resolution screenshots, requiring models to interpret interface elements, layout, and visual cues to answer questions or identify targets. It is commonly used to assess readiness for computer-use agents that must act based on what they “see” on a screen.","L1: Visual Perception
L2: Visual Attention & Eye Movements, Spatial Representation & Mapping, Scene Understanding & Visual Reasoning, Sensorimotor Coordination (minor)
L3: ",L2
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning measures a model’s ability to reason over figures and visual content drawn from scientific papers (e.g., plots, diagrams, or figure panels) together with accompanying questions. It emphasizes extracting structured information from scientific visuals and integrating it with domain context to reach correct conclusions.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Logical Reasoning, Semantic Understanding & Context Recognition (minor), Multisensory Integration (minor)
L3: ",L2
OmniDocBench 1.5,https://github.com/opendatalab/OmniDocBench,https://arxiv.org/abs/2412.07626,"OmniDocBench 1.5 evaluates document understanding/OCR pipelines across complex page elements such as text blocks, tables, formulas, and reading order. Scores reflect how faithfully models reconstruct and structure document content, stressing robustness to varied layouts and visual artifacts.","L1: Visual Perception, Language Comprehension (minor)
L2: Attention, Semantic Understanding & Context Recognition, Scene Understanding & Visual Reasoning (minor)
L3: ",L2
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU extends multimodal reasoning to the temporal domain, requiring models to answer questions about videos that may demand tracking events, actions, and state changes across time. It tests whether models can integrate information across frames rather than relying on single-image cues.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Working Memory, Multisensory Integration (minor)
L3: Cognitive Timing & Predictive Modeling",L3
LiveCodeBench Pro,https://livecodebenchpro.com/projects/livecodebench-pro/leaderboard,https://arxiv.org/abs/2506.11928,"LiveCodeBench Pro evaluates coding ability on up-to-date programming problems with standardized execution-based grading, often framed as competitive-programming or practical coding tasks. It emphasizes producing correct, runnable solutions under time-like constraints and is designed to reduce leakage via careful curation and recency.","L1: Language Production (minor)
L2: Logical Reasoning, Planning, Working Memory, Adaptive Error Correction
L3: ",L2
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"The FACTS Benchmark Suite systematically evaluates factuality-related behaviors, including accuracy, groundedness, and susceptibility to producing unsupported claims across a variety of tasks. It is intended to be more comprehensive than single QA sets by covering multiple failure modes and measurement styles.","L1: Language Comprehension (minor), Language Production (minor)
L2: Semantic Understanding & Context Recognition
L3: Inhibitory Control, Self-reflection (minor)",L3
SimpleQA Verified,https://www.kaggle.com/benchmarks/deepmind/simpleqa-verified,https://arxiv.org/abs/2509.07968,"SimpleQA Verified is a factual question-answering benchmark with verification designed to reduce ambiguous items and improve label reliability. It targets straightforward fact retrieval and penalizes confident hallucinations, making it useful for measuring baseline factual precision.","L1: Language Comprehension (minor)
L2: Semantic Understanding & Context Recognition, Working Memory (minor)
L3: Inhibitory Control",L3
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA evaluates physical commonsense and everyday procedural reasoning across many languages, aiming to measure whether models can answer practical “what happens if…” style questions without English-centric bias. It stresses cross-lingual robustness and grounding in intuitive physics and affordances.","L1: Language Comprehension
L2: Spatial Representation & Mapping, Logical Reasoning, Semantic Understanding & Context Recognition, Multisensory Integration (minor)
L3: ",L2
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) measures long-context retrieval and multi-round coreference resolution by embedding repeated, similar “needle” interactions within long “haystack” conversations and asking the model to reproduce the correct referenced response. The 8-needle variant increases interference, stressing attention control and retention over extended contexts.","L1: Language Comprehension (minor)
L2: Working Memory, Attention, Episodic Memory
L3: Inhibitory Control (minor)",L2
