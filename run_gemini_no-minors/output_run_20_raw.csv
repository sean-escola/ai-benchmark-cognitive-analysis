Benchmark,Website,Paper,Description,Cognitive Functions
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro evaluates real-world software engineering by giving a model an actual repository plus an issue and requiring it to produce a patch that passes tests. Compared to simpler coding benchmarks, it emphasizes end-to-end debugging, codebase navigation, and correct changes under realistic project constraints.","Planning, Adaptive Error Correction, Logical Reasoning, Working Memory, Language Comprehension (minor), Language Production (minor)"
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld measures an agent’s ability to complete tasks by interacting with a desktop operating system via a GUI, requiring it to interpret screens and take sequences of actions. It stresses robust multimodal perception-to-action loops, including navigation, form-filling, and tool/command usage across multiple steps.","Visual Perception, Scene Understanding & Visual Reasoning, Attention, Planning, Decision-making, Sensorimotor Coordination, Working Memory"
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI evaluates fluid reasoning on novel grid transformation puzzles, where models infer rules from a few input–output examples and generalize to a new case. The tasks are designed to emphasize abstraction, compositional pattern discovery, and generalization beyond memorized templates.","Logical Reasoning, Cognitive Flexibility, Working Memory, Spatial Representation & Mapping, Scene Understanding & Visual Reasoning, Attention (minor)"
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon agent performance in a simulated vending-machine business over an extended period, scoring outcomes such as final balance and business success. It probes whether an agent can maintain coherence, adapt strategy, and manage resources across many sequential decisions under uncertainty.","Planning, Decision-making, Reward Mechanisms, Working Memory, Episodic Memory (minor), Adaptive Error Correction (minor), Motivational Drives (minor)"
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas evaluates real-world tool use through the Model Context Protocol (MCP), requiring models to discover, call, and chain tools/APIs correctly in multi-step workflows. It emphasizes reliable action selection, parameterization, error handling, and synthesis of tool outputs into correct final results.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Language Comprehension (minor), Semantic Understanding & Context Recognition"
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity agent capabilities on tasks such as identifying known vulnerabilities from descriptions and discovering previously unknown issues in real open-source projects. It stresses systematic investigation, hypothesis testing, and producing actionable findings or fixes under realistic constraints.","Logical Reasoning, Planning, Adaptive Error Correction, Working Memory, Attention (minor), Semantic Understanding & Context Recognition (minor)"
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a broad, frontier-level benchmark spanning many subjects and often involving complex reasoning and (in some settings) multimodal inputs. It is intended to test deep problem solving and knowledge integration rather than narrow pattern matching, sometimes with optional tool use depending on evaluation setup.","Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Visual Perception (minor), Scene Understanding & Visual Reasoning (minor)"
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,GPQA Diamond is a difficult multiple-choice benchmark of graduate-level science questions chosen to be resistant to superficial retrieval and to require genuine reasoning. It focuses on high-quality items where experts reliably answer correctly while non-experts often fail.,"Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Language Comprehension (minor), Inhibitory Control (minor)"
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro measures expert-level multimodal understanding across many disciplines, requiring joint reasoning over images (e.g., diagrams, plots, figures) and text questions. It emphasizes cross-domain problem solving, careful visual interpretation, and integrating visual evidence with language and prior knowledge.","Visual Perception, Scene Understanding & Visual Reasoning, Multisensory Integration, Language Comprehension, Logical Reasoning, Attention (minor), Working Memory"
OmniDocBench 1.5,https://github.com/opendatalab/OmniDocBench,https://arxiv.org/abs/2412.07626,"OmniDocBench 1.5 evaluates document understanding and OCR-centric capabilities across complex page elements such as text, tables, formulas, and reading order. It targets robustness to layout, typography, and structured content where correct extraction depends on both perception and document-level organization.","Visual Perception, Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Semantic Understanding & Context Recognition, Working Memory (minor)"
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU evaluates multimodal reasoning over video, where solving questions depends on understanding actions, events, and temporal relationships across frames along with accompanying text. It probes whether models can integrate evidence over time rather than relying on single-frame cues.","Visual Perception, Scene Understanding & Visual Reasoning, Cognitive Timing & Predictive Modeling, Working Memory, Attention, Multisensory Integration (minor)"
LiveCodeBench Pro,https://livecodebenchpro.com/projects/livecodebench-pro/leaderboard,https://arxiv.org/abs/2506.11928,"LiveCodeBench Pro evaluates coding performance on challenging programming tasks, typically emphasizing correctness under time-like constraints and robustness across problem styles. It targets algorithmic reasoning, implementation quality, and iterative debugging behavior that resembles real coding workflows.","Logical Reasoning, Planning, Adaptive Error Correction, Working Memory, Language Production (minor), Language Comprehension (minor)"
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"FACTS Benchmark Suite evaluates factuality and grounding-related behaviors across a collection of tests designed to stress reliable truthfulness and resistance to hallucination. It focuses on whether a model can maintain factual consistency, appropriately express uncertainty, and avoid fabricating unsupported claims.","Semantic Understanding & Context Recognition, Inhibitory Control, Working Memory (minor), Logical Reasoning (minor), Self-reflection (minor)"
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA evaluates physical commonsense and practical reasoning across many languages, using non-parallel items to avoid trivial translation artifacts. It emphasizes understanding everyday physical interactions and selecting plausible outcomes based on intuitive physics and common sense.","Semantic Understanding & Context Recognition, Logical Reasoning, Language Comprehension, Spatial Representation & Mapping (minor)"
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) is a long-context retrieval and multi-round co-reference style evaluation where multiple similar “needle” requests are embedded within long “haystacks,” and the model must reproduce the correct referenced response. It stresses faithful integration of distant context and resistance to confusion among near-duplicates.","Working Memory, Attention, Episodic Memory, Semantic Understanding & Context Recognition"
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval evaluates economically valuable, well-specified professional knowledge-work tasks across many occupations, often requiring the creation of artifacts such as plans, spreadsheets, or presentations. It emphasizes end-to-end task execution quality as judged against expert professionals, including following requirements and producing usable deliverables.","Planning, Decision-making, Language Production, Semantic Understanding & Context Recognition, Working Memory, Adaptive Error Correction (minor), Social Reasoning & Theory of Mind (minor)"
SWE-Lancer,https://openai.com/index/swe-lancer/,https://arxiv.org/abs/2502.12115,"SWE-Lancer evaluates software engineering capability on realistic tasks that require making correct changes in a codebase and producing patches that meet requirements and tests. It targets agentic coding behaviors such as repository understanding, issue resolution, and reliable end-to-end completion under constraints.","Planning, Adaptive Error Correction, Logical Reasoning, Working Memory, Language Comprehension (minor)"
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath evaluates advanced mathematics problem solving, including difficult and less-memorization-prone items intended to probe genuine reasoning. It emphasizes multi-step derivations, formal manipulation, and sustained accuracy on expert-level math questions, sometimes with tool assistance depending on setup.","Logical Reasoning, Working Memory, Planning, Adaptive Error Correction (minor), Inhibitory Control (minor)"
