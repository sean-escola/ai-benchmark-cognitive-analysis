Benchmark,Website,Paper,Description,Cognitive Functions
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro evaluates software engineering agents on real-world GitHub issues by asking them to produce patches that make a target repository’s tests pass. Compared to SWE-bench Verified, it is designed to be harder and more contamination-resistant, and includes multiple programming languages and more complex engineering work (e.g., debugging, feature fixes, refactors).","Planning, Logical Reasoning, Adaptive Error Correction, Working Memory, Language Comprehension (minor), Language Production (minor)"
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld measures an agent’s ability to use a computer operating system through a GUI to complete multi-step tasks (e.g., navigating apps, editing files, changing settings). Agents must perceive screenshots, decide where to click/type, and recover from errors in a realistic, partially observable interactive environment.","Visual Perception, Scene Understanding & Visual Reasoning, Planning, Decision-making, Sensorimotor Coordination, Visual Attention & Eye Movements, Working Memory, Adaptive Error Correction (minor)"
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,ARC-AGI is a “fluid intelligence” benchmark based on abstract grid transformation puzzles where models infer rules from only a few examples and must generalize to a novel input. It aims to reduce reliance on memorized knowledge by emphasizing compositional pattern induction and transfer.,"Logical Reasoning, Cognitive Flexibility, Working Memory, Spatial Representation & Mapping, Attention (minor), Planning (minor)"
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon autonomy by placing an agent in a simulated year-long vending machine business with a starting budget. Success requires sustained coherence across thousands of decisions (inventory, pricing, supplier interaction) under changing market conditions.","Planning, Decision-making, Reward Mechanisms, Working Memory, Adaptive Error Correction, Semantic Understanding & Context Recognition (minor)"
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas benchmarks real-world tool use via the Model Context Protocol (MCP), where models must discover appropriate tools, call them with correct arguments, and combine results across multi-step workflows. It emphasizes robustness to API/tool errors, retries, and cross-tool coordination rather than pure text QA.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Language Comprehension (minor), Language Production (minor), Semantic Understanding & Context Recognition"
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity agents on tasks involving real software vulnerabilities at scale, including identifying known weaknesses and (in some settings) discovering new ones. The benchmark probes practical, multi-step security reasoning with codebases and exploit/patch-style workflows under time and tooling constraints.","Logical Reasoning, Planning, Adaptive Error Correction, Working Memory, Attention (minor), Inhibitory Control (minor)"
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam (HLE) is a difficult multimodal benchmark intended to sit near the frontier of human knowledge, spanning many domains and question styles. It tests whether models can integrate knowledge with multi-step reasoning (and, in some reported settings, benefit from tool use such as search or code).","Language Comprehension, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Visual Perception (minor), Scene Understanding & Visual Reasoning (minor)"
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,GPQA Diamond is a high-quality subset of the GPQA benchmark consisting of very challenging graduate-level science multiple-choice questions designed to be “Google-proof.” It is meant to stress deep scientific reasoning and careful reading rather than shallow retrieval or pattern matching.,"Language Comprehension, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory (minor)"
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro is a multimodal benchmark for expert-level understanding and reasoning across many disciplines, using problems that require combining text with images such as diagrams, charts, and scientific figures. The “Pro” setting is intended to be harder and more realistic for frontier evaluation than earlier MMMU variants.","Visual Perception, Scene Understanding & Visual Reasoning, Language Comprehension, Logical Reasoning, Working Memory, Spatial Representation & Mapping (minor)"
OmniDocBench 1.5,https://github.com/opendatalab/OmniDocBench,https://arxiv.org/abs/2412.07626,"OmniDocBench 1.5 evaluates document understanding/OCR systems on end-to-end extraction from complex documents, including text, formulas, tables, and reading order. It targets robustness to varied layouts and requires structured reconstruction rather than just recognizing isolated text snippets.","Visual Perception, Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Language Production, Language Comprehension (minor), Working Memory (minor)"
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU extends multimodal reasoning to the video domain, requiring models to answer questions that depend on temporal events, actions, and context across frames. It probes whether models can track and integrate information over time rather than relying on a single key frame.","Visual Perception, Scene Understanding & Visual Reasoning, Working Memory, Attention, Cognitive Timing & Predictive Modeling (minor), Multisensory Integration (minor)"
LiveCodeBench Pro,https://livecodebenchpro.com/projects/livecodebench-pro/leaderboard,https://arxiv.org/abs/2506.11928,"LiveCodeBench Pro evaluates coding capability on programming tasks where solutions are executed and scored (often via unit tests), aiming to reflect realistic coding performance under constraints. It is commonly summarized via aggregate success metrics and leaderboard-style ratings to compare models’ practical code generation and debugging ability.","Language Production, Logical Reasoning, Planning, Adaptive Error Correction, Working Memory, Language Comprehension (minor)"
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"The FACTS Benchmark Suite systematically evaluates factuality-related behaviors of language models, including correctness, consistency, and grounding under different prompt and context conditions. It is intended to characterize hallucination tendencies and reliability as a distinct capability dimension, not just overall task accuracy.","Semantic Understanding & Context Recognition, Inhibitory Control, Self-reflection (minor), Logical Reasoning (minor), Working Memory (minor)"
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA is a multilingual/non-parallel extension of physical commonsense reasoning questions, aimed at measuring whether models generalize “everyday physics” and intuitive action-effect knowledge across languages and cultures. It emphasizes understanding plausible interactions with objects and environments rather than factual recall.","Language Comprehension, Logical Reasoning, Semantic Understanding & Context Recognition, Spatial Representation & Mapping (minor)"
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) is a long-context evaluation where multiple similar “needle” requests are embedded within long “haystacks,” and the model must retrieve the correct referenced response across rounds. It stresses long-range dependency tracking, resistance to distractors, and faithful reproduction of the targeted span.","Working Memory, Attention, Episodic Memory (minor), Language Comprehension, Semantic Understanding & Context Recognition"
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval evaluates well-specified professional “knowledge work” by having models produce real work artifacts (e.g., spreadsheets, slides, plans) across many occupations, judged against outputs from human professionals. The focus is on end-to-end task execution quality, including following specifications, producing usable deliverables, and making sound tradeoffs.","Planning, Decision-making, Language Production, Semantic Understanding & Context Recognition, Working Memory, Adaptive Error Correction (minor), Social Reasoning & Theory of Mind (minor)"
SWE-Lancer,https://openai.com/index/swe-lancer/,https://arxiv.org/abs/2502.12115,"SWE-Lancer evaluates software engineering performance on realistic tasks framed around delivering changes in codebases, often emphasizing end-to-end completion quality rather than isolated algorithmic puzzles. It aims to capture practical engineering behaviors such as understanding requirements, implementing correct patches, and maintaining code quality.","Planning, Logical Reasoning, Adaptive Error Correction, Working Memory, Language Comprehension (minor), Language Production (minor)"
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath measures expert-level mathematical problem solving on difficult, research-adjacent or olympiad-style questions, often requiring multi-step derivations and careful symbolic reasoning. It is designed to distinguish strong reasoning models from those that rely on shallow heuristics, and can optionally be paired with tools like Python in some setups.","Logical Reasoning, Working Memory, Planning, Cognitive Flexibility (minor), Adaptive Error Correction (minor)"
