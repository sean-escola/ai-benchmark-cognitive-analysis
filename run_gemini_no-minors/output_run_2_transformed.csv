Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates software engineering agents on real GitHub issues where a model must produce a code patch that makes the repository’s tests pass and resolves the described bug or feature request. The “Verified” subset consists of problems validated by human annotators to be solvable given the repository state and task description, emphasizing end-to-end coding in realistic codebases.","L1: Language Comprehension (minor)
L2: Planning, Adaptive Error Correction, Working Memory, Logical Reasoning, Semantic Understanding & Context Recognition, Decision-making (minor)
L3: ",L2
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 measures how well an agent can complete practical tasks in a command-line environment (e.g., installing tools, manipulating files, running programs, debugging failures) under a fixed harness. It stresses multi-step tool use, iterative troubleshooting, and maintaining task state across longer interactions.","L1: Language Comprehension (minor)
L2: Planning, Adaptive Error Correction, Decision-making, Working Memory, Attention (minor)
L3: Inhibitory Control (minor)",L2
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench evaluates agentic tool use in multi-turn customer-support-style environments (e.g., retail, airline, telecom) where the agent must interact with a simulated user and programmatic APIs while following domain policies. Success requires consistent policy adherence across turns, correct API usage, and robust dialog management when the user’s goals conflict with constraints.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Working Memory (minor)
L3: Social Reasoning & Theory of Mind, Inhibitory Control",L3
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI is a “fluid intelligence” benchmark of abstract rule induction from a few examples, typically presented as input–output grid transformations. Models must infer the hidden transformation rule from 2–3 demonstrations and generalize it to a new input, penalizing reliance on memorized templates and rewarding flexible pattern discovery.","L1: Visual Perception
L2: Logical Reasoning, Working Memory, Spatial Representation & Mapping, Scene Understanding & Visual Reasoning, Attention (minor)
L3: Cognitive Flexibility",L3
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon autonomy by having an agent manage a simulated vending machine business over an extended period, making thousands of decisions under budget and market constraints. Performance is typically measured by final profit, requiring coherent strategy, adaptation to feedback, and sustained goal pursuit across long trajectories.","L1: 
L2: Planning, Decision-making, Reward Mechanisms, Working Memory, Adaptive Error Correction (minor)
L3: Cognitive Timing & Predictive Modeling, Self-reflection (minor)",L3
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam (HLE) is a difficult, broad-coverage benchmark intended to probe frontier knowledge and reasoning across many domains, with many items designed to be non-trivial even for strong models. It often includes tasks that benefit from careful multi-step reasoning and, in tool-enabled settings, selecting and using external tools like search or code execution appropriately.","L1: Language Comprehension, Visual Perception (minor)
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Planning (minor)
L3: ",L2
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 consists of competition-style mathematics problems that require multi-step derivations, clever transformations, and precise symbolic reasoning. As an evaluation, it primarily tests a model’s ability to sustain correct intermediate reasoning without tool assistance (unless explicitly allowed).","L1: 
L2: Logical Reasoning, Working Memory, Attention (minor), Adaptive Error Correction (minor)
L3: Cognitive Flexibility",L3
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a curated subset of very challenging, “Google-proof” graduate-level multiple-choice science questions (physics, chemistry, biology) designed to reduce easy lookup and emphasize genuine understanding. The Diamond subset prioritizes high-quality items where domain experts succeed but non-experts commonly fail.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory
L3: Inhibitory Control (minor)",L2
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends MMLU-style academic subject questions into multiple languages, measuring whether models retain knowledge and reasoning competence beyond English. It emphasizes multilingual comprehension of technical prompts and consistent selection among answer options across diverse language contexts.","L1: Language Comprehension, Language Production (minor)
L2: Semantic Understanding & Context Recognition, Logical Reasoning (minor), Working Memory (minor)
L3: ",L2
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro is a harder, more contamination-resistant version of MMMU that evaluates multimodal expert reasoning across many disciplines using images (e.g., diagrams, charts, figures) paired with text questions. It targets not only recognition but also higher-level visual reasoning, quantitative interpretation, and cross-referencing visual evidence with textual constraints.","L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Multisensory Integration, Spatial Representation & Mapping, Working Memory (minor)
L3: ",L2
MathArena Apex,https://matharena.ai/?view=problem&comp=apex--apex_2025,https://arxiv.org/abs/2505.23281,"MathArena Apex is a difficult math benchmark suite that aggregates advanced problems intended to separate strong mathematical reasoners, often emphasizing long solution chains and careful case analysis. It is commonly used to compare models under standardized conditions (tool-free or tool-assisted, depending on the evaluation setting).","L1: 
L2: Logical Reasoning, Working Memory, Planning (minor), Adaptive Error Correction (minor)
L3: Cognitive Flexibility",L3
ScreenShot-Pro,https://gui-agent.github.io/grounding-leaderboard/,https://arxiv.org/abs/2504.07981,"ScreenShot-Pro evaluates visual grounding on real software screenshots, where models must understand GUI elements, their spatial layout, and sometimes infer the correct target for an intended action. It emphasizes precise localization and interpretation of UI affordances rather than generic image captioning.","L1: Visual Perception
L2: Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Spatial Representation & Mapping, Decision-making (minor), Sensorimotor Coordination (minor)
L3: ",L2
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning tests whether a model can answer questions about scientific figures (often from arXiv-style papers), requiring chart reading, quantitative extraction, and reasoning over plotted relationships. Many items require integrating figure content with the question’s textual constraints and performing multi-step inference rather than surface recognition.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Semantic Understanding & Context Recognition, Logical Reasoning, Multisensory Integration, Working Memory (minor)
L3: ",L2
OmniDocBench 1.5,https://github.com/opendatalab/OmniDocBench,https://arxiv.org/abs/2412.07626,"OmniDocBench 1.5 evaluates document understanding and OCR across heterogeneous document elements such as text blocks, tables, formulas, and reading order. It probes whether models can faithfully extract structured content and preserve layout-sensitive semantics instead of producing fluent but inaccurate reconstructions.","L1: Visual Perception, Language Comprehension
L2: Attention, Scene Understanding & Visual Reasoning, Semantic Understanding & Context Recognition, Working Memory (minor)
L3: ",L2
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU extends multimodal understanding to time-varying video, requiring models to reason over events, actions, and temporal dependencies across frames. Tasks often require aggregating evidence across time, tracking entities, and answering questions that depend on what changed and when.","L1: Visual Perception
L2: Working Memory, Attention, Scene Understanding & Visual Reasoning, Multisensory Integration (minor)
L3: Cognitive Timing & Predictive Modeling",L3
LiveCodeBench Pro,https://livecodebenchpro.com/projects/livecodebench-pro/leaderboard,https://arxiv.org/abs/2506.11928,"LiveCodeBench Pro evaluates coding ability in settings closer to real development: reading a spec, writing correct code, and often handling iterative constraints akin to interactive programming. It is designed to reduce simple memorization effects and better reflect practical software-building competence under time/attempt constraints.","L1: Language Comprehension (minor)
L2: Planning, Logical Reasoning, Adaptive Error Correction, Working Memory, Decision-making (minor)
L3: ",L2
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"The FACTS Benchmark Suite systematically evaluates factuality by testing whether a model’s generated statements remain grounded and correct across diverse factuality-related tasks. It emphasizes avoiding hallucinations, preserving correctness under paraphrase and compositional prompts, and calibrating outputs when evidence is insufficient.","L1: Language Production (minor)
L2: Semantic Understanding & Context Recognition, Working Memory (minor)
L3: Inhibitory Control, Self-reflection",L3
SimpleQA Verified,https://www.kaggle.com/benchmarks/deepmind/simpleqa-verified,https://arxiv.org/abs/2509.07968,"SimpleQA Verified is a short-form question answering benchmark focused on factual, verifiable queries with strong attention to answer correctness and evaluation reliability. It targets precision on simple-looking questions that can expose subtle hallucination, overconfidence, or entity/attribute confusion.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition
L3: Inhibitory Control (minor), Self-reflection (minor)",L2
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA evaluates commonsense physical reasoning across languages and cultures, aiming to test whether models can select plausible actions or explanations in everyday situations without relying on English-centric priors. It emphasizes robust understanding of physical affordances and pragmatic context under multilingual phrasing.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning
L3: Social Reasoning & Theory of Mind (minor), Cognitive Flexibility (minor)",L2
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) is a long-context multi-round coreference resolution benchmark where multiple similar “needle” interactions are embedded within a long “haystack,” and the model must retrieve the correct referenced response. The 8-needle setting stresses maintaining and selecting among many competing references spread across long contexts.","L1: Language Comprehension
L2: Working Memory, Attention, Episodic Memory, Planning (minor)
L3: Inhibitory Control (minor)",L2
