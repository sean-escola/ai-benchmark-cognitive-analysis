Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates software-engineering agents on real GitHub issues by asking them to produce a code patch that makes the repository’s tests pass. The Verified subset uses tasks that have been human-checked for solvability and clearer evaluation, emphasizing end-to-end debugging, editing, and validation under realistic constraints.","L1: Language Comprehension (minor)
L2: Planning, Adaptive Error Correction, Logical Reasoning, Working Memory, Decision-making (minor)
L3: ",L2
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 measures an agent’s ability to complete practical tasks in a command-line environment (e.g., file manipulation, package/tool use, scripting, and diagnosing failures). It stresses iterative interaction: interpreting outputs, choosing next commands, and recovering from errors under time/step constraints.","L1: Language Comprehension (minor)
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory
L3: Inhibitory Control (minor)",L2
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench evaluates interactive tool-using customer-support agents in simulated domains (e.g., retail, airline, telecom) that must follow policies while using APIs over multiple turns. It probes robustness to user behavior, policy compliance, and sustained task execution across long dialogues with tool calls.","L1: Language Comprehension, Language Production
L2: Decision-making, Planning, Working Memory (minor)
L3: Social Reasoning & Theory of Mind, Inhibitory Control",L3
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI tests few-shot abstract reasoning on novel grid-based pattern transformation tasks, where models infer hidden rules from a small number of input–output examples. It is designed to emphasize generalization and problem solving under distribution shift rather than memorized domain knowledge.","L1: 
L2: Logical Reasoning, Working Memory, Spatial Representation & Mapping, Scene Understanding & Visual Reasoning, Attention (minor)
L3: Cognitive Flexibility",L3
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon agent competence in a simulated vending-machine business, requiring inventory management, vendor negotiation, pricing, and adaptation over many steps. The score is typically tied to business outcomes (e.g., final balance), rewarding coherent strategy and error recovery over extended trajectories.","L1: 
L2: Planning, Decision-making, Reward Mechanisms, Adaptive Error Correction, Working Memory
L3: Cognitive Timing & Predictive Modeling (minor), Motivational Drives (minor)",L2
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a high-difficulty, broad-coverage benchmark intended to approximate frontier academic and professional reasoning across many domains, often including multimodal questions. It emphasizes solving novel, knowledge-intensive problems and integrating information, sometimes with optional tool use depending on the evaluation setup.","L1: Language Comprehension, Visual Perception (minor)
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Scene Understanding & Visual Reasoning (minor)
L3: ",L2
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 evaluates competition-style mathematics problem solving on the 2025 AIME set, typically scored by exact final answers. It probes multi-step derivations, algebraic/number-theoretic reasoning, and careful constraint handling under limited context.","L1: 
L2: Logical Reasoning, Working Memory, Planning, Adaptive Error Correction (minor), Attention (minor)
L3: ",L2
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a curated subset of GPQA designed to be especially difficult and resistant to superficial lookup, using graduate-level science multiple-choice questions. It emphasizes deep conceptual understanding and multi-step scientific reasoning rather than rote recall.","L1: Language Comprehension
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory (minor)
L3: Inhibitory Control (minor)",L2
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends broad academic knowledge testing to multiple languages, covering many subjects with standardized question formats. It targets cross-lingual generalization and the ability to apply knowledge and reasoning consistently across linguistic contexts.","L1: Language Comprehension, Language Production (minor)
L2: Semantic Understanding & Context Recognition, Logical Reasoning (minor), Working Memory (minor)
L3: ",L2
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro is a more challenging variant of MMMU for multimodal expert reasoning, combining text with images from diverse disciplines (e.g., charts, diagrams, documents). It emphasizes grounded visual understanding plus domain reasoning, often with stricter protocols to reduce shortcut solving.","L1: Visual Perception, Language Comprehension (minor)
L2: Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Attention (minor)
L3: ",L2
MathArena Apex,https://matharena.ai/?view=problem&comp=apex--apex_2025,https://arxiv.org/abs/2505.23281,"MathArena Apex is a hard mathematics benchmark collection intended to stress advanced problem solving beyond standard contest questions, often evaluated with consistent prompting and scoring across models. It prioritizes difficult multi-step reasoning, proof-like planning, and robustness to tricky edge cases.","L1: 
L2: Logical Reasoning, Planning, Working Memory, Adaptive Error Correction (minor), Attention (minor)
L3: ",L2
ScreenShot-Pro,https://gui-agent.github.io/grounding-leaderboard/,https://arxiv.org/abs/2504.07981,"ScreenShot-Pro evaluates GUI understanding and grounding from screenshots, typically requiring models to identify interface elements, interpret layout, and answer questions or propose actions based on what is visible. It stresses high-resolution visual reading, spatial reasoning over UI structure, and mapping intent to interface affordances.","L1: Visual Perception
L2: Visual Attention & Eye Movements, Spatial Representation & Mapping, Scene Understanding & Visual Reasoning, Planning (minor), Decision-making (minor)
L3: ",L2
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning tests reasoning over figures from scientific papers (e.g., plots, diagrams, and experimental results) paired with questions that require interpretation and inference. It emphasizes extracting quantitative/structural information from visuals and integrating it with scientific context in text.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory (minor), Attention (minor)
L3: ",L2
OmniDocBench 1.5,https://github.com/opendatalab/OmniDocBench,https://arxiv.org/abs/2412.07626,"OmniDocBench 1.5 evaluates document understanding and OCR-centric capabilities across heterogeneous layouts, including text blocks, tables, formulas, and reading order. Scores commonly reflect edit distance or structure-aware metrics, emphasizing faithful extraction and layout-aware interpretation.","L1: Visual Perception, Language Comprehension (minor)
L2: Scene Understanding & Visual Reasoning, Attention, Spatial Representation & Mapping (minor)
L3: ",L2
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU extends multimodal reasoning to video, requiring models to answer questions that depend on temporal visual information (events, actions, and scene changes) along with accompanying text. It emphasizes integrating cues across frames and maintaining coherence over time.","L1: Visual Perception
L2: Multisensory Integration, Working Memory, Scene Understanding & Visual Reasoning, Attention (minor)
L3: Cognitive Timing & Predictive Modeling",L3
LiveCodeBench Pro,https://livecodebenchpro.com/projects/livecodebench-pro/leaderboard,https://arxiv.org/abs/2506.11928,"LiveCodeBench Pro evaluates code generation and problem solving on programming tasks designed for strong contamination resistance and realistic difficulty, often using hidden or time-split test sets. It focuses on producing correct, executable solutions under single-try constraints and diverse problem types.","L1: Language Comprehension (minor), Language Production (minor)
L2: Logical Reasoning, Planning, Adaptive Error Correction, Working Memory
L3: ",L2
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"FACTS Benchmark Suite measures factuality and grounding of model outputs across multiple subtests that probe whether responses align with provided evidence and real-world facts. It emphasizes detecting and avoiding hallucinations, maintaining consistency, and appropriately expressing uncertainty when information is missing.","L1: Language Comprehension (minor), Language Production (minor)
L2: Semantic Understanding & Context Recognition, Working Memory (minor)
L3: Inhibitory Control, Self-reflection",L3
SimpleQA Verified,https://www.kaggle.com/benchmarks/deepmind/simpleqa-verified,https://arxiv.org/abs/2509.07968,SimpleQA Verified is a short-form factual question answering benchmark with verification procedures aimed at reducing annotation noise and improving reliability of correctness labels. It targets precise retrieval-like knowledge and the ability to avoid confidently stating incorrect facts.,"L1: Language Comprehension (minor)
L2: Semantic Understanding & Context Recognition
L3: Inhibitory Control, Self-reflection (minor)",L3
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA evaluates physical commonsense reasoning across many languages using non-parallel or culturally varied formulations, aiming to test whether models can generalize practical “how-to”/affordance knowledge beyond English. It emphasizes grounded reasoning about everyday physical interactions and plausible outcomes.","L1: Language Comprehension (minor)
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Spatial Representation & Mapping (minor), Sensorimotor Coordination (minor)
L3: Cognitive Flexibility (minor)",L2
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) tests long-context multi-round co-reference and retrieval by embedding multiple similar “needle” interactions within long “haystack” conversations and asking models to reproduce the correct referenced content. The 8-needle setting stresses sustained attention, interference resistance, and accurate recall under heavy distractors.","L1: Language Comprehension
L2: Working Memory, Attention, Episodic Memory
L3: Inhibitory Control, Cognitive Timing & Predictive Modeling (minor)",L3
