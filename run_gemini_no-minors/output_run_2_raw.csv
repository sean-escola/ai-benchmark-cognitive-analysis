Benchmark,Website,Paper,Description,Cognitive Functions
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro is a large-scale software engineering benchmark where a model must generate patches that resolve real issues in open-source repositories under realistic constraints (tests, dependencies, and project conventions). It expands beyond Python and is designed to be more challenging and more resistant to contamination than earlier SWE-bench variants.","Logical Reasoning, Planning, Adaptive Error Correction, Working Memory, Decision-making, Language Comprehension (minor), Language Production (minor)"
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld evaluates multimodal “computer use” by requiring an agent to complete tasks in a full desktop operating-system environment using screenshots and UI interactions (e.g., clicking, typing, navigation). Success requires reliably perceiving interfaces, choosing actions over many steps, and recovering from tool/interaction errors.","Visual Perception, Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Sensorimotor Coordination, Planning, Decision-making, Working Memory, Inhibitory Control (minor)"
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,ARC-AGI is a fluid reasoning benchmark based on abstract grid transformation puzzles where the model must infer a latent rule from a few input–output examples and apply it to a new input. It emphasizes generalization to novel patterns with minimal task-specific prior exposure.,"Logical Reasoning, Cognitive Flexibility, Working Memory, Spatial Representation & Mapping, Attention (minor)"
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 measures long-horizon agent performance by simulating operation of a vending machine business over an extended time period, scoring by final balance/profit. The agent must make coherent decisions across many steps, adapting to changing conditions (inventory, suppliers, pricing) without losing goal consistency.","Planning, Decision-making, Reward Mechanisms, Working Memory, Adaptive Error Correction, Semantic Understanding & Context Recognition (minor), Motivational Drives (minor)"
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas evaluates real-world tool use via the Model Context Protocol by testing whether a model can discover, invoke, and chain tools across multi-step workflows. Tasks require correct API selection, parameterization, error handling, and synthesis of results into a final answer.","Planning, Decision-making, Working Memory, Adaptive Error Correction, Language Comprehension (minor), Language Production (minor), Inhibitory Control (minor)"
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates agentic cybersecurity capabilities on tasks grounded in real software vulnerabilities, including identifying known vulnerabilities and, in some settings, discovering new ones. It stresses iterative investigation, hypothesis testing, and producing actionable outputs (e.g., proof-of-concept reasoning or patch-level changes).","Logical Reasoning, Planning, Adaptive Error Correction, Decision-making, Working Memory, Language Comprehension (minor)"
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a multimodal benchmark of difficult questions intended to sit near the frontier of human knowledge, spanning many domains and often requiring multi-step reasoning. It is commonly evaluated both without tools and with tools (e.g., search and code execution), testing end-to-end problem solving under realistic assistance settings.","Language Comprehension, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Visual Perception (minor), Scene Understanding & Visual Reasoning (minor)"
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,GPQA Diamond is a subset of GPQA consisting of high-quality graduate-level science multiple-choice questions that are designed to be difficult to answer via superficial pattern matching or simple web search. It probes deep conceptual understanding and careful multi-step elimination among plausible distractors.,"Language Comprehension, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Decision-making (minor)"
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro is a challenging multimodal benchmark covering expert-level questions across diverse disciplines, where models must interpret images (figures, diagrams, charts) alongside text. It emphasizes robust visual grounding and reasoning rather than surface recognition alone.","Visual Perception, Scene Understanding & Visual Reasoning, Language Comprehension, Logical Reasoning, Working Memory, Spatial Representation & Mapping (minor)"
OmniDocBench 1.5,https://github.com/opendatalab/OmniDocBench,https://arxiv.org/abs/2412.07626,"OmniDocBench 1.5 evaluates document understanding/OCR by measuring how accurately models transcribe and structure content from complex documents (text, tables, formulas, and reading order). It focuses on faithful extraction under layout complexity rather than purely conversational ability.","Visual Perception, Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Spatial Representation & Mapping, Language Comprehension (minor), Working Memory (minor)"
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU extends multimodal evaluation to videos, requiring models to integrate information across time to answer questions about events, actions, and causal/temporal relations. It tests whether an agent can maintain and update hypotheses as new frames provide additional context.","Visual Perception, Attention, Working Memory, Cognitive Timing & Predictive Modeling, Scene Understanding & Visual Reasoning, Language Comprehension (minor), Auditory Processing (minor)"
LiveCodeBench Pro,https://livecodebenchpro.com/projects/livecodebench-pro/leaderboard,https://arxiv.org/abs/2506.11928,"LiveCodeBench Pro measures competitive-style coding performance on fresh, time-indexed programming problems, often reported via an ELO-style rating. It targets reliable code synthesis and debugging under constraints that discourage memorization of static benchmark items.","Language Production, Logical Reasoning, Planning, Adaptive Error Correction, Working Memory, Decision-making (minor)"
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"The FACTS Benchmark Suite systematically evaluates factuality by testing whether model outputs remain accurate, appropriately grounded, and resistant to hallucination across multiple factuality-related tasks. It is designed to separate true helpfulness from fluent but unsupported generation.","Semantic Understanding & Context Recognition, Inhibitory Control, Working Memory, Language Production, Adaptive Error Correction (minor), Self-reflection (minor)"
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA evaluates physical commonsense and everyday procedural reasoning across many languages, aiming to measure whether models preserve commonsense competence under multilingual expression. Items typically require selecting or generating plausible actions/explanations grounded in basic physical constraints and affordances.","Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory (minor), Cognitive Flexibility (minor)"
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 is a long-context multi-round coreference/retrieval benchmark where multiple similar “needle” interactions are embedded in a large “haystack,” and the model must reproduce the correct response corresponding to a specified needle. The 8-needle variant stresses robustness when many near-duplicate candidates compete for attention across long contexts.","Working Memory, Attention, Episodic Memory, Language Comprehension, Inhibitory Control (minor)"
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval evaluates economically valuable professional knowledge work by having models produce real work artifacts (e.g., spreadsheets, presentations, schedules) across many occupations, with expert judges comparing outputs to human professionals. It emphasizes end-to-end execution quality, instruction following, and practical decision-making under constraints.","Planning, Decision-making, Language Production, Semantic Understanding & Context Recognition, Working Memory, Adaptive Error Correction (minor), Social Reasoning & Theory of Mind (minor)"
SWE-Lancer,https://openai.com/index/swe-lancer/,https://arxiv.org/abs/2502.12115,"SWE-Lancer is a software engineering evaluation focused on end-to-end task completion in realistic coding workflows, emphasizing producing correct, reviewable changes that satisfy a task specification. It is intended to reflect practical engineering outcomes more directly than isolated coding puzzles.","Planning, Adaptive Error Correction, Logical Reasoning, Working Memory, Decision-making, Language Comprehension (minor), Language Production (minor)"
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath is a difficult mathematics benchmark targeting advanced, research-adjacent problem solving, often reported by tier to reflect increasing difficulty. It emphasizes sustained multi-step derivations, careful symbolic manipulation, and (when allowed) effective use of computation tools to verify or explore solutions.","Logical Reasoning, Working Memory, Planning, Adaptive Error Correction, Cognitive Flexibility (minor), Cognitive Timing & Predictive Modeling (minor)"
