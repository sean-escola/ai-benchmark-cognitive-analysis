Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro evaluates software engineering agents on realistic, multi-file repository tasks that require producing patches which satisfy hidden tests and project constraints. It emphasizes end-to-end debugging and implementation under industrially representative conditions, including dependency management and adherence to existing code conventions.","L1: Language Comprehension (minor)
L2: Planning, Adaptive Error Correction, Logical Reasoning, Working Memory, Decision-making (minor)
L3: ",L2
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld measures an agent’s ability to operate a computer-like environment by perceiving GUI state (screenshots/DOM) and executing actions (clicks, typing, shortcuts) to complete user goals. Tasks stress robust tool use over long action sequences with partial observability and frequent need for recovery from mistakes.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Sensorimotor Coordination, Planning, Attention, Working Memory (minor), Adaptive Error Correction (minor)
L3: ",L2
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI tests “fluid” abstract reasoning by requiring models to infer latent rules from a few input–output grid examples and generalize to a new grid. It is designed to minimize reliance on memorized knowledge and instead probe rapid induction, compositionality, and robust generalization.","L1: 
L2: Logical Reasoning, Working Memory, Spatial Representation & Mapping, Attention (minor)
L3: Cognitive Flexibility",L3
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon autonomous agency by having a model manage a simulated vending-machine business over many decisions (inventory, pricing, supplier negotiation, budgeting) to maximize final profit. Success requires sustained coherence, strategy adaptation to changing conditions, and consistent execution across a lengthy trajectory.","L1: 
L2: Planning, Decision-making, Reward Mechanisms, Working Memory, Adaptive Error Correction (minor)
L3: Motivational Drives (minor)",L2
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas evaluates real-world tool use via the Model Context Protocol (MCP), where models must discover relevant tools, call them correctly, handle errors, and compose multi-step workflows. It targets practical agent reliability: selecting the right API, sequencing calls, and integrating tool outputs into a final response.","L1: Language Comprehension (minor), Language Production (minor)
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory
L3: ",L2
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity agents on tasks such as identifying known vulnerabilities in real codebases from high-level descriptions and discovering previously unknown issues. It stresses reasoning about software behavior, navigating repositories, and producing correct, actionable findings under realistic constraints.","L1: Language Comprehension (minor)
L2: Logical Reasoning, Planning, Adaptive Error Correction, Working Memory, Decision-making (minor)
L3: ",L2
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a broad, frontier-knowledge benchmark spanning challenging questions across domains, often requiring multi-step reasoning and careful interpretation of problem statements. Its multimodal variant additionally requires extracting and integrating information from images alongside text.","L1: Language Comprehension, Visual Perception (minor)
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Scene Understanding & Visual Reasoning (minor)
L3: ",L2
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a high-difficulty subset of graduate-level science multiple-choice questions intended to be resistant to shallow pattern matching and simple web lookup. It probes careful reading, domain reasoning, and selection among closely competing answer choices.","L1: Language Comprehension
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory (minor), Decision-making (minor)
L3: ",L2
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro evaluates expert-level multimodal understanding across disciplines, requiring models to answer questions that combine text with diagrams, charts, screenshots, and other visuals. It emphasizes grounded reasoning from visual evidence, not just caption-level recognition.","L1: Visual Perception, Language Comprehension
L2: Multisensory Integration, Scene Understanding & Visual Reasoning, Logical Reasoning, Attention (minor)
L3: ",L2
OmniDocBench 1.5,https://github.com/opendatalab/OmniDocBench,https://arxiv.org/abs/2412.07626,"OmniDocBench 1.5 evaluates document AI systems on OCR and document understanding, including text recognition, formulas, tables, and reading order. It targets robust perception of complex layouts and faithful reconstruction/structuring of document content.","L1: Visual Perception, Language Comprehension (minor)
L2: Scene Understanding & Visual Reasoning, Spatial Representation & Mapping, Attention, Working Memory (minor)
L3: ",L2
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU measures multimodal reasoning over video, where the model must integrate information across frames and time to answer questions about events, actions, and causal relationships. It stresses temporal integration and maintaining coherent hypotheses as new visual evidence appears.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Working Memory, Attention, Multisensory Integration (minor)
L3: Cognitive Timing & Predictive Modeling",L3
LiveCodeBench Pro,https://livecodebenchpro.com/projects/livecodebench-pro/leaderboard,https://arxiv.org/abs/2506.11928,"LiveCodeBench Pro evaluates coding capability on contemporary programming problems, emphasizing correct executable solutions under time-evolving, contamination-aware conditions. It captures the ability to understand task specs, implement algorithms, and debug until passing tests.","L1: Language Comprehension (minor), Language Production (minor)
L2: Logical Reasoning, Planning, Adaptive Error Correction, Working Memory
L3: ",L2
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"The FACTS Benchmark Suite systematically evaluates factuality in LLM outputs, including whether models produce accurate statements, avoid unsupported claims, and properly use provided evidence. It targets robustness against hallucination and the ability to regulate generation when uncertain.","L1: 
L2: Semantic Understanding & Context Recognition, Working Memory (minor), Logical Reasoning (minor), Episodic Memory (minor)
L3: Inhibitory Control, Self-reflection (minor)",L3
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA evaluates commonsense physical reasoning across many languages using non-parallel (not direct translation) question sets, aiming to assess robustness beyond English-centric priors. It probes whether models can infer plausible physical interactions and outcomes from short descriptions.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning
L3: Cognitive Flexibility (minor), Social Reasoning & Theory of Mind (minor)",L2
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) evaluates long-context multi-round coreference and retrieval by placing multiple similar “needle” requests inside a long “haystack” of distractors, then asking the model to reproduce the correct referenced response. It measures whether models can maintain and use precise references over very long sequences without drifting.","L1: Language Comprehension
L2: Working Memory, Attention, Episodic Memory, Semantic Understanding & Context Recognition (minor)
L3: ",L2
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval evaluates economically meaningful, well-specified professional knowledge-work tasks (e.g., creating spreadsheets, presentations, and operational artifacts), judged against human professionals. It emphasizes producing usable work products that satisfy requirements, handle constraints, and maintain quality across multiple deliverables.","L1: Language Production
L2: Planning, Decision-making, Working Memory, Semantic Understanding & Context Recognition
L3: Social Reasoning & Theory of Mind (minor), Self-reflection (minor)",L2
SWE-Lancer,https://openai.com/index/swe-lancer/,https://arxiv.org/abs/2502.12115,"SWE-Lancer evaluates software engineering agents on larger-scope, more realistic development tasks that may require multi-step implementation, iteration, and integration across a codebase rather than isolated edits. It is designed to better reflect professional engineering workflows, including maintaining correctness while making changes under constraints.","L1: Language Comprehension (minor)
L2: Planning, Adaptive Error Correction, Logical Reasoning, Working Memory, Decision-making (minor)
L3: ",L2
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath evaluates advanced mathematical problem solving intended to be challenging for frontier models, often requiring multi-step derivations and careful symbolic manipulation. It emphasizes depth of reasoning over rote recall, and can stress reliability under high difficulty and long solution chains.","L1: 
L2: Logical Reasoning, Working Memory, Planning
L3: Cognitive Flexibility (minor), Inhibitory Control (minor)",L2
