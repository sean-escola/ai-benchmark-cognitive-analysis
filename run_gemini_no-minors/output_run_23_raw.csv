Benchmark,Website,Paper,Description,Cognitive Functions
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro evaluates software engineering agents on a large set of real-world issues across multiple programming languages and repositories. Models must read project context, implement correct patches, and satisfy tests under realistic constraints, emphasizing robustness and contamination resistance compared to simpler coding sets.","Logical Reasoning, Planning, Adaptive Error Correction, Working Memory, Language Comprehension, Language Production (minor), Inhibitory Control (minor)"
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld benchmarks multimodal “computer use” agents in a full operating-system environment, where tasks require interacting with GUIs, applications, and files over many steps. Success depends on perceiving screens, choosing actions, and recovering from mistakes in a partially observable, tool-driven setting.","Visual Perception, Scene Understanding & Visual Reasoning, Attention, Planning, Decision-making, Sensorimotor Coordination, Working Memory, Adaptive Error Correction (minor), Spatial Representation & Mapping (minor)"
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,ARC-AGI measures fluid reasoning on novel grid-based puzzles where models infer a transformation rule from a few input–output examples and apply it to a new input. It is designed to reward generalization and abstraction rather than memorized domain knowledge.,"Scene Understanding & Visual Reasoning, Logical Reasoning, Cognitive Flexibility, Working Memory, Attention (minor), Visual Perception (minor)"
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon agent coherence and business decision-making in a simulated vending-machine company over an extended time period. Agents must manage inventory, pricing, supplier negotiation, and strategy under budget constraints, where early choices compound into later outcomes.","Planning, Decision-making, Reward Mechanisms, Working Memory, Episodic Memory (minor), Language Comprehension (minor), Language Production (minor), Social Reasoning & Theory of Mind (minor)"
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas measures real-world tool use via the Model Context Protocol, requiring models to discover, call, and compose tools across multi-step workflows. Tasks stress correct parameterization, error handling, and synthesis of tool outputs into a coherent final response.","Planning, Decision-making, Language Comprehension, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition, Inhibitory Control (minor)"
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity agents on large-scale tasks involving identifying known vulnerabilities from descriptions and discovering previously unknown issues in real open-source codebases. The benchmark emphasizes reasoning over code, reproducing weaknesses, and producing actionable findings under realistic constraints.","Logical Reasoning, Planning, Adaptive Error Correction, Working Memory, Language Comprehension, Decision-making (minor), Inhibitory Control (minor)"
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a difficult multimodal benchmark spanning many expert domains, designed to probe frontier knowledge, reasoning, and synthesis on problems near the limits of current models. It includes questions where tool use (e.g., search or code) can be important in practical evaluation settings, while still stressing robust reasoning and grounding.","Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Scene Understanding & Visual Reasoning (minor), Visual Perception (minor)"
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a curated subset of extremely challenging graduate-level multiple-choice science questions intended to be “Google-proof.” It tests whether models can apply deep scientific knowledge and reasoning, rather than relying on shallow pattern matching or retrieval shortcuts.","Semantic Understanding & Context Recognition, Logical Reasoning, Language Comprehension, Working Memory (minor), Decision-making (minor)"
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro is a high-difficulty multimodal benchmark covering many disciplines, where models answer questions requiring joint interpretation of images/diagrams and text. It targets expert-level multimodal understanding and reasoning beyond basic recognition, often involving charts, figures, and domain context.","Visual Perception, Scene Understanding & Visual Reasoning, Multisensory Integration, Language Comprehension, Logical Reasoning, Working Memory (minor), Visual Attention & Eye Movements (minor)"
OmniDocBench 1.5,https://github.com/opendatalab/OmniDocBench,https://arxiv.org/abs/2412.07626,"OmniDocBench 1.5 evaluates document understanding and OCR-like capabilities across complex layouts, including text, formulas, tables, and reading order. It emphasizes faithful extraction and structural reconstruction from visually rich documents rather than simple plain-text transcription.","Visual Perception, Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Spatial Representation & Mapping, Language Comprehension, Working Memory (minor), Semantic Understanding & Context Recognition (minor)"
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU benchmarks multimodal understanding and reasoning over video, requiring models to integrate information across frames and time to answer questions. It stresses temporal grounding, event understanding, and multi-step reasoning based on dynamic visual content.","Visual Perception, Cognitive Timing & Predictive Modeling, Attention, Working Memory, Scene Understanding & Visual Reasoning, Language Comprehension (minor), Multisensory Integration (minor)"
LiveCodeBench Pro,https://livecodebenchpro.com/projects/livecodebench-pro/leaderboard,https://arxiv.org/abs/2506.11928,"LiveCodeBench Pro evaluates coding models on competitive-style programming tasks with an emphasis on up-to-date, hard-to-memorize problems and standardized scoring (e.g., Elo). It tests end-to-end program synthesis and iterative debugging toward passing hidden test cases.","Logical Reasoning, Planning, Adaptive Error Correction, Working Memory, Language Comprehension (minor), Language Production (minor)"
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"The FACTS Benchmark Suite systematically evaluates factuality, including whether model statements are supported by provided sources or general-world evidence under controlled settings. It targets hallucination resistance and calibrated, context-sensitive truthfulness across diverse tasks and domains.","Semantic Understanding & Context Recognition, Language Comprehension, Inhibitory Control, Logical Reasoning (minor), Working Memory (minor), Self-reflection (minor)"
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,Global PIQA is a multilingual / cross-cultural physical commonsense benchmark that tests whether models can choose the more plausible action or outcome in everyday physical interaction scenarios. It emphasizes robust commonsense generalization across languages and culturally diverse contexts rather than English-only priors.,"Semantic Understanding & Context Recognition, Logical Reasoning, Language Comprehension, Spatial Representation & Mapping (minor), Multisensory Integration (minor)"
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) is a long-context retrieval and multi-round coreference benchmark where multiple similar “needle” requests are embedded in long “haystack” conversations. Models must locate and reproduce the correct referenced response, stressing attention control and interference resistance over long inputs.","Working Memory, Attention, Episodic Memory, Language Comprehension, Semantic Understanding & Context Recognition"
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval evaluates economically meaningful professional work across many occupations by having models produce real deliverables (e.g., spreadsheets, presentations, schedules) judged by expert humans. It focuses on end-to-end task execution quality, including correctness, completeness, and professional standards under well-specified requirements.","Planning, Decision-making, Language Production, Semantic Understanding & Context Recognition, Social Reasoning & Theory of Mind (minor), Working Memory (minor), Self-reflection (minor)"
SWE-Lancer,https://openai.com/index/swe-lancer/,https://arxiv.org/abs/2502.12115,"SWE-Lancer evaluates agentic software engineering on realistic, higher-level development work where models must navigate repositories, implement changes, and produce patches that satisfy tests and project constraints. Compared to narrow coding problems, it stresses longer-horizon execution and engineering judgment under ambiguity.","Planning, Logical Reasoning, Adaptive Error Correction, Decision-making, Working Memory, Language Comprehension (minor), Language Production (minor)"
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath is a benchmark of very challenging, expert-level mathematics problems intended to measure genuine mathematical reasoning at the research frontier. Many items require multi-step derivations and careful symbolic reasoning, and are designed to be difficult to solve via memorization alone.","Logical Reasoning, Working Memory, Planning, Cognitive Flexibility (minor), Semantic Understanding & Context Recognition (minor)"
