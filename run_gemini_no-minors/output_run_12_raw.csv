Benchmark,Website,Paper,Description,Cognitive Functions
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"A software-engineering benchmark where a model is given a real GitHub repository plus an issue description and must produce a patch that makes tests pass. The “Verified” variant uses human-verified tasks intended to be solvable and to reduce spurious failures, emphasizing end-to-end debugging and code changes rather than isolated coding puzzles.","Planning, Adaptive Error Correction, Logical Reasoning, Working Memory, Language Comprehension (minor), Decision-making (minor)"
Terminal-Bench 2.0,https://www.tbench.ai/,,"An agentic benchmark of real tasks performed in a command-line environment (e.g., manipulating files, installing dependencies, running programs, diagnosing failures). Success depends on choosing and sequencing shell commands, interpreting tool outputs, and iterating until an objective is met.","Planning, Adaptive Error Correction, Decision-making, Working Memory, Semantic Understanding & Context Recognition (minor), Language Comprehension (minor)"
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"A multi-turn customer-support simulation where an agent must interact with a simulated user and programmatic APIs while following domain policies (e.g., retail, airline, telecom). It tests whether agents can maintain policy compliance and conversational coherence while performing tool-mediated actions across long dialogues.","Social Reasoning & Theory of Mind, Decision-making, Planning, Inhibitory Control, Language Comprehension, Language Production, Empathy (minor)"
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,A fluid-reasoning benchmark of abstract grid puzzles where a model must infer the transformation rule from a few input–output examples and apply it to a new grid. It is designed to emphasize generalization to novel patterns over memorized knowledge.,"Logical Reasoning, Spatial Representation & Mapping, Cognitive Flexibility, Working Memory, Scene Understanding & Visual Reasoning, Attention (minor)"
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"A long-horizon agent benchmark where a model runs a simulated vending-machine business over an extended period, aiming to maximize final balance. It requires sustained strategy (pricing, inventory, supplier negotiation) and adaptation to changing market conditions across many sequential decisions.","Planning, Decision-making, Working Memory, Reward Mechanisms, Cognitive Timing & Predictive Modeling (minor), Social Reasoning & Theory of Mind (minor)"
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"A difficult multi-domain benchmark intended to probe frontier knowledge and reasoning across text and (in many settings) multimodal inputs. Questions are often research-adjacent and can require synthesis, careful interpretation, and multi-step problem solving rather than direct lookup.","Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Scene Understanding & Visual Reasoning (minor), Multisensory Integration (minor)"
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"A competition-mathematics benchmark derived from the 2025 AIME problems, typically scored by exact final answers. It emphasizes multi-step symbolic reasoning, algebraic manipulation, and maintaining constraints across a solution.","Logical Reasoning, Working Memory, Cognitive Flexibility (minor), Planning (minor)"
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"A graduate-level, “Google-proof” multiple-choice science benchmark; the Diamond subset is a high-quality set where experts reliably answer correctly and non-experts often fail. It targets deep conceptual understanding and careful discrimination among plausible distractors.","Semantic Understanding & Context Recognition, Logical Reasoning, Decision-making, Working Memory (minor), Language Comprehension (minor)"
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"A multilingual extension of broad academic knowledge and reasoning tests spanning many subjects, evaluated across multiple languages. It probes whether models can transfer knowledge and reasoning patterns across linguistic contexts rather than relying on English-only competence.","Language Comprehension, Cognitive Flexibility, Semantic Understanding & Context Recognition, Logical Reasoning (minor), Working Memory (minor)"
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"A multimodal benchmark focused on expert-level understanding and reasoning over images and text (e.g., diagrams, charts, scientific visuals) across disciplines. The “Pro” setting generally increases difficulty and aims to better separate strong visual reasoning from surface recognition.","Visual Perception, Scene Understanding & Visual Reasoning, Multisensory Integration, Language Comprehension, Spatial Representation & Mapping (minor), Working Memory (minor)"
MathArena Apex,https://matharena.ai/?view=problem&comp=apex--apex_2025,https://arxiv.org/abs/2505.23281,"A high-difficulty mathematics benchmark suite used to compare frontier models on advanced problem solving (often beyond standard competition level). It emphasizes long derivations, proof-like reasoning, and robustness on hard instances rather than short pattern-matching.","Logical Reasoning, Working Memory, Planning (minor), Cognitive Flexibility (minor)"
ScreenShot-Pro,https://gui-agent.github.io/grounding-leaderboard/,https://arxiv.org/abs/2504.07981,"A visual grounding benchmark where models answer questions or take actions based on high-resolution GUI screenshots, requiring precise localization and interpretation of interface elements. It stresses spatially grounded understanding (what is where) and mapping visual layout to semantic actions or answers.","Visual Perception, Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Spatial Representation & Mapping, Decision-making (minor), Sensorimotor Coordination (minor)"
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"A benchmark of reasoning questions grounded in figures from scientific papers (e.g., plots, charts, multi-panel visuals), often requiring quantitative interpretation. It tests whether models can extract relevant visual evidence, connect it to the question, and carry out multi-step inference.","Scene Understanding & Visual Reasoning, Visual Perception, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory (minor), Attention (minor)"
OmniDocBench 1.5,https://github.com/opendatalab/OmniDocBench,https://arxiv.org/abs/2412.07626,"A document understanding and OCR benchmark spanning heterogeneous layouts (text, tables, formulas, reading order) and emphasizing faithful extraction/structuring. Scoring typically rewards correct content recovery and correct structural interpretation rather than merely recognizing characters.","Visual Perception, Attention, Semantic Understanding & Context Recognition, Working Memory (minor), Language Production (minor)"
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"A video-based multimodal benchmark where questions require integrating information across frames and time, often combining visual cues with textual context. It tests temporal coherence, event understanding, and retrieving the right moment(s) from a clip to justify an answer.","Visual Perception, Working Memory, Attention, Cognitive Timing & Predictive Modeling, Scene Understanding & Visual Reasoning, Multisensory Integration (minor)"
LiveCodeBench Pro,https://livecodebenchpro.com/projects/livecodebench-pro/leaderboard,https://arxiv.org/abs/2506.11928,"A coding benchmark designed to evaluate programming ability on more realistic, time-sensitive tasks, often using live/chronologically split problems to reduce leakage. The “Pro” setting emphasizes harder items and practical code correctness under single-attempt constraints.","Logical Reasoning, Planning, Adaptive Error Correction, Working Memory, Language Comprehension (minor), Language Production (minor)"
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"A suite for systematically evaluating factuality and grounding-related failure modes, aggregating diverse factuality tests rather than a single QA dataset. It targets whether models produce claims consistent with reliable evidence, avoid hallucinations, and calibrate answers under uncertainty.","Semantic Understanding & Context Recognition, Inhibitory Control, Self-reflection (minor), Working Memory (minor), Language Comprehension (minor)"
SimpleQA Verified,https://www.kaggle.com/benchmarks/deepmind/simpleqa-verified,https://arxiv.org/abs/2509.07968,"A short-answer factual QA benchmark with verified answers, intended to measure straightforward factual recall and reduce ambiguity in evaluation. It emphasizes correctness and abstaining from unsupported claims on simple, high-precision questions.","Semantic Understanding & Context Recognition, Language Comprehension, Inhibitory Control (minor), Working Memory (minor)"
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"A multilingual commonsense reasoning benchmark focused on physical interaction and everyday “how to” situations, evaluating whether models choose plausible actions/outcomes across languages. It tests transfer of embodied/physical commonsense and robustness to linguistic variation.","Logical Reasoning, Semantic Understanding & Context Recognition, Cognitive Flexibility, Spatial Representation & Mapping (minor), Language Comprehension"
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"A long-context, multi-round coreference resolution benchmark where multiple similar “needle” interactions are embedded within long “haystacks,” and the model must retrieve the correct referenced content. The 8-needle setting increases interference, stressing robust retrieval under distractors and long-range dependencies.","Working Memory, Attention, Language Comprehension, Semantic Understanding & Context Recognition, Episodic Memory (minor)"
