Benchmark,Website,Paper,Description,Cognitive Functions
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro evaluates software engineering agents on realistic tasks in real repositories, requiring them to produce patches that satisfy problem statements and pass tests across multiple programming languages. It stresses end-to-end debugging and implementation under constraints similar to professional development workflows.","Planning, Adaptive Error Correction, Logical Reasoning, Working Memory, Language Comprehension (minor), Language Production (minor)"
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld benchmarks multimodal “computer use” by asking an agent to complete tasks in a full operating-system environment via screenshots and actions (e.g., clicking, typing, navigating apps). Success requires robust perception of UI state and reliable multi-step interaction strategies over many steps.","Visual Perception, Scene Understanding & Visual Reasoning, Planning, Decision-making, Sensorimotor Coordination, Attention (minor), Working Memory (minor)"
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI measures fluid, example-based abstract reasoning on grid transformation puzzles where the underlying rule must be inferred from a few demonstrations. The tasks emphasize learning new symbolic-like rules on the fly rather than relying on memorized domain knowledge.","Logical Reasoning, Cognitive Flexibility, Working Memory, Spatial Representation & Mapping, Visual Perception (minor), Scene Understanding & Visual Reasoning (minor)"
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon agent coherence and strategy by simulating management of a vending machine business over an extended period with many decisions and delayed outcomes. Agents must balance inventory, pricing, supplier interactions, and cash constraints to maximize final value.","Planning, Decision-making, Reward Mechanisms, Working Memory, Cognitive Timing & Predictive Modeling (minor), Adaptive Error Correction (minor)"
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas measures real-world tool-use performance via the Model Context Protocol, emphasizing discovering appropriate tools, invoking them correctly, handling errors, and composing multi-step workflows. It targets practical agent reliability in production-like API/tool ecosystems rather than isolated Q&A.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Language Comprehension (minor), Language Production (minor)"
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity capability by testing agents on identifying known vulnerabilities and discovering new ones in real open-source codebases under realistic constraints. It stresses reasoning about program behavior, security implications, and producing actionable findings or patches.","Logical Reasoning, Planning, Adaptive Error Correction, Working Memory, Language Comprehension (minor), Language Production (minor)"
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a difficult multimodal benchmark spanning frontier academic and professional knowledge, designed to be challenging for top models and to resist superficial pattern matching. Questions often require integrating domain knowledge with multi-step reasoning and, in some settings, careful tool use.","Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory (minor), Visual Perception (minor), Planning (minor)"
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,GPQA Diamond is a high-quality subset of GPQA consisting of very challenging graduate-level science multiple-choice questions intended to be “Google-proof.” It emphasizes deep conceptual understanding and careful reasoning over memorized trivia.,"Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory (minor), Language Comprehension (minor)"
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro evaluates expert-level multimodal understanding across many disciplines by requiring models to answer questions grounded in images, diagrams, and accompanying text. It probes whether a model can extract relevant visual evidence and combine it with domain knowledge and reasoning.","Visual Perception, Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Working Memory (minor), Language Comprehension (minor)"
OmniDocBench 1.5,https://github.com/opendatalab/OmniDocBench,https://arxiv.org/abs/2412.07626,"OmniDocBench 1.5 benchmarks document understanding and OCR on complex, real-world documents containing text, tables, formulas, and layout structure. It emphasizes faithful extraction and structural reconstruction rather than only recognizing plain text.","Visual Perception, Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Language Comprehension (minor), Working Memory (minor)"
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU extends multimodal evaluation to video, requiring models to answer questions that depend on events, temporal order, and visual details across clips. It stresses integrating information over time instead of relying on a single frame or short snippet.","Visual Perception, Scene Understanding & Visual Reasoning, Cognitive Timing & Predictive Modeling, Working Memory, Attention (minor), Multisensory Integration (minor)"
LiveCodeBench Pro,https://livecodebenchpro.com/projects/livecodebench-pro/leaderboard,https://arxiv.org/abs/2506.11928,"LiveCodeBench Pro evaluates coding ability on problems curated to reduce contamination, typically requiring writing correct code under time-like constraints and diverse task formats. It stresses generating executable solutions and maintaining correctness across edge cases and hidden tests.","Logical Reasoning, Planning, Adaptive Error Correction, Working Memory, Language Production (minor), Language Comprehension (minor)"
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"The FACTS Benchmark Suite systematically evaluates factuality and grounding, probing whether model outputs stay consistent with given sources or established facts across diverse tasks. It targets hallucination avoidance, correct attribution, and resisting unsupported inferences.","Semantic Understanding & Context Recognition, Inhibitory Control, Logical Reasoning (minor), Working Memory (minor), Self-reflection (minor)"
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,Global PIQA evaluates multilingual physical commonsense reasoning by asking models to choose plausible solutions to everyday physical interaction scenarios across many languages and locales. It stresses whether reasoning transfers beyond English and remains robust to linguistic variation.,"Logical Reasoning, Semantic Understanding & Context Recognition, Language Comprehension, Cognitive Flexibility (minor), Working Memory (minor)"
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,MRCR v2 (8-needle) tests long-context multi-round coreference resolution by embedding multiple similar “needle” interactions in a large “haystack” and asking the model to reproduce the correct referenced response. It measures robust retrieval and disambiguation under heavy interference.,"Working Memory, Attention, Episodic Memory, Semantic Understanding & Context Recognition, Language Comprehension (minor)"
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval evaluates economically valuable, well-specified knowledge-work tasks spanning many occupations, where models produce artifacts like spreadsheets, presentations, and plans judged against expert professionals. It emphasizes end-to-end execution quality, adherence to constraints, and usefulness for real workflows.","Planning, Decision-making, Language Production, Adaptive Error Correction (minor), Working Memory (minor), Semantic Understanding & Context Recognition (minor)"
SWE-Lancer,https://openai.com/index/swe-lancer/,https://arxiv.org/abs/2502.12115,"SWE-Lancer measures agentic software engineering on tasks that resemble real product and engineering work, often requiring coordinating changes across a codebase and producing patches that satisfy objective checks. It targets reliability in longer, more realistic development loops than short coding puzzles.","Planning, Adaptive Error Correction, Decision-making, Working Memory, Logical Reasoning (minor)"
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath evaluates advanced mathematical problem solving on difficult, expert-level questions designed to be resistant to memorization and to require genuine derivations. It stresses sustained multi-step reasoning, precise symbolic manipulation, and error-prone long chains of inference.","Logical Reasoning, Working Memory, Planning, Adaptive Error Correction (minor), Cognitive Flexibility (minor)"
