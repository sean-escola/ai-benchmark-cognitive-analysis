Benchmark,Website,Paper,Description,Cognitive Functions
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates software engineering agents on real GitHub issues by asking the model to generate a patch that makes a failing test suite pass. The ""Verified"" subset uses human-validated tasks and evaluation scripts to reduce false positives/negatives and better reflect real maintenance workflows.","Planning, Logical Reasoning, Adaptive Error Correction, Working Memory, Language Comprehension (minor)"
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 tests agentic performance in a command-line environment, where models must navigate filesystems, run commands, install dependencies, and iteratively debug to reach a goal state. It emphasizes end-to-end autonomy under tool use, partial observability, and real execution feedback loops.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Inhibitory Control (minor)"
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench measures interactive tool-using agents in customer-support-like simulations (e.g., retail, airline, telecom), where the agent must follow policies while using APIs and handling multi-turn user interactions. It stresses robust dialogue management, policy adherence, and correct tool invocation across long trajectories.","Language Comprehension, Language Production, Planning, Decision-making, Inhibitory Control, Social Reasoning & Theory of Mind (minor)"
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI evaluates abstract, few-shot pattern induction using grid transformation tasks, where models infer hidden rules from a handful of input-output examples. It is designed to reduce reliance on memorized knowledge and instead probe novel reasoning and generalization under strong distribution shift.","Logical Reasoning, Cognitive Flexibility, Working Memory, Spatial Representation & Mapping, Visual Perception (minor)"
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon autonomous agency by having models run a simulated vending-machine business over an extended period, optimizing profit while managing inventory, suppliers, and pricing. Success depends on maintaining coherent strategy over many steps and adapting to a dynamic market simulation.","Planning, Decision-making, Reward Mechanisms, Working Memory, Adaptive Error Correction (minor)"
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam (HLE) is a frontier, multi-domain benchmark intended to probe difficult academic reasoning and broad knowledge, often with multimodal inputs and optional tools like search or code. It focuses on solving novel, expert-level questions rather than short-form trivia.","Language Comprehension, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory (minor), Visual Perception (minor)"
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 comprises competition-style math problems requiring multi-step derivations and careful symbolic manipulation, typically with short numeric final answers. It is commonly used to measure mathematical reasoning under time-like constraints and sensitivity to small logical errors.","Logical Reasoning, Working Memory, Planning (minor), Adaptive Error Correction (minor)"
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a high-quality subset of extremely difficult graduate-level science multiple-choice questions designed to be resistant to simple web search. It emphasizes precise domain reasoning in biology, chemistry, and physics, with distractors intended to trap shallow pattern matching.","Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory (minor), Inhibitory Control (minor)"
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends MMLU to multiple languages, testing broad academic knowledge and reasoning across many subjects in non-English settings. It is used to assess multilingual generalization, cross-lingual robustness, and culturally/linguistically grounded understanding.","Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning (minor), Language Production (minor)"
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro is a harder, more evaluation-focused variant of MMMU for multimodal expert reasoning, combining images/diagrams with domain questions across disciplines. It targets reliable visual understanding integrated with textual reasoning and reduces shortcuts via stronger curation and protocols.","Visual Perception, Scene Understanding & Visual Reasoning, Multisensory Integration, Language Comprehension, Logical Reasoning (minor)"
MathArena Apex,https://matharena.ai/?view=problem&comp=apex--apex_2025,https://arxiv.org/abs/2505.23281,"MathArena Apex is a high-difficulty mathematics evaluation suite intended to separate strong models at the top end via challenging problem sets and standardized scoring. It is often used to compare frontier mathematical problem solving, including multi-step reasoning and error sensitivity.","Logical Reasoning, Working Memory, Planning (minor), Adaptive Error Correction (minor)"
ScreenShot-Pro,https://gui-agent.github.io/grounding-leaderboard/,https://arxiv.org/abs/2504.07981,"ScreenShot-Pro evaluates GUI understanding and grounding from screenshots, requiring models to interpret UI elements, spatial layout, and sometimes generate actions or answers tied to exact interface regions. It targets robust visual-spatial grounding under high-resolution, real-world screen distributions.","Visual Perception, Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Spatial Representation & Mapping, Sensorimotor Coordination (minor)"
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning tests scientific-figure understanding from arXiv-style papers, asking models to answer questions that require interpreting plots, axes, legends, and quantitative relationships. It emphasizes chart/figure reasoning and accurate extraction of structured information from visuals.","Scene Understanding & Visual Reasoning, Visual Perception, Logical Reasoning, Language Comprehension (minor), Working Memory (minor)"
OmniDocBench 1.5,https://github.com/opendatalab/OmniDocBench,https://arxiv.org/abs/2412.07626,"OmniDocBench 1.5 evaluates document understanding and OCR-style extraction across mixed content like text, tables, formulas, and reading order. It measures whether models can faithfully transcribe and structure information from complex page layouts rather than paraphrase it.","Visual Perception, Scene Understanding & Visual Reasoning, Language Comprehension, Spatial Representation & Mapping (minor)"
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU extends multimodal understanding to video, requiring integration of information across frames to answer questions about events, objects, and temporal relations. It stresses temporal coherence and maintaining relevant context over long visual sequences.","Visual Perception, Cognitive Timing & Predictive Modeling, Working Memory, Multisensory Integration (minor), Attention (minor)"
LiveCodeBench Pro,https://livecodebenchpro.com/projects/livecodebench-pro/leaderboard,https://arxiv.org/abs/2506.11928,"LiveCodeBench Pro evaluates coding ability on competitively curated programming tasks with strong anti-contamination and time-based splits, often emphasizing problem-solving under realistic constraints. It measures not just code generation but also correctness across hidden tests and diverse problem types.","Logical Reasoning, Planning, Adaptive Error Correction, Working Memory, Language Comprehension (minor)"
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"FACTS Benchmark Suite systematically evaluates factuality and grounding, probing whether model outputs remain consistent with provided sources and avoid hallucinated or unsupported claims. It is designed to measure reliability across multiple factuality subtests rather than a single QA format.","Inhibitory Control, Semantic Understanding & Context Recognition, Language Comprehension, Self-reflection (minor), Adaptive Error Correction (minor)"
SimpleQA Verified,https://www.kaggle.com/benchmarks/deepmind/simpleqa-verified,https://arxiv.org/abs/2509.07968,SimpleQA Verified is a factual QA benchmark with verified answers and strict grading intended to capture whether models can respond accurately to straightforward information-seeking questions. It is commonly used as a signal for hallucination propensity under simple prompts.,"Semantic Understanding & Context Recognition, Language Comprehension, Inhibitory Control, Language Production (minor)"
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA is a multilingual commonsense reasoning benchmark derived from or inspired by physical interaction/common sense QA, emphasizing robust transfer across languages rather than memorized English-only patterns. It targets practical reasoning about everyday situations and plausible actions across diverse linguistic contexts.","Language Comprehension, Logical Reasoning, Semantic Understanding & Context Recognition, Multisensory Integration (minor)"
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) is a long-context retrieval-and-reasoning evaluation in which multiple similar ""needle"" interactions are embedded within long ""haystack"" conversations, and the model must reproduce the correct response for a specified needle. It measures robustness to distraction, interference, and cross-document coreference at long context lengths.","Working Memory, Attention, Episodic Memory, Language Comprehension, Inhibitory Control (minor)"
