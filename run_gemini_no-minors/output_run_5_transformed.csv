Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates software engineering agents on real GitHub issues by requiring them to generate code patches that pass repository tests. The Verified subset focuses on tasks that have been human-validated as solvable and uses a standardized harness to reduce false positives from flaky tests, emphasizing end-to-end debugging and implementation in realistic codebases.","L1: Language Comprehension (minor), Language Production (minor)
L2: Logical Reasoning, Planning, Working Memory, Adaptive Error Correction
L3: ",L2
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 measures an agent’s ability to complete real-world tasks in a command-line environment, typically requiring running commands, reading outputs, editing files, and iterating until success. It stresses practical tool use and recovery from errors under resource and time constraints typical of developer workflows.","L1: 
L2: Planning, Decision-making, Working Memory, Adaptive Error Correction
L3: Inhibitory Control (minor)",L2
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench evaluates interactive tool-using agents in customer-support-like simulations (e.g., retail, airline, telecom) where the agent must converse with a user and call APIs while adhering to domain policies. Success depends on maintaining coherent multi-turn state, selecting correct actions, and balancing helpfulness with policy compliance.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Working Memory
L3: Social Reasoning & Theory of Mind, Inhibitory Control",L3
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI tests abstract, few-shot pattern induction using small grid-based input–output examples, requiring the model to infer the underlying transformation rule and apply it to a new grid. It is designed to emphasize generalization to novel tasks rather than memorization of domain knowledge.","L1: Visual Perception (minor)
L2: Logical Reasoning, Working Memory, Spatial Representation & Mapping, Scene Understanding & Visual Reasoning
L3: Cognitive Flexibility",L3
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon agent coherence by simulating management of a vending-machine business over extended time (e.g., many decisions spanning a simulated year). Models must plan, negotiate, allocate budget, manage inventory, and adapt to changing conditions to maximize final balance.","L1: 
L2: Planning, Decision-making, Working Memory, Reward Mechanisms, Semantic Understanding & Context Recognition (minor), Adaptive Error Correction (minor)
L3: ",L2
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a difficult multimodal benchmark intended to probe frontier knowledge and reasoning across many disciplines, often including images, charts, or other non-text inputs. It emphasizes robust problem solving under uncertainty and (in some setups) benefits from tool use like search or code execution.","L1: Language Comprehension, Visual Perception (minor)
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory (minor), Scene Understanding & Visual Reasoning (minor)
L3: ",L2
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,AIME 2025 evaluates mathematical problem solving on competition-style questions that require multi-step derivations and careful symbolic manipulation. The problems are designed to be solvable without specialized external knowledge but demand precise reasoning and error-free execution.,"L1: 
L2: Logical Reasoning, Working Memory, Adaptive Error Correction (minor), Planning (minor)
L3: ",L2
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a curated subset of very challenging graduate-level science multiple-choice questions designed to be resistant to simple web lookup. It emphasizes deep conceptual understanding and multi-step scientific reasoning, with strong separation between expert and non-expert performance.","L1: 
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory (minor)
L3: Inhibitory Control (minor)",L2
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends broad academic knowledge testing to multiple languages, measuring whether a model can answer questions across many subjects beyond English. It stresses cross-lingual generalization, consistent reasoning across languages, and robustness to linguistic variation.","L1: Language Comprehension, Language Production (minor)
L2: Semantic Understanding & Context Recognition, Logical Reasoning (minor)
L3: Cognitive Flexibility (minor)",L2
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro is a challenging multimodal benchmark that tests expert-level reasoning over images and text across many academic domains, often involving diagrams, plots, and technical figures. It focuses on integrating visual evidence with domain knowledge and logical inference under multiple-choice or structured answering formats.","L1: Visual Perception
L2: Multisensory Integration, Scene Understanding & Visual Reasoning, Logical Reasoning, Visual Attention & Eye Movements (minor), Semantic Understanding & Context Recognition (minor)
L3: ",L2
MathArena Apex,https://matharena.ai/?view=problem&comp=apex--apex_2025,https://arxiv.org/abs/2505.23281,"MathArena Apex aggregates harder, research-adjacent or advanced competition-style math problems and is used to differentiate top-tier mathematical reasoning. It emphasizes long solution chains, proof-like structure, and maintaining correctness across many dependent steps.","L1: 
L2: Logical Reasoning, Working Memory, Planning (minor), Adaptive Error Correction (minor)
L3: ",L2
ScreenShot-Pro,https://gui-agent.github.io/grounding-leaderboard/,https://arxiv.org/abs/2504.07981,"ScreenShot-Pro evaluates visual grounding and reasoning over high-resolution screenshots of software interfaces, requiring identification of UI elements, interpreting layout, and answering questions or selecting targets. It is designed to reflect practical “computer use” understanding where spatial relationships and fine-grained visual cues matter.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Spatial Representation & Mapping, Decision-making (minor), Sensorimotor Coordination (minor)
L3: ",L2
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning evaluates reasoning over scientific paper figures and related artifacts, often requiring extracting quantitative or relational information from plots, tables, and diagrams. It stresses linking visual evidence to scientific semantics and performing multi-step inference rather than mere OCR.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Semantic Understanding & Context Recognition (minor), Working Memory (minor)
L3: ",L2
OmniDocBench 1.5,https://github.com/opendatalab/OmniDocBench,https://arxiv.org/abs/2412.07626,"OmniDocBench 1.5 evaluates document understanding and OCR quality across heterogeneous document elements such as text, formulas, tables, and reading order. It measures whether models can accurately transcribe and structurally interpret complex page layouts rather than only recognizing characters.","L1: Visual Perception
L2: Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Semantic Understanding & Context Recognition (minor), Working Memory (minor)
L3: ",L2
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU extends multimodal understanding to videos, testing whether models can answer questions that require temporal integration across frames and alignment with accompanying text. It emphasizes event understanding, temporal dependencies, and extracting salient visual details over time.","L1: Visual Perception
L2: Attention, Working Memory, Multisensory Integration, Scene Understanding & Visual Reasoning
L3: Cognitive Timing & Predictive Modeling (minor)",L2
LiveCodeBench Pro,https://livecodebenchpro.com/projects/livecodebench-pro/leaderboard,https://arxiv.org/abs/2506.11928,"LiveCodeBench Pro evaluates coding ability on up-to-date programming problems under conditions intended to reduce training-set leakage, often emphasizing realistic development and reasoning rather than memorized solutions. It stresses producing correct, executable code and handling edge cases with limited attempts.","L1: Language Production (minor)
L2: Logical Reasoning, Planning, Working Memory, Adaptive Error Correction
L3: ",L2
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"The FACTS Benchmark Suite systematically evaluates factuality by testing whether models produce correct, well-grounded statements across diverse factuality stressors (e.g., ambiguous prompts, long contexts, and knowledge-heavy queries). It aims to measure reliability, including tendencies to hallucinate or to overstate uncertainty.","L1: Language Comprehension (minor)
L2: Semantic Understanding & Context Recognition, Working Memory (minor)
L3: Inhibitory Control, Self-reflection (minor)",L3
SimpleQA Verified,https://www.kaggle.com/benchmarks/deepmind/simpleqa-verified,https://arxiv.org/abs/2509.07968,"SimpleQA Verified is a short-form question answering benchmark focused on verifiable factual questions with high-quality ground truth, designed to measure correctness rather than eloquence. It emphasizes precision and resisting plausible-sounding but incorrect answers.","L1: Language Comprehension (minor)
L2: Semantic Understanding & Context Recognition
L3: Inhibitory Control (minor)",L2
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,Global PIQA evaluates commonsense reasoning about physical interactions and everyday actions across many languages using non-parallel (language-specific) items to reduce translation artifacts. It measures whether models can generalize pragmatic physical reasoning and commonsense constraints across linguistic and cultural variation.,"L1: Language Comprehension
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Spatial Representation & Mapping (minor)
L3: Cognitive Flexibility (minor)",L2
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,MRCR v2 (8-needle) tests long-context multi-round coreference-style retrieval by inserting multiple similar “needle” requests into a long “haystack” of dialogue and asking for the response corresponding to a specific needle. It is designed to stress attention control and accurate retrieval when many distractors are present across very long contexts.,"L1: Language Comprehension (minor)
L2: Working Memory, Attention, Episodic Memory
L3: Inhibitory Control (minor)",L2
