Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"A software engineering benchmark where models receive a real GitHub issue plus a code repository snapshot and must produce a patch that passes the project’s tests. The Verified split consists of tasks validated by humans to be solvable and to have reliable evaluation, emphasizing end-to-end debugging and code change correctness.","L1: Language Comprehension
L2: Planning, Decision-making, Logical Reasoning, Working Memory, Adaptive Error Correction
L3: ",L2
Terminal-Bench 2.0,https://www.tbench.ai/,,"An agent benchmark for accomplishing real tasks in a command-line environment (e.g., using Unix tools, package managers, and scripts) with limited instructions. Success requires choosing and sequencing shell actions, interpreting errors, and iterating until the desired system state is achieved.","L1: Language Comprehension
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory
L3: ",L2
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"A web-research benchmark where models must answer questions by searching and reading sources, typically under constraints that favor careful evidence gathering over memorization. It stresses selecting queries, extracting relevant passages, and synthesizing a final answer from noisy or partial information.","L1: Language Comprehension
L2: Planning, Decision-making, Attention, Working Memory, Semantic Understanding & Context Recognition
L3: ",L2
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"An interactive customer-support benchmark in simulated domains (e.g., retail, airline, telecom) where the agent must follow policies while using tools/APIs and conversing with a user. It evaluates multi-turn robustness, policy compliance, and the ability to resolve a case end-to-end despite ambiguity or user pressure.","L1: Language Comprehension, Language Production
L2: Decision-making, Planning, Working Memory
L3: Social Reasoning & Theory of Mind, Inhibitory Control, Empathy",L3
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"A computer-use benchmark where agents operate a full operating system via a GUI to complete tasks across applications. It requires perceiving screenshots, navigating interfaces, and executing multi-step action sequences while handling UI variability and recoveries from mistakes.","L1: Visual Perception
L2: Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Sensorimotor Coordination, Planning, Decision-making, Working Memory, Adaptive Error Correction, Spatial Representation & Mapping
L3: ",L2
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"A fluid-reasoning benchmark of abstract grid transformation tasks: given a few input-output examples, the model must infer the hidden rule and produce the correct output for a new input. It targets generalization to novel patterns and compositional rule induction rather than domain knowledge.","L1: Visual Perception
L2: Logical Reasoning, Working Memory, Spatial Representation & Mapping, Scene Understanding & Visual Reasoning, Attention
L3: Cognitive Flexibility",L3
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"A long-horizon agent simulation where the model runs a vending-machine business over an extended period, making thousands of decisions (inventory, pricing, supplier negotiation, etc.). It measures sustained coherence, strategic adaptation, and goal-directed optimization under changing market conditions.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Working Memory, Reward Mechanisms
L3: Cognitive Flexibility, Self-reflection",L3
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"A tool-use benchmark centered on the Model Context Protocol (MCP), evaluating whether models can discover relevant tools, call them correctly, and chain multiple calls into a coherent workflow. It emphasizes reliability under realistic API surfaces, including error handling, retries, and result synthesis.","L1: Language Comprehension
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition
L3: ",L2
FinanceAgent,https://www.vals.ai/benchmarks/finance_agent,,"An evaluation of agent performance on tasks representative of entry-level financial analyst work (e.g., extracting figures, building analyses, and producing written summaries). It stresses numeracy, domain reasoning, and producing structured artifacts under professional constraints.","L1: Language Production, Language Comprehension
L2: Logical Reasoning, Decision-making, Planning, Working Memory, Semantic Understanding & Context Recognition
L3: ",L2
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"A cybersecurity benchmark that tests agents on identifying known vulnerabilities from descriptions and discovering previously-unknown issues in real open-source codebases. It rewards systematic investigation, hypothesis testing, and precise patch or exploit reasoning under realistic constraints.","L1: 
L2: Logical Reasoning, Planning, Adaptive Error Correction, Working Memory, Attention, Semantic Understanding & Context Recognition
L3: ",L2
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"A benchmark for spreadsheet understanding and manipulation, where models must navigate, compute, and edit complex spreadsheets derived from real-world use cases. It evaluates correctness of transformations and the ability to maintain consistency across interconnected cells and sheets.","L1: 
L2: Working Memory, Logical Reasoning, Planning, Adaptive Error Correction, Decision-making, Semantic Understanding & Context Recognition
L3: ",L2
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"A difficult multimodal benchmark designed to probe frontier academic and expert knowledge with questions that often require nontrivial reasoning rather than lookup. It evaluates how well models integrate text and visual information to produce accurate, well-supported answers across domains.","L1: Language Comprehension, Visual Perception
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Scene Understanding & Visual Reasoning
L3: ",L2
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"A set of competition mathematics problems from the 2025 AIME, typically requiring multi-step symbolic reasoning and careful arithmetic. It is used to assess mathematical problem-solving ability, with or without tool assistance such as Python.","L1: 
L2: Logical Reasoning, Working Memory, Planning, Adaptive Error Correction, Attention
L3: ",L2
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"A challenging multiple-choice science benchmark designed to be “Google-proof,” with the Diamond subset curated for high quality and difficulty. It tests deep understanding and reasoning in physics, chemistry, and biology under strong distractors.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory
L3: Inhibitory Control",L3
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,A multilingual extension of MMLU that evaluates knowledge and reasoning across many academic subjects and multiple non-English languages. It stresses robust comprehension across languages and consistent subject-matter competence under language variation.,"L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Working Memory
L3: Cognitive Flexibility",L3
MMMU,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2311.16502,"A multidisciplinary multimodal benchmark where questions combine text with images (charts, diagrams, screenshots, etc.) across many fields. It evaluates the ability to ground language in visual evidence and perform expert-style reasoning over multimodal inputs.","L1: Visual Perception, Language Comprehension
L2: Multisensory Integration, Scene Understanding & Visual Reasoning, Logical Reasoning, Working Memory, Attention
L3: ",L2
LAB-Bench FigQA,https://huggingface.co/datasets/futurehouse/lab-bench,https://arxiv.org/abs/2407.10362,"A visual reasoning benchmark drawn from biology research figures, where models must answer questions by correctly interpreting complex scientific plots and panels. It stresses fine-grained figure reading, scientific context integration, and multi-step inference from visual evidence.","L1: Visual Perception, Language Comprehension
L2: Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Logical Reasoning, Working Memory, Semantic Understanding & Context Recognition
L3: ",L2
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,A benchmark focused on reasoning over scientific paper artifacts (especially figures/charts) where correct answers require interpreting visual encodings and relating them to accompanying text. It often stresses quantitative comparisons and chained inferences grounded in the document’s content.,"L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Logical Reasoning, Working Memory, Semantic Understanding & Context Recognition, Attention
L3: ",L2
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"A multimodal benchmark that evaluates understanding and reasoning over video content, typically requiring integration of information across time and scenes. It measures whether models can track events, identify relevant moments, and answer questions that depend on temporal context.","L1: Visual Perception
L2: Visual Attention & Eye Movements, Working Memory, Scene Understanding & Visual Reasoning, Attention
L3: Cognitive Timing & Predictive Modeling",L3
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"A suite of factuality evaluations aimed at measuring whether model outputs are accurate, well-grounded, and resistant to hallucination across diverse settings. It emphasizes verifying claims, abstaining when uncertain, and maintaining consistency with provided sources or known facts.","L1: Language Production, Language Comprehension
L2: Semantic Understanding & Context Recognition, Working Memory
L3: Inhibitory Control, Self-reflection",L3
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,A physical commonsense reasoning benchmark that extends PIQA-style questions across many languages in a non-parallel setting. It tests whether models can choose plausible actions or outcomes in everyday physical scenarios under linguistic and cultural variation.,"L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Spatial Representation & Mapping
L3: Cognitive Flexibility",L3
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"A long-context, multi-round coreference/recall evaluation where multiple similar “needle” requests are embedded in long “haystack” dialogues, and the model must reproduce the response to a specified needle. The 8-needle variant stresses maintaining and retrieving the correct referent amid interference and distractors.","L1: 
L2: Working Memory, Attention, Episodic Memory, Semantic Understanding & Context Recognition
L3: Inhibitory Control",L3
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"An evaluation of well-specified professional knowledge work tasks spanning many occupations, judged by expert humans via pairwise comparisons (e.g., “wins or ties”). It tests the ability to produce high-quality work artifacts and decisions under realistic workplace constraints.","L1: Language Production
L2: Planning, Decision-making, Semantic Understanding & Context Recognition, Working Memory
L3: Social Reasoning & Theory of Mind",L3
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"A reasoning benchmark where models must follow paths and relationships in graph-structured data, often requiring multi-step traversal and state tracking. It targets systematic computation over discrete structures rather than surface-level pattern matching.","L1: 
L2: Spatial Representation & Mapping, Working Memory, Logical Reasoning, Planning, Attention
L3: ",L2
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"A benchmark for complex tool-using agents that must select from many tools, compose multi-step tool calls, and recover from failures to complete tasks. It emphasizes action selection under uncertainty, robust execution, and coherent synthesis of tool outputs.","L1: Language Comprehension
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition
L3: Inhibitory Control",L3
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"A difficult mathematics benchmark targeting advanced problem solving beyond standard competition sets, often requiring novel techniques and long derivations. It is used to probe the frontier of mathematical reasoning, sometimes with optional computational tools.","L1: 
L2: Logical Reasoning, Working Memory, Planning, Adaptive Error Correction, Attention
L3: Cognitive Flexibility",L3
