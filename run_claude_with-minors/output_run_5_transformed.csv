Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,SWE-bench Verified evaluates software engineering agents on real GitHub issues where the model must produce a patch that makes the project’s tests pass. The “Verified” subset contains tasks confirmed by human reviewers to be solvable and uses a standardized harness to reduce false positives from flaky tests and underspecified issues.,"L1: Language Comprehension
L2: Planning, Logical Reasoning, Working Memory, Adaptive Error Correction, Decision-making
L3: ",L2
SWE-bench Multilingual,https://www.swebench.com/multilingual.html,https://arxiv.org/abs/2504.21798,"SWE-bench Multilingual extends the SWE-bench setup beyond Python to multiple programming languages and ecosystems, requiring agents to understand diverse toolchains, build systems, and language-specific conventions. It measures whether a model can translate natural-language issue descriptions into correct multi-file edits and pass language-appropriate test suites.","L1: Language Comprehension
L2: Planning, Logical Reasoning, Working Memory, Adaptive Error Correction, Decision-making
L3: Cognitive Flexibility",L3
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro is a larger and harder software engineering benchmark designed to be more diverse and more resistant to contamination, typically spanning multiple languages and more industrially realistic tasks. Models must navigate complex repositories, implement or fix behavior, and satisfy rigorous evaluation criteria under an agent scaffold.","L1: Language Comprehension
L2: Planning, Logical Reasoning, Working Memory, Adaptive Error Correction, Decision-making
L3: Cognitive Flexibility",L3
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 measures an agent’s ability to accomplish real command-line tasks in sandboxed environments (e.g., debugging, installing dependencies, data manipulation, and system operations). Success depends on choosing correct commands, interpreting outputs and errors, iterating, and managing limited resources and timeouts.","L1: Language Comprehension
L2: Planning, Adaptive Error Correction, Working Memory, Decision-making, Logical Reasoning
L3: ",L2
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp evaluates web-browsing research agents on questions that require finding, synthesizing, and citing information from the web (or from a fixed index in reproducible variants). The benchmark stresses tool use (search/fetch), evidence tracking across multiple pages, and producing a grounded final answer under long-context constraints.","L1: Language Comprehension
L2: Planning, Decision-making, Semantic Understanding & Context Recognition, Working Memory, Episodic Memory
L3: Self-reflection",L3
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench evaluates interactive tool-using customer-support agents in simulated domains (e.g., retail, airline, telecom) with policies the agent must follow while solving multi-turn user requests. It tests whether agents can reliably call APIs, maintain conversational state, handle edge cases, and comply with constraints even when helpfulness pressures create policy-loophole temptations.","L1: Language Comprehension, Language Production
L2: Decision-making, Planning, Working Memory
L3: Social Reasoning & Theory of Mind, Empathy, Inhibitory Control",L3
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld evaluates multimodal “computer use” agents operating on a desktop-like environment to complete tasks across applications via screenshots and actions. Agents must perceive UI state, plan multi-step interactions, and execute sequences of clicks/typing while recovering from mistakes and dynamic interface changes.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Sensorimotor Coordination, Planning, Decision-making, Adaptive Error Correction, Working Memory
L3: ",L2
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI measures few-shot abstract reasoning on grid transformation puzzles where the model must infer latent rules from a handful of input-output examples and generalize to a novel input. It is designed to emphasize fluid reasoning and generalization rather than memorized knowledge, with strict correctness on produced grids.","L1: 
L2: Logical Reasoning, Working Memory, Planning, Spatial Representation & Mapping, Scene Understanding & Visual Reasoning
L3: Cognitive Flexibility",L3
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon agentic performance in a simulated vending machine business over an extended timeline, scoring based on final financial outcomes. Agents must make coherent strategic decisions (pricing, inventory, supplier negotiation) over many steps while adapting to market dynamics and delayed consequences.","L1: 
L2: Planning, Decision-making, Working Memory, Episodic Memory, Reward Mechanisms, Adaptive Error Correction
L3: Cognitive Timing & Predictive Modeling, Motivational Drives",L3
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas measures real-world tool use through the Model Context Protocol (MCP), requiring agents to discover tools, call APIs correctly, handle failures, and combine multi-step results into correct outputs. Tasks resemble production workflows where correctness depends on robust orchestration across multiple servers/tools rather than single-shot answering.","L1: Language Comprehension
L2: Planning, Decision-making, Working Memory, Adaptive Error Correction, Semantic Understanding & Context Recognition
L3: ",L2
FinanceAgent,https://www.vals.ai/benchmarks/finance_agent,,"FinanceAgent evaluates agents on tasks typical of an entry-level financial analyst, such as analyzing documents, building or checking analyses, and producing finance-oriented deliverables. It emphasizes multi-step reasoning with domain constraints, careful numerical consistency, and structured reporting aligned to professional standards.","L1: Language Comprehension, Language Production
L2: Logical Reasoning, Planning, Decision-making, Working Memory, Semantic Understanding & Context Recognition, Adaptive Error Correction
L3: ",L2
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity agent capabilities on large-scale tasks including identifying known vulnerabilities in real codebases from natural-language weakness descriptions and, in some settings, discovering previously unknown issues. Success requires code comprehension, hypothesis-driven debugging, iterative testing, and careful handling of adversarial or ambiguous evidence.","L1: Language Comprehension
L2: Logical Reasoning, Planning, Adaptive Error Correction, Working Memory, Decision-making
L3: Inhibitory Control",L3
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"SpreadsheetBench measures an agent’s ability to understand, edit, and compute with complex spreadsheets derived from real-world scenarios, often requiring multi-step transformations and formula reasoning. Agents must track dependencies, maintain consistent structure/formatting, and verify results through iterative corrections.","L1: 
L2: Logical Reasoning, Working Memory, Planning, Adaptive Error Correction, Decision-making, Semantic Understanding & Context Recognition
L3: ",L2
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a multimodal benchmark intended to probe frontier academic reasoning and knowledge across a broad set of difficult questions, including those that benefit from tools like search or code execution. It stresses multi-step inference, careful grounding, and integrating information from text and (in some subsets) images or other modalities.","L1: Language Comprehension
L2: Logical Reasoning, Working Memory, Semantic Understanding & Context Recognition, Planning, Decision-making, Multisensory Integration, Scene Understanding & Visual Reasoning
L3: ",L2
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,AIME 2025 evaluates mathematical problem-solving on competition-style questions that typically require multi-step symbolic reasoning and careful casework. Scores reflect exact-answer accuracy and are often reported with and without tools to separate pure reasoning from computation assistance.,"L1: 
L2: Logical Reasoning, Working Memory, Planning, Adaptive Error Correction
L3: ",L2
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a difficult, graduate-level multiple-choice science benchmark curated to be “Google-proof,” emphasizing reasoning over superficial recall. The Diamond subset focuses on high-quality questions where experts reliably agree on the correct answer and non-experts tend to fail, probing deep scientific understanding under time/format constraints.","L1: Language Comprehension
L2: Logical Reasoning, Working Memory, Semantic Understanding & Context Recognition, Decision-making
L3: ",L2
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends broad academic knowledge testing to multiple languages, measuring whether models can transfer reasoning and subject knowledge beyond English across many domains. It typically uses standardized multiple-choice questions to evaluate multilingual comprehension, cross-lingual generalization, and consistency of knowledge access.","L1: Language Comprehension
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory
L3: Cognitive Flexibility",L3
MMMU,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2311.16502,"MMMU is a multimodal benchmark spanning many disciplines where models must answer questions that combine text with images such as diagrams, charts, or scientific figures. It targets expert-level multimodal reasoning, requiring models to parse visual information, integrate it with textual context, and produce a correct selection or response.","L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Working Memory, Visual Attention & Eye Movements
L3: ",L2
LAB-Bench FigQA,https://huggingface.co/datasets/futurehouse/lab-bench,https://arxiv.org/abs/2407.10362,"LAB-Bench FigQA evaluates whether models can correctly interpret and reason over complex biology paper figures (e.g., plots, panels, and annotations) to answer scientific questions. It stresses fine-grained visual analysis, mapping visual cues to domain semantics, and resisting plausible-but-wrong interpretations common in scientific figure reading.","L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Working Memory, Visual Attention & Eye Movements, Semantic Understanding & Context Recognition
L3: ",L2
WebArena,https://webarena.dev/,https://arxiv.org/abs/2307.13854,"WebArena evaluates autonomous web agents in realistic, self-hosted web apps (e-commerce, CMS, forums, Git repositories, maps) where tasks require navigation, form filling, search, and multi-step workflows. It measures an agent’s ability to plan and execute actions from pixel/DOM observations, maintain task state, and recover from mistakes under interactive dynamics.","L1: Visual Perception, Language Comprehension, Language Production
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory, Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Sensorimotor Coordination
L3: ",L2
