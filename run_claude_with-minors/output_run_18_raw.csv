Benchmark,Website,Paper,Description,Cognitive Functions
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates software engineering ability by giving a real GitHub repository, an issue description, and tests that currently fail. The model must produce a patch that makes the repository’s test suite pass; the “Verified” subset focuses on tasks that have been manually validated as solvable and correctly specified.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Language Comprehension, Language Production, Logical Reasoning"
SWE-bench Multilingual,https://www.swebench.com/multilingual.html,https://arxiv.org/abs/2504.21798,"SWE-bench Multilingual extends SWE-bench-style bugfix tasks beyond Python to multiple programming languages, emphasizing cross-language transfer and tool-driven debugging. Models must interpret natural-language issues and repository context, then generate correct code changes that satisfy test-based evaluation across languages.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Language Comprehension, Language Production, Cognitive Flexibility, Logical Reasoning"
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro is a larger, harder software engineering benchmark designed to better reflect professional development work and reduce easy shortcuts. It emphasizes more diverse repositories and realistic fixes, typically requiring deeper codebase understanding, multi-file edits, and reliable test-driven iteration.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Language Comprehension, Language Production, Logical Reasoning, Cognitive Flexibility"
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 measures an agent’s ability to complete real-world tasks inside a command-line environment, such as installing dependencies, editing files, running programs, and troubleshooting failures. Success depends on choosing correct shell actions, interpreting noisy outputs/logs, and iterating until the task’s objective is met.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Language Comprehension, Logical Reasoning"
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp evaluates deep research and browsing competence: the model must search, read, and synthesize information from the web (or a controlled document collection, depending on the setup) to answer challenging questions. It stresses selecting promising sources, cross-checking evidence, and composing grounded answers with citations or supporting details.","Planning, Decision-making, Attention, Working Memory, Episodic Memory, Semantic Understanding & Context Recognition, Language Comprehension, Language Production, Logical Reasoning"
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench evaluates interactive, policy-constrained tool use in customer-support-like simulations (e.g., retail, airline, telecom) with multi-turn users and APIs. Agents must follow domain rules while resolving requests, requiring robust dialog state tracking, correct API sequencing, and refusal/constraint handling when necessary.","Social Reasoning & Theory of Mind, Empathy, Inhibitory Control, Decision-making, Planning, Working Memory, Language Comprehension, Language Production, Adaptive Error Correction"
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld evaluates multimodal computer-use agents operating in full desktop environments, requiring navigation of GUIs, apps, and web pages to complete tasks. Models must interpret screenshots, plan action sequences (click/type/scroll), recover from mistakes, and manage multi-step workflows under step limits.","Visual Perception, Scene Understanding & Visual Reasoning, Attention, Visual Attention & Eye Movements, Sensorimotor Coordination, Planning, Decision-making, Working Memory, Adaptive Error Correction"
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI measures abstraction and generalization from a few input–output grid examples, where each task defines a novel transformation rule. Models must infer the latent rule and produce the correct output grid for new inputs, rewarding flexible pattern discovery rather than memorized domain knowledge.","Cognitive Flexibility, Spatial Representation & Mapping, Visual Perception, Scene Understanding & Visual Reasoning, Working Memory, Logical Reasoning, Planning"
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon agent coherence by having the model run a simulated vending-machine business over many decisions and interactions. The agent must manage inventory, pricing, supplier negotiation, and cashflow, adapting to changing conditions to maximize final balance.","Planning, Decision-making, Episodic Memory, Working Memory, Reward Mechanisms, Adaptive Error Correction, Social Reasoning & Theory of Mind"
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas evaluates real-world tool use through the Model Context Protocol, where models must discover relevant tools, call them with correct schemas, handle errors, and compose results. Tasks typically require multi-step workflows across multiple tool calls and integrations, emphasizing robust execution over single-turn Q&A.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Language Comprehension, Language Production, Logical Reasoning"
FinanceAgent,https://www.vals.ai/benchmarks/finance_agent,,"FinanceAgent measures how well a model performs tasks typical of an entry-level financial analyst, such as analyzing statements, building/validating models, summarizing findings, and answering finance-domain questions. It stresses structured reasoning, careful constraint following, and producing usable professional outputs (often across multiple steps).","Semantic Understanding & Context Recognition, Logical Reasoning, Decision-making, Planning, Working Memory, Language Comprehension, Language Production"
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity agent capability on real software vulnerabilities, including finding known issues from descriptions and attempting discovery of previously unknown vulnerabilities. Agents must reason over codebases, reproduce or validate weaknesses, and iteratively refine hypotheses based on tool feedback.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Language Comprehension, Logical Reasoning, Cognitive Flexibility"
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"SpreadsheetBench tests an agent’s ability to navigate and manipulate realistic spreadsheets, including formulas, tables, formatting, and multi-sheet dependencies. Successful solutions require extracting the right information, performing correct transformations or calculations, and validating outputs against expected results.","Planning, Decision-making, Working Memory, Adaptive Error Correction, Language Comprehension, Logical Reasoning, Semantic Understanding & Context Recognition"
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a difficult multimodal benchmark spanning frontier-level questions across many domains, designed to test deep knowledge, reasoning, and synthesis rather than rote recall. Depending on the evaluation setting, models may also be assessed on tool use (e.g., search or code) while maintaining grounded, correct answers.","Language Comprehension, Language Production, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Visual Perception, Scene Understanding & Visual Reasoning, Planning, Decision-making"
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 evaluates competition-style mathematical problem solving using questions from the American Invitational Mathematics Examination. The benchmark stresses multi-step symbolic reasoning, precise arithmetic/algebra, and maintaining correctness under tight problem constraints.","Logical Reasoning, Working Memory, Planning, Attention"
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a high-quality subset of graduate-level, “Google-proof” multiple-choice science questions intended to resist shallow retrieval strategies. It probes scientific reasoning and conceptual understanding across physics, chemistry, and biology under adversarially difficult distractors.","Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Attention, Decision-making"
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends MMLU to multiple languages, evaluating broad academic knowledge and reasoning across many subjects and non-English settings. It tests whether models can preserve competence under translation, differing cultural/linguistic contexts, and varied writing systems.","Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Cognitive Flexibility, Attention"
MMMU,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2311.16502,"MMMU measures multimodal expert-level understanding across many disciplines using problems that require reasoning over both text and images (e.g., diagrams, charts, scientific visuals). It emphasizes integrating visual evidence with domain knowledge to answer questions correctly.","Multisensory Integration, Visual Perception, Scene Understanding & Visual Reasoning, Language Comprehension, Logical Reasoning, Working Memory, Attention"
LAB-Bench FigQA,https://huggingface.co/datasets/futurehouse/lab-bench,https://arxiv.org/abs/2407.10362,"LAB-Bench FigQA tests whether models can correctly interpret complex scientific figures from biology papers, including plots, schematics, and multi-panel graphics. It emphasizes precise visual reading, mapping figure elements to scientific claims, and avoiding unsupported inferences when visuals are ambiguous.","Visual Perception, Scene Understanding & Visual Reasoning, Language Comprehension, Logical Reasoning, Semantic Understanding & Context Recognition, Attention, Working Memory"
WebArena,https://webarena.dev/,https://arxiv.org/abs/2307.13854,"WebArena evaluates autonomous web agents performing realistic tasks across multiple web applications (e-commerce, CMS, forums, repositories, maps) in a controlled environment. Agents must navigate dynamic pages, fill forms, click and scroll correctly, track goals across steps, and recover from errors to complete end-to-end objectives.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Visual Perception, Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Sensorimotor Coordination, Language Comprehension, Language Production"
