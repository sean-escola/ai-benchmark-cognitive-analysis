Benchmark,Website,Paper,Description,Cognitive Functions
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates software-engineering agents on real GitHub issues where the model must produce a patch that makes a project’s tests pass. The “Verified” subset contains tasks validated by humans to be solvable and to have reliable evaluation, reducing ambiguity and flaky grading.","Planning, Logical Reasoning, Working Memory, Adaptive Error Correction, Language Comprehension, Language Production"
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 measures how well an agent can complete real-world, command-line tasks inside a sandboxed terminal environment. Tasks require choosing and chaining shell commands, inspecting files and outputs, and recovering from errors under resource and time constraints.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Language Comprehension, Language Production"
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp evaluates “deep research” behavior: answering questions that require searching, reading, and synthesizing information across multiple web sources. It emphasizes robust evidence gathering, cross-checking, and producing grounded final answers rather than single-hop retrieval.","Planning, Decision-making, Attention, Working Memory, Semantic Understanding & Context Recognition, Language Comprehension, Language Production"
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench (Tau2-bench) evaluates tool-using customer-support agents in simulated multi-turn interactions (e.g., retail, airline, telecom) with policies and API-like tools. Success depends on following rules consistently, choosing correct tool calls, and maintaining coherent dialogue across many turns.","Social Reasoning & Theory of Mind, Decision-making, Planning, Inhibitory Control, Working Memory, Language Comprehension, Language Production"
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld evaluates multimodal “computer use” agents that must complete tasks in full operating system environments using screenshots and UI interactions. It tests navigation, form filling, multi-app workflows, and error recovery under step limits.","Visual Perception, Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Sensorimotor Coordination, Planning, Decision-making, Working Memory"
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI measures abstract reasoning and generalization from a handful of examples using grid-based pattern transformation puzzles. Models must infer latent rules and apply them to new inputs, emphasizing novelty rather than memorized skills.","Logical Reasoning, Working Memory, Cognitive Flexibility, Planning, Spatial Representation & Mapping, Scene Understanding & Visual Reasoning"
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon autonomous agent behavior in a simulated business-management setting (running a vending machine business over an extended period). Agents must make sequential decisions about inventory, pricing, suppliers, and adaptation to changing conditions to maximize final outcomes.","Planning, Decision-making, Working Memory, Reward Mechanisms, Adaptive Error Correction, Self-reflection"
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas measures real-world tool use through the Model Context Protocol (MCP), where models must discover and correctly invoke tools across multi-step workflows. Tasks emphasize selecting appropriate APIs, handling failures/retries, and synthesizing tool outputs into correct answers.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Language Comprehension, Language Production"
FinanceAgent,https://www.vals.ai/benchmarks/finance_agent,,"FinanceAgent evaluates agentic performance on tasks typical of an entry-level financial analyst, often requiring spreadsheet-like reasoning, document synthesis, and quantitative checks. It stresses correctness, structured outputs, and end-to-end task completion rather than only answering questions.","Logical Reasoning, Semantic Understanding & Context Recognition, Planning, Decision-making, Working Memory, Language Production"
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity agents on vulnerability understanding and discovery across real software projects, including finding known vulnerabilities from descriptions and identifying new issues. Tasks require program analysis, hypothesis testing, and precise reporting of findings.","Logical Reasoning, Planning, Adaptive Error Correction, Working Memory, Language Comprehension, Language Production"
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"SpreadsheetBench tests whether models can navigate and manipulate realistic spreadsheets to solve applied problems (e.g., data cleaning, formulas, transformations, and summarization). It typically requires structured reasoning over tabular data and correct execution of multi-step edits.","Logical Reasoning, Working Memory, Planning, Adaptive Error Correction, Semantic Understanding & Context Recognition, Decision-making"
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam (HLE) is a broad, frontier-focused benchmark designed to test advanced reasoning and knowledge across many domains, often with multimodal inputs. It aims to probe hard questions where superficial pattern matching is insufficient and tool use may matter depending on the setting.","Language Comprehension, Language Production, Logical Reasoning, Working Memory, Semantic Understanding & Context Recognition, Visual Perception, Multisensory Integration"
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 evaluates competition-style mathematics problem solving drawn from the American Invitational Mathematics Examination. Problems demand multi-step derivations, careful symbolic manipulation, and exact final answers under strict formatting.","Logical Reasoning, Working Memory, Planning, Adaptive Error Correction"
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a hard subset of graduate-level science multiple-choice questions intended to be difficult to answer via shallow retrieval. It emphasizes rigorous reasoning over expert knowledge in physics, chemistry, and biology with distractors that punish imprecise understanding.","Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Language Comprehension"
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends MMLU to multiple languages, testing broad academic knowledge and reasoning across many subjects in non-English settings. It stresses cross-lingual robustness, comprehension, and consistent reasoning despite language shifts.","Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning, Cognitive Flexibility, Working Memory"
MMMU,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2311.16502,"MMMU is a multimodal benchmark covering many disciplines where models must answer questions grounded in images (charts, diagrams, screenshots) plus text. It tests integrated visual-text reasoning and accurate interpretation of complex visual artifacts.","Visual Perception, Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Visual Attention & Eye Movements, Working Memory"
LAB-Bench FigQA,https://huggingface.co/datasets/futurehouse/lab-bench,https://arxiv.org/abs/2407.10362,"LAB-Bench FigQA evaluates whether models can interpret and reason about scientific figures from biology papers, including plots and experimental schematics. It focuses on extracting correct relationships from visuals and using them to answer research-style questions.","Visual Perception, Scene Understanding & Visual Reasoning, Logical Reasoning, Multisensory Integration, Working Memory, Semantic Understanding & Context Recognition"
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning tests reasoning over figures and content from scientific papers, often requiring careful interpretation of plots, tables, and experimental setups. It emphasizes multi-step inference grounded in the provided scientific context rather than generic visual recognition.","Scene Understanding & Visual Reasoning, Visual Perception, Logical Reasoning, Working Memory, Multisensory Integration, Semantic Understanding & Context Recognition"
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU extends multimodal understanding to videos, requiring models to integrate information across frames and time to answer questions. It tests temporal grounding, multi-step reasoning over dynamic scenes, and retention of relevant details over longer sequences.","Visual Perception, Scene Understanding & Visual Reasoning, Multisensory Integration, Cognitive Timing & Predictive Modeling, Attention, Working Memory"
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"FACTS Benchmark Suite systematically evaluates factuality, including whether models make unsupported claims, mishandle uncertainty, or contradict sources. It aims to separate fluent generation from reliably grounded, evidence-consistent answers across varied scenarios.","Semantic Understanding & Context Recognition, Language Comprehension, Language Production, Inhibitory Control, Self-reflection, Working Memory"
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA evaluates commonsense physical reasoning across many languages, focusing on understanding everyday situations and plausible actions/outcomes. It probes whether models preserve commonsense and reasoning consistency under multilingual variation.","Logical Reasoning, Semantic Understanding & Context Recognition, Language Comprehension, Cognitive Flexibility, Working Memory"
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) evaluates long-context retrieval and multi-round coreference resolution by embedding multiple similar “needles” within long “haystacks” and asking for a specific referenced response. The 8-needle setting stresses sustained attention, interference resistance, and accurate selection among many near-duplicates.","Working Memory, Attention, Semantic Understanding & Context Recognition, Episodic Memory, Inhibitory Control, Logical Reasoning"
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval evaluates well-specified professional knowledge-work tasks across many occupations, often requiring producing artifacts like spreadsheets, plans, analyses, and written deliverables judged against expert work. It emphasizes end-to-end task execution quality, correctness, and usefulness under real-world constraints.","Planning, Decision-making, Language Production, Semantic Understanding & Context Recognition, Self-reflection, Social Reasoning & Theory of Mind, Working Memory"
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks tests multi-step reasoning over graph-structured data, such as traversals or relational queries that require following edges across many hops. It emphasizes precise state tracking over long chains and resisting shortcuts when distractor paths exist.","Spatial Representation & Mapping, Logical Reasoning, Working Memory, Attention, Adaptive Error Correction"
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon evaluates an agent’s ability to solve tasks by selecting and orchestrating many tools under realistic constraints (correct tool choice, parameters, sequencing, and verification). It emphasizes robustness to tool errors, partial results, and the need to plan multi-step workflows.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Inhibitory Control, Language Comprehension, Language Production"
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath is a benchmark of advanced mathematics problems intended to be challenging for state-of-the-art models, often requiring long derivations and careful verification. It targets deep mathematical reasoning rather than short contest tricks, with strict correctness requirements.","Logical Reasoning, Working Memory, Planning, Adaptive Error Correction"
