Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates software engineering agents on real GitHub issues by requiring a patch that passes the repository’s tests. The “Verified” subset emphasizes tasks that have been manually checked to be solvable and correctly scored, reducing noise from ambiguous or broken tasks.","L1: Language Comprehension, Language Production
L2: Planning, Logical Reasoning, Adaptive Error Correction, Working Memory
L3: ",L2
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 measures how well an agent can complete real command-line tasks inside a sandboxed terminal environment, typically requiring iterative execution, inspection, and repair. Success depends on choosing correct commands, interpreting errors, and converging on a working solution under resource and time constraints.","L1: Language Comprehension
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory
L3: ",L2
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp tests “deep research” agents on questions that require searching, reading, and synthesizing information from a curated document corpus or controlled web-like setting. It emphasizes multi-step information gathering, evidence tracking, and final answer generation that is consistent with retrieved sources.","L1: Language Comprehension, Language Production
L2: Planning, Attention, Episodic Memory, Semantic Understanding & Context Recognition, Decision-making
L3: ",L2
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench evaluates interactive tool-using agents in customer-support style environments (e.g., retail, airline, telecom) with policy constraints and multi-turn dialogues. Agents must coordinate conversation, API calls, and policy compliance to resolve the user’s request while avoiding prohibited actions.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making
L3: Inhibitory Control, Social Reasoning & Theory of Mind, Empathy",L3
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld benchmarks autonomous “computer use” by giving agents tasks on a desktop-like operating system that require navigating GUIs, launching apps, and completing multi-step workflows. Performance depends on accurately interpreting screen state and executing sequences of interface actions reliably.","L1: Visual Perception
L2: Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Planning, Decision-making, Working Memory, Sensorimotor Coordination
L3: ",L2
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI measures abstract reasoning and generalization from few examples using grid-based input–output puzzles where the underlying rule is hidden. Models must infer the transformation and apply it to new inputs, emphasizing novelty and systematic pattern induction.","L1: Visual Perception
L2: Logical Reasoning, Working Memory, Spatial Representation & Mapping
L3: Cognitive Flexibility",L3
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon agent coherence by simulating management of a vending-machine business over extended time, including procurement, pricing, inventory, and communication. Agents must sustain a strategy, react to market dynamics, and compound gains over many decisions.","L1: 
L2: Planning, Decision-making, Reward Mechanisms, Working Memory, Adaptive Error Correction, Semantic Understanding & Context Recognition
L3: ",L2
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas measures real-world tool use through the Model Context Protocol (MCP), requiring agents to discover, invoke, and chain tools across multi-step workflows. It stresses correct tool selection, parameterization, error handling, and synthesis of tool outputs into final responses.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory
L3: ",L2
FinanceAgent,https://www.vals.ai/benchmarks/finance_agent,,"FinanceAgent assesses how well a model performs tasks typical of an entry-level financial analyst, such as analyzing company information, creating structured outputs, and performing finance-oriented reasoning. It emphasizes accurate interpretation of problem context and producing actionable, well-structured deliverables.","L1: Language Production
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Planning, Decision-making, Working Memory
L3: ",L2
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity capability by testing whether agents can identify known vulnerabilities from descriptions and discover new issues in real open-source codebases. Tasks often require code navigation, hypothesis testing, and iterative refinement based on failures or partial findings.","L1: 
L2: Logical Reasoning, Planning, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition
L3: ",L2
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"SpreadsheetBench tests an agent’s ability to understand, modify, and generate complex spreadsheets based on real-world-like tasks. It involves transforming data, applying formulas, maintaining layout/formatting constraints, and verifying outputs against task requirements.","L1: 
L2: Working Memory, Planning, Decision-making, Logical Reasoning, Adaptive Error Correction, Semantic Understanding & Context Recognition
L3: ",L2
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a broad, frontier-level benchmark spanning difficult questions across domains and modalities, aiming to probe advanced knowledge and reasoning rather than routine recall. Many items require multi-step inference, careful interpretation of problem statements, and sometimes integrating visual or structured information.","L1: Language Comprehension, Visual Perception
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory
L3: ",L2
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 evaluates competition-style mathematics problem solving using the AIME exam format, typically requiring multi-step derivations and precise numerical answers. The benchmark stresses symbolic manipulation, constraint tracking, and error-free arithmetic under complex reasoning chains.","L1: 
L2: Logical Reasoning, Working Memory, Planning, Adaptive Error Correction
L3: ",L2
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a high-difficulty subset of graduate-level science multiple-choice questions designed to be resistant to shallow pattern matching. It probes deep conceptual understanding and multi-step reasoning across biology, chemistry, and physics topics.","L1: Language Comprehension
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory
L3: ",L2
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends broad academic evaluation across many subjects into multiple languages, testing both knowledge and reasoning under multilingual prompts. It emphasizes robust understanding across linguistic variations and domain contexts rather than English-only performance.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory
L3: ",L2
MMMU,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2311.16502,"MMMU measures multimodal expert-level understanding and reasoning by combining text questions with images from diverse disciplines. Success requires interpreting visual content, integrating it with textual context, and performing domain-relevant reasoning to select or generate correct answers.","L1: Visual Perception, Language Comprehension
L2: Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Working Memory
L3: ",L2
LAB-Bench FigQA,https://huggingface.co/datasets/futurehouse/lab-bench,https://arxiv.org/abs/2407.10362,"LAB-Bench FigQA focuses on scientific figure question answering, requiring models to interpret complex plots/diagrams from biology papers and answer targeted questions. It stresses accurate extraction of visual evidence and reasoning that connects figure content to the question’s intent.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory
L3: ",L2
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning evaluates reasoning over scientific paper figures and related contextual text, emphasizing quantitative and structural interpretation rather than surface caption reading. Many items require extracting relationships from charts/diagrams and composing a justified conclusion.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory
L3: ",L2
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU benchmarks multimodal reasoning over videos paired with text questions, often requiring temporal integration of events and visual details across frames. It tests whether models can maintain coherence over time and answer based on evidence distributed throughout the clip.","L1: Visual Perception
L2: Attention, Working Memory, Scene Understanding & Visual Reasoning, Multisensory Integration
L3: Cognitive Timing & Predictive Modeling",L3
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,FACTS Benchmark Suite systematically evaluates factuality by testing whether a model’s statements are supported by reliable evidence and whether it avoids hallucinating unsupported claims. It targets calibration-like behavior: abstaining or qualifying answers when information is uncertain or missing.,"L1: Language Production
L2: Semantic Understanding & Context Recognition, Episodic Memory
L3: Inhibitory Control, Self-reflection",L3
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA evaluates commonsense and physical reasoning across many languages and cultural contexts, emphasizing whether models generalize beyond English-centric priors. It probes practical inference about everyday situations while controlling for cross-lingual variation.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning
L3: Social Reasoning & Theory of Mind",L3
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) measures long-context, multi-round coreference resolution by embedding multiple similar “needles” inside long distractor contexts and requiring retrieval of the correct referenced answer. It stresses precise tracking of entities and dependencies over very long inputs without conflation.","L1: Language Comprehension
L2: Working Memory, Attention, Episodic Memory, Semantic Understanding & Context Recognition
L3: ",L2
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval evaluates economically relevant, well-specified knowledge work across many occupations by having models produce real work artifacts (e.g., spreadsheets, presentations, schedules) and comparing outputs to expert baselines. It emphasizes end-to-end task completion quality, including structure, correctness, and usefulness.","L1: Language Production
L2: Planning, Decision-making, Semantic Understanding & Context Recognition, Working Memory, Adaptive Error Correction
L3: ",L2
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks tests reasoning over graph-structured data by requiring models to follow paths, apply traversal rules, and answer questions that depend on multi-step navigation. It emphasizes faithful, stepwise tracking of nodes/edges under distractors and longer sequences.","L1: 
L2: Spatial Representation & Mapping, Working Memory, Logical Reasoning, Planning, Attention
L3: ",L2
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon evaluates tool-using agents on diverse, multi-step tasks that require choosing among tools, calling them correctly, and integrating results into a final output. It stresses robustness to tool errors, compositional workflows, and correct execution under evolving state.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory
L3: ",L2
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath benchmarks advanced mathematics at the research frontier, designed to be challenging and less susceptible to memorization. Problems often require long derivations, creative strategy selection, and careful management of intermediate results.","L1: 
L2: Logical Reasoning, Working Memory, Planning, Adaptive Error Correction
L3: Cognitive Flexibility",L3
