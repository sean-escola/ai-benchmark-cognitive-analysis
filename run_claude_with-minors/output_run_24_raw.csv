Benchmark,Website,Paper,Description,Cognitive Functions
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates whether a model can solve real GitHub issues by producing a correct patch that passes the repository’s tests, using a curated set of tasks verified to be solvable. It measures end-to-end software debugging and implementation skill under realistic constraints (reading code, localizing bugs, editing files, and validating fixes).","Language Comprehension, Logical Reasoning, Planning, Working Memory, Adaptive Error Correction, Decision-making, Semantic Understanding & Context Recognition"
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 measures an agent’s ability to complete practical tasks in a command-line environment (e.g., installing tools, manipulating files, running programs, and interpreting outputs). It emphasizes iterative diagnosis and recovery from errors, where progress depends on choosing the right shell actions based on feedback from the environment.","Planning, Decision-making, Working Memory, Adaptive Error Correction, Language Comprehension, Semantic Understanding & Context Recognition, Sensorimotor Coordination, Attention"
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp evaluates deep research and browsing competence by requiring models to answer questions using information found via web browsing/search and multi-step investigation. It stresses decomposing an information need into sub-queries, integrating evidence across sources, and producing a justified final answer under time and context constraints.","Planning, Decision-making, Working Memory, Attention, Semantic Understanding & Context Recognition, Language Comprehension, Language Production, Self-reflection"
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench evaluates interactive tool-using agents in customer-support style domains (e.g., retail, airline, telecom), where the agent must follow policies while calling APIs and conversing over multiple turns. It probes policy compliance, robust dialogue management, and the ability to resolve user problems through correct sequences of actions.","Language Comprehension, Language Production, Planning, Decision-making, Working Memory, Inhibitory Control, Social Reasoning & Theory of Mind, Empathy, Attention"
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld evaluates multimodal “computer use” by asking agents to complete tasks on a real operating system via screenshots and actions (mouse/keyboard), such as configuring settings, navigating apps, or filling forms. Success requires interpreting UI state, planning action sequences, and recovering from misclicks or unexpected interface changes.","Visual Perception, Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Sensorimotor Coordination, Planning, Decision-making, Working Memory, Adaptive Error Correction, Attention"
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,ARC-AGI measures fluid reasoning by presenting few-shot grid transformation puzzles where the model must infer abstract rules from a handful of examples and apply them to new inputs. It is designed to reduce reliance on memorized knowledge and emphasize generalization to novel pattern-learning tasks.,"Logical Reasoning, Cognitive Flexibility, Working Memory, Attention, Spatial Representation & Mapping, Scene Understanding & Visual Reasoning, Planning"
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon agent coherence by simulating operation of a vending-machine business over many steps, scoring the final financial outcome. The agent must manage inventory, pricing, procurement, and adaptation to changing conditions while maintaining consistent objectives over extended interactions.","Planning, Decision-making, Working Memory, Episodic Memory, Reward Mechanisms, Adaptive Error Correction, Semantic Understanding & Context Recognition, Motivational Drives"
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas measures real-world tool use via the Model Context Protocol by requiring models to discover, invoke, and chain calls across production-like tool servers. Tasks emphasize correct API selection and parameterization, error handling, and synthesis of multi-tool results into reliable final outputs.","Planning, Decision-making, Working Memory, Adaptive Error Correction, Language Comprehension, Semantic Understanding & Context Recognition, Sensorimotor Coordination, Attention"
FinanceAgent,https://www.vals.ai/benchmarks/finance_agent,,"FinanceAgent evaluates whether a model can perform tasks typical of an entry-level financial analyst, such as interpreting financial statements, building analyses, and producing decision-relevant summaries. It targets practical reasoning with domain constraints, often requiring careful handling of assumptions, calculations, and structured outputs.","Logical Reasoning, Decision-making, Planning, Working Memory, Language Comprehension, Language Production, Semantic Understanding & Context Recognition, Self-reflection"
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity agent skills on large-scale tasks involving vulnerability identification, reproduction, and in some settings discovery within real open-source codebases. It emphasizes reading and reasoning about code, forming exploitation hypotheses, and iteratively testing/patching based on empirical feedback.","Logical Reasoning, Planning, Decision-making, Working Memory, Adaptive Error Correction, Language Comprehension, Semantic Understanding & Context Recognition, Inhibitory Control"
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"SpreadsheetBench evaluates an agent’s ability to understand, edit, and compute over complex spreadsheets that mirror real workplace artifacts. Tasks typically require multi-step transformations, formula reasoning, and validation of results in a structured, high-precision environment.","Logical Reasoning, Working Memory, Planning, Decision-making, Adaptive Error Correction, Semantic Understanding & Context Recognition, Language Comprehension, Attention"
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a challenging multimodal benchmark spanning frontier academic and professional knowledge, designed to stress models on difficult questions rather than routine recall. It evaluates deep reasoning, cross-domain synthesis, and (when enabled) tool-assisted problem solving across text and images.","Language Comprehension, Language Production, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Planning, Visual Perception, Scene Understanding & Visual Reasoning, Attention"
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 evaluates competition-style mathematics problem solving using the 2025 AIME questions, typically requiring multi-step derivations and careful algebraic/number-theoretic reasoning. It emphasizes exactness, intermediate-state tracking, and avoidance of subtle logical errors.","Logical Reasoning, Working Memory, Planning, Attention, Adaptive Error Correction"
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a subset of the GPQA science multiple-choice benchmark curated to be especially difficult, where experts succeed and non-experts often fail. It evaluates deep scientific reasoning and the ability to disambiguate plausible distractors under knowledge-intensive constraints.","Language Comprehension, Logical Reasoning, Working Memory, Semantic Understanding & Context Recognition, Attention"
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU is a multilingual extension of broad academic knowledge testing across many subjects and languages, measuring understanding and reasoning in non-English settings. It probes whether knowledge and problem-solving transfer across languages rather than relying on English-only patterns.","Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Attention"
MMMU,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2311.16502,"MMMU evaluates expert-level multimodal understanding across many disciplines, where models must combine text with diagrams, charts, or images to answer questions. It stresses visual reasoning grounded in domain context and the integration of information across modalities.","Visual Perception, Scene Understanding & Visual Reasoning, Multisensory Integration, Language Comprehension, Logical Reasoning, Working Memory, Attention"
LAB-Bench FigQA,https://huggingface.co/datasets/futurehouse/lab-bench,https://arxiv.org/abs/2407.10362,"LAB-Bench FigQA tests whether models can interpret and reason over scientific figures from biology papers, including plots, schematics, and multi-panel graphics. Success requires extracting relevant visual evidence and mapping it to domain concepts and questions.","Visual Perception, Scene Understanding & Visual Reasoning, Logical Reasoning, Semantic Understanding & Context Recognition, Attention, Working Memory"
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning evaluates reasoning over charts and figure-like visuals commonly found in scientific documents, often requiring quantitative or relational inference beyond surface recognition. It probes long-form visual-to-text mapping, where correct answers depend on interpreting axes, legends, and plotted relationships.","Visual Perception, Scene Understanding & Visual Reasoning, Logical Reasoning, Working Memory, Attention, Language Comprehension, Semantic Understanding & Context Recognition"
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU evaluates multimodal understanding and reasoning over videos paired with questions, requiring models to integrate information across time. It stresses tracking events, relationships, and state changes across frames rather than relying on a single static image.","Visual Perception, Attention, Working Memory, Scene Understanding & Visual Reasoning, Cognitive Timing & Predictive Modeling, Multisensory Integration"
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"FACTS Benchmark Suite systematically evaluates factuality by testing whether model outputs remain grounded, avoid hallucinations, and preserve correctness under varied prompting and retrieval settings. It emphasizes faithfulness to provided evidence and reliable attribution when generating claims.","Language Comprehension, Language Production, Semantic Understanding & Context Recognition, Inhibitory Control, Self-reflection, Working Memory, Attention"
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA evaluates physical commonsense reasoning (how objects and actions interact) across diverse linguistic and cultural settings, aiming to test robustness beyond English-centric distributions. It probes whether models can select plausible solutions to everyday physical interaction scenarios under multilingual variation.","Semantic Understanding & Context Recognition, Logical Reasoning, Spatial Representation & Mapping, Scene Understanding & Visual Reasoning, Language Comprehension, Working Memory"
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,MRCR v2 (8-needle) tests long-context retrieval and multi-round coreference by embedding multiple similar “needle” interactions within long “haystack” conversations and asking for the correct referenced response. It stresses attention control and accurate retrieval when many distractors are highly confusable.,"Working Memory, Episodic Memory, Attention, Semantic Understanding & Context Recognition, Language Comprehension, Adaptive Error Correction"
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval evaluates well-specified knowledge-work tasks across many occupations, judged by experts on whether the model’s produced work product wins, ties, or loses against professional baselines. Tasks include creating structured artifacts (e.g., plans, spreadsheets, presentations) where quality depends on correct constraints, clarity, and usefulness.","Planning, Decision-making, Language Production, Language Comprehension, Semantic Understanding & Context Recognition, Working Memory, Social Reasoning & Theory of Mind, Self-reflection"
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,Graphwalks evaluates reasoning over graph-structured data by requiring models to follow traversal rules and answer questions that depend on multi-step navigation and relational composition. It stresses consistent state tracking over long sequences and resistance to local distractors.,"Logical Reasoning, Working Memory, Attention, Spatial Representation & Mapping, Planning, Semantic Understanding & Context Recognition"
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon evaluates general tool-using competence across diverse APIs and multi-step workflows, where the model must decide which tools to call, in what order, and how to recover from failures. It emphasizes robust orchestration under realistic tool errors, partial information, and iterative refinement.","Planning, Decision-making, Working Memory, Adaptive Error Correction, Language Comprehension, Semantic Understanding & Context Recognition, Sensorimotor Coordination, Attention"
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath evaluates expert-level mathematics with problems designed to be difficult and contamination-resistant, often requiring deep multi-step reasoning and precise computation. It probes advanced proof-like reasoning, careful abstraction, and error-checking over long solution chains.","Logical Reasoning, Working Memory, Planning, Attention, Cognitive Flexibility, Adaptive Error Correction"
