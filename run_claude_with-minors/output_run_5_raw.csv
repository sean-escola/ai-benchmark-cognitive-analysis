Benchmark,Website,Paper,Description,Cognitive Functions
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"A software engineering benchmark where models receive a real GitHub issue plus a code repository snapshot and must produce a patch that passes the project’s tests. The Verified split consists of tasks validated by humans to be solvable and to have reliable evaluation, emphasizing end-to-end debugging and code change correctness.","Planning, Decision-making, Logical Reasoning, Working Memory, Adaptive Error Correction, Language Comprehension"
Terminal-Bench 2.0,https://www.tbench.ai/,,"An agent benchmark for accomplishing real tasks in a command-line environment (e.g., using Unix tools, package managers, and scripts) with limited instructions. Success requires choosing and sequencing shell actions, interpreting errors, and iterating until the desired system state is achieved.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Language Comprehension"
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"A web-research benchmark where models must answer questions by searching and reading sources, typically under constraints that favor careful evidence gathering over memorization. It stresses selecting queries, extracting relevant passages, and synthesizing a final answer from noisy or partial information.","Planning, Decision-making, Attention, Working Memory, Semantic Understanding & Context Recognition, Language Comprehension"
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"An interactive customer-support benchmark in simulated domains (e.g., retail, airline, telecom) where the agent must follow policies while using tools/APIs and conversing with a user. It evaluates multi-turn robustness, policy compliance, and the ability to resolve a case end-to-end despite ambiguity or user pressure.","Social Reasoning & Theory of Mind, Inhibitory Control, Decision-making, Planning, Language Comprehension, Language Production, Working Memory, Empathy"
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"A computer-use benchmark where agents operate a full operating system via a GUI to complete tasks across applications. It requires perceiving screenshots, navigating interfaces, and executing multi-step action sequences while handling UI variability and recoveries from mistakes.","Visual Perception, Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Sensorimotor Coordination, Planning, Decision-making, Working Memory, Adaptive Error Correction, Spatial Representation & Mapping"
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"A fluid-reasoning benchmark of abstract grid transformation tasks: given a few input-output examples, the model must infer the hidden rule and produce the correct output for a new input. It targets generalization to novel patterns and compositional rule induction rather than domain knowledge.","Logical Reasoning, Working Memory, Cognitive Flexibility, Spatial Representation & Mapping, Visual Perception, Scene Understanding & Visual Reasoning, Attention"
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"A long-horizon agent simulation where the model runs a vending-machine business over an extended period, making thousands of decisions (inventory, pricing, supplier negotiation, etc.). It measures sustained coherence, strategic adaptation, and goal-directed optimization under changing market conditions.","Planning, Decision-making, Working Memory, Reward Mechanisms, Cognitive Flexibility, Self-reflection, Language Comprehension, Language Production"
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"A tool-use benchmark centered on the Model Context Protocol (MCP), evaluating whether models can discover relevant tools, call them correctly, and chain multiple calls into a coherent workflow. It emphasizes reliability under realistic API surfaces, including error handling, retries, and result synthesis.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Language Comprehension, Semantic Understanding & Context Recognition"
FinanceAgent,https://www.vals.ai/benchmarks/finance_agent,,"An evaluation of agent performance on tasks representative of entry-level financial analyst work (e.g., extracting figures, building analyses, and producing written summaries). It stresses numeracy, domain reasoning, and producing structured artifacts under professional constraints.","Logical Reasoning, Decision-making, Planning, Working Memory, Semantic Understanding & Context Recognition, Language Production, Language Comprehension"
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"A cybersecurity benchmark that tests agents on identifying known vulnerabilities from descriptions and discovering previously-unknown issues in real open-source codebases. It rewards systematic investigation, hypothesis testing, and precise patch or exploit reasoning under realistic constraints.","Logical Reasoning, Planning, Adaptive Error Correction, Working Memory, Attention, Semantic Understanding & Context Recognition"
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"A benchmark for spreadsheet understanding and manipulation, where models must navigate, compute, and edit complex spreadsheets derived from real-world use cases. It evaluates correctness of transformations and the ability to maintain consistency across interconnected cells and sheets.","Working Memory, Logical Reasoning, Planning, Adaptive Error Correction, Decision-making, Semantic Understanding & Context Recognition"
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"A difficult multimodal benchmark designed to probe frontier academic and expert knowledge with questions that often require nontrivial reasoning rather than lookup. It evaluates how well models integrate text and visual information to produce accurate, well-supported answers across domains.","Language Comprehension, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Visual Perception, Scene Understanding & Visual Reasoning"
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"A set of competition mathematics problems from the 2025 AIME, typically requiring multi-step symbolic reasoning and careful arithmetic. It is used to assess mathematical problem-solving ability, with or without tool assistance such as Python.","Logical Reasoning, Working Memory, Planning, Adaptive Error Correction, Attention"
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"A challenging multiple-choice science benchmark designed to be “Google-proof,” with the Diamond subset curated for high quality and difficulty. It tests deep understanding and reasoning in physics, chemistry, and biology under strong distractors.","Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Inhibitory Control"
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,A multilingual extension of MMLU that evaluates knowledge and reasoning across many academic subjects and multiple non-English languages. It stresses robust comprehension across languages and consistent subject-matter competence under language variation.,"Language Comprehension, Semantic Understanding & Context Recognition, Working Memory, Cognitive Flexibility"
MMMU,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2311.16502,"A multidisciplinary multimodal benchmark where questions combine text with images (charts, diagrams, screenshots, etc.) across many fields. It evaluates the ability to ground language in visual evidence and perform expert-style reasoning over multimodal inputs.","Multisensory Integration, Visual Perception, Scene Understanding & Visual Reasoning, Language Comprehension, Logical Reasoning, Working Memory, Attention"
LAB-Bench FigQA,https://huggingface.co/datasets/futurehouse/lab-bench,https://arxiv.org/abs/2407.10362,"A visual reasoning benchmark drawn from biology research figures, where models must answer questions by correctly interpreting complex scientific plots and panels. It stresses fine-grained figure reading, scientific context integration, and multi-step inference from visual evidence.","Visual Perception, Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Language Comprehension, Logical Reasoning, Working Memory, Semantic Understanding & Context Recognition"
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,A benchmark focused on reasoning over scientific paper artifacts (especially figures/charts) where correct answers require interpreting visual encodings and relating them to accompanying text. It often stresses quantitative comparisons and chained inferences grounded in the document’s content.,"Visual Perception, Scene Understanding & Visual Reasoning, Language Comprehension, Logical Reasoning, Working Memory, Semantic Understanding & Context Recognition, Attention"
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"A multimodal benchmark that evaluates understanding and reasoning over video content, typically requiring integration of information across time and scenes. It measures whether models can track events, identify relevant moments, and answer questions that depend on temporal context.","Visual Perception, Visual Attention & Eye Movements, Working Memory, Cognitive Timing & Predictive Modeling, Scene Understanding & Visual Reasoning, Attention"
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"A suite of factuality evaluations aimed at measuring whether model outputs are accurate, well-grounded, and resistant to hallucination across diverse settings. It emphasizes verifying claims, abstaining when uncertain, and maintaining consistency with provided sources or known facts.","Semantic Understanding & Context Recognition, Inhibitory Control, Self-reflection, Working Memory, Language Production, Language Comprehension"
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,A physical commonsense reasoning benchmark that extends PIQA-style questions across many languages in a non-parallel setting. It tests whether models can choose plausible actions or outcomes in everyday physical scenarios under linguistic and cultural variation.,"Language Comprehension, Cognitive Flexibility, Semantic Understanding & Context Recognition, Logical Reasoning, Spatial Representation & Mapping"
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"A long-context, multi-round coreference/recall evaluation where multiple similar “needle” requests are embedded in long “haystack” dialogues, and the model must reproduce the response to a specified needle. The 8-needle variant stresses maintaining and retrieving the correct referent amid interference and distractors.","Working Memory, Attention, Episodic Memory, Semantic Understanding & Context Recognition, Inhibitory Control"
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"An evaluation of well-specified professional knowledge work tasks spanning many occupations, judged by expert humans via pairwise comparisons (e.g., “wins or ties”). It tests the ability to produce high-quality work artifacts and decisions under realistic workplace constraints.","Planning, Decision-making, Language Production, Semantic Understanding & Context Recognition, Working Memory, Social Reasoning & Theory of Mind"
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"A reasoning benchmark where models must follow paths and relationships in graph-structured data, often requiring multi-step traversal and state tracking. It targets systematic computation over discrete structures rather than surface-level pattern matching.","Spatial Representation & Mapping, Working Memory, Logical Reasoning, Planning, Attention"
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"A benchmark for complex tool-using agents that must select from many tools, compose multi-step tool calls, and recover from failures to complete tasks. It emphasizes action selection under uncertainty, robust execution, and coherent synthesis of tool outputs.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Language Comprehension, Inhibitory Control, Semantic Understanding & Context Recognition"
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"A difficult mathematics benchmark targeting advanced problem solving beyond standard competition sets, often requiring novel techniques and long derivations. It is used to probe the frontier of mathematical reasoning, sometimes with optional computational tools.","Logical Reasoning, Working Memory, Planning, Cognitive Flexibility, Adaptive Error Correction, Attention"
