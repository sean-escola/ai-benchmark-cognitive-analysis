Benchmark,Website,Paper,Description,Cognitive Functions
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,SWE-bench Verified evaluates software engineering agents on real GitHub issues where the model must produce a patch that makes the project’s tests pass. The “Verified” subset contains tasks confirmed by human reviewers to be solvable and uses a standardized harness to reduce false positives from flaky tests and underspecified issues.,"Language Comprehension, Planning, Logical Reasoning, Working Memory, Adaptive Error Correction, Decision-making"
SWE-bench Multilingual,https://www.swebench.com/multilingual.html,https://arxiv.org/abs/2504.21798,"SWE-bench Multilingual extends the SWE-bench setup beyond Python to multiple programming languages and ecosystems, requiring agents to understand diverse toolchains, build systems, and language-specific conventions. It measures whether a model can translate natural-language issue descriptions into correct multi-file edits and pass language-appropriate test suites.","Language Comprehension, Planning, Logical Reasoning, Working Memory, Adaptive Error Correction, Cognitive Flexibility, Decision-making"
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro is a larger and harder software engineering benchmark designed to be more diverse and more resistant to contamination, typically spanning multiple languages and more industrially realistic tasks. Models must navigate complex repositories, implement or fix behavior, and satisfy rigorous evaluation criteria under an agent scaffold.","Language Comprehension, Planning, Logical Reasoning, Working Memory, Adaptive Error Correction, Cognitive Flexibility, Decision-making"
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 measures an agent’s ability to accomplish real command-line tasks in sandboxed environments (e.g., debugging, installing dependencies, data manipulation, and system operations). Success depends on choosing correct commands, interpreting outputs and errors, iterating, and managing limited resources and timeouts.","Planning, Adaptive Error Correction, Working Memory, Decision-making, Logical Reasoning, Language Comprehension"
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp evaluates web-browsing research agents on questions that require finding, synthesizing, and citing information from the web (or from a fixed index in reproducible variants). The benchmark stresses tool use (search/fetch), evidence tracking across multiple pages, and producing a grounded final answer under long-context constraints.","Planning, Decision-making, Language Comprehension, Semantic Understanding & Context Recognition, Working Memory, Episodic Memory, Self-reflection"
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench evaluates interactive tool-using customer-support agents in simulated domains (e.g., retail, airline, telecom) with policies the agent must follow while solving multi-turn user requests. It tests whether agents can reliably call APIs, maintain conversational state, handle edge cases, and comply with constraints even when helpfulness pressures create policy-loophole temptations.","Social Reasoning & Theory of Mind, Empathy, Decision-making, Planning, Language Comprehension, Language Production, Working Memory, Inhibitory Control"
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld evaluates multimodal “computer use” agents operating on a desktop-like environment to complete tasks across applications via screenshots and actions. Agents must perceive UI state, plan multi-step interactions, and execute sequences of clicks/typing while recovering from mistakes and dynamic interface changes.","Visual Perception, Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Sensorimotor Coordination, Planning, Decision-making, Adaptive Error Correction, Working Memory"
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI measures few-shot abstract reasoning on grid transformation puzzles where the model must infer latent rules from a handful of input-output examples and generalize to a novel input. It is designed to emphasize fluid reasoning and generalization rather than memorized knowledge, with strict correctness on produced grids.","Logical Reasoning, Working Memory, Cognitive Flexibility, Planning, Spatial Representation & Mapping, Scene Understanding & Visual Reasoning"
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon agentic performance in a simulated vending machine business over an extended timeline, scoring based on final financial outcomes. Agents must make coherent strategic decisions (pricing, inventory, supplier negotiation) over many steps while adapting to market dynamics and delayed consequences.","Planning, Decision-making, Working Memory, Episodic Memory, Reward Mechanisms, Cognitive Timing & Predictive Modeling, Adaptive Error Correction, Motivational Drives"
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas measures real-world tool use through the Model Context Protocol (MCP), requiring agents to discover tools, call APIs correctly, handle failures, and combine multi-step results into correct outputs. Tasks resemble production workflows where correctness depends on robust orchestration across multiple servers/tools rather than single-shot answering.","Planning, Decision-making, Working Memory, Adaptive Error Correction, Language Comprehension, Semantic Understanding & Context Recognition"
FinanceAgent,https://www.vals.ai/benchmarks/finance_agent,,"FinanceAgent evaluates agents on tasks typical of an entry-level financial analyst, such as analyzing documents, building or checking analyses, and producing finance-oriented deliverables. It emphasizes multi-step reasoning with domain constraints, careful numerical consistency, and structured reporting aligned to professional standards.","Logical Reasoning, Planning, Decision-making, Working Memory, Language Comprehension, Language Production, Semantic Understanding & Context Recognition, Adaptive Error Correction"
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity agent capabilities on large-scale tasks including identifying known vulnerabilities in real codebases from natural-language weakness descriptions and, in some settings, discovering previously unknown issues. Success requires code comprehension, hypothesis-driven debugging, iterative testing, and careful handling of adversarial or ambiguous evidence.","Logical Reasoning, Planning, Adaptive Error Correction, Working Memory, Language Comprehension, Decision-making, Inhibitory Control"
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"SpreadsheetBench measures an agent’s ability to understand, edit, and compute with complex spreadsheets derived from real-world scenarios, often requiring multi-step transformations and formula reasoning. Agents must track dependencies, maintain consistent structure/formatting, and verify results through iterative corrections.","Logical Reasoning, Working Memory, Planning, Adaptive Error Correction, Decision-making, Semantic Understanding & Context Recognition"
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a multimodal benchmark intended to probe frontier academic reasoning and knowledge across a broad set of difficult questions, including those that benefit from tools like search or code execution. It stresses multi-step inference, careful grounding, and integrating information from text and (in some subsets) images or other modalities.","Language Comprehension, Logical Reasoning, Working Memory, Semantic Understanding & Context Recognition, Planning, Decision-making, Multisensory Integration, Scene Understanding & Visual Reasoning"
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,AIME 2025 evaluates mathematical problem-solving on competition-style questions that typically require multi-step symbolic reasoning and careful casework. Scores reflect exact-answer accuracy and are often reported with and without tools to separate pure reasoning from computation assistance.,"Logical Reasoning, Working Memory, Planning, Adaptive Error Correction"
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a difficult, graduate-level multiple-choice science benchmark curated to be “Google-proof,” emphasizing reasoning over superficial recall. The Diamond subset focuses on high-quality questions where experts reliably agree on the correct answer and non-experts tend to fail, probing deep scientific understanding under time/format constraints.","Language Comprehension, Logical Reasoning, Working Memory, Semantic Understanding & Context Recognition, Decision-making"
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends broad academic knowledge testing to multiple languages, measuring whether models can transfer reasoning and subject knowledge beyond English across many domains. It typically uses standardized multiple-choice questions to evaluate multilingual comprehension, cross-lingual generalization, and consistency of knowledge access.","Language Comprehension, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Cognitive Flexibility"
MMMU,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2311.16502,"MMMU is a multimodal benchmark spanning many disciplines where models must answer questions that combine text with images such as diagrams, charts, or scientific figures. It targets expert-level multimodal reasoning, requiring models to parse visual information, integrate it with textual context, and produce a correct selection or response.","Visual Perception, Scene Understanding & Visual Reasoning, Multisensory Integration, Language Comprehension, Logical Reasoning, Working Memory, Visual Attention & Eye Movements"
LAB-Bench FigQA,https://huggingface.co/datasets/futurehouse/lab-bench,https://arxiv.org/abs/2407.10362,"LAB-Bench FigQA evaluates whether models can correctly interpret and reason over complex biology paper figures (e.g., plots, panels, and annotations) to answer scientific questions. It stresses fine-grained visual analysis, mapping visual cues to domain semantics, and resisting plausible-but-wrong interpretations common in scientific figure reading.","Visual Perception, Scene Understanding & Visual Reasoning, Multisensory Integration, Language Comprehension, Logical Reasoning, Working Memory, Visual Attention & Eye Movements, Semantic Understanding & Context Recognition"
WebArena,https://webarena.dev/,https://arxiv.org/abs/2307.13854,"WebArena evaluates autonomous web agents in realistic, self-hosted web apps (e-commerce, CMS, forums, Git repositories, maps) where tasks require navigation, form filling, search, and multi-step workflows. It measures an agent’s ability to plan and execute actions from pixel/DOM observations, maintain task state, and recover from mistakes under interactive dynamics.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Visual Perception, Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Sensorimotor Coordination, Language Comprehension, Language Production"
