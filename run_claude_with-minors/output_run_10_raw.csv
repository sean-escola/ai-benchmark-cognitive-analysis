Benchmark,Website,Paper,Description,Cognitive Functions
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates software engineering agents on real GitHub issues by requiring a patch that passes the repository’s tests. The “Verified” subset emphasizes tasks that have been manually checked to be solvable and correctly scored, reducing noise from ambiguous or broken tasks.","Planning, Logical Reasoning, Adaptive Error Correction, Working Memory, Language Comprehension, Language Production"
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 measures how well an agent can complete real command-line tasks inside a sandboxed terminal environment, typically requiring iterative execution, inspection, and repair. Success depends on choosing correct commands, interpreting errors, and converging on a working solution under resource and time constraints.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Language Comprehension"
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp tests “deep research” agents on questions that require searching, reading, and synthesizing information from a curated document corpus or controlled web-like setting. It emphasizes multi-step information gathering, evidence tracking, and final answer generation that is consistent with retrieved sources.","Planning, Attention, Episodic Memory, Semantic Understanding & Context Recognition, Language Comprehension, Language Production, Decision-making"
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench evaluates interactive tool-using agents in customer-support style environments (e.g., retail, airline, telecom) with policy constraints and multi-turn dialogues. Agents must coordinate conversation, API calls, and policy compliance to resolve the user’s request while avoiding prohibited actions.","Language Comprehension, Language Production, Planning, Decision-making, Inhibitory Control, Social Reasoning & Theory of Mind, Empathy"
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld benchmarks autonomous “computer use” by giving agents tasks on a desktop-like operating system that require navigating GUIs, launching apps, and completing multi-step workflows. Performance depends on accurately interpreting screen state and executing sequences of interface actions reliably.","Visual Perception, Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Planning, Decision-making, Working Memory, Sensorimotor Coordination"
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI measures abstract reasoning and generalization from few examples using grid-based input–output puzzles where the underlying rule is hidden. Models must infer the transformation and apply it to new inputs, emphasizing novelty and systematic pattern induction.","Logical Reasoning, Cognitive Flexibility, Working Memory, Spatial Representation & Mapping, Visual Perception"
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon agent coherence by simulating management of a vending-machine business over extended time, including procurement, pricing, inventory, and communication. Agents must sustain a strategy, react to market dynamics, and compound gains over many decisions.","Planning, Decision-making, Reward Mechanisms, Working Memory, Adaptive Error Correction, Semantic Understanding & Context Recognition"
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas measures real-world tool use through the Model Context Protocol (MCP), requiring agents to discover, invoke, and chain tools across multi-step workflows. It stresses correct tool selection, parameterization, error handling, and synthesis of tool outputs into final responses.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Language Comprehension, Language Production"
FinanceAgent,https://www.vals.ai/benchmarks/finance_agent,,"FinanceAgent assesses how well a model performs tasks typical of an entry-level financial analyst, such as analyzing company information, creating structured outputs, and performing finance-oriented reasoning. It emphasizes accurate interpretation of problem context and producing actionable, well-structured deliverables.","Semantic Understanding & Context Recognition, Logical Reasoning, Planning, Decision-making, Working Memory, Language Production"
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity capability by testing whether agents can identify known vulnerabilities from descriptions and discover new issues in real open-source codebases. Tasks often require code navigation, hypothesis testing, and iterative refinement based on failures or partial findings.","Logical Reasoning, Planning, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition"
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"SpreadsheetBench tests an agent’s ability to understand, modify, and generate complex spreadsheets based on real-world-like tasks. It involves transforming data, applying formulas, maintaining layout/formatting constraints, and verifying outputs against task requirements.","Working Memory, Planning, Decision-making, Logical Reasoning, Adaptive Error Correction, Semantic Understanding & Context Recognition"
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a broad, frontier-level benchmark spanning difficult questions across domains and modalities, aiming to probe advanced knowledge and reasoning rather than routine recall. Many items require multi-step inference, careful interpretation of problem statements, and sometimes integrating visual or structured information.","Language Comprehension, Visual Perception, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory"
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 evaluates competition-style mathematics problem solving using the AIME exam format, typically requiring multi-step derivations and precise numerical answers. The benchmark stresses symbolic manipulation, constraint tracking, and error-free arithmetic under complex reasoning chains.","Logical Reasoning, Working Memory, Planning, Adaptive Error Correction"
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a high-difficulty subset of graduate-level science multiple-choice questions designed to be resistant to shallow pattern matching. It probes deep conceptual understanding and multi-step reasoning across biology, chemistry, and physics topics.","Language Comprehension, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory"
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends broad academic evaluation across many subjects into multiple languages, testing both knowledge and reasoning under multilingual prompts. It emphasizes robust understanding across linguistic variations and domain contexts rather than English-only performance.","Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory"
MMMU,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2311.16502,"MMMU measures multimodal expert-level understanding and reasoning by combining text questions with images from diverse disciplines. Success requires interpreting visual content, integrating it with textual context, and performing domain-relevant reasoning to select or generate correct answers.","Visual Perception, Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Multisensory Integration, Language Comprehension, Logical Reasoning, Working Memory"
LAB-Bench FigQA,https://huggingface.co/datasets/futurehouse/lab-bench,https://arxiv.org/abs/2407.10362,"LAB-Bench FigQA focuses on scientific figure question answering, requiring models to interpret complex plots/diagrams from biology papers and answer targeted questions. It stresses accurate extraction of visual evidence and reasoning that connects figure content to the question’s intent.","Visual Perception, Scene Understanding & Visual Reasoning, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory"
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning evaluates reasoning over scientific paper figures and related contextual text, emphasizing quantitative and structural interpretation rather than surface caption reading. Many items require extracting relationships from charts/diagrams and composing a justified conclusion.","Visual Perception, Scene Understanding & Visual Reasoning, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory"
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU benchmarks multimodal reasoning over videos paired with text questions, often requiring temporal integration of events and visual details across frames. It tests whether models can maintain coherence over time and answer based on evidence distributed throughout the clip.","Visual Perception, Attention, Working Memory, Cognitive Timing & Predictive Modeling, Scene Understanding & Visual Reasoning, Multisensory Integration"
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,FACTS Benchmark Suite systematically evaluates factuality by testing whether a model’s statements are supported by reliable evidence and whether it avoids hallucinating unsupported claims. It targets calibration-like behavior: abstaining or qualifying answers when information is uncertain or missing.,"Semantic Understanding & Context Recognition, Episodic Memory, Inhibitory Control, Self-reflection, Language Production"
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA evaluates commonsense and physical reasoning across many languages and cultural contexts, emphasizing whether models generalize beyond English-centric priors. It probes practical inference about everyday situations while controlling for cross-lingual variation.","Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning, Social Reasoning & Theory of Mind"
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) measures long-context, multi-round coreference resolution by embedding multiple similar “needles” inside long distractor contexts and requiring retrieval of the correct referenced answer. It stresses precise tracking of entities and dependencies over very long inputs without conflation.","Working Memory, Attention, Episodic Memory, Language Comprehension, Semantic Understanding & Context Recognition"
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval evaluates economically relevant, well-specified knowledge work across many occupations by having models produce real work artifacts (e.g., spreadsheets, presentations, schedules) and comparing outputs to expert baselines. It emphasizes end-to-end task completion quality, including structure, correctness, and usefulness.","Planning, Decision-making, Language Production, Semantic Understanding & Context Recognition, Working Memory, Adaptive Error Correction"
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks tests reasoning over graph-structured data by requiring models to follow paths, apply traversal rules, and answer questions that depend on multi-step navigation. It emphasizes faithful, stepwise tracking of nodes/edges under distractors and longer sequences.","Spatial Representation & Mapping, Working Memory, Logical Reasoning, Planning, Attention"
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon evaluates tool-using agents on diverse, multi-step tasks that require choosing among tools, calling them correctly, and integrating results into a final output. It stresses robustness to tool errors, compositional workflows, and correct execution under evolving state.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Language Comprehension, Language Production"
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath benchmarks advanced mathematics at the research frontier, designed to be challenging and less susceptible to memorization. Problems often require long derivations, creative strategy selection, and careful management of intermediate results.","Logical Reasoning, Working Memory, Planning, Cognitive Flexibility, Adaptive Error Correction"
