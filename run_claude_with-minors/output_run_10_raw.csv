Benchmark,Website,Paper,Description,Cognitive Functions
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates an AI system’s ability to solve real software engineering issues by generating patches that make repository tests pass, using issues drawn from real GitHub projects. The “Verified” split adds additional human validation that tasks are solvable and that evaluation outcomes are reliable, emphasizing end-to-end debugging and code changes rather than isolated code generation.","Language Comprehension, Language Production, Logical Reasoning, Planning, Working Memory, Adaptive Error Correction, Decision-making"
SWE-bench Multilingual,https://www.swebench.com/multilingual.html,https://arxiv.org/abs/2504.21798,"SWE-bench Multilingual extends the SWE-bench paradigm beyond Python to multiple programming languages, requiring models to understand diverse ecosystems, tooling conventions, and language-specific idioms. It stresses generalization across languages and repositories, where correct solutions often require mapping an issue description to code locations and producing consistent patches under different build/test setups.","Language Comprehension, Language Production, Logical Reasoning, Planning, Working Memory, Adaptive Error Correction, Cognitive Flexibility"
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro is a larger and more challenging software engineering benchmark designed to be more contamination-resistant and industrially representative than earlier variants. Tasks typically require deeper repository understanding, longer patch sequences, and more robust debugging across multiple languages and project configurations, making it a stronger proxy for real professional engineering work.","Language Comprehension, Language Production, Logical Reasoning, Planning, Working Memory, Adaptive Error Correction, Decision-making, Cognitive Flexibility"
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 measures an agent’s ability to complete realistic command-line tasks in sandboxed environments, such as installing dependencies, manipulating files, running programs, and diagnosing failures. Success requires iterative interaction (execute commands, interpret outputs, revise approach) under resource and time constraints, mimicking real terminal-based workflows.","Language Comprehension, Planning, Decision-making, Working Memory, Adaptive Error Correction, Attention"
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp evaluates research-style question answering with browsing/search over a controlled document corpus to improve reproducibility and reduce dependence on live web variance. Models must plan queries, sift retrieved snippets, reconcile evidence, and produce grounded answers, emphasizing information seeking, citation-like grounding, and multi-step synthesis.","Planning, Attention, Working Memory, Semantic Understanding & Context Recognition, Language Comprehension, Language Production, Adaptive Error Correction"
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench evaluates tool-using customer-support agents interacting with simulated users and APIs across domains (e.g., retail, airline, telecom) while following domain policies. It tests whether agents can maintain policy compliance over multi-turn dialogues, correctly invoke tools, and resolve user problems despite ambiguity, exceptions, and adversarial or emotionally charged situations.","Language Comprehension, Language Production, Planning, Decision-making, Inhibitory Control, Social Reasoning & Theory of Mind, Empathy, Working Memory, Reward Mechanisms"
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld benchmarks multimodal “computer use” agents on realistic tasks performed in a full operating system environment (e.g., browsing, editing, managing settings), typically with step limits and high-resolution UI observations. It stresses perception-to-action loops: recognizing UI state, choosing actions (click/type/shortcut), and recovering from errors across long task sequences.","Visual Perception, Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Planning, Decision-making, Working Memory, Adaptive Error Correction, Sensorimotor Coordination"
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI measures fluid reasoning by requiring inference of novel abstract rules from a few input-output grid examples and applying them to unseen grids. Unlike knowledge-heavy tests, it emphasizes rapid pattern induction, systematic generalization, and robustness to distribution shift, making it a canonical benchmark for abstraction and compositional reasoning.","Logical Reasoning, Working Memory, Cognitive Flexibility, Attention, Spatial Representation & Mapping, Scene Understanding & Visual Reasoning"
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon agent coherence and business management in a simulated vending-machine enterprise over extended time (e.g., a year of decisions). Agents must manage inventory, pricing, supplier negotiations, and cash flow, adapting to changing conditions and compounding consequences across many steps.","Planning, Decision-making, Working Memory, Episodic Memory, Reward Mechanisms, Cognitive Timing & Predictive Modeling, Self-reflection"
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas measures real-world tool use with Model Context Protocol servers, requiring models to discover relevant tools, call them correctly, handle errors, and integrate outputs across multi-step workflows. The benchmark emphasizes reliable orchestration across heterogeneous APIs and realistic tool-call failure modes typical of production agent systems.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Language Comprehension, Language Production, Attention"
FinanceAgent,https://www.vals.ai/benchmarks/finance_agent,,"FinanceAgent evaluates agent performance on tasks representative of an entry-level financial analyst, such as extracting information from documents, building analyses, and producing finance-oriented deliverables. It emphasizes structured reasoning over numeric and textual evidence, tool-assisted analysis, and producing decisions or recommendations that remain consistent with constraints and assumptions.","Logical Reasoning, Planning, Decision-making, Working Memory, Semantic Understanding & Context Recognition, Language Production, Attention"
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity agents on vulnerability-related tasks at scale, including finding known vulnerabilities from high-level descriptions and, in some settings, discovering new issues. It stresses iterative investigation in real codebases, hypothesis-driven debugging, and careful validation to avoid false positives while operating under realistic constraints.","Logical Reasoning, Planning, Adaptive Error Correction, Working Memory, Attention, Inhibitory Control, Decision-making"
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"SpreadsheetBench measures an agent’s ability to navigate, edit, and compute with complex spreadsheets derived from realistic tasks, often requiring multi-step transformations and formula reasoning. Success involves tracking dependencies across cells/sheets, applying consistent data manipulations, and validating outputs against task requirements.","Working Memory, Planning, Decision-making, Adaptive Error Correction, Semantic Understanding & Context Recognition, Logical Reasoning, Attention"
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a frontier, multi-modal benchmark designed to probe advanced reasoning and expert-level knowledge across many domains with difficult questions and strict grading. It is intended to be challenging even for strong models, often requiring multi-step derivations, careful grounding to provided materials, and robust handling of ambiguity or missing information.","Language Comprehension, Language Production, Logical Reasoning, Working Memory, Semantic Understanding & Context Recognition, Scene Understanding & Visual Reasoning, Multisensory Integration, Attention"
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 is a competition-math evaluation derived from the American Invitational Mathematics Examination, emphasizing nontrivial algebra, number theory, geometry, and combinatorics. It tests precise multi-step derivations under tight correctness requirements, where small logical gaps typically lead to wrong final answers.","Logical Reasoning, Working Memory, Planning, Attention, Adaptive Error Correction"
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a hard multiple-choice science benchmark composed of high-quality graduate-level questions where experts succeed and non-experts often fail. It is designed to be “Google-proof,” requiring genuine reasoning and domain understanding rather than simple retrieval, with distractors that penalize shallow pattern matching.","Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Attention, Decision-making"
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends broad academic knowledge and reasoning evaluation across many subjects and multiple languages, stressing multilingual understanding rather than English-only competence. It tests whether models can transfer conceptual knowledge and reasoning patterns across linguistic contexts, maintaining consistent performance despite translation and cultural/terminology variation.","Language Comprehension, Language Production, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Cognitive Flexibility"
MMMU,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2311.16502,"MMMU is a multimodal benchmark spanning many disciplines that requires models to answer questions using both text and images, including diagrams, charts, and technical visuals. It emphasizes integrating visual evidence with domain knowledge and reasoning, often requiring attention to spatial layout and fine-grained visual details.","Visual Perception, Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Attention, Working Memory, Spatial Representation & Mapping"
LAB-Bench FigQA,https://huggingface.co/datasets/futurehouse/lab-bench,https://arxiv.org/abs/2407.10362,LAB-Bench FigQA evaluates whether models can interpret complex scientific figures (common in biology papers) and answer questions that require extracting and reasoning over plotted or annotated information. It stresses careful reading of axes/legends/experimental conditions and combining visual evidence with scientific context to reach correct conclusions.,"Visual Perception, Scene Understanding & Visual Reasoning, Attention, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory"
WebArena,https://webarena.dev/,https://arxiv.org/abs/2307.13854,"WebArena evaluates autonomous web agents on realistic multi-step tasks across several self-hosted web applications (e-commerce, forums, code hosting, CMS, maps). Agents must perceive changing webpages, navigate and fill forms, and recover from mistakes, making it a standard benchmark for end-to-end web-based planning and execution.","Visual Perception, Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Planning, Decision-making, Working Memory, Adaptive Error Correction, Sensorimotor Coordination"
