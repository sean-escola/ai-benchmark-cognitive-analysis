Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates end-to-end software engineering by asking a model/agent to produce patches that fix real issues in GitHub repositories, validated by tests. The “Verified” subset emphasizes tasks confirmed solvable and scored by whether the submitted patch passes the repository’s test suite under the harness.","L1: Language Comprehension
L2: Planning, Logical Reasoning, Working Memory, Adaptive Error Correction, Decision-making
L3: ",L2
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 measures autonomous command-line problem solving, where agents must navigate a terminal environment, run commands, edit files, and iteratively debug to reach a goal state. It stresses long-horizon tool interaction under realistic constraints such as dependency issues, environment errors, and partial progress.","L1: Language Comprehension
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory
L3: Inhibitory Control",L3
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp evaluates deep research behavior: answering questions that require multi-step web-style information gathering, synthesis, and citation-like justification rather than single-hop recall. The benchmark is designed to test whether agents can plan searches, filter noisy evidence, and integrate findings into a coherent final answer.","L1: Language Comprehension, Language Production
L2: Planning, Attention, Semantic Understanding & Context Recognition, Working Memory, Decision-making
L3: Inhibitory Control",L3
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench evaluates agentic tool use in multi-turn customer-service-style simulations (e.g., retail, airline, telecom) where models must follow policies while using APIs to complete tasks. Success depends on reliable action sequencing, state tracking across turns, and policy adherence under pressure from user requests.","L1: Language Comprehension, Language Production
L2: Decision-making, Planning, Working Memory, Attention
L3: Social Reasoning & Theory of Mind, Inhibitory Control",L3
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld benchmarks multimodal computer-use agents that must operate graphical desktop environments to accomplish tasks across applications. It emphasizes perception of UI state (screenshots), action execution (click/type), and robust multi-step planning with error recovery.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Sensorimotor Coordination, Planning, Decision-making, Working Memory, Adaptive Error Correction
L3: ",L2
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI tests fluid reasoning by requiring inference of abstract rules from a small set of input–output grid examples and applying the rule to a new grid. It aims to measure generalization to novel patterns with minimal data, rather than domain knowledge or memorization.","L1: 
L2: Logical Reasoning, Working Memory, Spatial Representation & Mapping, Attention
L3: Cognitive Flexibility",L3
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 measures long-horizon autonomy in a simulated business management setting, where an agent runs a vending machine operation over many steps and must grow its balance. High performance requires sustained coherence, strategic planning, negotiation/communication, and adaptation to changing market signals.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Working Memory, Adaptive Error Correction
L3: Motivational Drives",L3
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas evaluates real-world tool use via the Model Context Protocol by testing whether models can discover tools, call them correctly, handle errors, and compose multi-step workflows across services. It focuses on execution reliability and integration of tool outputs into accurate final responses.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Working Memory, Adaptive Error Correction
L3: Inhibitory Control",L3
FinanceAgent,https://www.vals.ai/benchmarks/finance_agent,,"FinanceAgent assesses agent performance on tasks typical of an entry-level financial analyst, such as extracting information from documents, performing calculations, building structured analyses, and producing professional summaries. It emphasizes correct reasoning with numbers, adherence to task constraints, and coherent reporting.","L1: Language Comprehension, Language Production
L2: Logical Reasoning, Decision-making, Planning, Working Memory, Semantic Understanding & Context Recognition, Attention
L3: ",L2
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity capability by asking agents to identify known vulnerabilities in real projects and, in some settings, to discover new issues under realistic constraints. It stresses iterative investigation, hypothesis testing, and patch/reproduction workflows.","L1: 
L2: Logical Reasoning, Planning, Adaptive Error Correction, Working Memory, Attention, Decision-making
L3: Inhibitory Control",L3
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"SpreadsheetBench measures the ability to understand, edit, and compute over complex spreadsheets drawn from realistic scenarios. Agents must interpret layout and formulas, perform structured transformations, and maintain correctness across interdependent cells and sheets.","L1: 
L2: Logical Reasoning, Working Memory, Attention, Semantic Understanding & Context Recognition, Adaptive Error Correction, Planning, Decision-making
L3: ",L2
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a challenging multimodal benchmark spanning advanced questions at the frontier of human knowledge across many domains. It tests not only recall but multi-step reasoning, synthesis of evidence, and, in tool-enabled settings, effective use of external resources to reach correct answers.","L1: Language Comprehension, Language Production, Visual Perception
L2: Logical Reasoning, Working Memory, Attention, Scene Understanding & Visual Reasoning, Semantic Understanding & Context Recognition
L3: ",L2
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 consists of competition-style mathematics problems requiring precise multi-step derivations and careful handling of algebra, geometry, combinatorics, and number theory. It probes consistency under symbolic manipulation and the ability to avoid small logical slips.","L1: 
L2: Logical Reasoning, Working Memory, Attention, Adaptive Error Correction, Planning
L3: ",L2
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,GPQA Diamond is a subset of graduate-level science multiple-choice questions designed to be difficult to answer via simple web search or superficial pattern matching. It emphasizes deep scientific reasoning and careful discrimination among plausible distractors.,"L1: 
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Attention, Decision-making
L3: ",L2
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends broad academic knowledge evaluation to multiple languages, testing whether models can understand and answer subject-matter questions across diverse linguistic contexts. It measures multilingual comprehension and cross-lingual robustness of reasoning and knowledge access.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Attention
L3: ",L2
MMMU,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2311.16502,"MMMU is a multimodal benchmark where questions require integrating text with images (diagrams, charts, figures) across many disciplines. It emphasizes visual understanding, grounding language in visual evidence, and performing multi-step reasoning from that evidence.","L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Working Memory, Attention
L3: ",L2
LAB-Bench FigQA,https://huggingface.co/datasets/futurehouse/lab-bench,https://arxiv.org/abs/2407.10362,"LAB-Bench FigQA tests scientific figure understanding in biology papers, requiring models to read complex plots and experimental figures and answer questions about them. It targets practical research-relevant interpretation rather than generic image captioning.","L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Logical Reasoning, Working Memory, Attention
L3: ",L2
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning evaluates reasoning over scientific paper figures and related context, often benefiting from quantitative inspection and structured analysis. It targets careful interpretation of visual scientific artifacts (e.g., plots) and mapping them to correct textual conclusions.","L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Logical Reasoning, Working Memory, Attention
L3: ",L2
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU extends multimodal understanding to video, requiring temporal integration of visual evidence and language to answer questions about dynamic scenes. It stresses retaining relevant events across time and reasoning over actions, causality, and context shifts.","L1: Visual Perception, Language Comprehension
L2: Visual Attention & Eye Movements, Working Memory, Scene Understanding & Visual Reasoning, Attention
L3: Cognitive Timing & Predictive Modeling",L3
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"FACTS Benchmark Suite systematically evaluates factuality by probing whether model outputs remain grounded, avoid unsupported claims, and appropriately handle uncertainty. It typically stresses correctness under long-form generation where hallucination risks increase.","L1: Language Production, Language Comprehension
L2: Semantic Understanding & Context Recognition, Adaptive Error Correction, Working Memory, Attention
L3: Inhibitory Control",L3
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA evaluates practical physical and everyday reasoning across languages and cultures, aiming to reduce geographic and cultural bias in “common-sense” evaluations. It tests whether models can choose or generate plausible actions/explanations in diverse real-world contexts.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Decision-making, Working Memory
L3: Social Reasoning & Theory of Mind",L3
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,MRCR v2 (8-needle) measures long-context multi-round coreference resolution by embedding multiple similar ‘needle’ requests in large ‘haystacks’ and asking the model to recover the correct referenced response. It stresses robust retrieval of the right instance among distractors across very long contexts.,"L1: 
L2: Working Memory, Attention, Episodic Memory, Semantic Understanding & Context Recognition
L3: Inhibitory Control",L3
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval evaluates economically valuable, well-specified knowledge work across many occupations by judging the quality of produced work artifacts (e.g., plans, analyses, presentations, spreadsheets). It emphasizes end-to-end task execution quality, adherence to constraints, and professional communication.","L1: Language Production, Language Comprehension
L2: Planning, Decision-making, Working Memory, Attention, Adaptive Error Correction
L3: Self-reflection",L3
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks tests reasoning over graph-structured data where models must follow paths, track connectivity, and answer queries that require multi-step traversal. It stresses structured reasoning, maintaining intermediate states, and resisting distractor branches.","L1: 
L2: Spatial Representation & Mapping, Logical Reasoning, Working Memory, Attention, Planning
L3: ",L2
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon evaluates tool-using agents across a diverse suite of tasks requiring correct selection, invocation, and composition of tools under constraints. It emphasizes reliability in multi-step workflows, error handling, and integrating tool outputs into final answers.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Working Memory, Adaptive Error Correction
L3: Inhibitory Control",L3
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath evaluates expert-level mathematics designed to be difficult for models without genuine problem-solving, often requiring long chains of derivation and careful verification. It targets robustness on novel problems and the ability to sustain correct reasoning over extended solutions.","L1: 
L2: Logical Reasoning, Working Memory, Planning, Adaptive Error Correction, Attention
L3: ",L2
