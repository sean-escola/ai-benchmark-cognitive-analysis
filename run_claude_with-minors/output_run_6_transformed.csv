Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,SWE-bench Verified evaluates software engineering agents on real GitHub issues where the model must generate a code patch that fixes the bug or implements the requested change and passes the repository’s tests. The “Verified” subset focuses on tasks that have been human-validated as solvable and checks correctness via deterministic test execution.,"L1: Language Comprehension
L2: Planning, Logical Reasoning, Adaptive Error Correction, Working Memory
L3: ",L2
SWE-bench Multilingual,https://www.swebench.com/multilingual.html,https://arxiv.org/abs/2504.21798,"SWE-bench Multilingual extends SWE-bench-style patch generation beyond Python to multiple programming languages, testing whether agents can understand issues and modify unfamiliar codebases across language ecosystems. It emphasizes transfer of debugging and maintenance skills across different syntactic and tooling conventions.","L1: Language Comprehension
L2: Planning, Logical Reasoning, Adaptive Error Correction, Working Memory
L3: Cognitive Flexibility",L3
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro is a harder, more contamination-resistant software engineering benchmark with a larger and more diverse set of tasks spanning multiple languages and more realistic industry maintenance work. Agents must plan and implement robust multi-file changes that satisfy hidden unit/integration tests.","L1: Language Comprehension
L2: Planning, Decision-making, Logical Reasoning, Adaptive Error Correction, Working Memory
L3: ",L2
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 evaluates agent performance on real command-line tasks (e.g., debugging, building, configuring, data wrangling) executed in a sandboxed terminal environment. Success requires selecting and sequencing shell commands, interpreting outputs/errors, and iterating toward a working end state.","L1: 
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory, Attention, Sensorimotor Coordination
L3: ",L2
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp measures an agent’s ability to answer difficult questions by searching and synthesizing information from a controlled document collection, aiming for reproducible “browsing” evaluation. It tests whether the agent can formulate queries, gather evidence, resolve conflicts, and produce a grounded final answer.","L1: Language Comprehension
L2: Planning, Attention, Working Memory, Semantic Understanding & Context Recognition, Logical Reasoning
L3: ",L2
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench evaluates interactive tool-using agents in customer-support-like domains (e.g., retail, airline, telecom) where the model must follow policies while calling APIs and coordinating multi-turn dialogs with a simulated user. The benchmark stresses robustness to long-horizon interaction, policy adherence, and recovery from tool or user-induced complications.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Working Memory
L3: Social Reasoning & Theory of Mind, Inhibitory Control, Empathy",L3
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld benchmarks multimodal “computer use” agents on tasks performed inside an operating-system-like environment with graphical interfaces. Models must perceive screenshots, navigate UI state, and execute multi-step actions to accomplish goals under step limits and partial observability.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Planning, Decision-making, Sensorimotor Coordination, Working Memory
L3: ",L2
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI tests fluid, few-shot abstraction by asking models to infer hidden transformations from a handful of input–output grid examples and then produce the correct output grid for a new input. It is designed to reduce reliance on memorized knowledge and emphasize novel pattern induction and generalization.","L1: 
L2: Logical Reasoning, Working Memory, Attention, Spatial Representation & Mapping
L3: Cognitive Flexibility",L3
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon autonomous agent performance by simulating operation of a vending-machine business over an extended period, scoring on final financial outcomes. Agents must make coherent sequences of decisions (pricing, inventory, supplier negotiation) while adapting to changing market conditions.","L1: 
L2: Planning, Decision-making, Working Memory, Reward Mechanisms, Adaptive Error Correction
L3: Motivational Drives",L3
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas evaluates real-world tool use via the Model Context Protocol, requiring models to discover relevant tools, call them correctly, handle failures, and compose multi-step workflows across different MCP servers. It emphasizes dependable API interaction and integration of tool results into accurate end outputs.","L1: Language Comprehension
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory
L3: ",L2
FinanceAgent,https://www.vals.ai/benchmarks/finance_agent,,"FinanceAgent assesses agent performance on tasks typical of an entry-level financial analyst, such as financial modeling, analysis, and report generation, often requiring structured reasoning with domain constraints. It tests whether models can interpret financial context, execute multi-step analyses, and present decisions with supporting rationale.","L1: Language Production
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Planning, Decision-making, Working Memory
L3: ",L2
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity agents on tasks including identifying known vulnerabilities in real open-source projects from high-level descriptions and discovering new vulnerabilities. Performance depends on analyzing code and program behavior, hypothesizing weaknesses, and iteratively testing and refining findings.","L1: 
L2: Logical Reasoning, Planning, Adaptive Error Correction, Working Memory, Attention
L3: Inhibitory Control",L3
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"SpreadsheetBench measures an agent’s ability to work with complex spreadsheets, including reading/writing cells, transforming data, building formulas, and producing correct computed results. It probes whether models can maintain consistency across many interdependent fields and avoid cascading errors.","L1: 
L2: Logical Reasoning, Planning, Working Memory, Attention, Adaptive Error Correction
L3: ",L2
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a large, frontier-oriented multimodal benchmark spanning many disciplines and including difficult questions intended to stress expert-level reasoning and breadth of knowledge. It often requires integrating text and visual inputs, handling ambiguity, and producing well-justified answers under uncertainty.","L1: Language Comprehension, Language Production, Visual Perception
L2: Scene Understanding & Visual Reasoning, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Attention
L3: ",L2
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 evaluates mathematical problem solving on competition-style questions that demand multi-step derivations rather than rote recall. It tests correctness of symbolic manipulation, structured reasoning, and the ability to track intermediate constraints to reach a single numeric answer.","L1: 
L2: Logical Reasoning, Working Memory, Planning, Attention
L3: ",L2
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a subset of graduate-level, multiple-choice science questions selected to be especially resistant to superficial pattern matching and to require genuine scientific reasoning. It stresses precise reading, elimination of distractors, and integrating domain knowledge with logical inference.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Attention
L3: ",L2
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends MMLU to multilingual settings, evaluating knowledge and reasoning across many academic subjects in multiple non-English languages. It probes whether models can preserve reasoning competence while operating in different linguistic and cultural contexts.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory
L3: Cognitive Flexibility",L3
MMMU,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2311.16502,"MMMU is a multimodal benchmark spanning many disciplines where questions require interpreting images (e.g., diagrams, charts, figures) alongside text. It emphasizes grounding language in visual evidence and performing multi-step reasoning over combined modalities.","L1: Visual Perception, Language Comprehension
L2: Multisensory Integration, Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Logical Reasoning, Working Memory
L3: ",L2
LAB-Bench FigQA,https://huggingface.co/datasets/futurehouse/lab-bench,https://arxiv.org/abs/2407.10362,"LAB-Bench FigQA evaluates whether models can correctly answer questions about complex scientific figures from biology papers, including interpreting plots, annotations, and experimental schematics. It targets the kind of visual-semantic reasoning needed for practical scientific reading and figure-based inference.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Semantic Understanding & Context Recognition, Logical Reasoning, Attention, Working Memory
L3: ",L2
WebArena,https://webarena.dev/,https://arxiv.org/abs/2307.13854,"WebArena evaluates autonomous web agents on realistic multi-step tasks across several web applications (e-commerce, forums, CMS, code hosting, maps) using a browser-like interface. Models must navigate dynamic pages, fill forms, follow task constraints, and recover from mistakes over long interaction sequences.","L1: Visual Perception, Language Comprehension
L2: Planning, Decision-making, Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Sensorimotor Coordination, Working Memory
L3: ",L2
