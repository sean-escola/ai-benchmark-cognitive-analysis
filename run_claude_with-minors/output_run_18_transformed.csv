Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates software-engineering agents on real GitHub issues where the model must produce a patch that makes a project’s tests pass. The “Verified” subset contains tasks validated by humans to be solvable and to have reliable evaluation, reducing ambiguity and flaky grading.","L1: Language Comprehension, Language Production
L2: Planning, Logical Reasoning, Working Memory, Adaptive Error Correction
L3: ",L2
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 measures how well an agent can complete real-world, command-line tasks inside a sandboxed terminal environment. Tasks require choosing and chaining shell commands, inspecting files and outputs, and recovering from errors under resource and time constraints.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory
L3: ",L2
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp evaluates “deep research” behavior: answering questions that require searching, reading, and synthesizing information across multiple web sources. It emphasizes robust evidence gathering, cross-checking, and producing grounded final answers rather than single-hop retrieval.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Attention, Working Memory, Semantic Understanding & Context Recognition
L3: ",L2
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench (Tau2-bench) evaluates tool-using customer-support agents in simulated multi-turn interactions (e.g., retail, airline, telecom) with policies and API-like tools. Success depends on following rules consistently, choosing correct tool calls, and maintaining coherent dialogue across many turns.","L1: Language Comprehension, Language Production
L2: Decision-making, Planning, Working Memory
L3: Social Reasoning & Theory of Mind, Inhibitory Control",L3
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld evaluates multimodal “computer use” agents that must complete tasks in full operating system environments using screenshots and UI interactions. It tests navigation, form filling, multi-app workflows, and error recovery under step limits.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Sensorimotor Coordination, Planning, Decision-making, Working Memory
L3: ",L2
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI measures abstract reasoning and generalization from a handful of examples using grid-based pattern transformation puzzles. Models must infer latent rules and apply them to new inputs, emphasizing novelty rather than memorized skills.","L1: 
L2: Logical Reasoning, Working Memory, Planning, Spatial Representation & Mapping, Scene Understanding & Visual Reasoning
L3: Cognitive Flexibility",L3
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon autonomous agent behavior in a simulated business-management setting (running a vending machine business over an extended period). Agents must make sequential decisions about inventory, pricing, suppliers, and adaptation to changing conditions to maximize final outcomes.","L1: 
L2: Planning, Decision-making, Working Memory, Reward Mechanisms, Adaptive Error Correction
L3: Self-reflection",L3
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas measures real-world tool use through the Model Context Protocol (MCP), where models must discover and correctly invoke tools across multi-step workflows. Tasks emphasize selecting appropriate APIs, handling failures/retries, and synthesizing tool outputs into correct answers.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory
L3: ",L2
FinanceAgent,https://www.vals.ai/benchmarks/finance_agent,,"FinanceAgent evaluates agentic performance on tasks typical of an entry-level financial analyst, often requiring spreadsheet-like reasoning, document synthesis, and quantitative checks. It stresses correctness, structured outputs, and end-to-end task completion rather than only answering questions.","L1: Language Production
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Planning, Decision-making, Working Memory
L3: ",L2
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity agents on vulnerability understanding and discovery across real software projects, including finding known vulnerabilities from descriptions and identifying new issues. Tasks require program analysis, hypothesis testing, and precise reporting of findings.","L1: Language Comprehension, Language Production
L2: Logical Reasoning, Planning, Adaptive Error Correction, Working Memory
L3: ",L2
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"SpreadsheetBench tests whether models can navigate and manipulate realistic spreadsheets to solve applied problems (e.g., data cleaning, formulas, transformations, and summarization). It typically requires structured reasoning over tabular data and correct execution of multi-step edits.","L1: 
L2: Logical Reasoning, Working Memory, Planning, Adaptive Error Correction, Semantic Understanding & Context Recognition, Decision-making
L3: ",L2
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam (HLE) is a broad, frontier-focused benchmark designed to test advanced reasoning and knowledge across many domains, often with multimodal inputs. It aims to probe hard questions where superficial pattern matching is insufficient and tool use may matter depending on the setting.","L1: Language Comprehension, Language Production, Visual Perception
L2: Logical Reasoning, Working Memory, Semantic Understanding & Context Recognition, Multisensory Integration
L3: ",L2
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 evaluates competition-style mathematics problem solving drawn from the American Invitational Mathematics Examination. Problems demand multi-step derivations, careful symbolic manipulation, and exact final answers under strict formatting.","L1: 
L2: Logical Reasoning, Working Memory, Planning, Adaptive Error Correction
L3: ",L2
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a hard subset of graduate-level science multiple-choice questions intended to be difficult to answer via shallow retrieval. It emphasizes rigorous reasoning over expert knowledge in physics, chemistry, and biology with distractors that punish imprecise understanding.","L1: Language Comprehension
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory
L3: ",L2
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends MMLU to multiple languages, testing broad academic knowledge and reasoning across many subjects in non-English settings. It stresses cross-lingual robustness, comprehension, and consistent reasoning despite language shifts.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory
L3: Cognitive Flexibility",L3
MMMU,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2311.16502,"MMMU is a multimodal benchmark covering many disciplines where models must answer questions grounded in images (charts, diagrams, screenshots) plus text. It tests integrated visual-text reasoning and accurate interpretation of complex visual artifacts.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Visual Attention & Eye Movements, Working Memory
L3: ",L2
LAB-Bench FigQA,https://huggingface.co/datasets/futurehouse/lab-bench,https://arxiv.org/abs/2407.10362,"LAB-Bench FigQA evaluates whether models can interpret and reason about scientific figures from biology papers, including plots and experimental schematics. It focuses on extracting correct relationships from visuals and using them to answer research-style questions.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Logical Reasoning, Multisensory Integration, Working Memory, Semantic Understanding & Context Recognition
L3: ",L2
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning tests reasoning over figures and content from scientific papers, often requiring careful interpretation of plots, tables, and experimental setups. It emphasizes multi-step inference grounded in the provided scientific context rather than generic visual recognition.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Logical Reasoning, Working Memory, Multisensory Integration, Semantic Understanding & Context Recognition
L3: ",L2
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU extends multimodal understanding to videos, requiring models to integrate information across frames and time to answer questions. It tests temporal grounding, multi-step reasoning over dynamic scenes, and retention of relevant details over longer sequences.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Multisensory Integration, Attention, Working Memory
L3: Cognitive Timing & Predictive Modeling",L3
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"FACTS Benchmark Suite systematically evaluates factuality, including whether models make unsupported claims, mishandle uncertainty, or contradict sources. It aims to separate fluent generation from reliably grounded, evidence-consistent answers across varied scenarios.","L1: Language Comprehension, Language Production
L2: Semantic Understanding & Context Recognition, Working Memory
L3: Inhibitory Control, Self-reflection",L3
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA evaluates commonsense physical reasoning across many languages, focusing on understanding everyday situations and plausible actions/outcomes. It probes whether models preserve commonsense and reasoning consistency under multilingual variation.","L1: Language Comprehension
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory
L3: Cognitive Flexibility",L3
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) evaluates long-context retrieval and multi-round coreference resolution by embedding multiple similar “needles” within long “haystacks” and asking for a specific referenced response. The 8-needle setting stresses sustained attention, interference resistance, and accurate selection among many near-duplicates.","L1: 
L2: Working Memory, Attention, Semantic Understanding & Context Recognition, Episodic Memory, Logical Reasoning
L3: Inhibitory Control",L3
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval evaluates well-specified professional knowledge-work tasks across many occupations, often requiring producing artifacts like spreadsheets, plans, analyses, and written deliverables judged against expert work. It emphasizes end-to-end task execution quality, correctness, and usefulness under real-world constraints.","L1: Language Production
L2: Planning, Decision-making, Semantic Understanding & Context Recognition, Working Memory
L3: Self-reflection, Social Reasoning & Theory of Mind",L3
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks tests multi-step reasoning over graph-structured data, such as traversals or relational queries that require following edges across many hops. It emphasizes precise state tracking over long chains and resisting shortcuts when distractor paths exist.","L1: 
L2: Spatial Representation & Mapping, Logical Reasoning, Working Memory, Attention, Adaptive Error Correction
L3: ",L2
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon evaluates an agent’s ability to solve tasks by selecting and orchestrating many tools under realistic constraints (correct tool choice, parameters, sequencing, and verification). It emphasizes robustness to tool errors, partial results, and the need to plan multi-step workflows.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory
L3: Inhibitory Control",L3
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath is a benchmark of advanced mathematics problems intended to be challenging for state-of-the-art models, often requiring long derivations and careful verification. It targets deep mathematical reasoning rather than short contest tricks, with strict correctness requirements.","L1: 
L2: Logical Reasoning, Working Memory, Planning, Adaptive Error Correction
L3: ",L2
