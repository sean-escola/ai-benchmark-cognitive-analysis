Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates whether a model can fix real-world GitHub issues by generating a code patch that makes the project’s tests pass. The “Verified” subset contains problems that were manually confirmed to be solvable, and is commonly used to measure end-to-end software engineering and debugging ability under realistic repository context.","L1: Language Comprehension
L2: Planning, Decision-making, Logical Reasoning, Working Memory, Adaptive Error Correction
L3: ",L2
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 evaluates agentic problem-solving in a command-line environment, where models must execute shell commands, inspect files, and iteratively repair mistakes to complete tasks. It emphasizes interactive, tool-mediated workflows and robustness to environment feedback and errors.","L1: Language Comprehension
L2: Planning, Decision-making, Working Memory, Adaptive Error Correction, Sensorimotor Coordination, Attention
L3: ",L2
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp measures deep-research performance by requiring models to locate and synthesize evidence from a curated web document collection, typically via search and retrieval tools. Tasks reward accurate attribution, multi-step information gathering, and coherent synthesis across multiple sources.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Attention, Working Memory, Semantic Understanding & Context Recognition
L3: ",L2
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench evaluates interactive customer-support agents operating in simulated domains (e.g., retail, airline, telecom) with policies and tool/API calls. Agents must follow domain-specific rules, manage multi-turn dialogue, and complete user goals reliably despite ambiguity and policy constraints.","L1: Language Comprehension, Language Production
L2: Decision-making, Planning, Working Memory
L3: Social Reasoning & Theory of Mind, Inhibitory Control",L3
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld evaluates multimodal computer-use agents on tasks performed in a full operating-system environment (e.g., navigating GUIs, filling forms, configuring settings). Success depends on perceiving on-screen state, selecting correct actions, and recovering from UI or tool errors over long action sequences.","L1: Visual Perception
L2: Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Planning, Decision-making, Working Memory, Sensorimotor Coordination, Adaptive Error Correction
L3: ",L2
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI is a “fluid intelligence” benchmark of abstract pattern induction on grid transformation tasks, where models infer a hidden rule from a few examples and apply it to new inputs. It is designed to probe generalization to novel problems rather than memorized knowledge.","L1: 
L2: Logical Reasoning, Working Memory, Spatial Representation & Mapping, Attention
L3: Cognitive Flexibility",L3
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon agent coherence and business decision-making in a simulated vending-machine enterprise over many steps. Agents must plan inventory and pricing, negotiate or purchase supplies, and adapt strategies to maximize long-run outcomes.","L1: 
L2: Planning, Decision-making, Working Memory, Reward Mechanisms, Adaptive Error Correction
L3: Cognitive Timing & Predictive Modeling, Self-reflection",L3
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas evaluates real-world tool use via the Model Context Protocol (MCP), requiring models to discover relevant tools, call them correctly, handle failures, and compose multi-step workflows across services. It targets practical API competence and reliable orchestration in production-like setups.","L1: Language Comprehension
L2: Planning, Decision-making, Working Memory, Adaptive Error Correction, Attention
L3: ",L2
FinanceAgent,https://www.vals.ai/benchmarks/finance_agent,,"FinanceAgent evaluates tasks expected of an entry-level financial analyst, such as interpreting financial documents, performing calculations, producing structured analyses, and (often) using tools. It emphasizes applied reasoning and producing decision-relevant outputs under domain constraints.","L1: Language Production, Language Comprehension
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Planning, Decision-making, Working Memory
L3: ",L2
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,CyberGym evaluates cybersecurity agent capabilities on vulnerability identification and vulnerability discovery tasks grounded in real software projects. It tests whether models can reason about codebases and security hints to find flaws and produce correct exploit-relevant conclusions or fixes.,"L1: 
L2: Logical Reasoning, Planning, Working Memory, Adaptive Error Correction, Attention
L3: Inhibitory Control",L3
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"SpreadsheetBench evaluates a model’s ability to understand and manipulate complex spreadsheets, often requiring multi-step edits, formula reasoning, and consistent structuring. Tasks resemble real workplace spreadsheet operations where small mistakes can propagate across dependent cells.","L1: 
L2: Logical Reasoning, Working Memory, Planning, Adaptive Error Correction, Semantic Understanding & Context Recognition, Attention
L3: ",L2
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam (HLE) is a multimodal benchmark intended to probe frontier academic reasoning across a wide variety of difficult questions. It rewards correct answers under high uncertainty and often tests the ability to integrate knowledge, reasoning, and (when enabled) tool-based verification.","L1: Language Comprehension, Language Production, Visual Perception
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory
L3: ",L2
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 is a competition-math benchmark derived from the American Invitational Mathematics Examination problems. It focuses on multi-step symbolic reasoning, careful arithmetic, and solution verification under constrained output formats.","L1: 
L2: Logical Reasoning, Working Memory, Attention
L3: Cognitive Flexibility",L3
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a subset of the GPQA benchmark containing especially high-quality, graduate-level multiple-choice science questions that are hard to answer via shallow pattern matching. It targets deep scientific reasoning and precise discrimination among plausible options.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Attention
L3: ",L2
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends MMLU into many languages, testing whether a model’s knowledge and reasoning transfer across linguistic contexts. It probes robust multilingual understanding of academic topics and the ability to select correct answers under varied language phrasing.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Working Memory, Attention
L3: ",L2
MMMU,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2311.16502,"MMMU evaluates expert-level multimodal understanding across many disciplines, combining images (e.g., diagrams, plots, figures) with text questions. It tests whether models can ground language in visual evidence and perform reasoning over both modalities.","L1: Visual Perception, Language Comprehension
L2: Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Logical Reasoning, Working Memory, Multisensory Integration
L3: ",L2
LAB-Bench FigQA,https://huggingface.co/datasets/futurehouse/lab-bench,https://arxiv.org/abs/2407.10362,"LAB-Bench FigQA evaluates whether models can answer questions about complex scientific figures from biology papers, often requiring careful reading of axes, legends, and experimental layouts. It emphasizes figure-grounding, scientific interpretation, and avoiding plausible-but-unsupported claims.","L1: Visual Perception
L2: Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory
L3: ",L2
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning evaluates reasoning over scientific charts and figures associated with papers, requiring models to extract quantitative/relational information and answer structured questions. It stresses precise visual-text grounding and multi-step inference rather than generic captioning.","L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Logical Reasoning, Working Memory
L3: ",L2
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU extends multimodal reasoning to video, requiring models to track events, states, and causal relations over time. It probes temporal integration of visual evidence with text questions, often demanding cross-frame consistency and long-range dependency tracking.","L1: Visual Perception
L2: Attention, Working Memory, Scene Understanding & Visual Reasoning, Multisensory Integration
L3: Cognitive Timing & Predictive Modeling",L3
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"FACTS Benchmark Suite evaluates factuality and groundedness across a collection of tasks that stress precise claim generation, attribution, and resistance to hallucination. It aims to measure whether a model can remain faithful to sources and avoid fabricating details when uncertain.","L1: Language Production, Language Comprehension
L2: Semantic Understanding & Context Recognition, Working Memory
L3: Inhibitory Control, Self-reflection",L3
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,Global PIQA evaluates physical commonsense reasoning—choosing actions or explanations consistent with everyday physics—across multiple languages in a non-parallel setting. It tests whether models maintain consistent intuitive physics and commonsense across linguistic variation.,"L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Spatial Representation & Mapping, Working Memory
L3: ",L2
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,MRCR v2 (8-needle) tests long-context multi-round coreference and retrieval by embedding multiple similar “needle” requests into large “haystack” conversations and requiring the model to reproduce the correct referenced response. It stresses robust tracking of entities and instructions across very long contexts.,"L1: Language Comprehension
L2: Working Memory, Attention, Episodic Memory, Semantic Understanding & Context Recognition
L3: ",L2
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval evaluates economically valuable professional knowledge work across many occupations by asking models to produce real deliverables (e.g., spreadsheets, presentations, plans) judged against human professionals. It emphasizes end-to-end task execution quality, structure, and practical decision support.","L1: Language Production
L2: Planning, Decision-making, Semantic Understanding & Context Recognition, Working Memory
L3: Social Reasoning & Theory of Mind",L3
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks evaluates a model’s ability to follow graph-structured relationships (e.g., traversals, parent/neighbor queries) described in text, often over long contexts. It targets systematic traversal and consistent bookkeeping under distractors and near-duplicate nodes.","L1: 
L2: Spatial Representation & Mapping, Working Memory, Logical Reasoning, Attention, Semantic Understanding & Context Recognition
L3: ",L2
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon evaluates tool-using agents on diverse multi-step tasks that require selecting among many tools, calling them correctly, and composing results into a final answer. It probes reliability under tool errors, interface variation, and long action chains.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Working Memory, Adaptive Error Correction, Attention
L3: ",L2
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath is an advanced mathematics benchmark intended to measure progress on expert-level problems beyond routine competition math, often requiring creative multi-step reasoning. It targets deep mathematical problem solving and verification under tight correctness standards.","L1: 
L2: Logical Reasoning, Working Memory, Attention
L3: Cognitive Flexibility",L3
