Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates software-engineering agents on real GitHub issues where the task is to produce a patch that makes a failing test suite pass. The “Verified” subset uses human validation to filter to problems that are actually solvable and correctly specified, emphasizing reliable end-to-end bug fixing.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Planning, Decision-making, Logical Reasoning, Working Memory, Adaptive Error Correction
L3: ",L2
SWE-bench Multilingual,https://www.swebench.com/multilingual.html,https://arxiv.org/abs/2504.21798,"SWE-bench Multilingual extends SWE-bench-style repository patching beyond Python into multiple programming languages, stressing cross-language transfer and debugging. Systems must interpret issue descriptions, localize faults, modify code, and satisfy tests across diverse language ecosystems.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Planning, Decision-making, Logical Reasoning, Working Memory, Adaptive Error Correction
L3: Cognitive Flexibility",L3
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro is a larger and more difficult software-engineering benchmark designed to better reflect industrial complexity and reduce shortcutting via contamination. Tasks typically require deeper codebase understanding, multi-file changes, and more robust validation to produce correct patches.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Planning, Decision-making, Logical Reasoning, Working Memory, Adaptive Error Correction
L3: ",L2
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 measures agent performance on real command-line tasks executed in sandboxed environments, such as installing dependencies, editing files, running programs, and diagnosing failures. Success requires choosing effective sequences of shell actions and iteratively correcting mistakes based on tool feedback.","L1: 
L2: Planning, Decision-making, Working Memory, Adaptive Error Correction, Logical Reasoning, Semantic Understanding & Context Recognition
L3: ",L2
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp evaluates deep-research behavior by requiring models to answer questions using browsing/search over a controlled document collection to improve reproducibility. It emphasizes iterative information seeking, source triangulation, and synthesis into a final grounded response.","L1: Language Comprehension
L2: Planning, Decision-making, Working Memory, Episodic Memory, Semantic Understanding & Context Recognition, Logical Reasoning, Adaptive Error Correction
L3: ",L2
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench evaluates interactive tool-using agents in multi-turn customer-support simulations (e.g., retail, airline, telecom) with domain policies and programmatic APIs. Agents must resolve user requests while adhering to constraints, managing dialogue state, and avoiding policy violations or loopholes.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Working Memory, Adaptive Error Correction
L3: Social Reasoning & Theory of Mind, Empathy, Inhibitory Control",L3
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld benchmarks multimodal computer-use agents operating a desktop-like environment, where the model must interpret screenshots and perform UI actions over many steps. Tasks require robust GUI navigation, state tracking, and recovery from mis-clicks or unexpected interface states.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Sensorimotor Coordination, Planning, Decision-making, Working Memory, Adaptive Error Correction
L3: ",L2
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,ARC-AGI is a “few examples” abstract reasoning benchmark where models infer the transformation rule mapping input grids to output grids and apply it to a new input. It is intended to probe fluid reasoning and generalization to novel tasks with minimal training-like exposure.,"L1: Visual Perception
L2: Logical Reasoning, Spatial Representation & Mapping, Working Memory, Attention, Scene Understanding & Visual Reasoning
L3: Cognitive Flexibility",L3
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon agent coherence by simulating the operation of a vending-machine business across an extended time period with many decisions. Agents must manage inventory, pricing, procurement/negotiation, and cash flow to maximize final performance under uncertainty.","L1: 
L2: Planning, Decision-making, Reward Mechanisms, Working Memory, Episodic Memory, Adaptive Error Correction
L3: ",L2
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas assesses real-world tool use through the Model Context Protocol by requiring multi-step workflows across external tools and APIs. The benchmark emphasizes selecting the right tools, making correct calls, handling errors/retries, and integrating results into an accurate final output.","L1: Language Comprehension
L2: Planning, Decision-making, Working Memory, Adaptive Error Correction, Semantic Understanding & Context Recognition
L3: ",L2
FinanceAgent,https://www.vals.ai/benchmarks/finance_agent,,"FinanceAgent evaluates agentic performance on tasks representative of entry-level financial analysis, often requiring multi-step reasoning over documents, tables, and calculations. It stresses producing decision-relevant outputs (e.g., analyses, summaries, or models) while maintaining numerical and contextual consistency.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Planning, Decision-making, Working Memory, Adaptive Error Correction
L3: ",L2
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity agent skills on real open-source projects, including identifying known vulnerabilities from descriptions and discovering new issues. The tasks require reading and reasoning about code, forming hypotheses about weaknesses, and iteratively validating findings.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Planning, Decision-making, Working Memory, Adaptive Error Correction
L3: ",L2
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"SpreadsheetBench measures an agent’s ability to work with complex spreadsheets, including navigating sheets, editing values/formulas, and generating correct computations and outputs. It reflects practical office-automation demands where success depends on maintaining state, applying constraints, and checking results.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Planning, Decision-making, Working Memory, Logical Reasoning, Adaptive Error Correction
L3: ",L2
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a difficult, broad, and often multimodal benchmark intended to probe frontier academic reasoning and knowledge across many domains. Questions frequently demand careful reading, multi-step inference, and (in tool-enabled variants) disciplined grounding and verification.","L1: Language Comprehension, Visual Perception
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Planning, Multisensory Integration
L3: ",L2
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 is a competition-math benchmark drawn from the American Invitational Mathematics Examination, featuring problems that require nontrivial algebraic and combinational reasoning. It primarily tests structured multi-step derivations under tight precision requirements.","L1: 
L2: Logical Reasoning, Working Memory, Planning, Attention
L3: ",L2
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a high-difficulty multiple-choice science QA benchmark intended to be resistant to simple web search and superficial pattern matching. The “Diamond” subset focuses on especially challenging, high-quality questions where expert reasoning is necessary.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Attention
L3: ",L2
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends broad academic knowledge testing to many non-English languages and subject areas, assessing whether models can reason and answer reliably across linguistic contexts. It measures both multilingual understanding and cross-domain factual/analytical competence.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory
L3: Cognitive Flexibility",L3
MMMU,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2311.16502,"MMMU is a multimodal benchmark requiring models to answer expert-level questions using both text and images (e.g., diagrams, charts, figures) spanning many disciplines. It stresses integrating visual evidence with domain knowledge and reasoning to produce correct answers.","L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Multisensory Integration, Logical Reasoning, Working Memory
L3: ",L2
LAB-Bench FigQA,https://huggingface.co/datasets/futurehouse/lab-bench,https://arxiv.org/abs/2407.10362,"LAB-Bench FigQA evaluates whether models can interpret complex scientific figures from biology papers and answer questions that require extracting and reasoning over visual evidence. It emphasizes precise figure reading, relating visual signals to textual scientific context, and avoiding confabulation.","L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Multisensory Integration, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory
L3: ",L2
WebArena,https://webarena.dev/,https://arxiv.org/abs/2307.13854,"WebArena benchmarks autonomous web agents on realistic tasks across multiple self-hosted web applications (e-commerce, forums, GitLab-like workflows, etc.). Agents must navigate dynamic pages, fill forms, manage multi-step goals, and recover from interaction or state-tracking errors.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Sensorimotor Coordination, Planning, Decision-making, Working Memory, Adaptive Error Correction
L3: ",L2
