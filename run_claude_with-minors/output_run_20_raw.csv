Benchmark,Website,Paper,Description,Cognitive Functions
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates an AI model’s ability to resolve real-world GitHub software issues by generating patches that pass the project’s tests, with a curated set of tasks verified by humans to be solvable. It targets end-to-end bug fixing and feature implementation in Python repositories under a standardized, reliability-focused evaluation setup.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Logical Reasoning, Semantic Understanding & Context Recognition, Language Comprehension, Language Production"
SWE-bench Multilingual,https://www.swebench.com/multilingual.html,https://arxiv.org/abs/2504.21798,"SWE-bench Multilingual extends the SWE-bench paradigm beyond Python to multiple programming languages, assessing whether models can diagnose and patch issues across diverse ecosystems and toolchains. It emphasizes transfer of debugging and code-editing competence across languages and project conventions.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Logical Reasoning, Semantic Understanding & Context Recognition, Language Comprehension, Language Production, Cognitive Flexibility"
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro is a larger and more challenging software engineering benchmark designed to be more realistic and more resistant to contamination, spanning multiple languages and harder issue types. It evaluates whether models can produce correct patches under stricter, industrially relevant conditions and broader repository diversity.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Logical Reasoning, Semantic Understanding & Context Recognition, Language Comprehension, Language Production, Cognitive Flexibility"
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 evaluates agentic performance on real tasks executed in a command-line environment, where models must iteratively run commands, inspect outputs, and modify files to achieve a goal. It measures practical tool use, debugging, and execution-aware reasoning under environmental constraints and potential flakiness.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Logical Reasoning, Semantic Understanding & Context Recognition, Language Comprehension, Language Production, Attention"
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp evaluates deep-research agents on questions that require searching, reading, and synthesizing information from a fixed document collection to enable reproducible scoring. It probes multi-step retrieval, evidence integration across sources, and maintaining coherence while navigating long or noisy information.","Planning, Decision-making, Attention, Working Memory, Episodic Memory, Semantic Understanding & Context Recognition, Language Comprehension, Language Production, Logical Reasoning"
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench evaluates interactive customer-support-style agents operating over multi-turn dialogs with simulated users and APIs, while following domain policies (e.g., retail, airline, telecom). It measures robust tool calling, policy adherence under pressure, and consistent conversational behavior across long task horizons.","Planning, Decision-making, Inhibitory Control, Social Reasoning & Theory of Mind, Empathy, Working Memory, Attention, Language Comprehension, Language Production, Adaptive Error Correction"
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld measures computer-use agency in a realistic desktop environment, where models must interpret screenshots and execute actions to complete tasks across applications. It emphasizes visually grounded interaction, multi-step navigation, and recovery from mistakes in a dynamic GUI setting.","Visual Perception, Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Planning, Decision-making, Sensorimotor Coordination, Working Memory, Adaptive Error Correction"
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,ARC-AGI evaluates “fluid” reasoning by requiring models to infer abstract rules from a few grid-based input-output examples and apply them to new inputs. The tasks are designed to reward generalization and compositional pattern discovery rather than memorization or domain knowledge.,"Logical Reasoning, Cognitive Flexibility, Working Memory, Attention, Spatial Representation & Mapping, Scene Understanding & Visual Reasoning"
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon autonomous decision-making in a simulated vending-machine business, where an agent must manage inventory, pricing, supplier negotiation, and cash flow over an extended timeline. It tests sustained coherence, strategy formation, and adaptation to changing market signals across many steps.","Planning, Decision-making, Reward Mechanisms, Working Memory, Episodic Memory, Adaptive Error Correction, Motivational Drives"
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas evaluates real-world tool-use through the Model Context Protocol, requiring models to discover tools, call APIs correctly, manage multi-step workflows, and synthesize results. It emphasizes execution reliability, error handling, and orchestrating sequences of tool calls to complete tasks.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Attention, Language Comprehension, Language Production, Logical Reasoning"
FinanceAgent,https://www.vals.ai/benchmarks/finance_agent,,"FinanceAgent assesses performance on tasks typical of an entry-level financial analyst, including retrieving/transforming financial information, performing calculations, and producing analysis-ready outputs. It measures domain-grounded reasoning and the ability to structure multi-step analytical workflows into correct deliverables.","Logical Reasoning, Planning, Decision-making, Working Memory, Semantic Understanding & Context Recognition, Language Comprehension, Language Production, Adaptive Error Correction"
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity capabilities by asking agents to identify known vulnerabilities from descriptions and to discover new vulnerabilities in real open-source projects at scale. It probes technical reasoning over codebases, hypothesis-driven exploration, and iterative verification of findings.","Logical Reasoning, Planning, Decision-making, Adaptive Error Correction, Working Memory, Attention, Semantic Understanding & Context Recognition"
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"SpreadsheetBench evaluates spreadsheet-centric task completion using real-world-inspired spreadsheets, requiring agents to read, modify, compute, and validate structured data. It targets procedural competence in manipulating complex artifacts and maintaining correctness across interdependent cells and sheets.","Planning, Decision-making, Working Memory, Adaptive Error Correction, Logical Reasoning, Semantic Understanding & Context Recognition, Attention"
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam (HLE) is a frontier, multi-domain benchmark with challenging questions that span advanced reasoning and expert knowledge, often in multimodal formats. It is designed to stress models’ ability to synthesize information, reason under uncertainty, and produce well-justified answers beyond routine exam-style QA.","Language Comprehension, Language Production, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Attention, Multisensory Integration, Scene Understanding & Visual Reasoning, Visual Perception"
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 consists of competition-style mathematics problems that require multi-step symbolic reasoning and careful constraint management, typically with short numeric final answers. It is used to measure mathematical problem solving under tight precision demands and limited opportunity for partial credit.","Logical Reasoning, Working Memory, Planning, Attention, Adaptive Error Correction"
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a difficult multiple-choice science benchmark curated to be “Google-proof,” with questions requiring deep understanding and reasoning rather than surface recall. The Diamond subset focuses on high-quality items where experts succeed and non-experts often fail, stressing robust scientific reasoning.","Language Comprehension, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Attention, Decision-making"
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends broad academic knowledge evaluation across many subjects and multiple non-English languages, testing both content understanding and reasoning under multilingual variation. It is commonly used to assess cross-lingual generalization and robustness of instruction-following and QA capabilities.","Language Comprehension, Language Production, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Attention, Cognitive Flexibility"
MMMU,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2311.16502,"MMMU is a multimodal benchmark spanning many disciplines where models must answer questions that require jointly interpreting text and images (e.g., diagrams, charts, scientific visuals). It stresses visual grounding, cross-modal integration, and reasoning about specialized content in context.","Visual Perception, Scene Understanding & Visual Reasoning, Multisensory Integration, Visual Attention & Eye Movements, Language Comprehension, Language Production, Logical Reasoning, Working Memory, Attention"
LAB-Bench FigQA,https://huggingface.co/datasets/futurehouse/lab-bench,https://arxiv.org/abs/2407.10362,"LAB-Bench FigQA evaluates whether models can accurately interpret complex scientific figures from biology papers and answer questions that depend on visual evidence and domain context. It targets figure literacy—extracting relationships, trends, and experimental implications from real research visuals.","Visual Perception, Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Multisensory Integration, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Attention, Language Comprehension, Language Production"
WebArena,https://webarena.dev/,https://arxiv.org/abs/2307.13854,"WebArena evaluates autonomous web agents performing realistic multi-step tasks across interactive websites (e-commerce, CMS, code hosting, social platforms), requiring navigation, form filling, and goal tracking. It measures end-to-end planning and robust interaction with dynamic web UIs under partial observability.","Visual Perception, Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Planning, Decision-making, Sensorimotor Coordination, Working Memory, Adaptive Error Correction, Attention, Language Comprehension, Language Production"
