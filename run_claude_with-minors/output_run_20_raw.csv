Benchmark,Website,Paper,Description,Cognitive Functions
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates software engineering agents on real GitHub issues where the task is to produce a patch that passes project tests. The “Verified” subset uses problems that have been filtered/confirmed to be solvable and reduces noise from ambiguous tasks, emphasizing end-to-end bug fixing and implementation reliability.","Planning, Logical Reasoning, Working Memory, Adaptive Error Correction, Decision-making, Language Comprehension"
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 tests agentic coding and systems skills in a command-line environment, requiring models to run commands, inspect files, install dependencies, and iteratively debug under realistic constraints. Tasks reward robust exploration, error recovery, and correct tool-mediated execution rather than single-shot code generation.","Planning, Adaptive Error Correction, Decision-making, Working Memory, Logical Reasoning, Language Comprehension"
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp measures a model’s ability to answer difficult questions by conducting web research: issuing searches, opening sources, extracting evidence, and synthesizing a final answer. It is designed to test deep retrieval, source triage, and multi-step information integration under tool-use constraints.","Attention, Planning, Working Memory, Semantic Understanding & Context Recognition, Language Comprehension, Decision-making"
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench evaluates interactive customer-support style agents that must follow domain policies while using APIs and conversing with a simulated user across multiple turns. The benchmark emphasizes consistent policy adherence, correct tool/API usage, and maintaining task state over long dialogues.","Social Reasoning & Theory of Mind, Language Comprehension, Language Production, Planning, Decision-making, Inhibitory Control, Working Memory"
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld evaluates multimodal computer-use agents on completing tasks in a full operating-system desktop environment (e.g., browsing, file management, app workflows). Success requires interpreting screenshots/GUI state, selecting actions (click/type), and recovering from UI errors or unexpected states over many steps.","Visual Perception, Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Planning, Decision-making, Sensorimotor Coordination, Working Memory, Adaptive Error Correction"
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,ARC-AGI presents few-shot grid transformation puzzles where a model must infer hidden rules from a small set of input–output examples and apply them to a new input. It targets generalization to novel patterns and compositional rule induction rather than memorized knowledge.,"Logical Reasoning, Cognitive Flexibility, Working Memory, Scene Understanding & Visual Reasoning, Spatial Representation & Mapping, Planning"
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 measures long-horizon autonomy by having an agent run a simulated vending machine business over an extended time period, including inventory, pricing, supplier interaction, and adaptation to market dynamics. It stresses sustained coherence and strategic decision-making across thousands of sequential actions.","Planning, Decision-making, Working Memory, Reward Mechanisms, Adaptive Error Correction, Semantic Understanding & Context Recognition"
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas evaluates real-world tool use through the Model Context Protocol, where agents must discover, call, and chain tools across multi-step workflows using production-like servers. It focuses on correct API selection, argument formatting, error handling, and synthesis of tool outputs into final responses.","Planning, Decision-making, Working Memory, Adaptive Error Correction, Language Comprehension, Semantic Understanding & Context Recognition"
FinanceAgent,https://www.vals.ai/benchmarks/finance_agent,,"FinanceAgent assesses whether models can perform tasks typical of an entry-level financial analyst, such as extracting figures, reasoning about financial statements, and producing analysis-ready outputs. It emphasizes structured reasoning with domain constraints and careful handling of numerical and document context.","Logical Reasoning, Decision-making, Working Memory, Semantic Understanding & Context Recognition, Planning, Language Production"
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity agents on tasks involving identifying known vulnerabilities from descriptions and, in some settings, finding previously unknown vulnerabilities in real software. The benchmark rewards systematic investigation, hypothesis testing, and safe iterative debugging of exploit-relevant behaviors.","Logical Reasoning, Planning, Adaptive Error Correction, Working Memory, Decision-making, Inhibitory Control"
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"SpreadsheetBench tests end-to-end spreadsheet manipulation and reasoning, including reading complex workbooks, applying transformations, and generating correct computed outputs. The benchmark stresses structured, tool-mediated workflows and multi-step consistency across formulas, tables, and formatting constraints.","Working Memory, Planning, Logical Reasoning, Adaptive Error Correction, Decision-making, Language Comprehension"
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a multi-domain, frontier-difficulty benchmark with questions spanning advanced knowledge and reasoning, often including multimodal inputs. It is designed to probe general problem solving, synthesis, and robustness on items that exceed typical academic benchmark difficulty.","Language Comprehension, Logical Reasoning, Working Memory, Semantic Understanding & Context Recognition, Scene Understanding & Visual Reasoning, Planning"
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 evaluates competition-style mathematics problem solving under time-limited, trick-resistant question design. It emphasizes symbolic manipulation, multi-step derivations, and precise final answers (often integer-valued) without relying on external tools.","Logical Reasoning, Working Memory, Planning, Cognitive Flexibility"
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a high-difficulty multiple-choice science benchmark curated to be “Google-proof,” requiring deep reasoning rather than shallow lookup. The Diamond subset is a quality-filtered split intended to better separate expert-level scientific understanding from surface heuristics.","Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Decision-making, Language Comprehension"
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends broad academic knowledge testing across many subjects into multiple languages, measuring whether models can generalize knowledge and reasoning beyond English. It stresses multilingual understanding while keeping question styles similar to standard MMLU.","Language Comprehension, Semantic Understanding & Context Recognition, Working Memory, Logical Reasoning"
MMMU,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2311.16502,"MMMU evaluates expert-level multimodal understanding across many disciplines by combining images (diagrams, charts, figures) with text questions. It targets cross-modal grounding and reasoning, requiring models to extract visual evidence and integrate it with domain knowledge.","Visual Perception, Scene Understanding & Visual Reasoning, Multisensory Integration, Language Comprehension, Logical Reasoning, Working Memory"
LAB-Bench FigQA,https://huggingface.co/datasets/futurehouse/lab-bench,https://arxiv.org/abs/2407.10362,"LAB-Bench FigQA measures whether models can correctly interpret complex scientific figures from biology papers and answer questions that depend on visual evidence and experimental context. It emphasizes faithful figure reading, cross-referencing captions/labels, and reasoning about scientific claims.","Visual Perception, Scene Understanding & Visual Reasoning, Semantic Understanding & Context Recognition, Language Comprehension, Logical Reasoning, Working Memory"
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning tests chart and figure understanding using scientific plots/figures (e.g., from arXiv-style documents), requiring models to answer questions grounded in visualized data. It stresses quantitative and relational reasoning over axes, legends, and trends rather than generic image captioning.","Visual Perception, Scene Understanding & Visual Reasoning, Logical Reasoning, Working Memory, Language Comprehension"
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU extends multimodal understanding to the temporal domain, asking questions that require integrating information across video frames and associated text. It targets temporal consistency, event understanding, and multi-step reasoning over dynamic visual scenes.","Visual Perception, Scene Understanding & Visual Reasoning, Working Memory, Attention, Cognitive Timing & Predictive Modeling, Multisensory Integration, Language Comprehension"
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"FACTS Benchmark Suite systematically evaluates factuality in large language models across diverse settings, including attribution/grounding, resistance to hallucination, and consistency under prompting. It aims to distinguish models that are merely fluent from those that reliably track truth and evidence.","Semantic Understanding & Context Recognition, Language Comprehension, Inhibitory Control, Adaptive Error Correction, Working Memory"
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA is a multilingual, non-parallel extension of physical interaction question answering, testing common-sense reasoning about everyday actions and object affordances across many languages and cultures. It probes whether models preserve grounded physical intuition under linguistic variation.","Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning, Spatial Representation & Mapping"
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,MRCR v2 (8-needle) evaluates long-context multi-round coreference resolution by embedding multiple similar “needle” requests across a long “haystack” of distractors and asking the model to reproduce the correct referenced content. The 8-needle setting stresses precise retrieval and disambiguation under heavy interference.,"Working Memory, Attention, Language Comprehension, Semantic Understanding & Context Recognition, Inhibitory Control"
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval evaluates economically relevant knowledge work by having models produce real professional artifacts (e.g., slides, spreadsheets, plans) across many occupations and scoring them via expert human comparisons. It focuses on end-to-end task execution quality, including structure, correctness, and usability of deliverables.","Planning, Decision-making, Language Production, Semantic Understanding & Context Recognition, Working Memory, Social Reasoning & Theory of Mind"
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks tests reasoning over graph-structured data, such as following paths, retrieving parents/ancestors, or performing constrained traversals from textual descriptions. It emphasizes multi-step state tracking and systematic traversal rather than free-form association.","Spatial Representation & Mapping, Working Memory, Planning, Logical Reasoning, Attention"
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon evaluates general tool-using agents across diverse APIs and tasks, emphasizing selecting appropriate tools, composing multi-step tool chains, and handling tool failures. It is designed to measure robust agentic execution rather than isolated function calling.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Language Comprehension"
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath measures advanced mathematical problem solving at research-adjacent difficulty, with tiers intended to separate incremental capability gains at the frontier. It emphasizes rigorous multi-step derivations, abstraction, and error-sensitive reasoning that often defeats pattern-matching approaches.","Logical Reasoning, Working Memory, Planning, Cognitive Flexibility, Adaptive Error Correction"
