Benchmark,Website,Paper,Description,Cognitive Functions
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates an LLM’s ability to solve real software engineering issues by generating patches in real GitHub repositories, typically validated by running the project’s test suite. The “Verified” subset consists of problems confirmed by human annotators to be solvable and to have reliable evaluation signals, improving comparability across systems.","Planning, Decision-making, Logical Reasoning, Working Memory, Adaptive Error Correction, Language Comprehension"
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 measures agent performance on real-world command-line tasks executed in sandboxed terminal environments (e.g., configuring tools, manipulating files, running programs, debugging). Success depends on choosing correct shell commands, interpreting outputs, and iterating until the objective is met.","Planning, Decision-making, Working Memory, Adaptive Error Correction, Language Comprehension, Semantic Understanding & Context Recognition"
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp evaluates “deep research” style browsing: models must search a fixed corpus of web documents, identify relevant sources, and synthesize an answer that matches ground truth. It emphasizes multi-step information seeking under constraints (limited retrieval, noisy snippets) and rewards robust citation-like grounding behavior.","Planning, Attention, Working Memory, Semantic Understanding & Context Recognition, Decision-making, Language Comprehension, Language Production"
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench evaluates interactive agent behavior in simulated customer support domains (e.g., retail, airline, telecom), combining natural language dialogue with API/tool actions. Agents must follow domain policies while completing multi-turn tasks, handling constraints, and resolving user issues without breaking rules.","Social Reasoning & Theory of Mind, Language Comprehension, Language Production, Planning, Decision-making, Inhibitory Control, Working Memory"
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld is a multimodal computer-use benchmark where agents control a desktop-like environment via screenshots and action primitives (mouse/keyboard) to complete realistic tasks across applications. It stresses grounding in visual UI state, long-horizon task completion, and recovery from mistakes under step limits.","Visual Perception, Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Sensorimotor Coordination, Planning, Decision-making, Working Memory, Adaptive Error Correction, Spatial Representation & Mapping"
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI measures fluid, few-shot abstract reasoning using grid transformation puzzles: models infer latent rules from a handful of input–output examples and must produce the correct output for a new grid. It is designed to reduce reliance on memorized knowledge and emphasize rapid rule induction and generalization.","Logical Reasoning, Cognitive Flexibility, Working Memory, Spatial Representation & Mapping, Attention"
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon autonomy in a business-management simulation: the agent runs a vending-machine business over many turns (e.g., procurement, pricing, inventory, negotiation) and is scored by final profit/balance. It probes sustained coherence, strategy under uncertainty, and iterative improvement over extended time.","Planning, Decision-making, Working Memory, Reward Mechanisms, Cognitive Timing & Predictive Modeling, Adaptive Error Correction"
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas evaluates real-world tool use via the Model Context Protocol (MCP): models must discover appropriate tools, call them correctly across multiple steps, handle errors, and integrate results into a final response. It targets robust tool-selection and tool-argument grounding rather than purely text-only reasoning.","Planning, Decision-making, Working Memory, Adaptive Error Correction, Language Comprehension, Semantic Understanding & Context Recognition"
FinanceAgent,https://www.vals.ai/benchmarks/finance_agent,,"FinanceAgent measures an agent’s ability to perform tasks resembling entry-level financial analysis, such as interpreting financial documents, extracting figures, performing calculations, and producing structured deliverables. Strong performance requires combining domain knowledge with multi-step quantitative reasoning and artifact-focused reporting.","Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Planning, Decision-making, Language Production, Language Comprehension"
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity capabilities by testing whether agents can identify known vulnerabilities from descriptions and also discover previously unknown issues in real open-source projects. It emphasizes code comprehension, hypothesis-driven debugging, and careful iterative validation under a pass@1-style setting.","Logical Reasoning, Planning, Working Memory, Adaptive Error Correction, Attention, Semantic Understanding & Context Recognition"
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"SpreadsheetBench evaluates the ability to understand, edit, and generate complex spreadsheets drawn from realistic workflows (e.g., cleaning data, building formulas, restructuring tables, and producing correct outputs). It tests procedural accuracy and error recovery when manipulating structured artifacts rather than free-form text.","Planning, Decision-making, Working Memory, Adaptive Error Correction, Logical Reasoning, Semantic Understanding & Context Recognition"
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,Humanity’s Last Exam is a large multimodal benchmark intended to probe frontier-level knowledge and reasoning across diverse expert domains. Questions often require integrating text with visual inputs and performing multi-step inference rather than recalling isolated facts.,"Language Comprehension, Visual Perception, Scene Understanding & Visual Reasoning, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory"
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 evaluates competition-style mathematics problem solving using the 2025 American Invitational Mathematics Examination questions. Problems typically require multi-step derivations, careful algebraic manipulation, and consistency checks under time-like constraints.","Logical Reasoning, Working Memory, Planning, Adaptive Error Correction, Attention"
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a difficult graduate-level science multiple-choice benchmark designed to be resistant to simple web search and pattern matching. The “Diamond” subset focuses on high-quality questions that strongly separate experts from non-experts, requiring deep scientific reasoning and precise reading.","Language Comprehension, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Attention"
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends MMLU to multiple languages, testing broad academic knowledge and reasoning across many subjects in non-English settings. It stresses cross-lingual generalization and robust comprehension of domain-specific terminology and question styles.","Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Attention"
MMMU,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2311.16502,"MMMU is a multimodal, multi-discipline benchmark where questions require jointly using images (diagrams, charts, figures) and text to answer expert-level prompts. It targets visual grounding plus domain reasoning across a broad set of professional and academic areas.","Visual Perception, Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Multisensory Integration, Language Comprehension, Logical Reasoning, Working Memory"
LAB-Bench FigQA,https://huggingface.co/datasets/futurehouse/lab-bench,https://arxiv.org/abs/2407.10362,"LAB-Bench FigQA evaluates whether models can correctly interpret scientific figures from biology papers, including reading axes/legends, comparing conditions, and drawing conclusions supported by the figure. It focuses on figure-grounded reasoning rather than free-form biological recall.","Visual Perception, Scene Understanding & Visual Reasoning, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Attention"
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning tests chart/figure understanding and quantitative reasoning over figures drawn from arXiv papers, often requiring careful reading of plotted trends and experimental comparisons. Many questions are designed to penalize shallow caption-based guessing and reward figure-grounded inference.","Visual Perception, Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Logical Reasoning, Working Memory, Semantic Understanding & Context Recognition"
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU extends multimodal understanding to video, requiring models to answer questions grounded in temporal sequences, actions, and state changes. It probes whether an agent can integrate information across frames and maintain coherent context over time.","Visual Perception, Scene Understanding & Visual Reasoning, Attention, Working Memory, Cognitive Timing & Predictive Modeling, Multisensory Integration, Language Comprehension"
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"The FACTS Benchmark Suite evaluates factuality and grounding, measuring how reliably models produce statements supported by evidence and how often they hallucinate or confabulate details. It typically combines tasks that test citation-like attribution, conflict handling, and robustness to misleading contexts.","Semantic Understanding & Context Recognition, Language Comprehension, Language Production, Inhibitory Control, Working Memory, Self-reflection"
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA evaluates commonsense physical reasoning across many languages using non-parallel variants, reducing the chance that translation artifacts dominate results. Items focus on selecting plausible actions or outcomes in everyday physical situations, testing grounded commonsense rather than specialist knowledge.","Language Comprehension, Logical Reasoning, Semantic Understanding & Context Recognition, Spatial Representation & Mapping, Attention"
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 is a long-context, multi-round co-reference/recall evaluation where multiple similar “needle” interactions are embedded within long “haystacks,” and the model must reproduce the correct response for a specified needle. The 8-needle setting increases interference and tests robust retrieval under high distractor density.","Working Memory, Attention, Episodic Memory, Language Comprehension, Semantic Understanding & Context Recognition"
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval measures performance on well-specified professional knowledge-work tasks spanning many occupations, judged by experts via head-to-head comparisons against human outputs. Tasks often require producing structured artifacts (e.g., plans, presentations, spreadsheets) that satisfy constraints and stakeholder needs.","Planning, Decision-making, Language Production, Semantic Understanding & Context Recognition, Working Memory, Social Reasoning & Theory of Mind, Self-reflection"
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks evaluates algorithmic reasoning over graph-structured data presented in text, such as following edges, computing paths, or answering node-relation queries at scale. It is commonly used to test systematic traversal behavior and long-context consistency when many nodes and edges are in play.","Spatial Representation & Mapping, Working Memory, Logical Reasoning, Attention, Planning"
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon evaluates general tool-use competence across diverse tools and tasks, emphasizing correct tool selection, parameterization, multi-step orchestration, and robust handling of tool errors. It aims to measure agentic execution quality beyond single-call function invocation.","Planning, Decision-making, Working Memory, Adaptive Error Correction, Inhibitory Control, Semantic Understanding & Context Recognition"
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath is an expert-level mathematics benchmark designed to be difficult and more resistant to memorization, with tiered problem difficulty and an emphasis on rigorous reasoning. High scores require multi-step derivations, precise symbolic manipulation, and strong verification habits (often aided by computation tools).","Logical Reasoning, Working Memory, Planning, Cognitive Flexibility, Adaptive Error Correction, Attention"
