Benchmark,Website,Paper,Description,Cognitive Functions
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates software-engineering agents on real GitHub issues, where the model must produce a patch that makes a project’s tests pass. The Verified subset emphasizes tasks that have been human-checked for solvability and evaluation reliability, reducing noise from ambiguous or flaky issues.","Language Comprehension, Logical Reasoning, Planning, Working Memory, Adaptive Error Correction"
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 tests agents in command-line environments on multi-step, real-world style tasks (e.g., using shell tools, editing files, running programs, and interpreting outputs). Success requires choosing actions based on tool feedback and recovering from errors under resource and time constraints.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Language Comprehension"
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp measures deep web-browsing competence on questions that require searching, reading, and synthesizing information from multiple sources. Agent performance depends on decomposing the query, selecting which pages to open, and integrating evidence while avoiding distractors and stale context.","Planning, Decision-making, Semantic Understanding & Context Recognition, Working Memory, Language Comprehension, Attention"
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench evaluates customer-support agents that must interact with simulated users and APIs while adhering to domain policies (e.g., retail/airline/telecom). It stresses multi-turn dialogue consistency, correct tool use, and policy compliance under adversarial or emotionally charged user requests.","Language Comprehension, Language Production, Social Reasoning & Theory of Mind, Decision-making, Planning, Inhibitory Control"
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld benchmarks computer-use agents that complete tasks in realistic desktop operating systems via screenshots and action APIs. Tasks require understanding UI state, navigating multi-step workflows, and executing precise interactions (clicking, typing, switching windows) with feedback-driven correction.","Visual Perception, Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Sensorimotor Coordination, Planning, Adaptive Error Correction"
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,ARC-AGI is a few-shot abstract reasoning benchmark where models infer transformation rules from small sets of input–output grid examples and apply them to new inputs. It targets fluid reasoning and generalization to novel patterns rather than memorized domain knowledge.,"Logical Reasoning, Working Memory, Cognitive Flexibility, Spatial Representation & Mapping, Attention"
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon agent coherence in a simulated business setting where an agent runs a vending machine operation over an extended timeline. The agent must plan inventory and pricing decisions, interact with simulated counterparts, and adapt strategy based on outcomes.","Planning, Decision-making, Working Memory, Reward Mechanisms, Adaptive Error Correction"
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas measures real-world tool use through the Model Context Protocol by requiring multi-step workflows across multiple tools/services. It emphasizes selecting appropriate tools, issuing correct calls, handling tool errors, and composing tool outputs into a final answer.","Planning, Decision-making, Language Comprehension, Working Memory, Adaptive Error Correction, Semantic Understanding & Context Recognition"
FinanceAgent,https://www.vals.ai/benchmarks/finance_agent,,"FinanceAgent assesses whether a model can complete tasks typical of an entry-level financial analyst, often involving multi-document reading, quantitative reasoning, and structured deliverables. It probes the ability to follow finance conventions, maintain assumptions consistently, and produce actionable analyses.","Semantic Understanding & Context Recognition, Logical Reasoning, Planning, Decision-making, Working Memory, Language Production"
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity agent capabilities on tasks involving finding known vulnerabilities from descriptions and discovering new issues in real open-source codebases. It stresses code comprehension, hypothesis-driven investigation, and precise patch or exploit reasoning under realistic constraints.","Logical Reasoning, Planning, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition"
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"SpreadsheetBench tests agents on complex spreadsheet tasks derived from real workflows, such as editing formulas, cleaning data, building models, and validating outputs. It requires understanding spreadsheet structure, manipulating files via tools, and maintaining consistency across interconnected cells.","Logical Reasoning, Working Memory, Planning, Adaptive Error Correction, Semantic Understanding & Context Recognition"
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a difficult, often multimodal benchmark spanning frontier academic and professional questions that demand synthesis rather than recall. Tasks commonly require combining domain knowledge with careful reasoning, and when tools are enabled, integrating tool outputs into a coherent response.","Language Comprehension, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Visual Perception, Planning"
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 evaluates competition-style mathematics problem solving, typically requiring multi-step symbolic reasoning and careful algebraic manipulation. The benchmark emphasizes correctness under tight problem specifications and punishes small logical or arithmetic slips.","Logical Reasoning, Working Memory, Planning, Adaptive Error Correction"
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a subset of high-quality, graduate-level science multiple-choice questions designed to be resistant to superficial search or memorization. It tests deep conceptual understanding and multi-step scientific reasoning across physics, chemistry, and biology.","Language Comprehension, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory"
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends broad academic knowledge testing across many subjects into multiple languages, probing both knowledge and reasoning under multilingual prompts. It measures whether models can maintain performance when linguistic surface form changes while the underlying concepts remain the same.","Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory"
MMMU,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2311.16502,MMMU is a multi-discipline multimodal benchmark where models answer expert-level questions using both images and text. It stresses interpreting diagrams/charts and combining visual evidence with textual reasoning across diverse domains.,"Multisensory Integration, Visual Perception, Scene Understanding & Visual Reasoning, Language Comprehension, Logical Reasoning, Working Memory"
LAB-Bench FigQA,https://huggingface.co/datasets/futurehouse/lab-bench,https://arxiv.org/abs/2407.10362,LAB-Bench FigQA evaluates the ability to interpret scientific figures from biology papers and answer targeted questions about them. It focuses on extracting quantitative/illustrative information from complex plots and linking visual evidence to scientific claims in text.,"Visual Perception, Scene Understanding & Visual Reasoning, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Attention"
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,CharXiv Reasoning evaluates scientific figure and document understanding by requiring models to answer reasoning questions grounded in content from research papers. It probes whether a model can connect visual elements (plots/tables/diagrams) with accompanying text and perform multi-step inference.,"Language Comprehension, Visual Perception, Scene Understanding & Visual Reasoning, Logical Reasoning, Working Memory"
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU benchmarks multimodal understanding and reasoning over videos, requiring models to track events, entities, and temporal relationships. It stresses integrating information across frames and maintaining coherent interpretations over longer sequences.","Visual Perception, Scene Understanding & Visual Reasoning, Working Memory, Attention, Cognitive Timing & Predictive Modeling, Multisensory Integration"
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"FACTS Benchmark Suite systematically evaluates factuality and grounding across multiple tasks that probe whether model outputs are supported by provided sources or reliable knowledge. It emphasizes resisting hallucination, maintaining consistency, and appropriately abstaining or correcting when uncertain.","Semantic Understanding & Context Recognition, Language Comprehension, Inhibitory Control, Adaptive Error Correction, Working Memory"
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA measures practical commonsense and physical reasoning across many languages, aiming to separate real-world understanding from English-centric artifacts. Items typically require choosing the most plausible action or outcome in everyday scenarios under multilingual phrasing.","Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning, Cognitive Flexibility"
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) tests long-context retrieval and multi-round coreference by embedding multiple similar “needle” requests inside a long “haystack” and asking for the response corresponding to a specific needle. It stresses precise recall, disambiguation among near-duplicates, and robustness as context length scales.","Working Memory, Attention, Episodic Memory, Semantic Understanding & Context Recognition"
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval evaluates economically valuable, well-specified professional tasks (e.g., creating spreadsheets, presentations, schedules, or analyses) judged against human professional outputs. It emphasizes producing correct, usable artifacts and following constraints, often across multi-step workflows.","Planning, Decision-making, Language Production, Semantic Understanding & Context Recognition, Working Memory"
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks evaluates reasoning over graph-structured data, commonly requiring multi-step traversal, parent/neighbor retrieval, and following specified walk procedures. It stresses maintaining state across steps and accurately executing algorithmic instructions over long sequences.","Spatial Representation & Mapping, Logical Reasoning, Working Memory, Attention, Planning"
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon evaluates general tool-using agents across diverse, multi-step tasks that require selecting among tools, calling them correctly, and stitching results into a final solution. It highlights reliability under tool errors, schema mismatches, and the need for iterative refinement.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Language Comprehension"
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath measures expert-level mathematics performance on problems intended to be difficult and less susceptible to training contamination. Many tasks require sustained multi-step derivations or proof-like reasoning, often benefiting from careful verification and backtracking.","Logical Reasoning, Working Memory, Planning, Adaptive Error Correction"
