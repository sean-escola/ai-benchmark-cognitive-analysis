Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates software engineering agents by giving them real GitHub issues and a code repository, requiring a patch that makes the project’s tests pass. The “Verified” subset uses human-validated tasks that are confirmed solvable and reduces noise from ambiguous or broken evaluations.","L1: Language Comprehension, Language Production
L2: Planning, Logical Reasoning, Working Memory, Adaptive Error Correction
L3: ",L2
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 measures an agent’s ability to solve practical, real-world tasks in a command-line (terminal) environment, typically by running commands, inspecting files, and iterating until goals are met. It stresses interactive problem solving under tool constraints and frequent feedback from program outputs/errors.","L1: Language Comprehension
L2: Planning, Decision-making, Working Memory, Adaptive Error Correction
L3: ",L2
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp evaluates deep-research agents on questions that require searching, reading, and synthesizing information from a constrained web/document collection. Successful solutions depend on finding relevant sources, integrating evidence across documents, and producing a concise grounded answer.","L1: Language Comprehension, Language Production
L2: Planning, Attention, Working Memory, Semantic Understanding & Context Recognition
L3: Self-reflection",L3
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench tests whether an agent can complete multi-turn customer-support tasks while interacting with simulated users and API tools under domain policies (e.g., retail, airline, telecom). It emphasizes robust tool use, policy compliance, and consistent dialogue over extended interactions.","L1: Language Comprehension, Language Production
L2: Decision-making, Planning, Working Memory
L3: Social Reasoning & Theory of Mind, Inhibitory Control",L3
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld evaluates multimodal “computer use” agents that must operate within a graphical desktop environment to complete tasks across applications. Agents perceive screenshots/GUI state, plan sequences of actions (click/type/navigation), and adapt to changing UI feedback.","L1: Visual Perception
L2: Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Planning, Decision-making, Sensorimotor Coordination, Working Memory, Adaptive Error Correction
L3: ",L2
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,ARC-AGI is a fluid-intelligence benchmark of abstract grid transformation tasks where models infer a hidden rule from a few input–output examples and apply it to a new input. It is designed to prioritize generalization to novel patterns rather than memorized knowledge.,"L1: 
L2: Logical Reasoning, Spatial Representation & Mapping, Working Memory, Attention
L3: Cognitive Flexibility",L3
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon agent performance in a simulated vending-machine business over many decision steps (e.g., pricing, inventory, supplier negotiation). The score reflects sustained coherence, strategy, and adaptation over extended time horizons.","L1: 
L2: Planning, Decision-making, Working Memory, Reward Mechanisms, Adaptive Error Correction
L3: Self-reflection, Motivational Drives",L3
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas measures real-world tool use via the Model Context Protocol, requiring models to discover tools, call them correctly, manage multi-step workflows, and handle errors/retries. It focuses on whether an agent can reliably execute end-to-end tasks over multiple tool calls.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Working Memory, Adaptive Error Correction
L3: ",L2
FinanceAgent,https://www.vals.ai/benchmarks/finance_agent,,"FinanceAgent assesses performance on tasks typical of an entry-level financial analyst, such as analysis, reporting, and workflow-style problem solving over financial artifacts. It emphasizes correct reasoning under domain constraints, clear communication, and robust handling of multi-step requirements.","L1: Language Production
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Planning, Decision-making, Working Memory
L3: ",L2
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity agents on tasks such as identifying known vulnerabilities from descriptions and discovering vulnerabilities in real open-source projects. It stresses technical reasoning, iterative testing/verification, and precise execution of investigative steps.","L1: 
L2: Planning, Logical Reasoning, Working Memory, Adaptive Error Correction, Decision-making
L3: Inhibitory Control",L3
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"SpreadsheetBench evaluates an agent’s ability to understand, navigate, and manipulate complex spreadsheets (often via programmatic or tool-assisted operations). Tasks require extracting structure, applying transformations, and producing correct computed outputs across many dependent cells.","L1: 
L2: Logical Reasoning, Working Memory, Planning, Adaptive Error Correction, Attention, Semantic Understanding & Context Recognition
L3: ",L2
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a multimodal benchmark aimed at frontier difficulty, spanning challenging questions across domains and often requiring synthesis rather than recall. It tests broad knowledge, reasoning, and—in tool-enabled settings—verification and iterative refinement.","L1: Language Comprehension, Language Production, Visual Perception
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Scene Understanding & Visual Reasoning
L3: Self-reflection",L3
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 uses problems from the American Invitational Mathematics Examination to evaluate competition-style mathematical problem solving. It rewards correct multi-step reasoning, algebraic manipulation, and careful constraint tracking under time-like pressure.","L1: 
L2: Logical Reasoning, Working Memory, Attention, Planning
L3: ",L2
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,GPQA Diamond is a high-quality subset of GPQA consisting of very difficult graduate-level science multiple-choice questions designed to be “Google-proof.” It targets deep scientific reasoning and disambiguation rather than surface-level recall.,"L1: Language Comprehension
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Attention
L3: ",L2
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends the MMLU-style multi-subject test to multiple languages, evaluating academic knowledge and reasoning across diverse linguistic contexts. It probes whether competence transfers across languages and scripts while preserving accuracy and instruction following.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory
L3: ",L2
MMMU,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2311.16502,"MMMU is a multimodal benchmark spanning many disciplines where questions require integrating text with images (charts, diagrams, photos) to answer. It emphasizes visual understanding plus domain reasoning under varied formats and question types.","L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Working Memory
L3: ",L2
LAB-Bench FigQA,https://huggingface.co/datasets/futurehouse/lab-bench,https://arxiv.org/abs/2407.10362,LAB-Bench FigQA evaluates whether models can correctly interpret complex scientific figures from biology papers and answer questions grounded in those visuals. It stresses extracting quantitative/structural information from plots and diagrams and linking it to scientific context.,"L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Logical Reasoning, Working Memory, Attention, Semantic Understanding & Context Recognition
L3: ",L2
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning tests reasoning over scientific figures and charts (drawn from arXiv-style papers), often requiring careful reading of axes/legends and multi-step inference. In tool-enabled variants, models may also use computation to support quantitative conclusions.","L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Logical Reasoning, Working Memory, Attention
L3: ",L2
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU evaluates multimodal understanding and reasoning over video, where answers depend on events unfolding across time rather than a single frame. It stresses temporal integration, maintaining relevant details, and grounding language in dynamic visual evidence.","L1: Visual Perception, Language Comprehension
L2: Visual Attention & Eye Movements, Working Memory, Scene Understanding & Visual Reasoning
L3: Cognitive Timing & Predictive Modeling",L3
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"The FACTS Benchmark Suite systematically evaluates factuality, including whether model outputs are supported by provided context and whether they avoid unsupported claims. It targets reliable attribution, uncertainty handling, and minimizing hallucinations across diverse settings.","L1: Language Comprehension, Language Production
L2: Semantic Understanding & Context Recognition, Attention
L3: Inhibitory Control, Self-reflection",L3
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,Global PIQA is a multilingual physical commonsense reasoning benchmark that extends PIQA-style questions across languages (non-parallel). It probes whether models can choose plausible physical interactions/solutions while maintaining robustness to linguistic variation.,"L1: Language Comprehension
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory
L3: ",L2
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,MRCR v2 (8-needle) measures long-context multi-round coreference resolution by embedding multiple similar “needle” requests across long “haystack” conversations and asking for the response corresponding to a specific needle. It stresses precise retrieval and interference resistance under heavy context.,"L1: Language Comprehension
L2: Working Memory, Attention, Episodic Memory
L3: Inhibitory Control",L3
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval evaluates well-specified professional knowledge-work tasks across many occupations, scored by expert human judges via head-to-head comparisons with professional outputs. Tasks often require producing realistic artifacts (e.g., plans, analyses, presentations/spreadsheets) with strong adherence to constraints.","L1: Language Production
L2: Planning, Decision-making, Semantic Understanding & Context Recognition, Working Memory, Adaptive Error Correction
L3: Self-reflection",L3
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks evaluates whether models can reliably execute graph traversal and relational reasoning tasks (e.g., BFS-like navigation) described in text, often with many entities and steps. It is designed to expose failures in maintaining state and following multi-step structured constraints.","L1: 
L2: Spatial Representation & Mapping, Working Memory, Logical Reasoning, Attention, Planning
L3: ",L2
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon benchmarks agentic tool use across diverse APIs and multi-step workflows, focusing on selecting the right tools, calling them correctly, and recovering from tool errors. It emphasizes robust orchestration, iteration, and synthesis into a final user-facing result.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Working Memory, Adaptive Error Correction
L3: ",L2
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath is a benchmark of expert-level mathematics problems intended to be substantially beyond routine contest sets, emphasizing hard reasoning and proof-like multi-step derivations. Tool-enabled variants additionally test whether models can use computation appropriately without losing mathematical rigor.","L1: 
L2: Logical Reasoning, Working Memory, Planning, Attention, Adaptive Error Correction
L3: ",L2
