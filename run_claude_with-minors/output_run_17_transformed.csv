Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates software engineering agents on real GitHub issues by asking them to produce a patch that makes the project’s tests pass. The “Verified” subset consists of tasks validated by human reviewers to be solvable and to have reliable evaluation, emphasizing end-to-end debugging and code change accuracy.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Adaptive Error Correction, Logical Reasoning, Working Memory, Semantic Understanding & Context Recognition
L3: ",L2
SWE-bench Multilingual,https://www.swebench.com/multilingual.html,https://arxiv.org/abs/2504.21798,"SWE-bench Multilingual extends SWE-bench-style issue resolution beyond Python to multiple programming languages and ecosystems. It tests whether an agent can generalize debugging, code reading, and patch creation skills across varied syntax, tooling, and project conventions while maintaining correctness.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Adaptive Error Correction, Logical Reasoning, Working Memory, Semantic Understanding & Context Recognition
L3: Cognitive Flexibility",L3
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro is a larger, more challenging software engineering benchmark designed to better reflect industrial complexity and reduce easy shortcuts. Tasks typically require deeper repository understanding, multi-file changes, and robust fixes under stricter evaluation, stressing reliable agentic coding performance.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Adaptive Error Correction, Logical Reasoning, Working Memory, Semantic Understanding & Context Recognition
L3: Cognitive Flexibility",L3
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 measures an agent’s ability to solve real-world tasks in a command-line environment, such as installing tools, manipulating files, running scripts, and diagnosing failures. Success depends on iterative experimentation, interpreting tool output, and executing correct sequences of shell actions.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory, Attention, Logical Reasoning, Semantic Understanding & Context Recognition
L3: ",L2
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp evaluates “deep research” style web-browsing agents on questions that require searching, reading, and synthesizing information from multiple sources. It stresses reliable retrieval, source triangulation, and maintaining a coherent intermediate knowledge state while navigating noisy or conflicting evidence.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Attention, Episodic Memory, Working Memory, Semantic Understanding & Context Recognition, Logical Reasoning
L3: ",L2
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench evaluates interactive tool-using agents in customer-support-like simulations (e.g., retail, airline, telecom) where the agent must use APIs and follow domain policies. It probes long multi-turn consistency, policy adherence under pressure, and choosing actions that satisfy user goals while respecting constraints.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Working Memory, Attention, Semantic Understanding & Context Recognition
L3: Inhibitory Control, Social Reasoning & Theory of Mind, Empathy",L3
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld benchmarks multimodal computer-use agents on realistic operating-system tasks (e.g., browsing, editing, configuration) using screenshots and interactive actions. It tests whether an agent can perceive UI state, plan multi-step workflows, and recover from mistakes in a dynamic desktop environment.","L1: Visual Perception
L2: Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Sensorimotor Coordination, Planning, Decision-making, Adaptive Error Correction, Working Memory, Attention
L3: ",L2
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI is a few-shot abstract reasoning benchmark where models infer the rule mapping input grids to output grids from a small set of examples. It emphasizes discovering latent structure, composing transformations, and generalizing to novel tasks with minimal prior task-specific training.","L1: 
L2: Logical Reasoning, Working Memory, Spatial Representation & Mapping, Scene Understanding & Visual Reasoning, Planning, Attention
L3: Cognitive Flexibility",L3
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon agent coherence and strategy by having a model run a simulated vending-machine business over extended time (e.g., months). The agent must manage inventory, pricing, supplier interactions, and budgets, adapting its policy based on outcomes and constraints.","L1: 
L2: Planning, Decision-making, Reward Mechanisms, Episodic Memory, Working Memory, Adaptive Error Correction
L3: Cognitive Timing & Predictive Modeling, Self-reflection, Motivational Drives",L3
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas measures real-world tool-use performance via the Model Context Protocol (MCP), requiring agents to discover tools, call them correctly, handle errors, and integrate results across multi-step workflows. It focuses on robust orchestration across heterogeneous APIs and realistic failure modes.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory, Attention, Semantic Understanding & Context Recognition, Logical Reasoning
L3: ",L2
FinanceAgent,https://www.vals.ai/benchmarks/finance_agent,,"FinanceAgent evaluates agent performance on tasks typical of an entry-level financial analyst, such as extracting relevant details from documents, performing quantitative analyses, and producing structured outputs. It stresses domain-grounded reasoning, correctness under constraints, and reliable multi-step workflow execution.","L1: Language Comprehension, Language Production
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Planning, Decision-making, Working Memory, Attention
L3: ",L2
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity capabilities by testing agents on vulnerability identification, exploitation reasoning, and secure-fix understanding across real software projects at scale. It emphasizes interpreting technical artifacts, forming hypotheses about weaknesses, and iterating based on tool feedback and partial evidence.","L1: Language Comprehension, Language Production
L2: Logical Reasoning, Planning, Decision-making, Adaptive Error Correction, Working Memory, Attention, Semantic Understanding & Context Recognition
L3: ",L2
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"SpreadsheetBench measures how well agents can read, edit, and compute within complex spreadsheets, often requiring multi-step transformations and formula reasoning. It tests precision on structured data manipulation, error recovery, and producing correct, well-formed spreadsheet artifacts.","L1: 
L2: Working Memory, Attention, Planning, Decision-making, Adaptive Error Correction, Logical Reasoning, Semantic Understanding & Context Recognition, Scene Understanding & Visual Reasoning
L3: ",L2
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a difficult, broad benchmark intended to probe frontier-level knowledge and reasoning across many domains, often including multimodal questions. It stresses synthesis, careful constraint-following, and handling questions that require integrating information rather than recalling a single fact.","L1: Language Comprehension, Language Production
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Attention, Scene Understanding & Visual Reasoning, Multisensory Integration
L3: ",L2
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 evaluates competition-style mathematics problem solving on the 2025 AIME set (often via an evaluation harness). It emphasizes multi-step derivations, algebraic and combinatorial reasoning, and maintaining correctness across long solution chains under time/attempt constraints.","L1: 
L2: Logical Reasoning, Working Memory, Planning, Adaptive Error Correction, Attention
L3: ",L2
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a high-difficulty multiple-choice science QA benchmark designed to be “Google-proof,” with the Diamond split representing the most reliable and challenging items. It tests deep conceptual understanding and careful elimination of distractors across physics, chemistry, and biology.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Attention, Decision-making
L3: ",L2
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends broad academic knowledge testing to multiple non-English languages across many subjects. It evaluates whether models can reason and answer questions accurately across languages, measuring cross-lingual generalization beyond simple translation.","L1: Language Comprehension, Language Production
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Attention
L3: Cognitive Flexibility",L3
MMMU,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2311.16502,"MMMU is a multimodal benchmark spanning many disciplines where solving questions requires combining textual information with images (e.g., diagrams, charts, screenshots). It probes multimodal grounding, visual reasoning, and integrating evidence across modalities to answer expert-level questions.","L1: Visual Perception, Language Comprehension, Language Production
L2: Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Working Memory, Attention
L3: ",L2
LAB-Bench FigQA,https://huggingface.co/datasets/futurehouse/lab-bench,https://arxiv.org/abs/2407.10362,"LAB-Bench FigQA evaluates scientific figure question answering, requiring models to interpret complex figures from biology papers and connect them to the question context. It focuses on extracting relevant visual evidence (plots, labels, multi-panel layouts) and performing domain-grounded reasoning from figures.","L1: Visual Perception, Language Comprehension, Language Production
L2: Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Multisensory Integration, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Attention
L3: ",L2
WebArena,https://webarena.dev/,https://arxiv.org/abs/2307.13854,"WebArena evaluates autonomous web agents on realistic tasks across multiple interactive sites (e-commerce, forums, code hosting, etc.), requiring multi-step navigation and form interactions. It tests end-to-end web action policies, including interpreting UI state, planning, and recovering from errors under partial observability.","L1: Visual Perception
L2: Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Sensorimotor Coordination, Planning, Decision-making, Adaptive Error Correction, Working Memory, Attention, Episodic Memory
L3: ",L2
