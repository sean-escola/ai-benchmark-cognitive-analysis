Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates software engineering agents on real GitHub issues where the task is to produce a patch that passes project tests. The “Verified” subset uses problems that have been filtered/confirmed to be solvable and reduces noise from ambiguous tasks, emphasizing end-to-end bug fixing and implementation reliability.","L1: Language Comprehension
L2: Planning, Logical Reasoning, Working Memory, Adaptive Error Correction, Decision-making
L3: ",L2
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 tests agentic coding and systems skills in a command-line environment, requiring models to run commands, inspect files, install dependencies, and iteratively debug under realistic constraints. Tasks reward robust exploration, error recovery, and correct tool-mediated execution rather than single-shot code generation.","L1: Language Comprehension
L2: Planning, Adaptive Error Correction, Decision-making, Working Memory, Logical Reasoning
L3: ",L2
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp measures a model’s ability to answer difficult questions by conducting web research: issuing searches, opening sources, extracting evidence, and synthesizing a final answer. It is designed to test deep retrieval, source triage, and multi-step information integration under tool-use constraints.","L1: Language Comprehension
L2: Attention, Planning, Working Memory, Semantic Understanding & Context Recognition, Decision-making
L3: ",L2
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench evaluates interactive customer-support style agents that must follow domain policies while using APIs and conversing with a simulated user across multiple turns. The benchmark emphasizes consistent policy adherence, correct tool/API usage, and maintaining task state over long dialogues.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Working Memory
L3: Social Reasoning & Theory of Mind, Inhibitory Control",L3
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld evaluates multimodal computer-use agents on completing tasks in a full operating-system desktop environment (e.g., browsing, file management, app workflows). Success requires interpreting screenshots/GUI state, selecting actions (click/type), and recovering from UI errors or unexpected states over many steps.","L1: Visual Perception
L2: Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Planning, Decision-making, Sensorimotor Coordination, Working Memory, Adaptive Error Correction
L3: ",L2
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,ARC-AGI presents few-shot grid transformation puzzles where a model must infer hidden rules from a small set of input–output examples and apply them to a new input. It targets generalization to novel patterns and compositional rule induction rather than memorized knowledge.,"L1: 
L2: Logical Reasoning, Working Memory, Scene Understanding & Visual Reasoning, Spatial Representation & Mapping, Planning
L3: Cognitive Flexibility",L3
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 measures long-horizon autonomy by having an agent run a simulated vending machine business over an extended time period, including inventory, pricing, supplier interaction, and adaptation to market dynamics. It stresses sustained coherence and strategic decision-making across thousands of sequential actions.","L1: 
L2: Planning, Decision-making, Working Memory, Reward Mechanisms, Adaptive Error Correction, Semantic Understanding & Context Recognition
L3: ",L2
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas evaluates real-world tool use through the Model Context Protocol, where agents must discover, call, and chain tools across multi-step workflows using production-like servers. It focuses on correct API selection, argument formatting, error handling, and synthesis of tool outputs into final responses.","L1: Language Comprehension
L2: Planning, Decision-making, Working Memory, Adaptive Error Correction, Semantic Understanding & Context Recognition
L3: ",L2
FinanceAgent,https://www.vals.ai/benchmarks/finance_agent,,"FinanceAgent assesses whether models can perform tasks typical of an entry-level financial analyst, such as extracting figures, reasoning about financial statements, and producing analysis-ready outputs. It emphasizes structured reasoning with domain constraints and careful handling of numerical and document context.","L1: Language Production
L2: Logical Reasoning, Decision-making, Working Memory, Semantic Understanding & Context Recognition, Planning
L3: ",L2
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity agents on tasks involving identifying known vulnerabilities from descriptions and, in some settings, finding previously unknown vulnerabilities in real software. The benchmark rewards systematic investigation, hypothesis testing, and safe iterative debugging of exploit-relevant behaviors.","L1: 
L2: Logical Reasoning, Planning, Adaptive Error Correction, Working Memory, Decision-making
L3: Inhibitory Control",L3
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"SpreadsheetBench tests end-to-end spreadsheet manipulation and reasoning, including reading complex workbooks, applying transformations, and generating correct computed outputs. The benchmark stresses structured, tool-mediated workflows and multi-step consistency across formulas, tables, and formatting constraints.","L1: Language Comprehension
L2: Working Memory, Planning, Logical Reasoning, Adaptive Error Correction, Decision-making
L3: ",L2
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a multi-domain, frontier-difficulty benchmark with questions spanning advanced knowledge and reasoning, often including multimodal inputs. It is designed to probe general problem solving, synthesis, and robustness on items that exceed typical academic benchmark difficulty.","L1: Language Comprehension
L2: Logical Reasoning, Working Memory, Semantic Understanding & Context Recognition, Scene Understanding & Visual Reasoning, Planning
L3: ",L2
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 evaluates competition-style mathematics problem solving under time-limited, trick-resistant question design. It emphasizes symbolic manipulation, multi-step derivations, and precise final answers (often integer-valued) without relying on external tools.","L1: 
L2: Logical Reasoning, Working Memory, Planning
L3: Cognitive Flexibility",L3
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a high-difficulty multiple-choice science benchmark curated to be “Google-proof,” requiring deep reasoning rather than shallow lookup. The Diamond subset is a quality-filtered split intended to better separate expert-level scientific understanding from surface heuristics.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Decision-making
L3: ",L2
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends broad academic knowledge testing across many subjects into multiple languages, measuring whether models can generalize knowledge and reasoning beyond English. It stresses multilingual understanding while keeping question styles similar to standard MMLU.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Working Memory, Logical Reasoning
L3: ",L2
MMMU,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2311.16502,"MMMU evaluates expert-level multimodal understanding across many disciplines by combining images (diagrams, charts, figures) with text questions. It targets cross-modal grounding and reasoning, requiring models to extract visual evidence and integrate it with domain knowledge.","L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Working Memory
L3: ",L2
LAB-Bench FigQA,https://huggingface.co/datasets/futurehouse/lab-bench,https://arxiv.org/abs/2407.10362,"LAB-Bench FigQA measures whether models can correctly interpret complex scientific figures from biology papers and answer questions that depend on visual evidence and experimental context. It emphasizes faithful figure reading, cross-referencing captions/labels, and reasoning about scientific claims.","L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory
L3: ",L2
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning tests chart and figure understanding using scientific plots/figures (e.g., from arXiv-style documents), requiring models to answer questions grounded in visualized data. It stresses quantitative and relational reasoning over axes, legends, and trends rather than generic image captioning.","L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Logical Reasoning, Working Memory
L3: ",L2
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU extends multimodal understanding to the temporal domain, asking questions that require integrating information across video frames and associated text. It targets temporal consistency, event understanding, and multi-step reasoning over dynamic visual scenes.","L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Working Memory, Attention, Multisensory Integration
L3: Cognitive Timing & Predictive Modeling",L3
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"FACTS Benchmark Suite systematically evaluates factuality in large language models across diverse settings, including attribution/grounding, resistance to hallucination, and consistency under prompting. It aims to distinguish models that are merely fluent from those that reliably track truth and evidence.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Adaptive Error Correction, Working Memory
L3: Inhibitory Control",L3
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA is a multilingual, non-parallel extension of physical interaction question answering, testing common-sense reasoning about everyday actions and object affordances across many languages and cultures. It probes whether models preserve grounded physical intuition under linguistic variation.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Spatial Representation & Mapping
L3: ",L2
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,MRCR v2 (8-needle) evaluates long-context multi-round coreference resolution by embedding multiple similar “needle” requests across a long “haystack” of distractors and asking the model to reproduce the correct referenced content. The 8-needle setting stresses precise retrieval and disambiguation under heavy interference.,"L1: Language Comprehension
L2: Working Memory, Attention, Semantic Understanding & Context Recognition
L3: Inhibitory Control",L3
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval evaluates economically relevant knowledge work by having models produce real professional artifacts (e.g., slides, spreadsheets, plans) across many occupations and scoring them via expert human comparisons. It focuses on end-to-end task execution quality, including structure, correctness, and usability of deliverables.","L1: Language Production
L2: Planning, Decision-making, Semantic Understanding & Context Recognition, Working Memory
L3: Social Reasoning & Theory of Mind",L3
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks tests reasoning over graph-structured data, such as following paths, retrieving parents/ancestors, or performing constrained traversals from textual descriptions. It emphasizes multi-step state tracking and systematic traversal rather than free-form association.","L1: 
L2: Spatial Representation & Mapping, Working Memory, Planning, Logical Reasoning, Attention
L3: ",L2
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon evaluates general tool-using agents across diverse APIs and tasks, emphasizing selecting appropriate tools, composing multi-step tool chains, and handling tool failures. It is designed to measure robust agentic execution rather than isolated function calling.","L1: Language Comprehension
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory
L3: ",L2
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath measures advanced mathematical problem solving at research-adjacent difficulty, with tiers intended to separate incremental capability gains at the frontier. It emphasizes rigorous multi-step derivations, abstraction, and error-sensitive reasoning that often defeats pattern-matching approaches.","L1: 
L2: Logical Reasoning, Working Memory, Planning, Adaptive Error Correction
L3: Cognitive Flexibility",L3
