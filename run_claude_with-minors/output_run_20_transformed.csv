Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates an AI model’s ability to resolve real-world GitHub software issues by generating patches that pass the project’s tests, with a curated set of tasks verified by humans to be solvable. It targets end-to-end bug fixing and feature implementation in Python repositories under a standardized, reliability-focused evaluation setup.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory, Logical Reasoning, Semantic Understanding & Context Recognition
L3: ",L2
SWE-bench Multilingual,https://www.swebench.com/multilingual.html,https://arxiv.org/abs/2504.21798,"SWE-bench Multilingual extends the SWE-bench paradigm beyond Python to multiple programming languages, assessing whether models can diagnose and patch issues across diverse ecosystems and toolchains. It emphasizes transfer of debugging and code-editing competence across languages and project conventions.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory, Logical Reasoning, Semantic Understanding & Context Recognition
L3: Cognitive Flexibility",L3
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro is a larger and more challenging software engineering benchmark designed to be more realistic and more resistant to contamination, spanning multiple languages and harder issue types. It evaluates whether models can produce correct patches under stricter, industrially relevant conditions and broader repository diversity.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory, Logical Reasoning, Semantic Understanding & Context Recognition
L3: Cognitive Flexibility",L3
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 evaluates agentic performance on real tasks executed in a command-line environment, where models must iteratively run commands, inspect outputs, and modify files to achieve a goal. It measures practical tool use, debugging, and execution-aware reasoning under environmental constraints and potential flakiness.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory, Logical Reasoning, Semantic Understanding & Context Recognition, Attention
L3: ",L2
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp evaluates deep-research agents on questions that require searching, reading, and synthesizing information from a fixed document collection to enable reproducible scoring. It probes multi-step retrieval, evidence integration across sources, and maintaining coherence while navigating long or noisy information.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Attention, Working Memory, Episodic Memory, Semantic Understanding & Context Recognition, Logical Reasoning
L3: ",L2
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench evaluates interactive customer-support-style agents operating over multi-turn dialogs with simulated users and APIs, while following domain policies (e.g., retail, airline, telecom). It measures robust tool calling, policy adherence under pressure, and consistent conversational behavior across long task horizons.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Working Memory, Attention, Adaptive Error Correction
L3: Inhibitory Control, Social Reasoning & Theory of Mind, Empathy",L3
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld measures computer-use agency in a realistic desktop environment, where models must interpret screenshots and execute actions to complete tasks across applications. It emphasizes visually grounded interaction, multi-step navigation, and recovery from mistakes in a dynamic GUI setting.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Planning, Decision-making, Sensorimotor Coordination, Working Memory, Adaptive Error Correction
L3: ",L2
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,ARC-AGI evaluates “fluid” reasoning by requiring models to infer abstract rules from a few grid-based input-output examples and apply them to new inputs. The tasks are designed to reward generalization and compositional pattern discovery rather than memorization or domain knowledge.,"L1: 
L2: Logical Reasoning, Working Memory, Attention, Spatial Representation & Mapping, Scene Understanding & Visual Reasoning
L3: Cognitive Flexibility",L3
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon autonomous decision-making in a simulated vending-machine business, where an agent must manage inventory, pricing, supplier negotiation, and cash flow over an extended timeline. It tests sustained coherence, strategy formation, and adaptation to changing market signals across many steps.","L1: 
L2: Planning, Decision-making, Reward Mechanisms, Working Memory, Episodic Memory, Adaptive Error Correction
L3: Motivational Drives",L3
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas evaluates real-world tool-use through the Model Context Protocol, requiring models to discover tools, call APIs correctly, manage multi-step workflows, and synthesize results. It emphasizes execution reliability, error handling, and orchestrating sequences of tool calls to complete tasks.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory, Attention, Logical Reasoning
L3: ",L2
FinanceAgent,https://www.vals.ai/benchmarks/finance_agent,,"FinanceAgent assesses performance on tasks typical of an entry-level financial analyst, including retrieving/transforming financial information, performing calculations, and producing analysis-ready outputs. It measures domain-grounded reasoning and the ability to structure multi-step analytical workflows into correct deliverables.","L1: Language Comprehension, Language Production
L2: Logical Reasoning, Planning, Decision-making, Working Memory, Semantic Understanding & Context Recognition, Adaptive Error Correction
L3: ",L2
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity capabilities by asking agents to identify known vulnerabilities from descriptions and to discover new vulnerabilities in real open-source projects at scale. It probes technical reasoning over codebases, hypothesis-driven exploration, and iterative verification of findings.","L1: 
L2: Logical Reasoning, Planning, Decision-making, Adaptive Error Correction, Working Memory, Attention, Semantic Understanding & Context Recognition
L3: ",L2
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"SpreadsheetBench evaluates spreadsheet-centric task completion using real-world-inspired spreadsheets, requiring agents to read, modify, compute, and validate structured data. It targets procedural competence in manipulating complex artifacts and maintaining correctness across interdependent cells and sheets.","L1: 
L2: Planning, Decision-making, Working Memory, Adaptive Error Correction, Logical Reasoning, Semantic Understanding & Context Recognition, Attention
L3: ",L2
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam (HLE) is a frontier, multi-domain benchmark with challenging questions that span advanced reasoning and expert knowledge, often in multimodal formats. It is designed to stress models’ ability to synthesize information, reason under uncertainty, and produce well-justified answers beyond routine exam-style QA.","L1: Language Comprehension, Language Production, Visual Perception
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Attention, Multisensory Integration, Scene Understanding & Visual Reasoning
L3: ",L2
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 consists of competition-style mathematics problems that require multi-step symbolic reasoning and careful constraint management, typically with short numeric final answers. It is used to measure mathematical problem solving under tight precision demands and limited opportunity for partial credit.","L1: 
L2: Logical Reasoning, Working Memory, Planning, Attention, Adaptive Error Correction
L3: ",L2
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a difficult multiple-choice science benchmark curated to be “Google-proof,” with questions requiring deep understanding and reasoning rather than surface recall. The Diamond subset focuses on high-quality items where experts succeed and non-experts often fail, stressing robust scientific reasoning.","L1: Language Comprehension
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Attention, Decision-making
L3: ",L2
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends broad academic knowledge evaluation across many subjects and multiple non-English languages, testing both content understanding and reasoning under multilingual variation. It is commonly used to assess cross-lingual generalization and robustness of instruction-following and QA capabilities.","L1: Language Comprehension, Language Production
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Attention
L3: Cognitive Flexibility",L3
MMMU,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2311.16502,"MMMU is a multimodal benchmark spanning many disciplines where models must answer questions that require jointly interpreting text and images (e.g., diagrams, charts, scientific visuals). It stresses visual grounding, cross-modal integration, and reasoning about specialized content in context.","L1: Visual Perception, Language Comprehension, Language Production
L2: Scene Understanding & Visual Reasoning, Multisensory Integration, Visual Attention & Eye Movements, Logical Reasoning, Working Memory, Attention
L3: ",L2
LAB-Bench FigQA,https://huggingface.co/datasets/futurehouse/lab-bench,https://arxiv.org/abs/2407.10362,"LAB-Bench FigQA evaluates whether models can accurately interpret complex scientific figures from biology papers and answer questions that depend on visual evidence and domain context. It targets figure literacy—extracting relationships, trends, and experimental implications from real research visuals.","L1: Visual Perception, Language Comprehension, Language Production
L2: Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Multisensory Integration, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Attention
L3: ",L2
WebArena,https://webarena.dev/,https://arxiv.org/abs/2307.13854,"WebArena evaluates autonomous web agents performing realistic multi-step tasks across interactive websites (e-commerce, CMS, code hosting, social platforms), requiring navigation, form filling, and goal tracking. It measures end-to-end planning and robust interaction with dynamic web UIs under partial observability.","L1: Visual Perception, Language Comprehension, Language Production
L2: Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Planning, Decision-making, Sensorimotor Coordination, Working Memory, Adaptive Error Correction, Attention
L3: ",L2
