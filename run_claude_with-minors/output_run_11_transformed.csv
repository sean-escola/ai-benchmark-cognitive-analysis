Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates software-engineering agents by asking them to generate patches that fix real issues in open-source Python repositories, with solutions validated by tests. The ""Verified"" subset focuses on instances that have been human-checked as solvable and well-specified, emphasizing reliable end-to-end bug fixing rather than speculative edits.","L1: Language Comprehension, Language Production
L2: Planning, Logical Reasoning, Working Memory, Adaptive Error Correction
L3: ",L2
SWE-bench Multilingual,https://www.swebench.com/multilingual.html,https://arxiv.org/abs/2504.21798,"SWE-bench Multilingual extends the SWE-bench patch-generation setting beyond Python to multiple programming languages and ecosystems. It measures whether a model can understand repository context, reason about failing behavior, and implement correct fixes under diverse language/tooling conventions.","L1: Language Comprehension, Language Production
L2: Planning, Logical Reasoning, Working Memory, Adaptive Error Correction
L3: ",L2
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro is a larger, more difficult software engineering benchmark intended to be more industrially representative and more resistant to contamination than earlier SWE-bench variants. It stresses multi-file changes, nuanced specifications, and robust patch quality judged by repository test suites and evaluation harnesses.","L1: Language Comprehension
L2: Planning, Logical Reasoning, Working Memory, Adaptive Error Correction
L3: Cognitive Flexibility",L3
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 evaluates agentic performance on real command-line tasks in sandboxed environments, where models must inspect files, run commands, and iteratively fix issues. It emphasizes multi-step execution, debugging under feedback from tools, and maintaining coherent state over a task episode.","L1: Language Comprehension
L2: Planning, Decision-making, Working Memory, Adaptive Error Correction
L3: ",L2
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp evaluates ""browse-and-answer"" capability: the agent must use a constrained web-like retrieval setting to locate evidence and produce correct answers to information-seeking questions. It tests search strategy, source triangulation, and faithful synthesis from retrieved documents rather than relying on parametric memory alone.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Attention, Working Memory, Semantic Understanding & Context Recognition
L3: ",L2
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench measures interactive tool-using agents in customer-support style domains (e.g., retail, airline, telecom) with policies and APIs, requiring multi-turn dialogue and correct procedure following. It probes whether an agent can balance user intent with constraints, maintain dialogue state, and execute correct tool calls end-to-end.","L1: Language Comprehension, Language Production
L2: Decision-making, Planning, Working Memory
L3: Social Reasoning & Theory of Mind, Inhibitory Control",L3
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld evaluates multimodal computer-use agents operating desktop-like environments, where tasks require navigating GUIs, reading on-screen content, and taking actions (click/type) to complete goals. It emphasizes perception-action loops with long-horizon planning and robustness to interface variation and execution errors.","L1: Visual Perception
L2: Visual Attention & Eye Movements, Sensorimotor Coordination, Planning, Decision-making, Adaptive Error Correction, Working Memory
L3: ",L2
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI tests generalization and fluid reasoning via small grid-based puzzles where a model must infer latent transformation rules from a few input-output examples. Success depends on abstract pattern discovery, compositional rule induction, and avoiding overfitting to superficial cues.","L1: 
L2: Logical Reasoning, Working Memory, Spatial Representation & Mapping, Scene Understanding & Visual Reasoning
L3: Cognitive Flexibility",L3
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon agent coherence and strategy in a simulated year-long vending-machine business, where the model must make many sequential decisions about inventory, pricing, suppliers, and operations. Performance is scored by economic outcomes, stressing sustained planning, adaptation, and consistent goal pursuit over thousands of steps.","L1: 
L2: Planning, Decision-making, Reward Mechanisms, Working Memory, Adaptive Error Correction
L3: Self-reflection",L3
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas evaluates real-world tool use through the Model Context Protocol (MCP), requiring models to discover tools, call APIs correctly, handle errors/retries, and compose multi-step workflows across servers. It targets practical reliability in orchestration and tool-mediated reasoning under realistic constraints.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory
L3: ",L2
FinanceAgent,https://www.vals.ai/benchmarks/finance_agent,,"FinanceAgent assesses agent performance on tasks typical of an entry-level financial analyst, such as analyzing financial documents, building or editing analyses, and producing structured outputs. It emphasizes domain-grounded reasoning, careful handling of constraints, and multi-step workflows that resemble professional knowledge work.","L1: Language Comprehension, Language Production
L2: Logical Reasoning, Planning, Decision-making, Working Memory, Semantic Understanding & Context Recognition
L3: ",L2
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity agent capability on real software vulnerability tasks, including finding known vulnerabilities from descriptions and discovering new ones in codebases. It stresses hypothesis-driven investigation, precise code reasoning, and iterative debugging/validation in realistic security workflows.","L1: Language Comprehension
L2: Planning, Logical Reasoning, Adaptive Error Correction, Working Memory, Decision-making
L3: ",L2
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"SpreadsheetBench evaluates an agent’s ability to navigate, manipulate, and compute within complex spreadsheets derived from realistic scenarios. It tests whether the model can maintain structured state, apply multi-step transformations, and verify correctness across formulae, formatting, and tabular constraints.","L1: 
L2: Working Memory, Planning, Adaptive Error Correction, Logical Reasoning, Semantic Understanding & Context Recognition, Decision-making
L3: ",L2
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a difficult multimodal benchmark designed to probe frontier academic and reasoning ability across many domains, often requiring deep synthesis rather than rote recall. It includes questions intended to be near the edge of expert human knowledge and tests careful, multi-step justification and interpretation of provided materials.","L1: Language Comprehension, Language Production, Visual Perception
L2: Logical Reasoning, Working Memory, Semantic Understanding & Context Recognition, Scene Understanding & Visual Reasoning
L3: ",L2
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 is a competition-math benchmark based on problems from the American Invitational Mathematics Examination, requiring exact numeric answers. It emphasizes multi-step symbolic reasoning, careful constraint tracking, and avoidance of arithmetic and logical slips.","L1: 
L2: Logical Reasoning, Working Memory, Planning, Adaptive Error Correction
L3: ",L2
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,GPQA Diamond is a curated subset of challenging graduate-level multiple-choice science questions designed to be difficult to answer via superficial pattern matching or simple web lookup. It probes deep scientific reasoning and precise comprehension of technical language under tight answer choices.,"L1: Language Comprehension
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory
L3: ",L2
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends broad academic knowledge evaluation to many non-English languages across a large set of subjects, using standardized question formats. It tests multilingual comprehension and reasoning while exposing gaps in cross-lingual transfer and domain-specific understanding.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory
L3: ",L2
MMMU,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2311.16502,"MMMU is a multidisciplinary multimodal benchmark where answers require integrating text with images such as diagrams, charts, and scientific figures. It tests visual understanding, cross-modal grounding, and complex reasoning in expert-like settings.","L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Working Memory
L3: ",L2
LAB-Bench FigQA,https://huggingface.co/datasets/futurehouse/lab-bench,https://arxiv.org/abs/2407.10362,"LAB-Bench FigQA evaluates whether models can correctly interpret complex biology figures from scientific papers and answer questions that depend on visual evidence. It targets scientific visual reasoning, cross-referencing figure elements with textual context, and drawing correct inferences from plotted/diagrammatic data.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Working Memory, Semantic Understanding & Context Recognition
L3: ",L2
WebArena,https://webarena.dev/,https://arxiv.org/abs/2307.13854,"WebArena evaluates autonomous web agents on realistic tasks across multiple interactive web applications (e-commerce, CMS, forums, code hosting, maps), requiring navigation, form filling, and multi-step actions. It emphasizes robust planning under partial observability, UI understanding from screenshots/DOM, and recovery from inevitable execution errors.","L1: Visual Perception, Language Comprehension, Language Production
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory, Visual Attention & Eye Movements, Sensorimotor Coordination
L3: ",L2
