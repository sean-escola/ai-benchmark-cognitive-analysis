Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates software engineering agents on real GitHub issues by requiring them to generate code patches that make the project’s tests pass. The Verified subset focuses on tasks that have been human-validated as solvable, aiming to reduce noise from ambiguous or underspecified issues.","L1: Language Comprehension
L2: Logical Reasoning, Planning, Working Memory, Adaptive Error Correction, Decision-making
L3: ",L2
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 measures an agent’s ability to complete real-world command-line tasks inside isolated environments, often requiring iterative diagnosis and execution of shell commands. It emphasizes end-to-end problem solving under tool constraints (files, processes, packages, permissions) rather than single-turn answers.","L1: Language Comprehension
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory, Attention
L3: ",L2
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,BrowseComp evaluates “deep research” agents that must search a fixed document collection and synthesize answers that require multi-step information gathering. It is designed to be more reproducible than open-web browsing by controlling the retrieval corpus and grading.,"L1: Language Production
L2: Planning, Decision-making, Attention, Working Memory, Semantic Understanding & Context Recognition
L3: ",L2
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench evaluates interactive tool-using agents in simulated customer-support domains (e.g., retail, airline, telecom) with policies and APIs. Success requires maintaining conversation state, following constraints, and choosing correct tool actions across multi-turn trajectories.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Working Memory
L3: Social Reasoning & Theory of Mind, Inhibitory Control",L3
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld evaluates multimodal computer-use agents that operate graphical desktop environments to complete tasks across applications and websites. Agents must perceive UI state from screenshots, plan sequences of interactions, and recover from mistakes over many steps.","L1: Visual Perception
L2: Visual Attention & Eye Movements, Sensorimotor Coordination, Planning, Decision-making, Adaptive Error Correction, Working Memory
L3: ",L2
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,ARC-AGI is a few-shot abstract reasoning benchmark where models infer hidden transformation rules from a small number of input-output grid examples. It emphasizes generalization to novel patterns and compositional rule discovery rather than memorized knowledge.,"L1: Visual Perception
L2: Logical Reasoning, Working Memory, Spatial Representation & Mapping, Attention
L3: Cognitive Flexibility",L3
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon autonomous agents managing a simulated vending machine business over an extended period. Agents must make repeated operational decisions (inventory, suppliers, pricing, messaging) and optimize a long-term objective under uncertainty.","L1: 
L2: Planning, Decision-making, Working Memory, Reward Mechanisms, Adaptive Error Correction
L3: Motivational Drives",L3
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas evaluates real-world tool use via the Model Context Protocol, requiring models to discover, call, and chain tools across multi-step workflows. Tasks reward robust execution, correct parameterization, and resilient recovery from tool errors.","L1: Language Comprehension
L2: Planning, Decision-making, Working Memory, Adaptive Error Correction
L3: ",L2
FinanceAgent,https://www.vals.ai/benchmarks/finance_agent,,"FinanceAgent measures performance on tasks representative of entry-level financial analyst work, such as analysis, modeling, and reporting using provided data and tools. It stresses correctness, structured outputs, and decision-relevant reasoning in business contexts.","L1: Language Production
L2: Logical Reasoning, Planning, Decision-making, Semantic Understanding & Context Recognition, Working Memory
L3: ",L2
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity agents on tasks involving vulnerability discovery and exploitation in real-world software contexts, including both known and previously-unknown weaknesses. It requires reading technical artifacts, forming hypotheses, and iterating with tooling to validate findings.","L1: 
L2: Logical Reasoning, Planning, Adaptive Error Correction, Working Memory, Attention
L3: ",L2
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"SpreadsheetBench evaluates agents on complex spreadsheet tasks grounded in realistic workflows, including editing, formula manipulation, and data transformation. It tests whether a model can maintain structured state, execute multi-step operations, and verify results.","L1: 
L2: Working Memory, Planning, Decision-making, Logical Reasoning, Adaptive Error Correction, Attention
L3: ",L2
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam (HLE) is a frontier, multimodal benchmark intended to probe advanced academic reasoning and specialized knowledge across many domains. Problems often require combining textual understanding with rigorous reasoning and, in some settings, tool-assisted verification.","L1: Language Comprehension
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Scene Understanding & Visual Reasoning, Working Memory
L3: ",L2
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 consists of competition mathematics problems that require multi-step derivations under time-like constraints and minimal context. It is commonly used to stress symbolic manipulation, careful case analysis, and error-avoidant reasoning.","L1: 
L2: Logical Reasoning, Working Memory, Attention
L3: Cognitive Flexibility",L3
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a subset of graduate-level science multiple-choice questions curated to be resistant to simple web lookup and superficial pattern matching. It tests deep domain understanding and the ability to reason through distractors in physics, chemistry, and biology.","L1: Language Comprehension
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Attention
L3: ",L2
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends broad academic knowledge testing to multiple non-English languages, probing both factual knowledge and reasoning across subjects. It highlights multilingual competence and robustness to linguistic variation in question framing.","L1: Language Comprehension, Language Production
L2: Semantic Understanding & Context Recognition, Working Memory
L3: ",L2
MMMU,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2311.16502,"MMMU is a multidisciplinary multimodal benchmark where models answer questions that require interpreting images (e.g., diagrams, plots, figures) alongside text. It stresses cross-modal grounding and reasoning rather than purely textual recall.","L1: Visual Perception, Language Comprehension
L2: Multisensory Integration, Scene Understanding & Visual Reasoning, Logical Reasoning, Working Memory
L3: ",L2
LAB-Bench FigQA,https://huggingface.co/datasets/futurehouse/lab-bench,https://arxiv.org/abs/2407.10362,"LAB-Bench FigQA evaluates whether models can correctly interpret complex scientific figures from biology papers and answer targeted questions about them. It rewards precise visual reading (labels, axes, experimental layouts) and reasoning that connects figure evidence to claims.","L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Logical Reasoning, Working Memory
L3: ",L2
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning tests reasoning over scientific visual content (e.g., charts/figures) paired with accompanying paper context, emphasizing faithful extraction and quantitative interpretation. It targets errors common in chart understanding such as misreading axes, legends, and trends.","L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Logical Reasoning, Working Memory, Attention
L3: ",L2
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU evaluates multimodal understanding and reasoning over video content, requiring models to integrate information across frames and time. It tests whether the model can track events, infer causality, and answer questions grounded in temporally extended visual evidence.","L1: Visual Perception, Language Comprehension
L2: Visual Attention & Eye Movements, Working Memory, Multisensory Integration, Scene Understanding & Visual Reasoning
L3: Cognitive Timing & Predictive Modeling",L3
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"The FACTS Benchmark Suite evaluates factuality and faithfulness of model outputs across multiple settings, including grounding to sources and resisting hallucination. It emphasizes whether models can maintain consistent claims, avoid unsupported assertions, and reflect uncertainty appropriately.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Working Memory
L3: Inhibitory Control, Self-reflection",L3
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA evaluates physical commonsense reasoning across many languages using non-parallel (not direct translations) data, stressing cultural and linguistic robustness. Items typically require selecting plausible actions or outcomes in everyday physical situations.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Spatial Representation & Mapping
L3: ",L2
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) is a long-context benchmark where multiple similar “needle” interactions are embedded within long “haystack” dialogues or documents, and the model must retrieve the correct target response. It probes whether models can maintain accurate cross-reference resolution under distractors and scale.","L1: Language Comprehension
L2: Working Memory, Attention, Episodic Memory, Semantic Understanding & Context Recognition
L3: ",L2
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval evaluates well-specified professional knowledge work across many occupations by judging the quality of produced work artifacts (e.g., plans, analyses, presentations, schedules). It is designed to reflect economically valuable end-to-end performance, including adherence to constraints and stakeholder intent.","L1: Language Production
L2: Planning, Decision-making, Semantic Understanding & Context Recognition, Working Memory
L3: Social Reasoning & Theory of Mind",L3
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks evaluates reasoning over graph-structured data, often requiring multi-step traversal or path-based queries under long-context conditions. It stresses maintaining intermediate state and applying consistent transition rules across many hops.","L1: 
L2: Logical Reasoning, Working Memory, Attention, Spatial Representation & Mapping
L3: Cognitive Flexibility",L3
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon evaluates models as tool-using agents across diverse tasks that require selecting appropriate tools, calling them correctly, and composing results into final answers. It emphasizes reliability under tool errors, correct orchestration, and sustained multi-step execution.","L1: Language Comprehension
L2: Planning, Decision-making, Working Memory, Adaptive Error Correction
L3: ",L2
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath is a benchmark of expert-level mathematics problems intended to be difficult for both models and many human solvers, often requiring deep multi-step proofs or computations. It targets robust mathematical reasoning under high complexity, including verification and avoidance of subtle logical errors.","L1: 
L2: Logical Reasoning, Working Memory, Attention, Planning
L3: Cognitive Flexibility",L3
