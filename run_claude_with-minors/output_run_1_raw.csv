Benchmark,Website,Paper,Description,Cognitive Functions
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates agentic software engineering on real GitHub issues by requiring a model to produce a patch that makes a hidden test suite pass. The “Verified” subset is manually curated to ensure tasks are solvable and that evaluation is reliable, emphasizing end-to-end debugging and code changes rather than short code snippets.","Planning, Logical Reasoning, Adaptive Error Correction, Working Memory, Language Comprehension, Language Production"
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 measures an agent’s ability to solve realistic tasks inside a command-line environment (e.g., manipulating files, running programs, configuring tools, and diagnosing failures). It stresses iterative trial-and-error with feedback from tool outputs under time/step constraints.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Language Comprehension"
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp evaluates web-browsing research agents on questions that require finding, verifying, and synthesizing information from documents discovered via search/browsing. It emphasizes source-grounded answering, multi-hop retrieval, and maintaining task context across many pages or snippets.","Planning, Decision-making, Attention, Working Memory, Semantic Understanding & Context Recognition, Language Comprehension, Language Production"
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench measures an agent’s ability to complete multi-turn customer-support workflows by interacting with simulated users and APIs while following domain policies (e.g., retail, airline, telecom). Success depends on maintaining state across turns, calling tools correctly, and balancing helpfulness with constraints.","Social Reasoning & Theory of Mind, Empathy, Decision-making, Planning, Working Memory, Language Comprehension, Language Production, Inhibitory Control"
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld tests computer-use agents in a realistic desktop environment where they must complete tasks through GUI interaction (clicking, typing, navigating apps, and interpreting screens). It stresses visual grounding and long-horizon action sequencing under partial observability and interface changes.","Visual Perception, Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Planning, Decision-making, Working Memory, Sensorimotor Coordination, Spatial Representation & Mapping"
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI evaluates “fluid” reasoning by giving a few input–output grid examples and asking the model to infer the hidden transformation rule for a new input. The tasks are designed to be novel and compositional, emphasizing generalization from very small data rather than memorized knowledge.","Logical Reasoning, Cognitive Flexibility, Working Memory, Attention, Spatial Representation & Mapping"
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon autonomous decision-making by simulating management of a vending-machine business over an extended period with many sequential choices. The score reflects the agent’s ability to plan, adapt to market dynamics, and stay coherent across thousands of steps.","Planning, Decision-making, Reward Mechanisms, Working Memory, Adaptive Error Correction, Semantic Understanding & Context Recognition, Self-reflection"
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas evaluates real-world tool use via the Model Context Protocol, requiring models to discover and call tools across multi-step workflows and integrate results into a correct final response. It emphasizes robust execution (correct parameters, error handling, retries) over purely verbal reasoning.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Language Comprehension, Language Production"
FinanceAgent,https://www.vals.ai/benchmarks/finance_agent,,"FinanceAgent measures performance on tasks typical of an entry-level financial analyst, such as analysis, reasoning over financial documents, and producing structured deliverables. It emphasizes correctness, defensible assumptions, and consistent quantitative reasoning across multi-step workflows.","Logical Reasoning, Planning, Decision-making, Working Memory, Semantic Understanding & Context Recognition, Language Production"
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity capabilities on real-world vulnerability tasks, including identifying known vulnerabilities from descriptions and discovering new ones. It stresses careful reasoning about code behavior, reproducing issues, and proposing effective fixes or exploit paths under realistic constraints.","Logical Reasoning, Planning, Adaptive Error Correction, Working Memory, Language Comprehension, Decision-making"
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"SpreadsheetBench evaluates an agent’s ability to understand, edit, and compute with complex spreadsheets derived from real-world scenarios. Tasks require correct formulas, structured organization, and iterative correction when outputs don’t match requirements.","Logical Reasoning, Working Memory, Planning, Adaptive Error Correction, Semantic Understanding & Context Recognition, Decision-making"
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam (HLE) is a large, frontier-level benchmark spanning difficult questions across many domains and modalities, intended to probe the limits of general reasoning and knowledge. Many questions require multi-step inference, careful reading, and (in some setups) effective use of tools like search or code to verify claims.","Language Comprehension, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Multisensory Integration, Planning, Decision-making, Visual Perception"
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 is a competition-math benchmark drawn from the American Invitational Mathematics Examination, featuring short-answer problems requiring nontrivial derivations. It emphasizes precise multi-step symbolic reasoning and error-free arithmetic or algebraic manipulation.","Logical Reasoning, Working Memory, Adaptive Error Correction, Planning"
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,GPQA Diamond is a high-quality subset of GPQA consisting of challenging graduate-level science multiple-choice questions designed to be difficult to answer by superficial pattern matching. It emphasizes deep domain understanding and careful reasoning to discriminate among plausible distractors.,"Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Inhibitory Control"
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends broad academic knowledge testing to multiple languages, measuring how well models maintain reasoning and subject knowledge beyond English. It stresses multilingual comprehension and consistent mapping of concepts across languages and domains.","Language Comprehension, Semantic Understanding & Context Recognition, Working Memory, Logical Reasoning"
MMMU,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2311.16502,"MMMU evaluates expert-level multimodal understanding by combining text with images (diagrams, charts, scientific visuals) across many disciplines. Models must integrate visual evidence with written context to answer questions requiring domain reasoning and visual interpretation.","Multisensory Integration, Visual Perception, Scene Understanding & Visual Reasoning, Language Comprehension, Logical Reasoning, Working Memory"
LAB-Bench FigQA,https://huggingface.co/datasets/futurehouse/lab-bench,https://arxiv.org/abs/2407.10362,LAB-Bench FigQA tests whether models can correctly interpret complex scientific figures from biology papers and answer targeted questions about experimental results and visual evidence. It emphasizes figure-grounded reasoning rather than recalling facts from memory alone.,"Visual Perception, Scene Understanding & Visual Reasoning, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Language Comprehension"
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning evaluates reasoning over scientific paper visuals (e.g., charts/figures) paired with textual prompts, often requiring quantitative or relational interpretation. The benchmark targets faithful extraction of information from figures and multi-step inference grounded in that evidence.","Visual Perception, Scene Understanding & Visual Reasoning, Logical Reasoning, Working Memory, Language Comprehension, Semantic Understanding & Context Recognition"
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU extends multimodal academic evaluation to videos, requiring temporal understanding of events, actions, and changes over time in addition to text. It stresses integrating information across multiple frames/clips and maintaining coherence over longer sequences.","Visual Perception, Scene Understanding & Visual Reasoning, Working Memory, Attention, Cognitive Timing & Predictive Modeling, Multisensory Integration, Language Comprehension"
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"The FACTS Benchmark Suite systematically evaluates LLM factuality across diverse tasks, focusing on whether generated statements are supported by given sources or by verifiable world knowledge (depending on the subtest). It emphasizes resisting hallucination, tracking evidence, and calibrating uncertainty when information is missing.","Semantic Understanding & Context Recognition, Inhibitory Control, Self-reflection, Working Memory, Language Comprehension, Logical Reasoning"
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA evaluates physical commonsense and everyday interaction reasoning across many languages, using non-parallel multilingual data to test true cross-lingual generalization. Questions typically require choosing the more plausible action or outcome in real-world physical situations.","Semantic Understanding & Context Recognition, Logical Reasoning, Spatial Representation & Mapping, Language Comprehension"
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) is a long-context, multi-round coreference retrieval benchmark where multiple similar “needle” requests are embedded in a long “haystack” conversation and the model must reproduce the correct response for a specified needle. It stresses precise reference tracking and robustness to distractors as context length increases.","Working Memory, Attention, Episodic Memory, Semantic Understanding & Context Recognition, Logical Reasoning, Inhibitory Control"
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval evaluates economically relevant knowledge work by asking models to produce realistic professional deliverables across many occupations (e.g., presentations, spreadsheets, schedules, analyses) judged by expert humans. It emphasizes end-to-end task completion quality, adherence to requirements, and usefulness under real workplace constraints.","Planning, Decision-making, Language Production, Semantic Understanding & Context Recognition, Working Memory, Social Reasoning & Theory of Mind, Adaptive Error Correction"
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks evaluates long-context reasoning over graph-structured data described in text, requiring the model to follow edges/paths and answer queries such as reachability or parent relationships. It stresses systematic traversal under distraction and careful state tracking across many nodes.","Spatial Representation & Mapping, Working Memory, Logical Reasoning, Attention, Planning"
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon measures tool-using agents on multi-step tasks that require selecting among many tools, invoking them correctly, and composing results into a final answer. It emphasizes robust orchestration, recovery from tool errors, and maintaining a coherent plan across calls.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Language Comprehension, Language Production"
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath evaluates advanced mathematics at or beyond typical competition levels, with tiers targeting progressively harder problems and (in some settings) permitting computation tools. It emphasizes deep multi-step reasoning, rigorous constraint handling, and avoiding subtle algebraic or logical mistakes.","Logical Reasoning, Working Memory, Planning, Adaptive Error Correction, Inhibitory Control"
