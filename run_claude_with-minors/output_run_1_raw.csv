Benchmark,Website,Paper,Description,Cognitive Functions
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates whether a model can solve real GitHub issues by generating a correct patch in an existing Python codebase and passing a human-validated test suite. The “Verified” subset focuses on problems confirmed solvable and aims to reduce noise from ambiguous tasks or incorrect tests, emphasizing reliable end-to-end software engineering.","Planning, Decision-making, Logical Reasoning, Working Memory, Adaptive Error Correction, Language Comprehension"
SWE-bench Multilingual,https://www.swebench.com/multilingual.html,https://arxiv.org/abs/2504.21798,"SWE-bench Multilingual extends the SWE-bench paradigm beyond Python to multiple programming languages, assessing whether models can navigate different tooling, libraries, and language idioms while fixing real repository issues. It emphasizes robust transfer of software-engineering behaviors across language ecosystems rather than overfitting to one stack.","Planning, Decision-making, Logical Reasoning, Working Memory, Adaptive Error Correction, Language Comprehension, Cognitive Flexibility"
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro is a larger and more difficult software engineering benchmark designed to be more contamination-resistant and industrially realistic, including tasks across several programming languages and tougher bug-fix/feature contexts. It stresses multi-step repo understanding, test-driven iteration, and higher difficulty issue resolution than standard SWE-bench variants.","Planning, Decision-making, Logical Reasoning, Working Memory, Adaptive Error Correction, Language Comprehension, Cognitive Flexibility"
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 evaluates agentic performance in command-line environments, where models must complete practical tasks by issuing shell commands, editing files, installing dependencies, and iterating based on outputs. Success requires reliable tool use, error recovery, and maintaining task state over multi-step workflows.","Planning, Decision-making, Working Memory, Adaptive Error Correction, Language Comprehension"
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp evaluates “deep research” style agents by requiring them to find and synthesize answers using browsing/search over a controlled document collection, supporting reproducible retrieval conditions. It tests whether an agent can plan a search strategy, integrate evidence across sources, and produce a grounded final answer under constraints.","Planning, Decision-making, Working Memory, Semantic Understanding & Context Recognition, Language Comprehension, Language Production, Adaptive Error Correction"
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench measures tool-using customer-support agents in simulated domains (e.g., retail, airline, telecom) where the model must interact with a user and APIs while following domain policies. It emphasizes multi-turn state tracking, policy adherence under pressure, and robust decision-making across long interaction trajectories.","Social Reasoning & Theory of Mind, Empathy, Inhibitory Control, Decision-making, Planning, Working Memory, Language Comprehension, Language Production, Adaptive Error Correction"
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld evaluates multimodal “computer use” agents that must operate within an operating-system-like environment by interpreting screens and taking UI actions over many steps. It tests grounded perception-to-action loops such as navigation, form filling, and application workflows under step limits and dynamic interfaces.","Visual Perception, Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Sensorimotor Coordination, Planning, Decision-making, Working Memory, Adaptive Error Correction"
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI tests abstract, few-shot pattern induction using small grid-based input–output examples, where the model must infer the latent rule and apply it to a new grid. The benchmark is designed to emphasize generalizable reasoning and compositional rule discovery rather than domain knowledge.","Logical Reasoning, Cognitive Flexibility, Working Memory, Spatial Representation & Mapping, Scene Understanding & Visual Reasoning, Attention"
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 measures long-horizon agent coherence in a simulated business environment where the model manages a vending-machine operation over extended time (e.g., a year). It requires sustained planning and adaptation—handling inventory, pricing, supplier interaction, and dynamic conditions to maximize outcomes.","Planning, Decision-making, Working Memory, Reward Mechanisms, Adaptive Error Correction, Self-reflection, Motivational Drives, Social Reasoning & Theory of Mind"
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas evaluates real-world tool use via the Model Context Protocol (MCP), emphasizing multi-step workflows across authentic APIs and servers. Tasks require discovering appropriate tools, invoking them with correct parameters, handling failures/retries, and synthesizing results into correct responses.","Planning, Decision-making, Working Memory, Adaptive Error Correction, Language Comprehension, Language Production"
FinanceAgent,https://www.vals.ai/benchmarks/finance_agent,,"FinanceAgent benchmarks agent performance on tasks typical of an entry-level financial analyst, often requiring structured reasoning over documents, tables, and quantitative constraints. It emphasizes producing correct, actionable outputs (e.g., analysis or artifact construction) while integrating domain rules and intermediate calculations.","Logical Reasoning, Decision-making, Planning, Working Memory, Semantic Understanding & Context Recognition, Language Comprehension, Language Production, Adaptive Error Correction"
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity agent capabilities on real-world vulnerability tasks, including identifying known issues from descriptions and attempting discovery of previously unknown vulnerabilities in open-source projects. It stresses iterative investigation, hypothesis testing, and correct patching or reporting under realistic code constraints.","Logical Reasoning, Planning, Decision-making, Working Memory, Adaptive Error Correction, Language Comprehension, Semantic Understanding & Context Recognition"
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"SpreadsheetBench measures how well models can navigate and manipulate complex spreadsheets derived from real-world scenarios, including reading/writing cells, applying formulas, and producing correct final artifacts. It emphasizes procedural accuracy, state tracking across many operations, and debugging when intermediate steps go wrong.","Working Memory, Planning, Decision-making, Adaptive Error Correction, Logical Reasoning, Semantic Understanding & Context Recognition"
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a frontier, multi-domain benchmark intended to probe advanced academic reasoning and knowledge, including multimodal questions and tasks that may benefit from tools (e.g., code execution or search) depending on the evaluation setup. It emphasizes synthesizing complex information and producing defensible final answers under high difficulty.","Language Comprehension, Language Production, Logical Reasoning, Working Memory, Semantic Understanding & Context Recognition, Scene Understanding & Visual Reasoning, Visual Perception, Planning, Decision-making"
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,AIME 2025 evaluates mathematical problem solving on competition-style questions that require multi-step derivations and careful symbolic/quantitative reasoning. It stresses accuracy under tight problem statements and the ability to maintain intermediate constraints over long solution chains.,"Logical Reasoning, Working Memory, Planning, Attention, Adaptive Error Correction"
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a difficult, graduate-level multiple-choice science QA benchmark designed to be “Google-proof,” focusing on questions that require expert reasoning rather than rote recall. The Diamond subset emphasizes high-quality items where experts reliably agree on the correct answer while non-experts often fail.","Language Comprehension, Logical Reasoning, Working Memory, Semantic Understanding & Context Recognition, Decision-making"
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends broad academic knowledge testing across many subjects and multiple languages, measuring multilingual understanding and reasoning in a standardized multiple-choice format. It probes whether models can transfer conceptual competence across languages and maintain consistent subject-matter performance.","Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Cognitive Flexibility"
MMMU,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2311.16502,"MMMU is a multimodal, multi-discipline benchmark where questions require jointly interpreting text and images (e.g., diagrams, plots, figures) to answer expert-level prompts. It emphasizes multimodal grounding, visual reasoning, and combining heterogeneous evidence to reach a correct conclusion.","Multisensory Integration, Visual Perception, Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Language Comprehension, Logical Reasoning, Working Memory"
LAB-Bench FigQA,https://huggingface.co/datasets/futurehouse/lab-bench,https://arxiv.org/abs/2407.10362,"LAB-Bench FigQA evaluates whether models can correctly interpret and reason over complex scientific figures from biology papers, answering questions that depend on extracting visual evidence and integrating it with domain context. It stresses precise figure reading (including axes/labels/layout) and multi-step scientific reasoning.","Visual Perception, Scene Understanding & Visual Reasoning, Multisensory Integration, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Visual Attention & Eye Movements"
WebArena,https://webarena.dev/,https://arxiv.org/abs/2307.13854,"WebArena evaluates autonomous web agents on realistic multi-step tasks across several interactive web apps (e-commerce, CMS, forums, Git workflows, maps). Models must navigate pages, manipulate forms, and complete goals while handling dynamic UI state and long action sequences.","Visual Perception, Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Sensorimotor Coordination, Planning, Decision-making, Working Memory, Adaptive Error Correction, Language Comprehension, Language Production"
