Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,SWE-bench Verified evaluates an LLM’s ability to solve real-world software engineering issues by generating patches that fix failing tests in open-source Python repositories. The “Verified” subset contains tasks that have been human-validated to be solvable and are scored by whether the produced patch passes the project’s test suite under a standardized harness.,"L1: Language Comprehension, Language Production
L2: Planning, Adaptive Error Correction, Logical Reasoning, Working Memory, Decision-making, Semantic Understanding & Context Recognition
L3: ",L2
SWE-bench Multilingual,https://www.swebench.com/multilingual.html,https://arxiv.org/abs/2504.21798,"SWE-bench Multilingual extends SWE-bench-style patch generation to multiple programming languages, testing whether models can perform repository-level debugging and implementation beyond Python. Tasks require reading issue descriptions and code context, producing correct edits, and passing language-appropriate tests/build checks.","L1: Language Comprehension, Language Production
L2: Planning, Adaptive Error Correction, Logical Reasoning, Working Memory, Decision-making, Semantic Understanding & Context Recognition
L3: Cognitive Flexibility",L3
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro is a larger, more challenging software engineering benchmark designed to be more contamination-resistant and industrially representative, spanning multiple languages and realistic engineering workflows. Models must propose code changes that satisfy hidden unit/integration checks, emphasizing robust reasoning under repository constraints and ambiguous bug reports.","L1: Language Comprehension, Language Production
L2: Planning, Adaptive Error Correction, Logical Reasoning, Working Memory, Decision-making, Semantic Understanding & Context Recognition
L3: Cognitive Flexibility",L3
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 evaluates agentic problem-solving in real command-line environments, where models must issue shell commands, inspect files, run programs, and iteratively fix issues to accomplish tasks. Success depends on managing state across many steps (filesystem, tool outputs, errors) and recovering from mistakes under resource constraints.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition
L3: Inhibitory Control",L3
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp evaluates “deep research” style question answering where models must search and read web documents (or a controlled corpus, in some variants) and synthesize a correct final answer. It emphasizes multi-step information gathering, source triangulation, and maintaining coherence over long contexts while using browsing tools.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Episodic Memory, Working Memory, Semantic Understanding & Context Recognition
L3: Self-reflection",L3
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench measures an agent’s ability to complete multi-turn customer-support tasks by interacting with simulated users and backend APIs while adhering to domain policies (e.g., retail, airline, telecom). The benchmark stresses reliable tool use, policy compliance, and resolving long interaction traces with changing user goals and constraints.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Working Memory, Semantic Understanding & Context Recognition
L3: Social Reasoning & Theory of Mind, Inhibitory Control",L3
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld evaluates multimodal computer-use agents in a realistic desktop operating system, requiring navigation of GUIs, reading screenshots, and executing actions over many steps. Tasks resemble end-user workflows (finding settings, editing files, using applications) and are scored by successful task completion under step limits.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Sensorimotor Coordination, Planning, Decision-making, Working Memory, Adaptive Error Correction
L3: ",L2
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI tests abstract pattern induction using small grids where a model must infer a hidden transformation from a few input–output examples and apply it to a new input. It targets generalization to novel rules rather than memorized knowledge, rewarding flexible reasoning over compositional visual structures.","L1: 
L2: Logical Reasoning, Working Memory, Spatial Representation & Mapping, Scene Understanding & Visual Reasoning, Attention
L3: Cognitive Flexibility",L3
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon autonomous agency in a simulated vending-machine business over extended time (e.g., a year), where the agent must manage inventory, pricing, suppliers, and communications to maximize final balance. It stresses persistence, strategic planning, and adapting decisions to changing market conditions across many turns.","L1: 
L2: Planning, Decision-making, Reward Mechanisms, Working Memory, Episodic Memory, Adaptive Error Correction
L3: Motivational Drives, Self-reflection",L3
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas measures real-world tool-use via Model Context Protocol (MCP) servers, requiring models to discover appropriate tools, invoke APIs correctly, handle errors, and compose multi-step workflows. It emphasizes execution correctness across heterogeneous services and robust recovery when tool calls fail or return unexpected data.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition
L3: ",L2
FinanceAgent,https://www.vals.ai/benchmarks/finance_agent,,"FinanceAgent evaluates agent performance on tasks representative of an entry-level financial analyst, such as building or auditing financial models, extracting information, and producing analysis with business context. It stresses accurate quantitative reasoning, structured outputs, and multi-step decomposition of finance workflows.","L1: Language Comprehension, Language Production
L2: Logical Reasoning, Planning, Decision-making, Working Memory, Semantic Understanding & Context Recognition, Adaptive Error Correction
L3: ",L2
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity agent capabilities at scale, including identifying known vulnerabilities from high-level descriptions and discovering new vulnerabilities in real open-source projects. Tasks require code comprehension, threat reasoning, and producing actionable findings or fixes under realistic constraints.","L1: Language Comprehension, Language Production
L2: Logical Reasoning, Planning, Decision-making, Adaptive Error Correction, Working Memory
L3: Inhibitory Control",L3
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"SpreadsheetBench evaluates an agent’s ability to navigate and manipulate complex spreadsheets derived from real-world scenarios, often requiring formula edits, table restructuring, and correct calculations. It stresses stepwise verification, error correction, and maintaining consistency across interdependent cells and sheets.","L1: 
L2: Working Memory, Planning, Decision-making, Adaptive Error Correction, Logical Reasoning, Semantic Understanding & Context Recognition, Attention
L3: ",L2
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam (HLE) is a multi-modal benchmark intended to probe frontier-level knowledge and reasoning across diverse expert domains, using challenging questions that may require synthesis, computation, and interpretation of provided materials. Variants may allow tools (e.g., search or code execution) to measure end-to-end problem solving under realistic workflows.","L1: Language Comprehension, Language Production, Visual Perception
L2: Logical Reasoning, Working Memory, Semantic Understanding & Context Recognition, Scene Understanding & Visual Reasoning, Planning
L3: ",L2
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 evaluates mathematical problem solving on competition-style questions from the American Invitational Mathematics Examination. Problems typically require multi-step derivations, precise algebraic/number-theoretic reasoning, and careful constraint management to produce a final numeric answer.","L1: 
L2: Logical Reasoning, Working Memory, Planning, Attention, Adaptive Error Correction
L3: ",L2
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a high-difficulty, multiple-choice science QA benchmark curated to be “Google-proof,” emphasizing questions that require expert-level reasoning rather than lookup. The Diamond subset focuses on the highest-quality items where experts reliably answer correctly and non-experts tend to fail.","L1: Language Comprehension
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Decision-making, Attention
L3: ",L2
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends the MMLU-style broad academic evaluation to multiple languages, testing multilingual knowledge and reasoning across many subjects. It stresses robust comprehension and consistent decision-making under varying linguistic forms and culturally different phrasing.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Decision-making
L3: Cognitive Flexibility",L3
MMMU,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2311.16502,"MMMU is a large-scale multimodal benchmark spanning many disciplines where models must answer questions using both text and images (e.g., diagrams, plots, tables, and natural images). It emphasizes integrating visual evidence with domain knowledge and multi-step reasoning to select or generate correct answers.","L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Working Memory, Attention
L3: ",L2
LAB-Bench FigQA,https://huggingface.co/datasets/futurehouse/lab-bench,https://arxiv.org/abs/2407.10362,"LAB-Bench FigQA evaluates whether models can correctly interpret and reason over complex scientific figures from biology papers, such as multi-panel plots and annotated diagrams. It probes figure literacy (reading axes, legends, visual encodings) and translating visual evidence into precise scientific answers.","L1: Visual Perception, Language Comprehension, Language Production
L2: Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Logical Reasoning, Working Memory, Semantic Understanding & Context Recognition
L3: ",L2
WebArena,https://webarena.dev/,https://arxiv.org/abs/2307.13854,"WebArena evaluates autonomous web agents in self-hosted, realistic web applications (e-commerce, CMS, forums, code hosting, maps), requiring navigation, form filling, and multi-step task completion. It stresses long-horizon planning, robust interaction with dynamic interfaces, and recovering from navigation or interpretation errors.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Sensorimotor Coordination, Planning, Decision-making, Working Memory, Adaptive Error Correction, Semantic Understanding & Context Recognition
L3: ",L2
