Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates software engineering agents by asking them to produce code patches that fix real issues in open-source repositories, validated by unit tests and human verification of solvability. The “Verified” subset aims to reduce noise (e.g., ambiguous tasks or brittle tests) and better reflect reliable end-to-end bug-fixing and change implementation.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Logical Reasoning, Working Memory, Adaptive Error Correction
L3: ",L2
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 measures an agent’s ability to complete real tasks in a command-line environment (e.g., inspecting files, running programs, debugging, and configuring systems) using tool calls and iterative feedback. It emphasizes practical autonomy: taking actions, interpreting outputs/errors, and converging on a working solution.","L1: Language Comprehension
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory, Sensorimotor Coordination
L3: ",L2
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp evaluates “deep research” behavior by requiring models to browse a document collection or the web to answer questions that typically need multi-step investigation and synthesis. It stresses selecting sources, extracting key evidence, and producing a final answer grounded in retrieved information.","L1: Language Comprehension
L2: Planning, Decision-making, Attention, Working Memory, Semantic Understanding & Context Recognition
L3: ",L2
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench (Tau2-bench) evaluates agentic customer-support style interactions where the model must follow domain policies while using tools/APIs across multi-turn dialogues. It tests robustness to policy constraints, conversational dynamics with a simulated user, and reliable completion of procedural workflows.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Working Memory
L3: Inhibitory Control, Social Reasoning & Theory of Mind",L3
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld evaluates multimodal “computer use” by giving tasks that require operating within an OS-like environment (e.g., navigating GUIs, opening apps, editing settings/files) based on screen observations. Success depends on interpreting interface state and executing correct action sequences over many steps.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Planning, Decision-making, Sensorimotor Coordination, Working Memory
L3: ",L2
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI measures abstract, few-shot pattern induction using grid-based input-output examples where the rule must be inferred and applied to a new grid. It is intended to isolate fluid reasoning and generalization rather than memorization of domain knowledge.","L1: 
L2: Logical Reasoning, Working Memory, Spatial Representation & Mapping, Attention
L3: Cognitive Flexibility",L3
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon autonomous agent performance in a year-long simulated business scenario (operating a vending-machine business) with many interdependent decisions. Models must manage inventory, pricing, supplier negotiation, and adapt to changing conditions to maximize final profit.","L1: 
L2: Planning, Decision-making, Working Memory, Reward Mechanisms, Adaptive Error Correction
L3: Cognitive Timing & Predictive Modeling",L3
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas evaluates real-world tool use via the Model Context Protocol, where agents must discover tools, invoke them correctly, handle errors/retries, and integrate tool outputs into coherent solutions. The benchmark emphasizes multi-step workflow execution across diverse tool servers and APIs.","L1: Language Comprehension
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory
L3: ",L2
FinanceAgent,https://www.vals.ai/benchmarks/finance_agent,,"FinanceAgent evaluates skills expected of an entry-level financial analyst, such as interpreting financial documents, performing calculations, producing analyses, and drafting structured outputs. Tasks often require combining domain knowledge with careful quantitative reasoning and artifact-quality reporting.","L1: Language Comprehension, Language Production
L2: Logical Reasoning, Planning, Decision-making, Working Memory, Semantic Understanding & Context Recognition
L3: ",L2
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity agent capabilities on tasks such as locating known vulnerabilities from descriptions and attempting discovery of unknown issues in real open-source projects. It stresses iterative debugging/investigation, correct interpretation of program behavior, and producing actionable findings.","L1: 
L2: Logical Reasoning, Planning, Adaptive Error Correction, Working Memory, Decision-making
L3: ",L2
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"SpreadsheetBench measures an agent’s ability to understand, edit, and compute over complex spreadsheets resembling real workplace artifacts. It tests whether models can plan multi-step transformations, maintain consistency across cells/sheets, and verify outputs programmatically.","L1: 
L2: Planning, Working Memory, Logical Reasoning, Adaptive Error Correction, Decision-making, Semantic Understanding & Context Recognition
L3: ",L2
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a frontier-level benchmark spanning challenging questions across many academic fields, designed to probe deep reasoning and advanced knowledge. It is multimodal in parts, requiring models to integrate text with visual information and produce precise answers.","L1: Language Comprehension, Visual Perception
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory
L3: ",L2
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 uses problems from a prestigious mathematics competition, emphasizing multi-step symbolic reasoning and careful error-free computation. It is commonly used to evaluate contest-math performance without requiring external tools.","L1: 
L2: Logical Reasoning, Working Memory, Planning
L3: Cognitive Flexibility",L3
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a high-difficulty, “Google-proof” set of graduate-level multiple-choice questions in science where superficial pattern matching tends to fail. It probes whether models can apply scientific understanding and reasoning under strong distractors.","L1: Language Comprehension
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory
L3: ",L2
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends broad academic testing to many non-English languages, measuring whether models retain knowledge and reasoning ability across multilingual contexts. It stresses consistent understanding of questions and options despite linguistic and cultural variation.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Working Memory, Logical Reasoning
L3: ",L2
MMMU,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2311.16502,"MMMU evaluates expert-level multimodal understanding across many disciplines, where problems combine images (diagrams, plots, screenshots) with text and multiple-choice reasoning. It targets integrated visual-and-text reasoning rather than vision-only recognition.","L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Logical Reasoning, Working Memory, Multisensory Integration
L3: ",L2
LAB-Bench FigQA,https://huggingface.co/datasets/futurehouse/lab-bench,https://arxiv.org/abs/2407.10362,"LAB-Bench FigQA tests whether models can interpret complex scientific figures (common in biology papers) and answer questions requiring reading plotted evidence, labels, and experimental context. It emphasizes faithful extraction of visual details and reasoning from figure-grounded evidence.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory
L3: ",L2
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning evaluates reasoning over scientific-paper content (often including charts/figures) where answers require structured interpretation of visual evidence and accompanying text. It emphasizes cross-referencing claims, extracting quantitative/relational information, and multi-step justification.","L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Logical Reasoning, Working Memory, Multisensory Integration
L3: ",L2
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU extends multimodal evaluation to video, requiring temporal understanding, event tracking, and reasoning about dynamic scenes. Questions often require integrating information across frames and aligning it with textual prompts or options.","L1: Visual Perception, Language Comprehension
L2: Visual Attention & Eye Movements, Working Memory, Scene Understanding & Visual Reasoning
L3: Cognitive Timing & Predictive Modeling",L3
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"The FACTS Benchmark Suite systematically evaluates factuality-related behaviors, including whether responses remain grounded, consistent, and resistant to hallucination across varied settings. It targets reliability in knowledge claims rather than raw generation fluency.","L1: Language Comprehension, Language Production
L2: Semantic Understanding & Context Recognition, Working Memory
L3: Inhibitory Control, Self-reflection",L3
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA evaluates practical, commonsense reasoning across many languages and locales, aiming to test whether models generalize beyond an English-centric worldview. It probes whether models select plausible actions/explanations under diverse everyday contexts.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning
L3: Social Reasoning & Theory of Mind",L3
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) tests long-context multi-round coreference resolution by embedding multiple similar “needle” requests within long “haystack” conversations/documents. Models must retrieve and reproduce the correct referenced response, stressing robust long-range dependency handling.","L1: Language Comprehension
L2: Working Memory, Attention, Semantic Understanding & Context Recognition, Episodic Memory
L3: ",L2
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval evaluates economically valuable professional work across many occupations by judging model-produced artifacts (e.g., spreadsheets, presentations, schedules) against expert human outputs. It emphasizes end-to-end task completion quality, including correctness, structure, and usefulness.","L1: Language Production
L2: Planning, Decision-making, Semantic Understanding & Context Recognition, Working Memory
L3: Self-reflection",L3
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks evaluates algorithmic reasoning over graph-structured data, such as following parent pointers or performing breadth-first style traversal under constraints. It probes whether models can maintain and manipulate structured state across many steps without losing track of dependencies.","L1: 
L2: Logical Reasoning, Working Memory, Spatial Representation & Mapping, Attention, Planning
L3: ",L2
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon evaluates multi-tool competence across diverse tool ecosystems, requiring models to select appropriate tools, compose tool calls, recover from failures, and integrate results into final answers. It focuses on robust agentic execution rather than single-step question answering.","L1: Language Comprehension
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory
L3: Inhibitory Control",L3
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath measures expert-level mathematical problem solving on difficult, research-adjacent items designed to resist memorization and shallow heuristics. It emphasizes deep multi-step reasoning, precise symbolic manipulation, and sustained focus over long solutions.","L1: 
L2: Logical Reasoning, Working Memory, Planning
L3: Cognitive Flexibility",L3
