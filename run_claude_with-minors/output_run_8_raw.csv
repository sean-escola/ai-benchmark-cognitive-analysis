Benchmark,Website,Paper,Description,Cognitive Functions
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates models on real, repository-based software engineering tasks where the goal is to produce a patch that makes previously failing tests pass. The “Verified” subset consists of issues that have been validated by human reviewers as solvable with the provided repository state and tests, making the benchmark more reliable for pass@1 agentic coding comparisons.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Language Comprehension, Language Production, Logical Reasoning, Semantic Understanding & Context Recognition"
SWE-bench Multilingual,https://www.swebench.com/multilingual.html,https://arxiv.org/abs/2504.21798,"SWE-bench Multilingual extends the SWE-bench paradigm beyond Python to multiple programming languages and ecosystems, testing whether models can transfer software-engineering skills across diverse tooling and conventions. Tasks still require generating correct code changes in real repos, but add cross-language generalization pressure and varied build/test workflows.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Language Comprehension, Language Production, Cognitive Flexibility, Logical Reasoning, Semantic Understanding & Context Recognition"
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro is a larger, harder suite of repository-level software engineering tasks designed to be more challenging and more resistant to superficial memorization than earlier variants. It emphasizes multi-file changes, realistic debugging and implementation work, and broader language/toolchain coverage to better approximate professional engineering workflows.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Cognitive Flexibility, Logical Reasoning, Language Comprehension, Language Production, Semantic Understanding & Context Recognition"
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 evaluates agentic performance in command-line environments, where models must complete practical tasks by issuing shell commands, editing files, installing dependencies, and iterating based on program output. It stresses end-to-end autonomy: choosing actions, interpreting errors, and converging on a working solution under time/step constraints.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Language Comprehension, Language Production, Logical Reasoning, Semantic Understanding & Context Recognition"
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp measures deep-research style question answering with controlled web retrieval, where agents must search, read, and synthesize information from a fixed document collection to produce verifiable answers. It focuses on tool-augmented reasoning over long contexts, including selecting good queries, aggregating evidence, and maintaining coherence across many retrieval steps.","Planning, Decision-making, Working Memory, Episodic Memory, Language Comprehension, Language Production, Logical Reasoning, Semantic Understanding & Context Recognition, Attention"
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench evaluates interactive customer-support agents in simulated domains (e.g., retail, airline, telecom) that must use tools/APIs while following policies and handling multi-turn dialogues. Success depends on maintaining state, resolving constraints, and producing compliant outcomes despite ambiguous user goals and evolving context.","Social Reasoning & Theory of Mind, Decision-making, Planning, Working Memory, Language Comprehension, Language Production, Inhibitory Control, Reward Mechanisms, Semantic Understanding & Context Recognition, Attention"
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld is a multimodal computer-use benchmark where agents operate within a desktop-like environment to complete tasks across applications, using screenshots and UI interactions. It tests whether models can perceive interfaces, plan action sequences, and recover from mistakes while navigating real software workflows under step limits.","Visual Perception, Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Planning, Decision-making, Working Memory, Adaptive Error Correction, Sensorimotor Coordination, Semantic Understanding & Context Recognition"
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI tests “fluid” abstract reasoning by asking models to infer transformation rules from a few input–output grid examples and apply them to new grids. Because tasks are intentionally out-of-distribution and compositional, performance reflects pattern induction and rule-like generalization rather than domain knowledge recall.","Logical Reasoning, Working Memory, Cognitive Flexibility, Spatial Representation & Mapping, Visual Perception, Scene Understanding & Visual Reasoning, Planning, Attention"
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon autonomous agency in a simulated vending-machine business over many decision steps (e.g., sourcing inventory, pricing, negotiation, and handling stochastic market dynamics). Scores reflect sustained coherence, strategic adaptation, and the ability to manage resources over extended time horizons.","Planning, Decision-making, Working Memory, Reward Mechanisms, Adaptive Error Correction, Semantic Understanding & Context Recognition, Motivational Drives"
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas measures real-world tool use via the Model Context Protocol by requiring models to discover, invoke, and compose multiple tools across realistic workflows and APIs. Tasks stress correct parameterization, multi-step orchestration, and robust handling of tool errors and partial results.","Planning, Decision-making, Working Memory, Adaptive Error Correction, Language Comprehension, Language Production, Semantic Understanding & Context Recognition"
FinanceAgent,https://www.vals.ai/benchmarks/finance_agent,,"FinanceAgent evaluates tool-using agents on tasks typical of an entry-level financial analyst, such as extracting information, building analyses, and producing finance-oriented deliverables under realistic constraints. It emphasizes structured reasoning with domain conventions, iterative refinement, and synthesis into business-ready outputs.","Logical Reasoning, Planning, Decision-making, Working Memory, Language Comprehension, Language Production, Semantic Understanding & Context Recognition, Adaptive Error Correction"
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity agents on large-scale, real-world vulnerability tasks, including identifying known vulnerability classes in open-source projects and, in some settings, discovering new issues. It measures how well models can reason about code behavior, reproduce exploits conceptually, and navigate complex technical artifacts under constrained attempts.","Logical Reasoning, Planning, Decision-making, Adaptive Error Correction, Working Memory, Language Comprehension, Language Production, Attention, Inhibitory Control"
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"SpreadsheetBench tests whether models can correctly navigate, edit, and compute within complex spreadsheets, often requiring multi-step transformations and formula logic consistent with real business artifacts. It stresses precision (small mistakes can cascade), procedural reliability, and the ability to translate instructions into structured spreadsheet operations.","Logical Reasoning, Planning, Decision-making, Working Memory, Adaptive Error Correction, Semantic Understanding & Context Recognition, Attention"
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a difficult, multi-modal benchmark intended to probe frontier-level academic reasoning and knowledge across many subjects, often requiring synthesis rather than lookup. Depending on the evaluation setup, models may be tested with or without tools (e.g., search or code), highlighting both raw reasoning and tool-augmented problem solving.","Language Comprehension, Language Production, Logical Reasoning, Working Memory, Semantic Understanding & Context Recognition, Multisensory Integration, Scene Understanding & Visual Reasoning, Planning, Decision-making"
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 evaluates mathematical problem solving using competition-style questions that require multi-step derivations, careful casework, and exact numeric answers. It is typically run without tools to assess internal reasoning reliability, though some reports also compare tool-augmented settings.","Logical Reasoning, Working Memory, Planning, Attention"
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a high-quality subset of GPQA consisting of challenging graduate-level multiple-choice questions in science designed to be difficult to answer via superficial recall. It targets rigorous reasoning over domain knowledge, with questions curated so non-experts tend to fail while experts can succeed.","Language Comprehension, Logical Reasoning, Working Memory, Semantic Understanding & Context Recognition, Decision-making, Attention"
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends broad academic knowledge testing into multiple languages, assessing whether a model’s understanding and reasoning generalize across linguistic contexts and cultural/terminological variation. It is commonly used to evaluate multilingual comprehension, consistency, and robustness of subject-matter knowledge.","Language Comprehension, Language Production, Semantic Understanding & Context Recognition, Logical Reasoning, Cognitive Flexibility, Working Memory"
MMMU,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2311.16502,"MMMU is a multimodal benchmark spanning many disciplines where models must answer questions that require jointly interpreting images (e.g., diagrams, charts, screenshots) and accompanying text. It probes integrated visual-language reasoning and the ability to ground conclusions in visual evidence rather than purely textual priors.","Visual Perception, Scene Understanding & Visual Reasoning, Multisensory Integration, Visual Attention & Eye Movements, Language Comprehension, Language Production, Logical Reasoning, Working Memory, Semantic Understanding & Context Recognition"
LAB-Bench FigQA,https://huggingface.co/datasets/futurehouse/lab-bench,https://arxiv.org/abs/2407.10362,"LAB-Bench FigQA evaluates scientific figure question answering, requiring models to extract and reason over information encoded in complex biology research figures (plots, schematics, multi-panel visuals). Performance reflects the ability to read visual scientific evidence, connect it to domain concepts, and answer precisely under ambiguity and noise.","Visual Perception, Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Multisensory Integration, Language Comprehension, Language Production, Logical Reasoning, Working Memory, Semantic Understanding & Context Recognition"
WebArena,https://webarena.dev/,https://arxiv.org/abs/2307.13854,"WebArena evaluates autonomous web agents on realistic tasks across multiple web apps (e-commerce, forums, code hosting, CMS) requiring navigation, form filling, and multi-step workflows. It stresses perception of dynamic pages, maintaining task state across long interactions, and robust recovery from UI or policy-induced failures.","Visual Perception, Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Planning, Decision-making, Working Memory, Adaptive Error Correction, Language Comprehension, Language Production, Sensorimotor Coordination, Semantic Understanding & Context Recognition"
