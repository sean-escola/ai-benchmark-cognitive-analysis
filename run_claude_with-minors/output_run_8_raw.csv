Benchmark,Website,Paper,Description,Cognitive Functions
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates software engineering agents on real GitHub issues by requiring them to produce a patch that makes a target test suite pass. The “Verified” subset filters to tasks confirmed solvable and reliably checkable with standardized evaluation harnesses, emphasizing end-to-end debugging and codebase navigation.","Planning, Decision-making, Logical Reasoning, Working Memory, Adaptive Error Correction, Semantic Understanding & Context Recognition"
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 measures how well an agent can complete real-world tasks in a command-line environment (e.g., installing tools, manipulating files, running programs, and debugging failures). It stresses iterative experimentation where the agent must interpret tool feedback (stdout/stderr) and recover from mistakes under resource constraints.","Planning, Decision-making, Working Memory, Adaptive Error Correction, Inhibitory Control, Semantic Understanding & Context Recognition"
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp evaluates “deep research” agents that must answer questions by searching, reading, and synthesizing information from a controlled web or document index. The benchmark emphasizes multi-step information foraging, evidence integration across sources, and maintaining coherence over long investigative trajectories.","Planning, Decision-making, Attention, Working Memory, Semantic Understanding & Context Recognition, Self-reflection"
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench evaluates interactive customer-support agents in simulated domains (e.g., retail, airline, telecom) where models must use tools/APIs while following policies. It emphasizes multi-turn dialogue management, adherence to constraints, and resolving user requests through structured actions and confirmations.","Language Comprehension, Language Production, Social Reasoning & Theory of Mind, Planning, Decision-making, Inhibitory Control"
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld evaluates multimodal “computer use” agents that operate desktop environments to complete tasks across applications and system settings. Agents must interpret UI screenshots, execute sequences of actions (clicks/typing), and adapt to dynamic states and errors during task execution.","Visual Perception, Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Sensorimotor Coordination, Planning, Decision-making, Working Memory, Adaptive Error Correction, Spatial Representation & Mapping"
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI measures few-shot abstract reasoning by asking models to infer latent rules from small sets of input–output grid examples and apply them to new inputs. It aims to minimize reliance on memorized knowledge, emphasizing novel pattern discovery, compositional transformations, and generalization.","Logical Reasoning, Cognitive Flexibility, Working Memory, Spatial Representation & Mapping, Planning"
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon agent coherence by placing an agent in a year-long simulated business management loop (inventory, pricing, supplier negotiation, and budgeting). Success depends on maintaining stable goals, adapting strategy over time, and avoiding compounding errors across many decisions.","Planning, Decision-making, Reward Mechanisms, Working Memory, Cognitive Timing & Predictive Modeling, Adaptive Error Correction"
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas evaluates real-world tool use via the Model Context Protocol (MCP), where an agent must discover tools, call them correctly, and integrate results across multi-step workflows. It emphasizes robust API interaction, error handling, and synthesizing outputs from multiple tool calls into correct final answers.","Planning, Decision-making, Working Memory, Adaptive Error Correction, Semantic Understanding & Context Recognition"
FinanceAgent,https://www.vals.ai/benchmarks/finance_agent,,"FinanceAgent benchmarks models on tasks resembling entry-level financial analysis, including interpreting financial documents, building/validating calculations, and producing justified recommendations. It stresses numeracy, structured reasoning, and producing professionally formatted outputs aligned with constraints.","Logical Reasoning, Decision-making, Planning, Working Memory, Semantic Understanding & Context Recognition, Language Production"
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity agents on large-scale tasks involving vulnerability understanding, reproduction, and discovery in real open-source software contexts. The benchmark emphasizes technical reasoning under uncertainty, iterative testing, and constructing correct exploitation or patching steps from partial clues.","Logical Reasoning, Planning, Working Memory, Adaptive Error Correction, Inhibitory Control, Semantic Understanding & Context Recognition"
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"SpreadsheetBench evaluates agents on manipulating complex spreadsheets drawn from realistic workflows (e.g., cleaning data, updating formulas, generating outputs). It stresses precise state tracking, structured transformation of tabular artifacts, and correcting mistakes without breaking dependencies.","Working Memory, Planning, Decision-making, Logical Reasoning, Adaptive Error Correction, Semantic Understanding & Context Recognition"
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a frontier multimodal benchmark designed to probe advanced academic reasoning and expert-level knowledge across many domains. Questions often require integrating text with figures or other modalities, applying multi-step inference, and resisting hallucinations when information is missing.","Language Comprehension, Visual Perception, Scene Understanding & Visual Reasoning, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory"
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,AIME 2025 evaluates competition-style mathematics problem solving on questions requiring multi-step derivations and careful algebraic/combinatorial reasoning. The tasks reward precise intermediate reasoning and error checking because small slips can invalidate the final numeric answer.,"Logical Reasoning, Working Memory, Planning, Adaptive Error Correction"
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a high-difficulty multiple-choice science benchmark curated to be “Google-proof,” focusing on questions that require expert reasoning rather than simple lookup. The Diamond subset emphasizes high-quality items where experts reliably answer correctly and non-experts often fail.","Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory"
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends broad academic knowledge testing into many non-English languages, evaluating whether models can reason and answer across subjects under multilingual prompts. It stresses robust multilingual understanding and the ability to maintain consistent competence across languages and topics.","Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Language Production"
MMMU,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2311.16502,"MMMU measures expert-level multimodal understanding across many disciplines by requiring reasoning over text plus images such as charts, diagrams, and technical figures. It emphasizes interpreting visual evidence, aligning it with textual context, and selecting answers that follow from both modalities.","Visual Perception, Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Working Memory"
LAB-Bench FigQA,https://huggingface.co/datasets/futurehouse/lab-bench,https://arxiv.org/abs/2407.10362,"LAB-Bench FigQA evaluates whether models can read and reason about complex scientific figures from biology papers, including plots, multi-panel diagrams, and annotations. The benchmark stresses extracting quantitative/qualitative relationships from visuals and connecting them to domain context to answer questions accurately.","Visual Perception, Scene Understanding & Visual Reasoning, Attention, Working Memory, Logical Reasoning, Semantic Understanding & Context Recognition"
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning evaluates model ability to answer reasoning questions grounded in scientific-paper visuals, often requiring careful reading of chart elements (axes, legends, markers) and linking them to textual descriptions. It stresses precise visual interpretation and multi-step inference rather than surface recognition.","Visual Perception, Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Working Memory, Logical Reasoning, Semantic Understanding & Context Recognition"
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU extends multimodal reasoning to videos, requiring models to integrate information across time and answer questions about events, actions, and causality. It stresses temporal integration, tracking state changes, and combining visual cues with language-based reasoning.","Visual Perception, Attention, Working Memory, Cognitive Timing & Predictive Modeling, Scene Understanding & Visual Reasoning, Multisensory Integration"
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"FACTS Benchmark Suite systematically evaluates factuality, including tendencies to hallucinate, to over-commit to uncertain claims, or to produce inconsistent statements across contexts. It stresses reliable grounding in provided evidence and disciplined handling of uncertainty during generation.","Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning, Inhibitory Control, Self-reflection"
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA evaluates physical commonsense and everyday interaction understanding across languages, focusing on whether models can reason about plausible actions and outcomes in the physical world. It stresses intuitive physics, affordances, and culturally/language-robust commonsense selection.","Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning, Spatial Representation & Mapping, Cognitive Timing & Predictive Modeling"
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) tests multi-round coreference resolution in long contexts by embedding repeated, similar “needle” interactions inside large “haystacks” and asking the model to retrieve the correct referenced content. It stresses precise long-context tracking and resisting interference from near-duplicate distractors.","Working Memory, Attention, Episodic Memory, Semantic Understanding & Context Recognition, Logical Reasoning"
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval evaluates economically valuable, well-specified professional knowledge-work tasks across many occupations using expert human judging and head-to-head comparisons. Tasks often require producing complete work artifacts (e.g., plans, spreadsheets, presentations) with correctness, formatting, and decision quality.","Planning, Decision-making, Language Production, Working Memory, Adaptive Error Correction, Semantic Understanding & Context Recognition, Social Reasoning & Theory of Mind"
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks evaluates whether models can follow and compute properties of graph traversals described in text (e.g., walking edges, retrieving parents, or performing BFS-like steps). It stresses structured state tracking, stepwise traversal logic, and robustness to long sequences of interdependent steps.","Working Memory, Logical Reasoning, Planning, Attention, Spatial Representation & Mapping"
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon evaluates tool-using agents across diverse tool ecosystems where success requires selecting appropriate tools, constructing correct calls, and composing multi-tool workflows. It stresses reliability under tool errors, adherence to tool schemas, and the ability to recover and continue toward a goal.","Planning, Decision-making, Working Memory, Adaptive Error Correction, Inhibitory Control, Semantic Understanding & Context Recognition"
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath evaluates advanced mathematics reasoning at or near research difficulty, often requiring deep multi-step derivations and careful handling of edge cases. It stresses sustained abstraction, rigorous validation of intermediate steps, and resisting brittle shortcuts that fail on hard problems.","Logical Reasoning, Working Memory, Planning, Cognitive Flexibility, Adaptive Error Correction"
