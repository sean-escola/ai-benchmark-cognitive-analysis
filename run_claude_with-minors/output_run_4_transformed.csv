Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates software engineering agents by asking them to produce patches that fix real issues in open-source repositories, with solutions verified as correct by a robust, human-validated harness. It emphasizes end-to-end debugging and implementation under realistic repo constraints rather than isolated code generation.","L1: 
L2: Planning, Decision-making, Logical Reasoning, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition
L3: ",L2
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 measures an agent’s ability to accomplish real tasks in a command-line environment (e.g., navigating files, running tools, installing dependencies, and diagnosing failures). Success requires iterative action selection with feedback from program outputs and errors.","L1: 
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition
L3: Inhibitory Control",L3
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp evaluates deep research behavior where models browse a controlled document collection to answer difficult questions that require multi-step information gathering. It stresses query formulation, evidence synthesis, and resisting premature conclusions when partial evidence is encountered.","L1: 
L2: Planning, Decision-making, Attention, Working Memory, Semantic Understanding & Context Recognition
L3: Self-reflection",L3
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench evaluates policy-following customer support agents interacting with simulated users and APIs across domains like retail, airline, and telecom. It emphasizes maintaining consistency with domain rules over multi-turn conversations while still solving user problems.","L1: Language Comprehension, Language Production
L2: Decision-making, Working Memory
L3: Social Reasoning & Theory of Mind, Inhibitory Control",L3
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld benchmarks multimodal “computer use” agents that must complete tasks on a real operating system using screenshots and interaction tools within a step budget. It evaluates grounding (reading UI state), planning action sequences, and recovering from UI or tool errors.","L1: Visual Perception
L2: Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Planning, Decision-making, Sensorimotor Coordination
L3: ",L2
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,ARC-AGI measures fluid reasoning by requiring models to infer latent rules from a handful of input-output grid examples and apply them to new grids. It is designed to reduce reliance on memorized knowledge and instead test flexible abstraction and generalization.,"L1: 
L2: Logical Reasoning, Working Memory, Attention, Spatial Representation & Mapping, Planning
L3: Cognitive Flexibility",L3
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon agent coherence and strategy by running a simulated vending machine business over an extended period with many decisions and changing conditions. Performance depends on sustained goal pursuit (profit), negotiation, inventory control, and adaptation.","L1: 
L2: Planning, Decision-making, Working Memory, Reward Mechanisms, Adaptive Error Correction
L3: Self-reflection",L3
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas measures real-world tool use via the Model Context Protocol, requiring models to discover tools, call them correctly, handle failures, and compose multi-step workflows across servers. It targets practical API literacy and robust execution rather than purely linguistic skill.","L1: 
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition
L3: Inhibitory Control",L3
FinanceAgent,https://www.vals.ai/benchmarks/finance_agent,,"FinanceAgent evaluates agent performance on tasks typical of an entry-level financial analyst, such as interpreting financial documents, building analyses, and producing structured outputs. It stresses quantitative reasoning, domain knowledge application, and careful synthesis under constraints.","L1: Language Production
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Planning, Decision-making, Working Memory
L3: ",L2
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity agents on tasks spanning vulnerability identification, exploitation reasoning, and in some settings discovering new vulnerabilities in real-world codebases. It rewards systematic investigation, hypothesis testing, and safe iteration when initial attempts fail.","L1: 
L2: Logical Reasoning, Planning, Decision-making, Adaptive Error Correction, Working Memory
L3: Inhibitory Control",L3
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"SpreadsheetBench measures the ability to understand and manipulate complex spreadsheets, often requiring multi-step transformations, formula handling, and verification of results. It reflects realistic office workflows where correctness, formatting, and dependency awareness matter.","L1: 
L2: Working Memory, Planning, Decision-making, Adaptive Error Correction, Semantic Understanding & Context Recognition, Attention
L3: ",L2
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a large, difficult benchmark spanning expert-level questions (often multimodal) intended to probe frontier knowledge and reasoning. It stresses integrating information, avoiding hallucinations, and producing defensible answers under uncertainty.","L1: Language Comprehension, Language Production, Visual Perception
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory
L3: ",L2
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 evaluates mathematical problem solving on competition-style questions that require multi-step derivations and careful algebraic or combinatorial reasoning. It is sensitive to logical consistency, intermediate-step accuracy, and symbolic manipulation.","L1: 
L2: Logical Reasoning, Working Memory, Planning, Adaptive Error Correction
L3: ",L2
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a hard multiple-choice science benchmark designed to be “Google-proof,” focusing on questions that require genuine understanding rather than retrieval. It tests disciplined reasoning over complex scientific content with distractors that punish shallow heuristics.","L1: Language Comprehension
L2: Logical Reasoning, Working Memory, Semantic Understanding & Context Recognition
L3: Inhibitory Control",L3
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends broad academic knowledge evaluation across many subjects and multiple languages, measuring both knowledge and reasoning in multilingual settings. It probes how well models generalize semantic understanding and logic beyond English.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory
L3: ",L2
MMMU,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2311.16502,"MMMU evaluates expert-level multimodal understanding across disciplines by requiring reasoning over images plus text (e.g., diagrams, charts, and domain figures). It emphasizes grounding, cross-modal integration, and structured inference rather than captioning alone.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Working Memory, Visual Attention & Eye Movements
L3: ",L2
LAB-Bench FigQA,https://huggingface.co/datasets/futurehouse/lab-bench,https://arxiv.org/abs/2407.10362,"LAB-Bench FigQA tests whether models can answer questions about complex scientific figures from biology papers, requiring careful reading of plots, labels, and experimental contexts. It rewards precise visual interpretation linked to domain reasoning and scientific language.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Attention
L3: ",L2
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning evaluates reasoning over scientific papers (often with technical notation) by asking questions that require linking claims, methods, and implications. It stresses faithful comprehension, multi-step inference, and synthesis across long technical contexts.","L1: Language Comprehension
L2: Logical Reasoning, Working Memory, Semantic Understanding & Context Recognition, Attention
L3: ",L2
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU evaluates multimodal reasoning over videos, requiring models to integrate information across time to answer questions about events, actions, and visual evidence. It tests temporal integration beyond single-frame perception and demands coherent cross-frame inference.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Working Memory, Attention, Multisensory Integration
L3: Cognitive Timing & Predictive Modeling",L3
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"FACTS Benchmark Suite systematically evaluates factuality and grounding across a collection of tasks that measure whether model outputs are supported by sources or reality constraints. It targets error avoidance, uncertainty handling, and resisting plausible-sounding fabrication.","L1: Language Production
L2: Semantic Understanding & Context Recognition, Decision-making, Working Memory
L3: Inhibitory Control, Self-reflection",L3
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA benchmarks practical reasoning about everyday physical and commonsense situations across many languages and cultural contexts. It focuses on selecting the most plausible solution or explanation for a scenario, emphasizing robust generalization.","L1: Language Comprehension
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory
L3: ",L2
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,MRCR v2 (8-needle) evaluates long-context retrieval and multi-round coreference by embedding multiple similar “needles” in long “haystacks” and asking the model to reproduce the correct referenced content. It stresses maintaining and indexing relevant details across extended contexts with many confounders.,"L1: 
L2: Working Memory, Attention, Episodic Memory, Semantic Understanding & Context Recognition
L3: Inhibitory Control",L3
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval measures performance on well-specified professional knowledge-work tasks spanning many occupations, judged by expert humans and focused on producing real work artifacts. It emphasizes planning, structured execution, and aligning outputs to practical constraints and evaluation criteria.","L1: Language Production
L2: Planning, Decision-making, Semantic Understanding & Context Recognition, Working Memory
L3: Self-reflection",L3
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks evaluates structured reasoning over graph problems (e.g., traversals and relational queries) that require following explicit constraints across many steps. It probes whether models can reliably maintain state and perform algorithmic-style inference without losing track of prior nodes.","L1: 
L2: Spatial Representation & Mapping, Logical Reasoning, Working Memory, Attention, Planning
L3: ",L2
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon evaluates general-purpose tool-using agents across diverse tasks that require selecting tools, calling them correctly, and combining results into final answers. It emphasizes robustness to tool errors, workflow composition, and disciplined control of multi-step execution.","L1: 
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory, Attention
L3: Inhibitory Control",L3
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath evaluates expert-level mathematics with problems designed to be difficult for models and resistant to memorization, often requiring deep multi-step derivations. It stresses precise symbolic reasoning, long chains of dependency, and careful checking of intermediate results.","L1: 
L2: Logical Reasoning, Working Memory, Planning, Adaptive Error Correction
L3: Inhibitory Control",L3
