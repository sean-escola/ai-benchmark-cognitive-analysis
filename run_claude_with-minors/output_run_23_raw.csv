Benchmark,Website,Paper,Description,Cognitive Functions
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates whether a model can fix real-world GitHub issues by generating a code patch that makes the project’s tests pass. The “Verified” subset contains problems that were manually confirmed to be solvable, and is commonly used to measure end-to-end software engineering and debugging ability under realistic repository context.","Planning, Decision-making, Logical Reasoning, Working Memory, Adaptive Error Correction, Language Comprehension"
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 evaluates agentic problem-solving in a command-line environment, where models must execute shell commands, inspect files, and iteratively repair mistakes to complete tasks. It emphasizes interactive, tool-mediated workflows and robustness to environment feedback and errors.","Planning, Decision-making, Working Memory, Adaptive Error Correction, Language Comprehension, Sensorimotor Coordination, Attention"
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp measures deep-research performance by requiring models to locate and synthesize evidence from a curated web document collection, typically via search and retrieval tools. Tasks reward accurate attribution, multi-step information gathering, and coherent synthesis across multiple sources.","Planning, Decision-making, Attention, Working Memory, Semantic Understanding & Context Recognition, Language Comprehension, Language Production"
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench evaluates interactive customer-support agents operating in simulated domains (e.g., retail, airline, telecom) with policies and tool/API calls. Agents must follow domain-specific rules, manage multi-turn dialogue, and complete user goals reliably despite ambiguity and policy constraints.","Social Reasoning & Theory of Mind, Decision-making, Planning, Inhibitory Control, Working Memory, Language Comprehension, Language Production"
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld evaluates multimodal computer-use agents on tasks performed in a full operating-system environment (e.g., navigating GUIs, filling forms, configuring settings). Success depends on perceiving on-screen state, selecting correct actions, and recovering from UI or tool errors over long action sequences.","Visual Perception, Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Planning, Decision-making, Working Memory, Sensorimotor Coordination, Adaptive Error Correction"
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI is a “fluid intelligence” benchmark of abstract pattern induction on grid transformation tasks, where models infer a hidden rule from a few examples and apply it to new inputs. It is designed to probe generalization to novel problems rather than memorized knowledge.","Logical Reasoning, Working Memory, Cognitive Flexibility, Spatial Representation & Mapping, Attention"
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon agent coherence and business decision-making in a simulated vending-machine enterprise over many steps. Agents must plan inventory and pricing, negotiate or purchase supplies, and adapt strategies to maximize long-run outcomes.","Planning, Decision-making, Working Memory, Reward Mechanisms, Cognitive Timing & Predictive Modeling, Adaptive Error Correction, Self-reflection"
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas evaluates real-world tool use via the Model Context Protocol (MCP), requiring models to discover relevant tools, call them correctly, handle failures, and compose multi-step workflows across services. It targets practical API competence and reliable orchestration in production-like setups.","Planning, Decision-making, Working Memory, Adaptive Error Correction, Attention, Language Comprehension"
FinanceAgent,https://www.vals.ai/benchmarks/finance_agent,,"FinanceAgent evaluates tasks expected of an entry-level financial analyst, such as interpreting financial documents, performing calculations, producing structured analyses, and (often) using tools. It emphasizes applied reasoning and producing decision-relevant outputs under domain constraints.","Logical Reasoning, Semantic Understanding & Context Recognition, Planning, Decision-making, Working Memory, Language Production, Language Comprehension"
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,CyberGym evaluates cybersecurity agent capabilities on vulnerability identification and vulnerability discovery tasks grounded in real software projects. It tests whether models can reason about codebases and security hints to find flaws and produce correct exploit-relevant conclusions or fixes.,"Logical Reasoning, Planning, Working Memory, Adaptive Error Correction, Inhibitory Control, Attention"
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"SpreadsheetBench evaluates a model’s ability to understand and manipulate complex spreadsheets, often requiring multi-step edits, formula reasoning, and consistent structuring. Tasks resemble real workplace spreadsheet operations where small mistakes can propagate across dependent cells.","Logical Reasoning, Working Memory, Planning, Adaptive Error Correction, Semantic Understanding & Context Recognition, Attention"
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam (HLE) is a multimodal benchmark intended to probe frontier academic reasoning across a wide variety of difficult questions. It rewards correct answers under high uncertainty and often tests the ability to integrate knowledge, reasoning, and (when enabled) tool-based verification.","Language Comprehension, Language Production, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Visual Perception"
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 is a competition-math benchmark derived from the American Invitational Mathematics Examination problems. It focuses on multi-step symbolic reasoning, careful arithmetic, and solution verification under constrained output formats.","Logical Reasoning, Working Memory, Cognitive Flexibility, Attention"
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a subset of the GPQA benchmark containing especially high-quality, graduate-level multiple-choice science questions that are hard to answer via shallow pattern matching. It targets deep scientific reasoning and precise discrimination among plausible options.","Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Attention"
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends MMLU into many languages, testing whether a model’s knowledge and reasoning transfer across linguistic contexts. It probes robust multilingual understanding of academic topics and the ability to select correct answers under varied language phrasing.","Language Comprehension, Semantic Understanding & Context Recognition, Working Memory, Attention"
MMMU,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2311.16502,"MMMU evaluates expert-level multimodal understanding across many disciplines, combining images (e.g., diagrams, plots, figures) with text questions. It tests whether models can ground language in visual evidence and perform reasoning over both modalities.","Visual Perception, Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Language Comprehension, Logical Reasoning, Working Memory, Multisensory Integration"
LAB-Bench FigQA,https://huggingface.co/datasets/futurehouse/lab-bench,https://arxiv.org/abs/2407.10362,"LAB-Bench FigQA evaluates whether models can answer questions about complex scientific figures from biology papers, often requiring careful reading of axes, legends, and experimental layouts. It emphasizes figure-grounding, scientific interpretation, and avoiding plausible-but-unsupported claims.","Visual Perception, Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory"
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning evaluates reasoning over scientific charts and figures associated with papers, requiring models to extract quantitative/relational information and answer structured questions. It stresses precise visual-text grounding and multi-step inference rather than generic captioning.","Visual Perception, Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Logical Reasoning, Working Memory, Language Comprehension"
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU extends multimodal reasoning to video, requiring models to track events, states, and causal relations over time. It probes temporal integration of visual evidence with text questions, often demanding cross-frame consistency and long-range dependency tracking.","Visual Perception, Attention, Working Memory, Scene Understanding & Visual Reasoning, Cognitive Timing & Predictive Modeling, Multisensory Integration"
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"FACTS Benchmark Suite evaluates factuality and groundedness across a collection of tasks that stress precise claim generation, attribution, and resistance to hallucination. It aims to measure whether a model can remain faithful to sources and avoid fabricating details when uncertain.","Semantic Understanding & Context Recognition, Inhibitory Control, Self-reflection, Working Memory, Language Production, Language Comprehension"
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,Global PIQA evaluates physical commonsense reasoning—choosing actions or explanations consistent with everyday physics—across multiple languages in a non-parallel setting. It tests whether models maintain consistent intuitive physics and commonsense across linguistic variation.,"Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning, Spatial Representation & Mapping, Working Memory"
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,MRCR v2 (8-needle) tests long-context multi-round coreference and retrieval by embedding multiple similar “needle” requests into large “haystack” conversations and requiring the model to reproduce the correct referenced response. It stresses robust tracking of entities and instructions across very long contexts.,"Working Memory, Attention, Episodic Memory, Language Comprehension, Semantic Understanding & Context Recognition"
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval evaluates economically valuable professional knowledge work across many occupations by asking models to produce real deliverables (e.g., spreadsheets, presentations, plans) judged against human professionals. It emphasizes end-to-end task execution quality, structure, and practical decision support.","Planning, Decision-making, Language Production, Semantic Understanding & Context Recognition, Working Memory, Social Reasoning & Theory of Mind"
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks evaluates a model’s ability to follow graph-structured relationships (e.g., traversals, parent/neighbor queries) described in text, often over long contexts. It targets systematic traversal and consistent bookkeeping under distractors and near-duplicate nodes.","Spatial Representation & Mapping, Working Memory, Logical Reasoning, Attention, Semantic Understanding & Context Recognition"
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon evaluates tool-using agents on diverse multi-step tasks that require selecting among many tools, calling them correctly, and composing results into a final answer. It probes reliability under tool errors, interface variation, and long action chains.","Planning, Decision-making, Working Memory, Adaptive Error Correction, Attention, Language Comprehension, Language Production"
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath is an advanced mathematics benchmark intended to measure progress on expert-level problems beyond routine competition math, often requiring creative multi-step reasoning. It targets deep mathematical problem solving and verification under tight correctness standards.","Logical Reasoning, Working Memory, Cognitive Flexibility, Attention"
