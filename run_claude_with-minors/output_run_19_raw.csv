Benchmark,Website,Paper,Description,Cognitive Functions
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates models on real GitHub issues by asking them to generate a patch that fixes a bug or implements a small feature, with solutions validated via the project’s tests. The “Verified” subset emphasizes human-vetted, solvable tasks and standardized evaluation pipelines to measure reliable end-to-end software engineering performance.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Logical Reasoning, Language Comprehension, Language Production"
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 measures an agent’s ability to complete practical command-line tasks in isolated environments (e.g., using shell tools, editing files, running programs, and debugging). It emphasizes iterative problem solving under tool constraints, where errors must be detected and corrected through interaction with the environment.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Logical Reasoning, Attention"
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp evaluates “deep research” by requiring models to answer questions using information spread across a fixed web-style corpus, typically through search and browsing actions. It tests whether an agent can find relevant sources, integrate evidence, and produce a supported final answer rather than relying on memorized knowledge.","Planning, Decision-making, Attention, Working Memory, Semantic Understanding & Context Recognition, Language Comprehension, Language Production"
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench evaluates agentic customer-support behavior in simulated domains (e.g., retail, airline, telecom), where the model must follow policies, use tools/APIs, and maintain coherent multi-turn interaction. Success requires balancing user goals with constraints and consistently applying rules across long dialogues.","Social Reasoning & Theory of Mind, Decision-making, Inhibitory Control, Planning, Working Memory, Language Comprehension, Language Production"
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld benchmarks multimodal “computer use” by asking agents to complete tasks in an operating-system-like environment via screenshots and tool actions (clicking, typing, navigating). It stresses grounded interface understanding and robust action sequencing over multiple steps in dynamic GUIs.","Visual Perception, Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Sensorimotor Coordination, Planning, Decision-making, Working Memory"
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,ARC-AGI presents few-shot grid transformation puzzles where the system must infer an abstract rule from a small set of input–output examples and apply it to a new input. The benchmark targets generalization to novel patterns and rule induction with minimal data.,"Logical Reasoning, Cognitive Flexibility, Working Memory, Spatial Representation & Mapping, Attention"
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon autonomous agency in a simulated business setting, where the agent manages a vending-machine operation over extended time (e.g., inventory, pricing, supplier negotiation). Performance is measured by final outcomes (e.g., profitability), requiring sustained coherence and strategy under uncertainty.","Planning, Decision-making, Reward Mechanisms, Working Memory, Adaptive Error Correction, Self-reflection"
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas evaluates real-world tool use via the Model Context Protocol by requiring models to discover tools, call APIs correctly, handle errors, and compose multi-step workflows across services. It targets reliable execution in production-like tool ecosystems rather than single-call function invocation.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Attention, Language Comprehension, Language Production"
FinanceAgent,https://www.vals.ai/benchmarks/finance_agent,,"FinanceAgent assesses how well models perform tasks typical of an entry-level financial analyst, such as interpreting financial documents, performing calculations, and producing structured analyses. It emphasizes professional reasoning over numbers, assumptions, and domain conventions to generate decision-relevant outputs.","Logical Reasoning, Decision-making, Planning, Working Memory, Semantic Understanding & Context Recognition, Language Production"
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity agent capabilities on real-world vulnerability tasks, including identifying known weaknesses from descriptions and discovering new vulnerabilities in codebases. It requires systematic exploration, hypothesis testing, and accurate technical reasoning under realistic constraints.","Logical Reasoning, Planning, Adaptive Error Correction, Working Memory, Attention"
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"SpreadsheetBench measures the ability to understand, edit, and compute with complex spreadsheets derived from realistic scenarios, often requiring formula creation, data cleaning, and multi-step transformations. It stresses precise manipulation, consistency across sheets, and correctness of computed outputs.","Working Memory, Planning, Logical Reasoning, Adaptive Error Correction, Attention, Language Comprehension"
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a multimodal benchmark designed to probe frontier academic reasoning and knowledge across many disciplines and formats (text, figures, diagrams). Questions often demand synthesis, careful constraint handling, and nontrivial inference rather than recall.","Language Comprehension, Language Production, Logical Reasoning, Working Memory, Scene Understanding & Visual Reasoning, Visual Perception, Semantic Understanding & Context Recognition"
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 comprises competition-style mathematics problems that require multi-step derivations, symbolic manipulation, and careful case analysis to produce exact answers. It is commonly used to measure mathematical reasoning without reliance on external tools.","Logical Reasoning, Working Memory, Attention, Planning"
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a difficult multiple-choice science QA benchmark curated to be “Google-proof,” emphasizing questions that non-experts usually miss. It targets robust scientific reasoning and deep conceptual understanding under time/format constraints.","Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Attention, Decision-making"
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends broad academic testing to multiple languages, evaluating whether models can answer subject-matter questions across diverse linguistic contexts. It probes multilingual understanding, cross-lingual knowledge transfer, and consistent reasoning across languages.","Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory"
MMMU,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2311.16502,"MMMU evaluates expert-level multimodal understanding by requiring models to answer questions that combine text with images such as charts, diagrams, and figures. It emphasizes integration of visual evidence with language-based reasoning across many domains.","Visual Perception, Scene Understanding & Visual Reasoning, Multisensory Integration, Language Comprehension, Logical Reasoning, Working Memory, Visual Attention & Eye Movements"
LAB-Bench FigQA,https://huggingface.co/datasets/futurehouse/lab-bench,https://arxiv.org/abs/2407.10362,LAB-Bench FigQA tests whether models can correctly interpret complex scientific figures from biology papers and answer questions grounded in those visuals. It focuses on extracting quantitative/structural information from figures and applying domain reasoning to reach correct conclusions.,"Visual Perception, Scene Understanding & Visual Reasoning, Multisensory Integration, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Visual Attention & Eye Movements"
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning evaluates complex reasoning grounded in scientific documents (e.g., interpreting research content that may include charts, tables, and technical passages) and answering validation-split questions. It stresses faithful extraction of evidence and multi-step inference over specialized scientific contexts.","Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Scene Understanding & Visual Reasoning, Visual Perception, Attention"
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU measures multimodal reasoning over video by asking questions that require integrating visual events across time with textual understanding. It targets temporal scene comprehension, tracking of entities/actions, and coherent summarization or inference from multi-frame evidence.","Visual Perception, Visual Attention & Eye Movements, Working Memory, Cognitive Timing & Predictive Modeling, Scene Understanding & Visual Reasoning, Multisensory Integration, Attention"
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"FACTS Benchmark Suite systematically evaluates factuality and grounding, measuring whether model outputs are supported by provided sources or true world knowledge under controlled settings. It aims to detect hallucinations, unsupported assertions, and failures to maintain factual consistency across responses.","Semantic Understanding & Context Recognition, Inhibitory Control, Working Memory, Language Production, Attention"
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA evaluates physical commonsense reasoning across many languages, focusing on everyday interaction scenarios and what actions or outcomes are plausible. It tests whether models maintain consistent physical intuition and reasoning when the same underlying problem is expressed in different languages.","Logical Reasoning, Semantic Understanding & Context Recognition, Spatial Representation & Mapping, Language Comprehension, Working Memory"
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,MRCR v2 (8-needle) evaluates long-context retrieval and multi-round coreference by embedding multiple similar “needle” interactions within a large “haystack” and asking the model to reproduce the correct referenced content. The 8-needle setting increases distractors and stresses precise selection under heavy context load.,"Working Memory, Attention, Semantic Understanding & Context Recognition, Inhibitory Control"
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval evaluates well-specified professional knowledge-work tasks across many occupations, often requiring the production of concrete artifacts (e.g., spreadsheets, plans, written deliverables) judged by experts. It is designed to measure economically relevant task completion quality rather than narrow academic accuracy.","Planning, Decision-making, Language Production, Semantic Understanding & Context Recognition, Working Memory, Self-reflection"
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks tests structured reasoning over graphs described in text, requiring models to follow paths, track relations, and answer queries about graph traversal or ancestry-like structure. It probes whether models can maintain and manipulate discrete relational state across many steps.","Spatial Representation & Mapping, Logical Reasoning, Working Memory, Attention, Planning"
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon evaluates general tool-using agents on diverse multi-step tasks that require selecting appropriate tools, executing calls, and integrating results into a final solution. It emphasizes reliability under tool errors, correct sequencing, and adherence to task constraints across heterogeneous environments.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Attention, Language Comprehension, Language Production"
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath is a set of advanced mathematics problems intended to test reasoning at the edge of current model capability, including proof-like and research-style problem solving. It measures sustained multi-step deduction and careful quantitative correctness, often benefiting from structured scratch work.","Logical Reasoning, Working Memory, Planning, Attention"
