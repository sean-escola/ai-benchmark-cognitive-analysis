Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates agentic software engineering on real GitHub issues by requiring a model to produce a patch that makes a hidden test suite pass. The “Verified” subset is manually curated to ensure tasks are solvable and that evaluation is reliable, emphasizing end-to-end debugging and code changes rather than short code snippets.","L1: Language Comprehension, Language Production
L2: Planning, Logical Reasoning, Adaptive Error Correction, Working Memory
L3: ",L2
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 measures an agent’s ability to solve realistic tasks inside a command-line environment (e.g., manipulating files, running programs, configuring tools, and diagnosing failures). It stresses iterative trial-and-error with feedback from tool outputs under time/step constraints.","L1: Language Comprehension
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory
L3: ",L2
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp evaluates web-browsing research agents on questions that require finding, verifying, and synthesizing information from documents discovered via search/browsing. It emphasizes source-grounded answering, multi-hop retrieval, and maintaining task context across many pages or snippets.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Attention, Working Memory, Semantic Understanding & Context Recognition
L3: ",L2
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench measures an agent’s ability to complete multi-turn customer-support workflows by interacting with simulated users and APIs while following domain policies (e.g., retail, airline, telecom). Success depends on maintaining state across turns, calling tools correctly, and balancing helpfulness with constraints.","L1: Language Comprehension, Language Production
L2: Decision-making, Planning, Working Memory
L3: Social Reasoning & Theory of Mind, Empathy, Inhibitory Control",L3
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld tests computer-use agents in a realistic desktop environment where they must complete tasks through GUI interaction (clicking, typing, navigating apps, and interpreting screens). It stresses visual grounding and long-horizon action sequencing under partial observability and interface changes.","L1: Visual Perception
L2: Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Planning, Decision-making, Working Memory, Sensorimotor Coordination, Spatial Representation & Mapping
L3: ",L2
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI evaluates “fluid” reasoning by giving a few input–output grid examples and asking the model to infer the hidden transformation rule for a new input. The tasks are designed to be novel and compositional, emphasizing generalization from very small data rather than memorized knowledge.","L1: 
L2: Logical Reasoning, Working Memory, Attention, Spatial Representation & Mapping
L3: Cognitive Flexibility",L3
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon autonomous decision-making by simulating management of a vending-machine business over an extended period with many sequential choices. The score reflects the agent’s ability to plan, adapt to market dynamics, and stay coherent across thousands of steps.","L1: 
L2: Planning, Decision-making, Reward Mechanisms, Working Memory, Adaptive Error Correction, Semantic Understanding & Context Recognition
L3: Self-reflection",L3
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas evaluates real-world tool use via the Model Context Protocol, requiring models to discover and call tools across multi-step workflows and integrate results into a correct final response. It emphasizes robust execution (correct parameters, error handling, retries) over purely verbal reasoning.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory
L3: ",L2
FinanceAgent,https://www.vals.ai/benchmarks/finance_agent,,"FinanceAgent measures performance on tasks typical of an entry-level financial analyst, such as analysis, reasoning over financial documents, and producing structured deliverables. It emphasizes correctness, defensible assumptions, and consistent quantitative reasoning across multi-step workflows.","L1: Language Production
L2: Logical Reasoning, Planning, Decision-making, Working Memory, Semantic Understanding & Context Recognition
L3: ",L2
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity capabilities on real-world vulnerability tasks, including identifying known vulnerabilities from descriptions and discovering new ones. It stresses careful reasoning about code behavior, reproducing issues, and proposing effective fixes or exploit paths under realistic constraints.","L1: Language Comprehension
L2: Logical Reasoning, Planning, Adaptive Error Correction, Working Memory, Decision-making
L3: ",L2
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"SpreadsheetBench evaluates an agent’s ability to understand, edit, and compute with complex spreadsheets derived from real-world scenarios. Tasks require correct formulas, structured organization, and iterative correction when outputs don’t match requirements.","L1: 
L2: Logical Reasoning, Working Memory, Planning, Adaptive Error Correction, Semantic Understanding & Context Recognition, Decision-making
L3: ",L2
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam (HLE) is a large, frontier-level benchmark spanning difficult questions across many domains and modalities, intended to probe the limits of general reasoning and knowledge. Many questions require multi-step inference, careful reading, and (in some setups) effective use of tools like search or code to verify claims.","L1: Language Comprehension, Visual Perception
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Multisensory Integration, Planning, Decision-making
L3: ",L2
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 is a competition-math benchmark drawn from the American Invitational Mathematics Examination, featuring short-answer problems requiring nontrivial derivations. It emphasizes precise multi-step symbolic reasoning and error-free arithmetic or algebraic manipulation.","L1: 
L2: Logical Reasoning, Working Memory, Adaptive Error Correction, Planning
L3: ",L2
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,GPQA Diamond is a high-quality subset of GPQA consisting of challenging graduate-level science multiple-choice questions designed to be difficult to answer by superficial pattern matching. It emphasizes deep domain understanding and careful reasoning to discriminate among plausible distractors.,"L1: 
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory
L3: Inhibitory Control",L3
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends broad academic knowledge testing to multiple languages, measuring how well models maintain reasoning and subject knowledge beyond English. It stresses multilingual comprehension and consistent mapping of concepts across languages and domains.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Working Memory, Logical Reasoning
L3: ",L2
MMMU,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2311.16502,"MMMU evaluates expert-level multimodal understanding by combining text with images (diagrams, charts, scientific visuals) across many disciplines. Models must integrate visual evidence with written context to answer questions requiring domain reasoning and visual interpretation.","L1: Visual Perception, Language Comprehension
L2: Multisensory Integration, Scene Understanding & Visual Reasoning, Logical Reasoning, Working Memory
L3: ",L2
LAB-Bench FigQA,https://huggingface.co/datasets/futurehouse/lab-bench,https://arxiv.org/abs/2407.10362,LAB-Bench FigQA tests whether models can correctly interpret complex scientific figures from biology papers and answer targeted questions about experimental results and visual evidence. It emphasizes figure-grounded reasoning rather than recalling facts from memory alone.,"L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory
L3: ",L2
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning evaluates reasoning over scientific paper visuals (e.g., charts/figures) paired with textual prompts, often requiring quantitative or relational interpretation. The benchmark targets faithful extraction of information from figures and multi-step inference grounded in that evidence.","L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Logical Reasoning, Working Memory, Semantic Understanding & Context Recognition
L3: ",L2
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU extends multimodal academic evaluation to videos, requiring temporal understanding of events, actions, and changes over time in addition to text. It stresses integrating information across multiple frames/clips and maintaining coherence over longer sequences.","L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Working Memory, Attention, Multisensory Integration
L3: Cognitive Timing & Predictive Modeling",L3
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"The FACTS Benchmark Suite systematically evaluates LLM factuality across diverse tasks, focusing on whether generated statements are supported by given sources or by verifiable world knowledge (depending on the subtest). It emphasizes resisting hallucination, tracking evidence, and calibrating uncertainty when information is missing.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Working Memory, Logical Reasoning
L3: Inhibitory Control, Self-reflection",L3
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA evaluates physical commonsense and everyday interaction reasoning across many languages, using non-parallel multilingual data to test true cross-lingual generalization. Questions typically require choosing the more plausible action or outcome in real-world physical situations.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Spatial Representation & Mapping
L3: ",L2
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) is a long-context, multi-round coreference retrieval benchmark where multiple similar “needle” requests are embedded in a long “haystack” conversation and the model must reproduce the correct response for a specified needle. It stresses precise reference tracking and robustness to distractors as context length increases.","L1: 
L2: Working Memory, Attention, Episodic Memory, Semantic Understanding & Context Recognition, Logical Reasoning
L3: Inhibitory Control",L3
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval evaluates economically relevant knowledge work by asking models to produce realistic professional deliverables across many occupations (e.g., presentations, spreadsheets, schedules, analyses) judged by expert humans. It emphasizes end-to-end task completion quality, adherence to requirements, and usefulness under real workplace constraints.","L1: Language Production
L2: Planning, Decision-making, Semantic Understanding & Context Recognition, Working Memory, Adaptive Error Correction
L3: Social Reasoning & Theory of Mind",L3
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks evaluates long-context reasoning over graph-structured data described in text, requiring the model to follow edges/paths and answer queries such as reachability or parent relationships. It stresses systematic traversal under distraction and careful state tracking across many nodes.","L1: 
L2: Spatial Representation & Mapping, Working Memory, Logical Reasoning, Attention, Planning
L3: ",L2
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon measures tool-using agents on multi-step tasks that require selecting among many tools, invoking them correctly, and composing results into a final answer. It emphasizes robust orchestration, recovery from tool errors, and maintaining a coherent plan across calls.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory
L3: ",L2
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath evaluates advanced mathematics at or beyond typical competition levels, with tiers targeting progressively harder problems and (in some settings) permitting computation tools. It emphasizes deep multi-step reasoning, rigorous constraint handling, and avoiding subtle algebraic or logical mistakes.","L1: 
L2: Logical Reasoning, Working Memory, Planning, Adaptive Error Correction
L3: Inhibitory Control",L3
