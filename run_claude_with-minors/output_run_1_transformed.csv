Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates whether a model can solve real GitHub issues by generating a correct patch in an existing Python codebase and passing a human-validated test suite. The “Verified” subset focuses on problems confirmed solvable and aims to reduce noise from ambiguous tasks or incorrect tests, emphasizing reliable end-to-end software engineering.","L1: Language Comprehension
L2: Planning, Decision-making, Logical Reasoning, Working Memory, Adaptive Error Correction
L3: ",L2
SWE-bench Multilingual,https://www.swebench.com/multilingual.html,https://arxiv.org/abs/2504.21798,"SWE-bench Multilingual extends the SWE-bench paradigm beyond Python to multiple programming languages, assessing whether models can navigate different tooling, libraries, and language idioms while fixing real repository issues. It emphasizes robust transfer of software-engineering behaviors across language ecosystems rather than overfitting to one stack.","L1: Language Comprehension
L2: Planning, Decision-making, Logical Reasoning, Working Memory, Adaptive Error Correction
L3: Cognitive Flexibility",L3
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro is a larger and more difficult software engineering benchmark designed to be more contamination-resistant and industrially realistic, including tasks across several programming languages and tougher bug-fix/feature contexts. It stresses multi-step repo understanding, test-driven iteration, and higher difficulty issue resolution than standard SWE-bench variants.","L1: Language Comprehension
L2: Planning, Decision-making, Logical Reasoning, Working Memory, Adaptive Error Correction
L3: Cognitive Flexibility",L3
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 evaluates agentic performance in command-line environments, where models must complete practical tasks by issuing shell commands, editing files, installing dependencies, and iterating based on outputs. Success requires reliable tool use, error recovery, and maintaining task state over multi-step workflows.","L1: Language Comprehension
L2: Planning, Decision-making, Working Memory, Adaptive Error Correction
L3: ",L2
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp evaluates “deep research” style agents by requiring them to find and synthesize answers using browsing/search over a controlled document collection, supporting reproducible retrieval conditions. It tests whether an agent can plan a search strategy, integrate evidence across sources, and produce a grounded final answer under constraints.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Working Memory, Semantic Understanding & Context Recognition, Adaptive Error Correction
L3: ",L2
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench measures tool-using customer-support agents in simulated domains (e.g., retail, airline, telecom) where the model must interact with a user and APIs while following domain policies. It emphasizes multi-turn state tracking, policy adherence under pressure, and robust decision-making across long interaction trajectories.","L1: Language Comprehension, Language Production
L2: Decision-making, Planning, Working Memory, Adaptive Error Correction
L3: Social Reasoning & Theory of Mind, Empathy, Inhibitory Control",L3
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld evaluates multimodal “computer use” agents that must operate within an operating-system-like environment by interpreting screens and taking UI actions over many steps. It tests grounded perception-to-action loops such as navigation, form filling, and application workflows under step limits and dynamic interfaces.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Sensorimotor Coordination, Planning, Decision-making, Working Memory, Adaptive Error Correction
L3: ",L2
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI tests abstract, few-shot pattern induction using small grid-based input–output examples, where the model must infer the latent rule and apply it to a new grid. The benchmark is designed to emphasize generalizable reasoning and compositional rule discovery rather than domain knowledge.","L1: 
L2: Logical Reasoning, Working Memory, Spatial Representation & Mapping, Scene Understanding & Visual Reasoning, Attention
L3: Cognitive Flexibility",L3
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 measures long-horizon agent coherence in a simulated business environment where the model manages a vending-machine operation over extended time (e.g., a year). It requires sustained planning and adaptation—handling inventory, pricing, supplier interaction, and dynamic conditions to maximize outcomes.","L1: 
L2: Planning, Decision-making, Working Memory, Reward Mechanisms, Adaptive Error Correction
L3: Self-reflection, Motivational Drives, Social Reasoning & Theory of Mind",L3
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas evaluates real-world tool use via the Model Context Protocol (MCP), emphasizing multi-step workflows across authentic APIs and servers. Tasks require discovering appropriate tools, invoking them with correct parameters, handling failures/retries, and synthesizing results into correct responses.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Working Memory, Adaptive Error Correction
L3: ",L2
FinanceAgent,https://www.vals.ai/benchmarks/finance_agent,,"FinanceAgent benchmarks agent performance on tasks typical of an entry-level financial analyst, often requiring structured reasoning over documents, tables, and quantitative constraints. It emphasizes producing correct, actionable outputs (e.g., analysis or artifact construction) while integrating domain rules and intermediate calculations.","L1: Language Comprehension, Language Production
L2: Logical Reasoning, Decision-making, Planning, Working Memory, Semantic Understanding & Context Recognition, Adaptive Error Correction
L3: ",L2
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity agent capabilities on real-world vulnerability tasks, including identifying known issues from descriptions and attempting discovery of previously unknown vulnerabilities in open-source projects. It stresses iterative investigation, hypothesis testing, and correct patching or reporting under realistic code constraints.","L1: Language Comprehension
L2: Logical Reasoning, Planning, Decision-making, Working Memory, Adaptive Error Correction, Semantic Understanding & Context Recognition
L3: ",L2
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"SpreadsheetBench measures how well models can navigate and manipulate complex spreadsheets derived from real-world scenarios, including reading/writing cells, applying formulas, and producing correct final artifacts. It emphasizes procedural accuracy, state tracking across many operations, and debugging when intermediate steps go wrong.","L1: 
L2: Working Memory, Planning, Decision-making, Adaptive Error Correction, Logical Reasoning, Semantic Understanding & Context Recognition
L3: ",L2
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a frontier, multi-domain benchmark intended to probe advanced academic reasoning and knowledge, including multimodal questions and tasks that may benefit from tools (e.g., code execution or search) depending on the evaluation setup. It emphasizes synthesizing complex information and producing defensible final answers under high difficulty.","L1: Language Comprehension, Language Production, Visual Perception
L2: Logical Reasoning, Working Memory, Semantic Understanding & Context Recognition, Scene Understanding & Visual Reasoning, Planning, Decision-making
L3: ",L2
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,AIME 2025 evaluates mathematical problem solving on competition-style questions that require multi-step derivations and careful symbolic/quantitative reasoning. It stresses accuracy under tight problem statements and the ability to maintain intermediate constraints over long solution chains.,"L1: 
L2: Logical Reasoning, Working Memory, Planning, Attention, Adaptive Error Correction
L3: ",L2
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a difficult, graduate-level multiple-choice science QA benchmark designed to be “Google-proof,” focusing on questions that require expert reasoning rather than rote recall. The Diamond subset emphasizes high-quality items where experts reliably agree on the correct answer while non-experts often fail.","L1: Language Comprehension
L2: Logical Reasoning, Working Memory, Semantic Understanding & Context Recognition, Decision-making
L3: ",L2
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends broad academic knowledge testing across many subjects and multiple languages, measuring multilingual understanding and reasoning in a standardized multiple-choice format. It probes whether models can transfer conceptual competence across languages and maintain consistent subject-matter performance.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory
L3: Cognitive Flexibility",L3
MMMU,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2311.16502,"MMMU is a multimodal, multi-discipline benchmark where questions require jointly interpreting text and images (e.g., diagrams, plots, figures) to answer expert-level prompts. It emphasizes multimodal grounding, visual reasoning, and combining heterogeneous evidence to reach a correct conclusion.","L1: Visual Perception, Language Comprehension
L2: Multisensory Integration, Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Logical Reasoning, Working Memory
L3: ",L2
LAB-Bench FigQA,https://huggingface.co/datasets/futurehouse/lab-bench,https://arxiv.org/abs/2407.10362,"LAB-Bench FigQA evaluates whether models can correctly interpret and reason over complex scientific figures from biology papers, answering questions that depend on extracting visual evidence and integrating it with domain context. It stresses precise figure reading (including axes/labels/layout) and multi-step scientific reasoning.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Multisensory Integration, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Visual Attention & Eye Movements
L3: ",L2
WebArena,https://webarena.dev/,https://arxiv.org/abs/2307.13854,"WebArena evaluates autonomous web agents on realistic multi-step tasks across several interactive web apps (e-commerce, CMS, forums, Git workflows, maps). Models must navigate pages, manipulate forms, and complete goals while handling dynamic UI state and long action sequences.","L1: Visual Perception, Language Comprehension, Language Production
L2: Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Sensorimotor Coordination, Planning, Decision-making, Working Memory, Adaptive Error Correction
L3: ",L2
