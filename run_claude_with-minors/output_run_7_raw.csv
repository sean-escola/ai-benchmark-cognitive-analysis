Benchmark,Website,Paper,Description,Cognitive Functions
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates software engineering agents on a curated set of real GitHub issues where each task has been validated as solvable and has a reliable test-based oracle. The model must understand the issue and repository context, edit code to produce a correct patch, and pass the project’s tests under a single-attempt pass@1 setting.","Language Comprehension, Language Production, Planning, Decision-making, Logical Reasoning, Working Memory, Adaptive Error Correction, Semantic Understanding & Context Recognition"
SWE-bench Multilingual,https://www.swebench.com/multilingual.html,https://arxiv.org/abs/2504.21798,"SWE-bench Multilingual extends the SWE-bench paradigm beyond Python to multiple programming languages, evaluating whether an agent can adapt its debugging and patching behavior across different ecosystems and toolchains. Tasks require interpreting natural-language issues, reasoning about codebases in diverse languages, and producing changes that satisfy test suites.","Language Comprehension, Language Production, Planning, Decision-making, Logical Reasoning, Working Memory, Adaptive Error Correction, Cognitive Flexibility, Semantic Understanding & Context Recognition"
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro is a larger and more difficult software engineering benchmark intended to be more industrially representative and contamination-resistant, spanning multiple languages and more complex repo contexts. Models must generate correct patches for realistic issues, often requiring deeper dependency reasoning and more robust iterative debugging to satisfy stricter evaluation harnesses.","Language Comprehension, Language Production, Planning, Decision-making, Logical Reasoning, Working Memory, Adaptive Error Correction, Cognitive Flexibility, Semantic Understanding & Context Recognition"
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 evaluates agentic performance on real command-line tasks (e.g., debugging, package/build issues, data wrangling, and system-style workflows) executed in sandboxed environments. The agent must choose commands, interpret outputs, and iteratively correct its approach until the task’s checks succeed.","Planning, Decision-making, Logical Reasoning, Working Memory, Adaptive Error Correction, Language Comprehension, Semantic Understanding & Context Recognition"
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp measures deep-research capability by requiring models to answer questions using web-style retrieval over a controlled document collection, emphasizing reproducibility and evidence-based answering. Strong performance requires decomposing information needs, iteratively searching, integrating evidence across sources, and producing grounded final responses.","Planning, Decision-making, Working Memory, Episodic Memory, Language Comprehension, Language Production, Semantic Understanding & Context Recognition, Adaptive Error Correction"
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench evaluates interactive tool-using agents in customer-support style environments (e.g., retail, airline, telecom) where the agent must follow policies while using APIs and handling multi-turn user interactions. It stresses robustness to user variability, policy constraints, and long-horizon task completion through correct tool calls and conversation management.","Language Comprehension, Language Production, Planning, Decision-making, Working Memory, Inhibitory Control, Social Reasoning & Theory of Mind, Empathy, Adaptive Error Correction"
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld evaluates multimodal computer-use agents operating in realistic desktop environments, where the agent must perceive UI state (screenshots) and complete tasks via a sequence of actions over multiple steps. Success depends on interpreting visual layouts, planning interaction sequences, and adapting when UI states or intermediate outcomes differ from expectations.","Visual Perception, Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Planning, Decision-making, Working Memory, Sensorimotor Coordination, Adaptive Error Correction"
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI tests fluid reasoning via few-shot abstract pattern induction on grid-based tasks, where models infer hidden transformations from a small set of input–output examples. It emphasizes generalization to novel rules, compositional reasoning over spatial structure, and avoiding reliance on domain-specific memorization.","Visual Perception, Scene Understanding & Visual Reasoning, Spatial Representation & Mapping, Logical Reasoning, Working Memory, Cognitive Flexibility, Planning"
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon agent coherence in a simulated business setting where the agent manages a vending-machine operation over an extended period. The agent must make strategic choices (inventory, pricing, supplier negotiation) and adapt to changing conditions to maximize final profit under persistent state and delayed consequences.","Planning, Decision-making, Working Memory, Episodic Memory, Reward Mechanisms, Adaptive Error Correction, Self-reflection"
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas measures real-world tool-use performance via the Model Context Protocol, requiring models to discover appropriate tools, execute multi-step workflows, handle errors, and synthesize results from authentic APIs. It targets end-to-end agent reliability under realistic tool schemas and multi-call dependency chains.","Planning, Decision-making, Working Memory, Adaptive Error Correction, Language Comprehension, Language Production, Semantic Understanding & Context Recognition"
FinanceAgent,https://www.vals.ai/benchmarks/finance_agent,,"FinanceAgent assesses an agent’s ability to complete tasks typical of an entry-level financial analyst, such as extracting facts from documents, performing calculations, building structured outputs, and producing analyst-style explanations. It often requires chaining quantitative reasoning with domain knowledge and clear reporting aligned to task constraints.","Language Comprehension, Language Production, Logical Reasoning, Planning, Decision-making, Working Memory, Semantic Understanding & Context Recognition, Adaptive Error Correction"
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity agent capabilities on large-scale, real-world vulnerability tasks, including identifying known weaknesses and discovering new vulnerabilities in open-source projects. Agents must reason about code behavior, craft exploits or fixes, and validate outcomes within a controlled evaluation harness.","Logical Reasoning, Planning, Decision-making, Working Memory, Adaptive Error Correction, Language Comprehension, Language Production, Inhibitory Control, Semantic Understanding & Context Recognition"
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"SpreadsheetBench evaluates whether models can navigate and manipulate complex spreadsheets to solve realistic business/data tasks, often involving formula edits, restructuring, and verification of computed outputs. It probes structured reasoning over tabular artifacts and iterative correction when intermediate spreadsheet states are inconsistent or incorrect.","Logical Reasoning, Planning, Decision-making, Working Memory, Adaptive Error Correction, Semantic Understanding & Context Recognition, Sensorimotor Coordination, Language Comprehension, Language Production"
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a difficult multimodal benchmark spanning frontier academic and professional questions, designed to stress broad knowledge plus multi-step reasoning under ambiguity. Tasks often require integrating textual prompts with figures/diagrams and producing concise, correct answers that reflect expert-level synthesis.","Language Comprehension, Language Production, Logical Reasoning, Working Memory, Semantic Understanding & Context Recognition, Scene Understanding & Visual Reasoning, Multisensory Integration"
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 (as used in modern model leaderboards) evaluates competitive mathematics problem solving from the American Invitational Mathematics Examination, requiring precise symbolic manipulation and multi-step derivations. It is typically scored by exact-answer matching, rewarding correctness and robustness over short, dense problem statements.","Logical Reasoning, Working Memory, Planning, Adaptive Error Correction"
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a high-quality subset of graduate-level, Google-resistant multiple-choice science questions where non-experts tend to fail and experts reliably succeed. It targets deep domain understanding and careful reasoning under distractors, emphasizing correctness over verbosity.","Language Comprehension, Logical Reasoning, Working Memory, Decision-making, Semantic Understanding & Context Recognition"
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends broad academic knowledge testing to many non-English languages across a wide range of subjects, probing whether models retain reasoning and factual competence beyond English. It stresses multilingual understanding, cross-lingual generalization, and consistent subject-matter reasoning under varied linguistic forms.","Language Comprehension, Logical Reasoning, Working Memory, Semantic Understanding & Context Recognition, Cognitive Flexibility, Language Production"
MMMU,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2311.16502,"MMMU is a multidisciplinary multimodal benchmark where questions require reasoning over both text and images (e.g., charts, diagrams, scientific figures) across expert domains. It probes integrated visual–textual understanding and multi-step inference rather than isolated perception or isolated language recall.","Multisensory Integration, Visual Perception, Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Language Comprehension, Logical Reasoning, Working Memory, Semantic Understanding & Context Recognition"
LAB-Bench FigQA,https://huggingface.co/datasets/futurehouse/lab-bench,https://arxiv.org/abs/2407.10362,"LAB-Bench FigQA focuses on scientific figure question answering from biology papers, requiring models to interpret complex plots and experimental diagrams and connect them to biological concepts. It stresses careful visual parsing, linking figure evidence to claims, and drawing correct inferences under domain constraints.","Visual Perception, Scene Understanding & Visual Reasoning, Language Comprehension, Language Production, Logical Reasoning, Working Memory, Semantic Understanding & Context Recognition, Multisensory Integration"
WebArena,https://webarena.dev/,https://arxiv.org/abs/2307.13854,"WebArena evaluates autonomous web agents in realistic, self-hosted web applications (e-commerce, CMS, forums, code hosting, maps), requiring multi-step navigation and form interactions to achieve goals. Agents must perceive changing pages, plan action sequences, recover from mistakes, and maintain task state across long trajectories.","Visual Perception, Visual Attention & Eye Movements, Planning, Decision-making, Working Memory, Sensorimotor Coordination, Adaptive Error Correction, Language Comprehension, Language Production"
