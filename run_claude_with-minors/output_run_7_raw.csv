Benchmark,Website,Paper,Description,Cognitive Functions
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates end-to-end software engineering by asking a model/agent to produce patches that fix real issues in GitHub repositories, validated by tests. The “Verified” subset emphasizes tasks confirmed solvable and scored by whether the submitted patch passes the repository’s test suite under the harness.","Planning, Logical Reasoning, Working Memory, Adaptive Error Correction, Decision-making, Language Comprehension"
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 measures autonomous command-line problem solving, where agents must navigate a terminal environment, run commands, edit files, and iteratively debug to reach a goal state. It stresses long-horizon tool interaction under realistic constraints such as dependency issues, environment errors, and partial progress.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Inhibitory Control, Language Comprehension"
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp evaluates deep research behavior: answering questions that require multi-step web-style information gathering, synthesis, and citation-like justification rather than single-hop recall. The benchmark is designed to test whether agents can plan searches, filter noisy evidence, and integrate findings into a coherent final answer.","Planning, Attention, Semantic Understanding & Context Recognition, Working Memory, Language Comprehension, Language Production, Decision-making, Inhibitory Control"
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench evaluates agentic tool use in multi-turn customer-service-style simulations (e.g., retail, airline, telecom) where models must follow policies while using APIs to complete tasks. Success depends on reliable action sequencing, state tracking across turns, and policy adherence under pressure from user requests.","Social Reasoning & Theory of Mind, Decision-making, Planning, Working Memory, Inhibitory Control, Language Comprehension, Language Production, Attention"
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld benchmarks multimodal computer-use agents that must operate graphical desktop environments to accomplish tasks across applications. It emphasizes perception of UI state (screenshots), action execution (click/type), and robust multi-step planning with error recovery.","Visual Perception, Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Sensorimotor Coordination, Planning, Decision-making, Working Memory, Adaptive Error Correction"
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI tests fluid reasoning by requiring inference of abstract rules from a small set of input–output grid examples and applying the rule to a new grid. It aims to measure generalization to novel patterns with minimal data, rather than domain knowledge or memorization.","Logical Reasoning, Cognitive Flexibility, Working Memory, Spatial Representation & Mapping, Attention"
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 measures long-horizon autonomy in a simulated business management setting, where an agent runs a vending machine operation over many steps and must grow its balance. High performance requires sustained coherence, strategic planning, negotiation/communication, and adaptation to changing market signals.","Planning, Decision-making, Working Memory, Adaptive Error Correction, Motivational Drives, Language Comprehension, Language Production"
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas evaluates real-world tool use via the Model Context Protocol by testing whether models can discover tools, call them correctly, handle errors, and compose multi-step workflows across services. It focuses on execution reliability and integration of tool outputs into accurate final responses.","Planning, Decision-making, Working Memory, Adaptive Error Correction, Inhibitory Control, Language Comprehension, Language Production"
FinanceAgent,https://www.vals.ai/benchmarks/finance_agent,,"FinanceAgent assesses agent performance on tasks typical of an entry-level financial analyst, such as extracting information from documents, performing calculations, building structured analyses, and producing professional summaries. It emphasizes correct reasoning with numbers, adherence to task constraints, and coherent reporting.","Logical Reasoning, Decision-making, Planning, Working Memory, Semantic Understanding & Context Recognition, Language Comprehension, Language Production, Attention"
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity capability by asking agents to identify known vulnerabilities in real projects and, in some settings, to discover new issues under realistic constraints. It stresses iterative investigation, hypothesis testing, and patch/reproduction workflows.","Logical Reasoning, Planning, Adaptive Error Correction, Working Memory, Attention, Decision-making, Inhibitory Control"
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"SpreadsheetBench measures the ability to understand, edit, and compute over complex spreadsheets drawn from realistic scenarios. Agents must interpret layout and formulas, perform structured transformations, and maintain correctness across interdependent cells and sheets.","Logical Reasoning, Working Memory, Attention, Semantic Understanding & Context Recognition, Adaptive Error Correction, Planning, Decision-making"
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a challenging multimodal benchmark spanning advanced questions at the frontier of human knowledge across many domains. It tests not only recall but multi-step reasoning, synthesis of evidence, and, in tool-enabled settings, effective use of external resources to reach correct answers.","Language Comprehension, Language Production, Logical Reasoning, Working Memory, Attention, Scene Understanding & Visual Reasoning, Visual Perception, Semantic Understanding & Context Recognition"
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 consists of competition-style mathematics problems requiring precise multi-step derivations and careful handling of algebra, geometry, combinatorics, and number theory. It probes consistency under symbolic manipulation and the ability to avoid small logical slips.","Logical Reasoning, Working Memory, Attention, Adaptive Error Correction, Planning"
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,GPQA Diamond is a subset of graduate-level science multiple-choice questions designed to be difficult to answer via simple web search or superficial pattern matching. It emphasizes deep scientific reasoning and careful discrimination among plausible distractors.,"Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Attention, Decision-making"
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends broad academic knowledge evaluation to multiple languages, testing whether models can understand and answer subject-matter questions across diverse linguistic contexts. It measures multilingual comprehension and cross-lingual robustness of reasoning and knowledge access.","Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Attention"
MMMU,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2311.16502,"MMMU is a multimodal benchmark where questions require integrating text with images (diagrams, charts, figures) across many disciplines. It emphasizes visual understanding, grounding language in visual evidence, and performing multi-step reasoning from that evidence.","Visual Perception, Scene Understanding & Visual Reasoning, Multisensory Integration, Language Comprehension, Logical Reasoning, Working Memory, Attention"
LAB-Bench FigQA,https://huggingface.co/datasets/futurehouse/lab-bench,https://arxiv.org/abs/2407.10362,"LAB-Bench FigQA tests scientific figure understanding in biology papers, requiring models to read complex plots and experimental figures and answer questions about them. It targets practical research-relevant interpretation rather than generic image captioning.","Scene Understanding & Visual Reasoning, Visual Perception, Language Comprehension, Logical Reasoning, Working Memory, Attention"
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning evaluates reasoning over scientific paper figures and related context, often benefiting from quantitative inspection and structured analysis. It targets careful interpretation of visual scientific artifacts (e.g., plots) and mapping them to correct textual conclusions.","Scene Understanding & Visual Reasoning, Visual Perception, Logical Reasoning, Working Memory, Attention, Language Comprehension"
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU extends multimodal understanding to video, requiring temporal integration of visual evidence and language to answer questions about dynamic scenes. It stresses retaining relevant events across time and reasoning over actions, causality, and context shifts.","Visual Perception, Visual Attention & Eye Movements, Cognitive Timing & Predictive Modeling, Working Memory, Scene Understanding & Visual Reasoning, Language Comprehension, Attention"
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"FACTS Benchmark Suite systematically evaluates factuality by probing whether model outputs remain grounded, avoid unsupported claims, and appropriately handle uncertainty. It typically stresses correctness under long-form generation where hallucination risks increase.","Semantic Understanding & Context Recognition, Inhibitory Control, Adaptive Error Correction, Working Memory, Attention, Language Production, Language Comprehension"
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA evaluates practical physical and everyday reasoning across languages and cultures, aiming to reduce geographic and cultural bias in “common-sense” evaluations. It tests whether models can choose or generate plausible actions/explanations in diverse real-world contexts.","Semantic Understanding & Context Recognition, Logical Reasoning, Social Reasoning & Theory of Mind, Language Comprehension, Decision-making, Working Memory"
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,MRCR v2 (8-needle) measures long-context multi-round coreference resolution by embedding multiple similar ‘needle’ requests in large ‘haystacks’ and asking the model to recover the correct referenced response. It stresses robust retrieval of the right instance among distractors across very long contexts.,"Working Memory, Attention, Episodic Memory, Semantic Understanding & Context Recognition, Inhibitory Control"
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval evaluates economically valuable, well-specified knowledge work across many occupations by judging the quality of produced work artifacts (e.g., plans, analyses, presentations, spreadsheets). It emphasizes end-to-end task execution quality, adherence to constraints, and professional communication.","Planning, Decision-making, Language Production, Language Comprehension, Working Memory, Attention, Adaptive Error Correction, Self-reflection"
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks tests reasoning over graph-structured data where models must follow paths, track connectivity, and answer queries that require multi-step traversal. It stresses structured reasoning, maintaining intermediate states, and resisting distractor branches.","Spatial Representation & Mapping, Logical Reasoning, Working Memory, Attention, Planning"
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon evaluates tool-using agents across a diverse suite of tasks requiring correct selection, invocation, and composition of tools under constraints. It emphasizes reliability in multi-step workflows, error handling, and integrating tool outputs into final answers.","Planning, Decision-making, Working Memory, Adaptive Error Correction, Inhibitory Control, Language Comprehension, Language Production"
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath evaluates expert-level mathematics designed to be difficult for models without genuine problem-solving, often requiring long chains of derivation and careful verification. It targets robustness on novel problems and the ability to sustain correct reasoning over extended solutions.","Logical Reasoning, Working Memory, Planning, Adaptive Error Correction, Attention"
