Benchmark,Website,Paper,Description,Cognitive Functions
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates LLMs as software engineering agents by requiring them to generate patches that resolve real GitHub issues in Python repositories, with success determined by repository tests and human verification of solvability. It emphasizes end-to-end debugging and code-change correctness under a single-attempt setting in many reports.","Planning, Logical Reasoning, Adaptive Error Correction, Working Memory, Language Comprehension, Language Production"
SWE-bench Multilingual,https://www.swebench.com/multilingual.html,https://arxiv.org/abs/2504.21798,"SWE-bench Multilingual extends the SWE-bench paradigm beyond Python by measuring whether a model can fix real issues across multiple programming languages and ecosystems. It stresses transfer of debugging and patch-planning skills across different tooling conventions, APIs, and code idioms.","Planning, Logical Reasoning, Adaptive Error Correction, Cognitive Flexibility, Working Memory, Language Comprehension, Language Production"
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro is a larger, more difficult software engineering benchmark designed to be more contamination-resistant and industrially representative, spanning multiple languages and complex repositories. It evaluates whether agents can perform sustained multi-step diagnosis and produce patches that satisfy tests in realistic codebases.","Planning, Decision-making, Logical Reasoning, Adaptive Error Correction, Cognitive Flexibility, Working Memory, Language Comprehension, Language Production"
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 evaluates autonomous agent performance in command-line environments on real-world tasks such as installing dependencies, manipulating files, running programs, and interpreting tool outputs. Scores reflect whether the agent completes objectives within constraints, requiring iterative trial-and-fix behavior over long action sequences.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Language Comprehension"
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp benchmarks “deep research” by having agents answer questions using browsing/search over a controlled document collection, emphasizing reproducibility across runs. It tests whether the agent can plan queries, gather evidence from documents, and synthesize a grounded final answer.","Planning, Decision-making, Attention, Working Memory, Semantic Understanding & Context Recognition, Language Comprehension, Language Production"
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench measures interactive agent competence in simulated customer-service domains (e.g., retail, airline, telecom) where the model must use tools/APIs while adhering to policies over multi-turn dialogues. It probes consistent policy-following, robust tool invocation, and recovery from user- or system-induced complications.","Social Reasoning & Theory of Mind, Decision-making, Planning, Inhibitory Control, Adaptive Error Correction, Working Memory, Language Comprehension, Language Production"
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld evaluates computer-use agents that must operate within a realistic desktop environment using multimodal observations (e.g., screenshots) and action interfaces (mouse/keyboard abstractions). Tasks require navigating GUIs, locating relevant information, and executing multi-step procedures reliably under step limits.","Visual Perception, Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Planning, Decision-making, Adaptive Error Correction, Working Memory, Sensorimotor Coordination"
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI tests fluid, example-driven abstract reasoning using grid-based input–output demonstrations where the model must infer hidden transformation rules and generalize to new inputs. It is designed to reduce reliance on memorized knowledge and highlight pattern induction and systematic generalization.","Logical Reasoning, Cognitive Flexibility, Spatial Representation & Mapping, Working Memory, Attention"
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon agent coherence by simulating management of a vending-machine business over an extended period, scoring by final financial outcomes. High performance requires strategic planning, adapting to market dynamics, and maintaining consistent objectives over thousands of decisions.","Planning, Decision-making, Working Memory, Reward Mechanisms, Adaptive Error Correction"
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas measures real-world tool-use skill via the Model Context Protocol, requiring agents to discover appropriate tools, call them with correct arguments, handle errors, and integrate results across multi-step workflows. It targets production-like API interaction rather than purely conversational correctness.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition, Language Comprehension, Language Production"
FinanceAgent,https://www.vals.ai/benchmarks/finance_agent,,"FinanceAgent evaluates capability on tasks typical of an entry-level financial analyst, such as extracting facts from financial materials, performing structured analysis, and producing professional outputs. It stresses grounded interpretation of domain documents and correct quantitative/logic-driven reasoning under task constraints.","Semantic Understanding & Context Recognition, Logical Reasoning, Planning, Decision-making, Working Memory, Language Comprehension, Language Production"
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity agent capabilities on real software projects, including identifying known vulnerabilities from descriptions and, in some settings, discovering new issues. It emphasizes reasoning over code and artifacts, selecting investigative actions, and producing correct findings under realistic constraints.","Logical Reasoning, Planning, Decision-making, Adaptive Error Correction, Working Memory, Attention"
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"SpreadsheetBench tests whether models can navigate and manipulate complex spreadsheets to solve practical problems, often requiring correct formulas, structured transformations, and precise edits. Success depends on reliably executing multi-step operations while maintaining consistency across interdependent cells and sheets.","Logical Reasoning, Planning, Decision-making, Adaptive Error Correction, Working Memory, Spatial Representation & Mapping"
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a challenging multi-modal benchmark spanning frontier academic and professional questions, designed to stress deep reasoning rather than rote recall. Evaluations often include tool-enabled variants (e.g., search or code execution) and measure whether the model can integrate evidence into correct answers.","Language Comprehension, Language Production, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Scene Understanding & Visual Reasoning"
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 evaluates competition-style mathematical problem solving drawn from the American Invitational Mathematics Examination, typically requiring multi-step derivations and careful case reasoning. It measures mathematical reasoning quality under strict answer formats, sometimes with and without tool assistance.","Logical Reasoning, Planning, Working Memory, Attention"
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a hard multiple-choice science benchmark curated to be “Google-proof,” with questions that typically require expert-level reasoning rather than surface pattern matching. The Diamond subset focuses on high-quality items where experts agree on the correct choice and non-experts often fail.","Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Attention"
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends broad academic knowledge testing into multiple languages, evaluating both multilingual understanding and cross-domain reasoning across many subjects. It probes whether models can maintain accuracy when questions and options are presented in diverse languages and culturally varied phrasing.","Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning, Cognitive Flexibility, Working Memory"
MMMU,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2311.16502,"MMMU evaluates multimodal expert-level understanding by combining images (e.g., diagrams, charts, photos) with text questions across many disciplines. It requires integrating visual evidence with linguistic prompts to answer questions that often involve multi-step reasoning or domain knowledge.","Visual Perception, Scene Understanding & Visual Reasoning, Multisensory Integration, Attention, Logical Reasoning, Working Memory, Language Comprehension, Language Production"
LAB-Bench FigQA,https://huggingface.co/datasets/futurehouse/lab-bench,https://arxiv.org/abs/2407.10362,"LAB-Bench FigQA focuses on interpreting complex scientific figures from biology papers, requiring extraction of quantitative/qualitative information from plots and schematics. It measures whether models can reason over figure content (often with sub-figure structure) and map it to correct scientific answers.","Visual Perception, Scene Understanding & Visual Reasoning, Attention, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Language Comprehension, Language Production"
WebArena,https://webarena.dev/,https://arxiv.org/abs/2307.13854,"WebArena evaluates autonomous web agents on realistic tasks across multiple web applications (e-commerce, forums, Git hosting, CMS, maps), requiring navigation and interaction with dynamic interfaces. It stresses long-horizon planning, robust recovery from mistakes, and reliable completion judged by functional graders.","Visual Perception, Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Planning, Decision-making, Adaptive Error Correction, Working Memory, Language Comprehension, Language Production"
