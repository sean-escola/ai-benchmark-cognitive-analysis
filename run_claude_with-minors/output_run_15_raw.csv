Benchmark,Website,Paper,Description,Cognitive Functions
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates software engineering agents on real GitHub issues by requiring them to generate a patch that makes a hidden test suite pass. The Verified split emphasizes reliably solvable tasks with stronger controls around task validity and evaluation rigor, aiming to measure end-to-end bug fixing and feature implementation in realistic repos.","Language Comprehension, Language Production, Logical Reasoning, Planning, Working Memory, Adaptive Error Correction"
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 measures an agent’s ability to complete practical tasks inside a command-line environment (e.g., using Unix tools, running programs, editing files, and debugging). It emphasizes iterative interaction: reading outputs, deciding next commands, and recovering from failures under realistic constraints.","Language Comprehension, Planning, Decision-making, Working Memory, Adaptive Error Correction"
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp evaluates deep web research agents on questions that typically require multi-step browsing, evidence gathering, and synthesis rather than single-document lookup. Systems must plan searches, navigate multiple sources, and produce a final answer that is supported by retrieved content.","Language Comprehension, Language Production, Planning, Decision-making, Attention, Working Memory, Semantic Understanding & Context Recognition"
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench evaluates agentic customer-support behavior in simulated domains (e.g., retail, airline, telecom) that require tool/API use over multi-turn conversations while following policies. It stresses reliable tool invocation, policy compliance, and coherent dialogue that adapts to user constraints and evolving state.","Language Comprehension, Language Production, Planning, Decision-making, Inhibitory Control, Social Reasoning & Theory of Mind, Working Memory"
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld is a computer-use benchmark where agents must operate a desktop-like OS to complete tasks by interacting with GUI elements (clicking, typing, navigating apps). It targets end-to-end perception-to-action competence under partial observability and step limits.","Visual Perception, Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Sensorimotor Coordination, Planning, Decision-making, Working Memory"
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,ARC-AGI tests fluid reasoning by asking models to infer abstract transformation rules from a few input-output grid examples and apply them to a new grid. It is designed to minimize dependence on memorized knowledge and emphasize novel pattern induction and generalization.,"Logical Reasoning, Cognitive Flexibility, Spatial Representation & Mapping, Working Memory"
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon autonomy by having an agent run a simulated vending machine business over an extended period, making many interdependent decisions. Success requires coherent strategy, resource management, and adaptation to changing market conditions across thousands of steps.","Planning, Decision-making, Working Memory, Reward Mechanisms, Cognitive Timing & Predictive Modeling"
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas evaluates real-world tool use via the Model Context Protocol by testing whether models can discover, call, and chain tools across multi-step workflows. It emphasizes robust API usage, error handling, and synthesis of tool outputs into correct final responses.","Language Comprehension, Planning, Decision-making, Working Memory, Adaptive Error Correction"
FinanceAgent,https://www.vals.ai/benchmarks/finance_agent,,"FinanceAgent assesses performance on tasks resembling entry-level financial analyst work, such as interpreting financial documents, computing metrics, and producing analyst-style outputs. It probes structured reasoning and decision-making grounded in domain constraints and numerical consistency.","Language Comprehension, Language Production, Logical Reasoning, Planning, Decision-making, Working Memory, Semantic Understanding & Context Recognition"
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity agents on tasks involving finding known vulnerabilities and discovering new ones in real open-source codebases. It stresses code understanding, adversarial thinking, and iterative debugging/verification under realistic constraints.","Language Comprehension, Logical Reasoning, Planning, Decision-making, Working Memory, Adaptive Error Correction, Inhibitory Control"
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"SpreadsheetBench measures an agent’s ability to navigate, edit, and compute with complex spreadsheets drawn from realistic scenarios. Tasks often require locating relevant cells, applying correct formulas/transformations, and verifying outputs after tool-driven edits.","Language Comprehension, Planning, Decision-making, Working Memory, Adaptive Error Correction, Semantic Understanding & Context Recognition"
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a challenging multimodal benchmark intended to probe frontier knowledge and reasoning across many domains, often beyond rote recall. Questions may require integrating text with images and performing multi-step inference under uncertainty.","Language Comprehension, Language Production, Logical Reasoning, Working Memory, Visual Perception, Scene Understanding & Visual Reasoning, Multisensory Integration"
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 evaluates mathematical problem solving using questions from a high-level competition setting, typically requiring multi-step derivations and careful case analysis. It is commonly used to compare advanced reasoning performance with and without tool assistance.","Logical Reasoning, Working Memory, Planning"
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a difficult multiple-choice science benchmark designed to be resistant to simple web search and superficial pattern matching, focusing on expert-level questions. The “Diamond” subset targets higher-quality items that require deep scientific reasoning and careful reading.","Language Comprehension, Logical Reasoning, Working Memory, Semantic Understanding & Context Recognition"
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends broad academic evaluation to multiple languages, testing knowledge and reasoning across many subjects and non-English contexts. It probes whether models can transfer understanding and reasoning beyond English into diverse linguistic settings.","Language Comprehension, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory"
MMMU,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2311.16502,"MMMU evaluates multidisciplinary understanding and reasoning with multimodal inputs, requiring models to interpret images (e.g., diagrams, charts) alongside text. It emphasizes cross-domain expert tasks that require combining visual evidence with textual reasoning.","Language Comprehension, Logical Reasoning, Visual Perception, Scene Understanding & Visual Reasoning, Multisensory Integration, Working Memory"
LAB-Bench FigQA,https://huggingface.co/datasets/futurehouse/lab-bench,https://arxiv.org/abs/2407.10362,"LAB-Bench FigQA focuses on answering questions about scientific figures from biology papers, requiring extraction of evidence from complex plots, panels, and annotations. It targets practical scientific figure literacy, including mapping visual cues to domain claims and conclusions.","Visual Perception, Scene Understanding & Visual Reasoning, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory"
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning evaluates reasoning over scientific figures and associated context from arXiv-style papers, often involving charts and quantitative visual evidence. Success requires interpreting plot structure, legends/axes, and relating visual signals to the question’s constraints.","Visual Perception, Scene Understanding & Visual Reasoning, Logical Reasoning, Working Memory, Language Comprehension"
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU evaluates multimodal understanding over videos, requiring models to integrate temporal visual information with text questions. It stresses event comprehension, temporal dependency tracking, and multi-step reasoning grounded in what occurs across frames.","Visual Perception, Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Working Memory, Multisensory Integration, Cognitive Timing & Predictive Modeling, Language Comprehension"
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"FACTS Benchmark Suite systematically evaluates factuality, including whether model outputs remain grounded, consistent, and resistant to hallucination across varied tasks. It targets reliability under real-world prompting conditions where plausible-but-wrong answers are tempting.","Language Comprehension, Language Production, Semantic Understanding & Context Recognition, Inhibitory Control, Self-reflection"
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA evaluates physical commonsense reasoning about how to accomplish everyday goals with objects and actions, with attention to cross-lingual or cross-cultural coverage. Items require selecting plausible interaction plans given constraints of the physical world.","Language Comprehension, Logical Reasoning, Semantic Understanding & Context Recognition, Spatial Representation & Mapping"
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,MRCR v2 (8-needle) tests long-context multi-round coreference/recall by embedding multiple similar “needle” interactions inside long “haystacks” and asking the model to reproduce the correct referenced response. It stresses robust retrieval under interference from highly similar distractors over very long contexts.,"Working Memory, Attention, Episodic Memory, Language Comprehension, Adaptive Error Correction"
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval evaluates well-specified professional knowledge-work tasks across many occupations, judged by expert humans via head-to-head comparisons. Tasks require producing realistic work artifacts (e.g., spreadsheets, presentations) and making practical decisions under business constraints.","Planning, Decision-making, Language Production, Language Comprehension, Working Memory, Semantic Understanding & Context Recognition"
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks evaluates graph-structured reasoning by asking models to follow paths, parents/ancestors, or other relational queries over graph descriptions. It emphasizes maintaining and updating a structured internal representation while executing multi-step traversals without losing state.","Spatial Representation & Mapping, Working Memory, Logical Reasoning, Planning, Attention"
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon evaluates tool-using agents across diverse tool APIs and multi-step tasks that require selecting the right tools, calling them correctly, and composing results. It focuses on reliable orchestration, error recovery, and end-to-end completion rather than isolated tool calls.","Planning, Decision-making, Working Memory, Adaptive Error Correction, Language Comprehension, Language Production"
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath evaluates expert-level mathematics beyond routine competition problems, emphasizing hard reasoning that often requires novel insight and long derivations. It is designed to be challenging even for strong models and is frequently analyzed by difficulty tiers.","Logical Reasoning, Working Memory, Planning, Cognitive Flexibility"
