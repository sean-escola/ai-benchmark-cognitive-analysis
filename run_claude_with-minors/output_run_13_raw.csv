Benchmark,Website,Paper,Description,Cognitive Functions
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates agentic software engineering by asking a model to generate patches for real GitHub issues, then running repository tests to verify correctness. The “Verified” subset emphasizes high-quality, human-validated tasks to reduce ambiguous or unsolvable instances and focuses on end-to-end code changes rather than isolated coding questions.","Planning, Logical Reasoning, Adaptive Error Correction, Working Memory, Language Comprehension, Language Production, Decision-making, Semantic Understanding & Context Recognition"
SWE-bench Multilingual,https://www.swebench.com/multilingual.html,https://arxiv.org/abs/2504.21798,"SWE-bench Multilingual extends SWE-bench-style repository repair to multiple programming languages beyond Python, measuring whether agents generalize debugging and patching behaviors across ecosystems. Tasks require understanding project conventions, applying language-specific tooling, and producing changes that satisfy tests in each language setting.","Planning, Logical Reasoning, Adaptive Error Correction, Working Memory, Cognitive Flexibility, Language Comprehension, Language Production, Decision-making, Semantic Understanding & Context Recognition"
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro is a larger and more difficult software engineering benchmark designed to be more contamination-resistant and industrially relevant, with complex repositories and realistic change requests. It emphasizes robust, multi-file modifications, correct dependency/tooling usage, and higher difficulty than SWE-bench Verified.","Planning, Logical Reasoning, Adaptive Error Correction, Working Memory, Decision-making, Cognitive Flexibility, Language Comprehension, Language Production, Semantic Understanding & Context Recognition"
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 evaluates agents operating in command-line environments on practical tasks such as installing dependencies, manipulating files, running programs, and debugging failures. Success depends on choosing correct shell commands, interpreting outputs/errors, and iterating toward a working solution under step and resource constraints.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Attention, Semantic Understanding & Context Recognition, Language Comprehension, Language Production"
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp measures deep research and information-seeking with browsing/search tools, requiring models to locate evidence in a large document collection and synthesize accurate answers. It tests whether agents can decompose a query, explore sources, track findings across steps, and avoid being misled by irrelevant or noisy documents.","Planning, Decision-making, Attention, Working Memory, Episodic Memory, Semantic Understanding & Context Recognition, Language Comprehension, Language Production, Adaptive Error Correction"
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench evaluates interactive tool-using agents in simulated customer-support domains (e.g., retail, airline, telecom) that must follow domain policies while using APIs over multi-turn dialogues. It probes whether an agent can maintain goals, adhere to constraints, and coordinate tool calls while handling user requests and edge cases.","Social Reasoning & Theory of Mind, Empathy, Inhibitory Control, Decision-making, Planning, Working Memory, Attention, Language Comprehension, Language Production, Reward Mechanisms"
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld evaluates multimodal “computer use” agents that operate a real or realistic OS GUI to complete tasks across apps and settings. Agents must perceive screens, navigate UI state, execute action sequences (click/type), and recover from mistakes to reach task goals.","Visual Perception, Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Sensorimotor Coordination, Planning, Decision-making, Working Memory, Adaptive Error Correction, Attention"
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI measures few-shot abstract reasoning on grid-based puzzles where models infer latent transformation rules from a small number of examples. It is designed to emphasize novel pattern induction and rule generalization rather than memorized knowledge, with solutions depending on compositional visual reasoning.","Logical Reasoning, Cognitive Flexibility, Working Memory, Visual Perception, Scene Understanding & Visual Reasoning, Spatial Representation & Mapping, Attention"
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon agent coherence by simulating management of a vending-machine business over many time steps, including procurement, pricing, negotiation, and inventory decisions. High scores require sustained planning, adapting strategy to changing conditions, and avoiding compounding errors across a lengthy trajectory.","Planning, Decision-making, Working Memory, Episodic Memory, Adaptive Error Correction, Reward Mechanisms, Self-reflection"
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas evaluates real-world tool use through the Model Context Protocol, where models must discover tools, call them correctly, and chain multiple calls to complete workflow tasks. It emphasizes practical API literacy, error handling, and orchestration across heterogeneous tool servers and data sources.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition, Language Comprehension, Language Production, Attention"
FinanceAgent,https://www.vals.ai/benchmarks/finance_agent,,"FinanceAgent benchmarks tasks typical of an entry-level financial analyst, such as interpreting financial documents, performing calculations, and producing justified recommendations or reports. It evaluates whether a model can integrate domain knowledge with structured reasoning and produce decision-support artifacts.","Logical Reasoning, Decision-making, Planning, Working Memory, Semantic Understanding & Context Recognition, Language Comprehension, Language Production, Attention"
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity agents on tasks such as finding known vulnerabilities from descriptions and discovering previously unknown issues in real open-source codebases. It stresses systematic investigation, hypothesis testing, safe exploitation reasoning, and iterative debugging to reach correct vulnerability reports or patches.","Logical Reasoning, Planning, Decision-making, Adaptive Error Correction, Working Memory, Attention, Semantic Understanding & Context Recognition, Inhibitory Control"
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"SpreadsheetBench evaluates agents’ ability to navigate and manipulate complex spreadsheets drawn from real-world settings, including formula editing, table operations, and data transformations. It tests whether models can maintain consistent structure, apply multi-step changes correctly, and validate outcomes after edits.","Planning, Decision-making, Working Memory, Attention, Adaptive Error Correction, Logical Reasoning, Semantic Understanding & Context Recognition"
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a multimodal benchmark positioned at the frontier of academic and professional knowledge, with questions spanning many disciplines and often requiring synthesis rather than recall. It evaluates advanced reasoning over text and images, and (in tool-enabled settings) robust use of search/code to justify answers.","Language Comprehension, Language Production, Logical Reasoning, Working Memory, Semantic Understanding & Context Recognition, Visual Perception, Scene Understanding & Visual Reasoning, Multisensory Integration, Attention"
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 evaluates competition-style mathematical problem solving under a fixed set of exam questions, typically requiring multi-step derivations and careful symbolic manipulation. It rewards precise reasoning, correct intermediate constraints, and avoidance of algebraic or arithmetic slips.","Logical Reasoning, Working Memory, Planning, Attention, Adaptive Error Correction"
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a high-difficulty multiple-choice science benchmark designed to be “Google-proof,” with questions that require expert-level reasoning and deep conceptual understanding. The Diamond subset emphasizes quality by selecting items where experts agree on the correct answer and non-experts frequently fail.","Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Attention, Language Comprehension, Decision-making"
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends MMLU to many languages, testing broad academic knowledge and reasoning across subjects in non-English settings. It probes whether models maintain consistent competence under multilingual inputs and varying cultural/linguistic phrasing of the same underlying concepts.","Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Attention, Cognitive Flexibility, Decision-making"
MMMU,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2311.16502,"MMMU is a multidisciplinary multimodal benchmark where questions require reasoning over images (charts, diagrams, tables, scenes) alongside text across many expert domains. It tests whether models can align visual evidence with language, perform compositional reasoning, and select answers based on multimodal cues.","Visual Perception, Scene Understanding & Visual Reasoning, Multisensory Integration, Language Comprehension, Language Production, Logical Reasoning, Working Memory, Attention, Decision-making"
LAB-Bench FigQA,https://huggingface.co/datasets/futurehouse/lab-bench,https://arxiv.org/abs/2407.10362,"LAB-Bench FigQA evaluates whether models can correctly interpret complex scientific figures from biology papers and answer targeted questions about them. It stresses extracting quantitative/structural information from visuals, mapping figure elements to domain concepts, and reasoning from evidence rather than surface text.","Visual Perception, Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Semantic Understanding & Context Recognition, Attention, Working Memory, Language Comprehension, Language Production"
WebArena,https://webarena.dev/,https://arxiv.org/abs/2307.13854,"WebArena evaluates autonomous web agents completing realistic multi-step tasks across several web apps (e-commerce, content management, forums, code hosting, maps). Agents must perceive dynamic pages, plan interaction sequences, execute UI actions reliably, and recover from navigation or tool errors to satisfy functional goals.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Attention, Visual Perception, Scene Understanding & Visual Reasoning, Sensorimotor Coordination, Language Comprehension, Language Production"
