Benchmark,Website,Paper,Description,Cognitive Functions
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates software engineering agents on real GitHub issues by asking them to produce a patch that makes the project’s tests pass. The “Verified” subset consists of tasks validated by human reviewers to be solvable and to have reliable evaluation, emphasizing end-to-end debugging and code change accuracy.","Planning, Decision-making, Adaptive Error Correction, Logical Reasoning, Working Memory, Language Comprehension, Language Production, Semantic Understanding & Context Recognition"
SWE-bench Multilingual,https://www.swebench.com/multilingual.html,https://arxiv.org/abs/2504.21798,"SWE-bench Multilingual extends SWE-bench-style issue resolution beyond Python to multiple programming languages and ecosystems. It tests whether an agent can generalize debugging, code reading, and patch creation skills across varied syntax, tooling, and project conventions while maintaining correctness.","Planning, Decision-making, Adaptive Error Correction, Logical Reasoning, Working Memory, Cognitive Flexibility, Language Comprehension, Language Production, Semantic Understanding & Context Recognition"
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro is a larger, more challenging software engineering benchmark designed to better reflect industrial complexity and reduce easy shortcuts. Tasks typically require deeper repository understanding, multi-file changes, and robust fixes under stricter evaluation, stressing reliable agentic coding performance.","Planning, Decision-making, Adaptive Error Correction, Logical Reasoning, Working Memory, Cognitive Flexibility, Language Comprehension, Language Production, Semantic Understanding & Context Recognition"
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 measures an agent’s ability to solve real-world tasks in a command-line environment, such as installing tools, manipulating files, running scripts, and diagnosing failures. Success depends on iterative experimentation, interpreting tool output, and executing correct sequences of shell actions.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Attention, Language Comprehension, Language Production, Logical Reasoning, Semantic Understanding & Context Recognition"
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp evaluates “deep research” style web-browsing agents on questions that require searching, reading, and synthesizing information from multiple sources. It stresses reliable retrieval, source triangulation, and maintaining a coherent intermediate knowledge state while navigating noisy or conflicting evidence.","Planning, Decision-making, Attention, Episodic Memory, Working Memory, Language Comprehension, Language Production, Semantic Understanding & Context Recognition, Logical Reasoning"
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench evaluates interactive tool-using agents in customer-support-like simulations (e.g., retail, airline, telecom) where the agent must use APIs and follow domain policies. It probes long multi-turn consistency, policy adherence under pressure, and choosing actions that satisfy user goals while respecting constraints.","Planning, Decision-making, Inhibitory Control, Social Reasoning & Theory of Mind, Empathy, Working Memory, Language Comprehension, Language Production, Attention, Semantic Understanding & Context Recognition"
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld benchmarks multimodal computer-use agents on realistic operating-system tasks (e.g., browsing, editing, configuration) using screenshots and interactive actions. It tests whether an agent can perceive UI state, plan multi-step workflows, and recover from mistakes in a dynamic desktop environment.","Visual Perception, Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Sensorimotor Coordination, Planning, Decision-making, Adaptive Error Correction, Working Memory, Attention"
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI is a few-shot abstract reasoning benchmark where models infer the rule mapping input grids to output grids from a small set of examples. It emphasizes discovering latent structure, composing transformations, and generalizing to novel tasks with minimal prior task-specific training.","Logical Reasoning, Cognitive Flexibility, Working Memory, Spatial Representation & Mapping, Scene Understanding & Visual Reasoning, Planning, Attention"
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon agent coherence and strategy by having a model run a simulated vending-machine business over extended time (e.g., months). The agent must manage inventory, pricing, supplier interactions, and budgets, adapting its policy based on outcomes and constraints.","Planning, Decision-making, Reward Mechanisms, Episodic Memory, Working Memory, Adaptive Error Correction, Cognitive Timing & Predictive Modeling, Self-reflection, Motivational Drives"
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas measures real-world tool-use performance via the Model Context Protocol (MCP), requiring agents to discover tools, call them correctly, handle errors, and integrate results across multi-step workflows. It focuses on robust orchestration across heterogeneous APIs and realistic failure modes.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Attention, Language Comprehension, Language Production, Semantic Understanding & Context Recognition, Logical Reasoning"
FinanceAgent,https://www.vals.ai/benchmarks/finance_agent,,"FinanceAgent evaluates agent performance on tasks typical of an entry-level financial analyst, such as extracting relevant details from documents, performing quantitative analyses, and producing structured outputs. It stresses domain-grounded reasoning, correctness under constraints, and reliable multi-step workflow execution.","Semantic Understanding & Context Recognition, Logical Reasoning, Planning, Decision-making, Working Memory, Attention, Language Comprehension, Language Production"
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity capabilities by testing agents on vulnerability identification, exploitation reasoning, and secure-fix understanding across real software projects at scale. It emphasizes interpreting technical artifacts, forming hypotheses about weaknesses, and iterating based on tool feedback and partial evidence.","Logical Reasoning, Planning, Decision-making, Adaptive Error Correction, Working Memory, Attention, Language Comprehension, Language Production, Semantic Understanding & Context Recognition"
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"SpreadsheetBench measures how well agents can read, edit, and compute within complex spreadsheets, often requiring multi-step transformations and formula reasoning. It tests precision on structured data manipulation, error recovery, and producing correct, well-formed spreadsheet artifacts.","Working Memory, Attention, Planning, Decision-making, Adaptive Error Correction, Logical Reasoning, Semantic Understanding & Context Recognition, Scene Understanding & Visual Reasoning"
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a difficult, broad benchmark intended to probe frontier-level knowledge and reasoning across many domains, often including multimodal questions. It stresses synthesis, careful constraint-following, and handling questions that require integrating information rather than recalling a single fact.","Language Comprehension, Language Production, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Attention, Scene Understanding & Visual Reasoning, Multisensory Integration"
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 evaluates competition-style mathematics problem solving on the 2025 AIME set (often via an evaluation harness). It emphasizes multi-step derivations, algebraic and combinatorial reasoning, and maintaining correctness across long solution chains under time/attempt constraints.","Logical Reasoning, Working Memory, Planning, Adaptive Error Correction, Attention"
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a high-difficulty multiple-choice science QA benchmark designed to be “Google-proof,” with the Diamond split representing the most reliable and challenging items. It tests deep conceptual understanding and careful elimination of distractors across physics, chemistry, and biology.","Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Attention, Decision-making, Language Comprehension"
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends broad academic knowledge testing to multiple non-English languages across many subjects. It evaluates whether models can reason and answer questions accurately across languages, measuring cross-lingual generalization beyond simple translation.","Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Attention, Cognitive Flexibility, Language Production"
MMMU,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2311.16502,"MMMU is a multimodal benchmark spanning many disciplines where solving questions requires combining textual information with images (e.g., diagrams, charts, screenshots). It probes multimodal grounding, visual reasoning, and integrating evidence across modalities to answer expert-level questions.","Visual Perception, Scene Understanding & Visual Reasoning, Multisensory Integration, Language Comprehension, Language Production, Logical Reasoning, Working Memory, Attention"
LAB-Bench FigQA,https://huggingface.co/datasets/futurehouse/lab-bench,https://arxiv.org/abs/2407.10362,"LAB-Bench FigQA evaluates scientific figure question answering, requiring models to interpret complex figures from biology papers and connect them to the question context. It focuses on extracting relevant visual evidence (plots, labels, multi-panel layouts) and performing domain-grounded reasoning from figures.","Visual Perception, Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Multisensory Integration, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Attention, Language Comprehension, Language Production"
WebArena,https://webarena.dev/,https://arxiv.org/abs/2307.13854,"WebArena evaluates autonomous web agents on realistic tasks across multiple interactive sites (e-commerce, forums, code hosting, etc.), requiring multi-step navigation and form interactions. It tests end-to-end web action policies, including interpreting UI state, planning, and recovering from errors under partial observability.","Visual Perception, Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Sensorimotor Coordination, Planning, Decision-making, Adaptive Error Correction, Working Memory, Attention, Episodic Memory"
