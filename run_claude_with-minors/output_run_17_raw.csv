Benchmark,Website,Paper,Description,Cognitive Functions
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates software engineering agents by giving them real GitHub issues and a code repository, requiring a patch that makes the project’s tests pass. The “Verified” subset uses human-validated tasks that are confirmed solvable and reduces noise from ambiguous or broken evaluations.","Planning, Logical Reasoning, Working Memory, Adaptive Error Correction, Language Comprehension, Language Production"
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 measures an agent’s ability to solve practical, real-world tasks in a command-line (terminal) environment, typically by running commands, inspecting files, and iterating until goals are met. It stresses interactive problem solving under tool constraints and frequent feedback from program outputs/errors.","Planning, Decision-making, Working Memory, Adaptive Error Correction, Language Comprehension"
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp evaluates deep-research agents on questions that require searching, reading, and synthesizing information from a constrained web/document collection. Successful solutions depend on finding relevant sources, integrating evidence across documents, and producing a concise grounded answer.","Planning, Attention, Working Memory, Semantic Understanding & Context Recognition, Language Comprehension, Language Production, Self-reflection"
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench tests whether an agent can complete multi-turn customer-support tasks while interacting with simulated users and API tools under domain policies (e.g., retail, airline, telecom). It emphasizes robust tool use, policy compliance, and consistent dialogue over extended interactions.","Social Reasoning & Theory of Mind, Decision-making, Planning, Inhibitory Control, Working Memory, Language Comprehension, Language Production"
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld evaluates multimodal “computer use” agents that must operate within a graphical desktop environment to complete tasks across applications. Agents perceive screenshots/GUI state, plan sequences of actions (click/type/navigation), and adapt to changing UI feedback.","Visual Perception, Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Planning, Decision-making, Sensorimotor Coordination, Working Memory, Adaptive Error Correction"
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,ARC-AGI is a fluid-intelligence benchmark of abstract grid transformation tasks where models infer a hidden rule from a few input–output examples and apply it to a new input. It is designed to prioritize generalization to novel patterns rather than memorized knowledge.,"Logical Reasoning, Cognitive Flexibility, Spatial Representation & Mapping, Working Memory, Attention"
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon agent performance in a simulated vending-machine business over many decision steps (e.g., pricing, inventory, supplier negotiation). The score reflects sustained coherence, strategy, and adaptation over extended time horizons.","Planning, Decision-making, Working Memory, Reward Mechanisms, Adaptive Error Correction, Self-reflection, Motivational Drives"
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas measures real-world tool use via the Model Context Protocol, requiring models to discover tools, call them correctly, manage multi-step workflows, and handle errors/retries. It focuses on whether an agent can reliably execute end-to-end tasks over multiple tool calls.","Planning, Decision-making, Working Memory, Adaptive Error Correction, Language Comprehension, Language Production"
FinanceAgent,https://www.vals.ai/benchmarks/finance_agent,,"FinanceAgent assesses performance on tasks typical of an entry-level financial analyst, such as analysis, reporting, and workflow-style problem solving over financial artifacts. It emphasizes correct reasoning under domain constraints, clear communication, and robust handling of multi-step requirements.","Semantic Understanding & Context Recognition, Logical Reasoning, Planning, Decision-making, Working Memory, Language Production"
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity agents on tasks such as identifying known vulnerabilities from descriptions and discovering vulnerabilities in real open-source projects. It stresses technical reasoning, iterative testing/verification, and precise execution of investigative steps.","Planning, Logical Reasoning, Working Memory, Adaptive Error Correction, Decision-making, Inhibitory Control"
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"SpreadsheetBench evaluates an agent’s ability to understand, navigate, and manipulate complex spreadsheets (often via programmatic or tool-assisted operations). Tasks require extracting structure, applying transformations, and producing correct computed outputs across many dependent cells.","Logical Reasoning, Working Memory, Planning, Adaptive Error Correction, Attention, Semantic Understanding & Context Recognition"
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a multimodal benchmark aimed at frontier difficulty, spanning challenging questions across domains and often requiring synthesis rather than recall. It tests broad knowledge, reasoning, and—in tool-enabled settings—verification and iterative refinement.","Language Comprehension, Language Production, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Scene Understanding & Visual Reasoning, Visual Perception, Self-reflection"
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 uses problems from the American Invitational Mathematics Examination to evaluate competition-style mathematical problem solving. It rewards correct multi-step reasoning, algebraic manipulation, and careful constraint tracking under time-like pressure.","Logical Reasoning, Working Memory, Attention, Planning"
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,GPQA Diamond is a high-quality subset of GPQA consisting of very difficult graduate-level science multiple-choice questions designed to be “Google-proof.” It targets deep scientific reasoning and disambiguation rather than surface-level recall.,"Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Attention, Language Comprehension"
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends the MMLU-style multi-subject test to multiple languages, evaluating academic knowledge and reasoning across diverse linguistic contexts. It probes whether competence transfers across languages and scripts while preserving accuracy and instruction following.","Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory"
MMMU,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2311.16502,"MMMU is a multimodal benchmark spanning many disciplines where questions require integrating text with images (charts, diagrams, photos) to answer. It emphasizes visual understanding plus domain reasoning under varied formats and question types.","Visual Perception, Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Working Memory, Language Comprehension"
LAB-Bench FigQA,https://huggingface.co/datasets/futurehouse/lab-bench,https://arxiv.org/abs/2407.10362,LAB-Bench FigQA evaluates whether models can correctly interpret complex scientific figures from biology papers and answer questions grounded in those visuals. It stresses extracting quantitative/structural information from plots and diagrams and linking it to scientific context.,"Visual Perception, Scene Understanding & Visual Reasoning, Logical Reasoning, Working Memory, Attention, Semantic Understanding & Context Recognition"
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning tests reasoning over scientific figures and charts (drawn from arXiv-style papers), often requiring careful reading of axes/legends and multi-step inference. In tool-enabled variants, models may also use computation to support quantitative conclusions.","Visual Perception, Scene Understanding & Visual Reasoning, Logical Reasoning, Working Memory, Attention, Language Comprehension"
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU evaluates multimodal understanding and reasoning over video, where answers depend on events unfolding across time rather than a single frame. It stresses temporal integration, maintaining relevant details, and grounding language in dynamic visual evidence.","Visual Perception, Visual Attention & Eye Movements, Working Memory, Cognitive Timing & Predictive Modeling, Scene Understanding & Visual Reasoning, Language Comprehension"
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"The FACTS Benchmark Suite systematically evaluates factuality, including whether model outputs are supported by provided context and whether they avoid unsupported claims. It targets reliable attribution, uncertainty handling, and minimizing hallucinations across diverse settings.","Semantic Understanding & Context Recognition, Language Comprehension, Language Production, Inhibitory Control, Self-reflection, Attention"
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,Global PIQA is a multilingual physical commonsense reasoning benchmark that extends PIQA-style questions across languages (non-parallel). It probes whether models can choose plausible physical interactions/solutions while maintaining robustness to linguistic variation.,"Logical Reasoning, Semantic Understanding & Context Recognition, Language Comprehension, Working Memory"
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,MRCR v2 (8-needle) measures long-context multi-round coreference resolution by embedding multiple similar “needle” requests across long “haystack” conversations and asking for the response corresponding to a specific needle. It stresses precise retrieval and interference resistance under heavy context.,"Working Memory, Attention, Episodic Memory, Language Comprehension, Inhibitory Control"
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval evaluates well-specified professional knowledge-work tasks across many occupations, scored by expert human judges via head-to-head comparisons with professional outputs. Tasks often require producing realistic artifacts (e.g., plans, analyses, presentations/spreadsheets) with strong adherence to constraints.","Planning, Decision-making, Language Production, Semantic Understanding & Context Recognition, Working Memory, Self-reflection, Adaptive Error Correction"
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks evaluates whether models can reliably execute graph traversal and relational reasoning tasks (e.g., BFS-like navigation) described in text, often with many entities and steps. It is designed to expose failures in maintaining state and following multi-step structured constraints.","Spatial Representation & Mapping, Working Memory, Logical Reasoning, Attention, Planning"
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon benchmarks agentic tool use across diverse APIs and multi-step workflows, focusing on selecting the right tools, calling them correctly, and recovering from tool errors. It emphasizes robust orchestration, iteration, and synthesis into a final user-facing result.","Planning, Decision-making, Working Memory, Adaptive Error Correction, Language Comprehension, Language Production"
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath is a benchmark of expert-level mathematics problems intended to be substantially beyond routine contest sets, emphasizing hard reasoning and proof-like multi-step derivations. Tool-enabled variants additionally test whether models can use computation appropriately without losing mathematical rigor.","Logical Reasoning, Working Memory, Planning, Attention, Adaptive Error Correction"
