Benchmark,Website,Paper,Description,Cognitive Functions
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates whether a model can solve real GitHub software engineering issues by generating code patches that make a provided test suite pass. The “Verified” subset consists of tasks that have been manually checked to be solvable and reliably testable, making results more comparable across systems.","Planning, Decision-making, Logical Reasoning, Working Memory, Adaptive Error Correction, Language Comprehension, Language Production"
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 measures agentic performance on real command-line tasks in containerized environments (e.g., installing packages, manipulating files, debugging, and running programs). Success depends on iteratively issuing correct shell commands, interpreting outputs, and recovering from errors under resource constraints.","Planning, Decision-making, Working Memory, Adaptive Error Correction, Attention, Language Comprehension"
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp benchmarks “deep research” behavior where a model must browse a document collection or the web (depending on setup) to answer difficult questions that require multi-hop evidence gathering. It emphasizes search strategy, source selection, and synthesizing a final answer grounded in retrieved evidence.","Planning, Decision-making, Working Memory, Attention, Language Comprehension, Semantic Understanding & Context Recognition, Self-reflection"
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench evaluates tool-using customer-support agents interacting with simulated users and APIs in domains like retail, airlines, and telecom. Models must follow policies, take multi-turn actions (e.g., refunds, rebookings, troubleshooting), and maintain consistency across long interactions.","Planning, Decision-making, Inhibitory Control, Social Reasoning & Theory of Mind, Empathy, Working Memory, Language Comprehension, Language Production"
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld evaluates multimodal “computer use” agents that complete tasks by operating a desktop OS through screenshots and UI actions (clicks, typing, navigation). It stresses robust perception of interfaces and long-horizon execution of multi-step workflows with feedback from the environment.","Visual Perception, Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Sensorimotor Coordination, Planning, Decision-making, Working Memory, Adaptive Error Correction"
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI tests fluid reasoning on abstract grid transformation puzzles: given a few input–output examples, the model must infer the hidden rule and produce the correct output for a new input. The benchmark is designed to minimize reliance on memorized knowledge and reward flexible pattern induction.","Logical Reasoning, Cognitive Flexibility, Working Memory, Spatial Representation & Mapping, Visual Perception, Attention"
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon agent coherence by running a simulated vending-machine business over an extended period, tracking outcomes like profit and business health. Agents must plan, negotiate, manage inventory/pricing, and adapt to changing conditions across many steps.","Planning, Decision-making, Working Memory, Reward Mechanisms, Cognitive Timing & Predictive Modeling, Adaptive Error Correction, Self-reflection"
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas measures real-world tool use through the Model Context Protocol (MCP), requiring models to discover, call, and chain tools across multi-step workflows. Tasks test correct API selection and parameterization, error handling, and synthesis of tool outputs into final responses.","Planning, Decision-making, Working Memory, Adaptive Error Correction, Language Comprehension, Language Production"
FinanceAgent,https://www.vals.ai/benchmarks/finance_agent,,"FinanceAgent assesses whether models can perform tasks typical of an entry-level financial analyst (e.g., extracting facts from documents, computing metrics, building simple financial artifacts, and explaining results). It emphasizes faithful analysis, numeric/financial reasoning, and structured reporting.","Logical Reasoning, Decision-making, Working Memory, Semantic Understanding & Context Recognition, Language Production, Attention"
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity agent skills on real-world vulnerability tasks, including identifying known vulnerabilities from descriptions and discovering new issues in open-source codebases. It typically requires code comprehension, hypothesis testing, and iterative debugging or exploitation reasoning.","Logical Reasoning, Planning, Decision-making, Working Memory, Adaptive Error Correction, Attention, Semantic Understanding & Context Recognition"
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"SpreadsheetBench tests an agent’s ability to understand and manipulate complex spreadsheets using programmatic or tool-based interactions (e.g., editing cells, building formulas, cleaning data, and producing correct outputs). It emphasizes multi-step procedural accuracy and consistency with spreadsheet semantics.","Planning, Decision-making, Working Memory, Adaptive Error Correction, Semantic Understanding & Context Recognition, Attention"
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,Humanity’s Last Exam is a difficult multimodal benchmark intended to probe frontier knowledge and reasoning across many domains. Questions may require integrating text with images/figures and carrying out multi-step inference rather than recalling a single fact.,"Language Comprehension, Logical Reasoning, Working Memory, Semantic Understanding & Context Recognition, Visual Perception, Scene Understanding & Visual Reasoning, Attention"
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 consists of competition-style math problems from the American Invitational Mathematics Examination, requiring exact numeric answers. It emphasizes multi-step symbolic reasoning, careful constraint handling, and error-free arithmetic/algebraic manipulation.","Logical Reasoning, Working Memory, Attention, Adaptive Error Correction"
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a high-difficulty subset of GPQA containing graduate-level, “Google-proof” multiple-choice science questions. It stresses deep conceptual understanding and reasoning in physics, chemistry, and biology under adversarially hard distractors.","Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Attention, Decision-making"
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends MMLU to multiple languages, testing broad academic knowledge and reasoning across many subjects via multilingual multiple-choice questions. It probes whether models can transfer competence across languages while maintaining consistent reasoning quality.","Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Attention"
MMMU,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2311.16502,"MMMU is a multidisciplinary multimodal benchmark where questions require joint reasoning over text and images (e.g., diagrams, charts, and visual scenes). It targets expert-level multimodal understanding across STEM, humanities, and applied domains.","Multisensory Integration, Visual Perception, Scene Understanding & Visual Reasoning, Language Comprehension, Logical Reasoning, Working Memory, Attention"
LAB-Bench FigQA,https://huggingface.co/datasets/futurehouse/lab-bench,https://arxiv.org/abs/2407.10362,"LAB-Bench FigQA evaluates whether models can answer questions about complex biology figures from scientific papers, requiring accurate extraction of visual evidence and domain-relevant interpretation. It stresses figure literacy (axes, legends, panels) and reasoning from experimental results.","Visual Perception, Scene Understanding & Visual Reasoning, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Attention"
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning tests scientific-figure understanding and reasoning using questions derived from research-paper artifacts (e.g., charts/plots) paired with contextual text. Success requires correctly reading graphical encodings and performing chained reasoning, often aided by quantitative calculation.","Visual Perception, Scene Understanding & Visual Reasoning, Logical Reasoning, Working Memory, Attention, Semantic Understanding & Context Recognition"
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU evaluates multimodal understanding and reasoning over video content paired with questions that may require temporal integration across scenes. It emphasizes tracking events over time, extracting salient visual cues, and using context to answer multi-step queries.","Visual Perception, Visual Attention & Eye Movements, Cognitive Timing & Predictive Modeling, Working Memory, Scene Understanding & Visual Reasoning, Language Comprehension, Multisensory Integration"
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"FACTS Benchmark Suite systematically evaluates factuality and grounding, measuring whether model outputs remain accurate, supported, and non-hallucinatory under various prompting and retrieval settings. It focuses on claim correctness, attribution to sources when available, and robustness to misleading contexts.","Semantic Understanding & Context Recognition, Language Comprehension, Language Production, Inhibitory Control, Self-reflection, Working Memory, Attention"
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA benchmarks physical commonsense reasoning across many languages and culturally diverse settings, typically via selecting the more plausible solution to a situation. It tests whether models maintain commonsense and pragmatic reasoning beyond English-centric distributions.","Logical Reasoning, Semantic Understanding & Context Recognition, Language Comprehension, Working Memory, Decision-making, Spatial Representation & Mapping"
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) is a long-context evaluation where multiple similar “needle” interactions are embedded in a long “haystack,” and the model must retrieve and reproduce the correct response for a specified needle. The 8-needle variant increases interference and tests robust in-context retrieval over long documents.","Working Memory, Attention, Episodic Memory, Language Comprehension, Semantic Understanding & Context Recognition"
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval measures performance on well-specified professional knowledge-work tasks spanning many occupations, judged by expert humans via pairwise comparisons. Tasks often require producing real work artifacts (e.g., spreadsheets, presentations, schedules) under realistic constraints and quality standards.","Planning, Decision-making, Language Production, Semantic Understanding & Context Recognition, Working Memory, Adaptive Error Correction, Self-reflection"
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks evaluates structured reasoning over graph-structured data, such as following edges, performing BFS-like traversals, or identifying parent/ancestor relationships. It stresses consistent step-by-step tracking and preventing drift when many similar entities compete for attention.","Spatial Representation & Mapping, Working Memory, Logical Reasoning, Planning, Attention, Adaptive Error Correction"
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon evaluates generalized tool-use competence across diverse tool APIs and multi-step tasks, emphasizing choosing the right tool, composing calls, and integrating results into a correct final output. It also stresses robustness to tool errors, partial failures, and ambiguous requirements.","Planning, Decision-making, Working Memory, Adaptive Error Correction, Attention, Inhibitory Control, Language Comprehension, Language Production"
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath is a high-difficulty mathematics benchmark targeting advanced problem-solving beyond standard contest sets, often requiring creative insight and long chains of derivation. It is designed to reduce contamination and better reflect frontier-level mathematical reasoning.","Logical Reasoning, Working Memory, Attention, Planning, Adaptive Error Correction"
