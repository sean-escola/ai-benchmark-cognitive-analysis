Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates software engineering agents on real GitHub issues, requiring models to generate code patches that fix bugs or implement small changes and pass repository tests. The “Verified” subset emphasizes tasks confirmed to be solvable and aims to reduce noise from ambiguous or broken test setups.","L1: Language Comprehension
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition
L3: ",L2
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 measures an agent’s ability to complete practical command-line tasks in sandboxed environments (e.g., debugging, file/system operations, installing dependencies, and running programs). It stresses iterative tool use, error recovery from failed commands, and maintaining a coherent plan over multi-step workflows.","L1: Language Comprehension
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory
L3: ",L2
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp evaluates “deep research” performance where agents must search within a controlled web-style corpus and synthesize answers to complex questions. It emphasizes long-horizon information gathering, source triangulation, and integrating evidence across many documents.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Planning, Decision-making, Working Memory
L3: ",L2
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench evaluates tool-using customer-support agents in simulated domains (e.g., retail/airline/telecom) that require multi-turn dialogue, policy adherence, and API interactions. Success depends on following constraints while still helping the user, including handling edge cases and long interaction histories.","L1: Language Comprehension, Language Production
L2: Decision-making, Planning, Working Memory
L3: Social Reasoning & Theory of Mind, Inhibitory Control",L3
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld benchmarks computer-use agents that must operate desktop-like GUIs to accomplish tasks (navigation, form filling, app usage, file manipulation) from visual observations. It tests end-to-end perception-to-action competence under long-horizon constraints and frequent interface ambiguity.","L1: Visual Perception
L2: Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Sensorimotor Coordination, Planning, Decision-making, Working Memory
L3: ",L2
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI measures fluid, example-efficient abstract reasoning via small grid-based puzzles where the model must infer hidden transformation rules from a few input-output examples. It emphasizes generalization to novel patterns rather than memorized task templates.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Logical Reasoning, Working Memory
L3: Cognitive Flexibility",L3
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon agent coherence and strategy in a simulated vending-machine business run over many sequential decisions (inventory, pricing, supplier negotiation, and adaptation to market changes). Scores reflect sustained planning, consistent execution, and reacting to feedback over extended time.","L1: 
L2: Planning, Decision-making, Working Memory, Reward Mechanisms, Adaptive Error Correction
L3: ",L2
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas measures real-world tool use through the Model Context Protocol, requiring models to discover tools, execute multi-step workflows across services, and handle tool errors/retries. It focuses on practical API orchestration and correct integration of tool outputs into final answers.","L1: Language Comprehension
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory
L3: ",L2
FinanceAgent,https://www.vals.ai/benchmarks/finance_agent,,"FinanceAgent benchmarks agent performance on tasks typical of an entry-level financial analyst, such as analysis, modeling, and report-style outputs grounded in provided materials. It emphasizes structured reasoning with domain constraints and producing professional-grade written artifacts.","L1: Language Production
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Planning, Decision-making, Working Memory
L3: ",L2
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity agents on vulnerability identification and discovery in real open-source projects, often requiring codebase navigation and exploit/bug reasoning. It stresses precise technical reasoning, iterative testing, and careful validation under uncertainty.","L1: 
L2: Logical Reasoning, Planning, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition
L3: Inhibitory Control",L3
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"SpreadsheetBench evaluates whether models can navigate and manipulate complex spreadsheets to solve real-world-style tasks (data cleaning, formulas, restructuring, and consistency checks). It emphasizes multi-step procedural accuracy and maintaining correct intermediate state across many edits.","L1: 
L2: Planning, Decision-making, Working Memory, Logical Reasoning, Adaptive Error Correction, Semantic Understanding & Context Recognition
L3: ",L2
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a multimodal benchmark designed to probe frontier academic reasoning and knowledge across a wide range of difficult questions. It often rewards careful synthesis, multi-step inference, and (when enabled in a harness) effective use of tools like search or code for verification.","L1: Language Comprehension, Visual Perception
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Scene Understanding & Visual Reasoning
L3: ",L2
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 evaluates competition-style mathematics problem solving, typically requiring multi-step derivations and precise final numeric answers. It stresses symbolic manipulation, strategic decomposition, and error-sensitive reasoning under tight correctness criteria.","L1: 
L2: Logical Reasoning, Working Memory, Planning, Adaptive Error Correction
L3: ",L2
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a high-difficulty multiple-choice science benchmark intended to be “Google-proof,” emphasizing reasoning rather than simple retrieval. The Diamond subset focuses on especially high-quality questions where experts succeed and non-experts usually fail.","L1: Language Comprehension
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory
L3: ",L2
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends broad academic knowledge testing across many subjects into multiple languages, assessing how well models understand and answer non-English prompts. It probes cross-lingual robustness of reasoning and knowledge application rather than English-only competence.","L1: Language Comprehension, Language Production
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory
L3: ",L2
MMMU,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2311.16502,"MMMU is a multimodal, multi-discipline benchmark where questions require jointly reasoning over images and text across expert domains. It evaluates whether models can integrate visual evidence with language-based knowledge and perform multi-step inference.","L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Working Memory
L3: ",L2
LAB-Bench FigQA,https://huggingface.co/datasets/futurehouse/lab-bench,https://arxiv.org/abs/2407.10362,"LAB-Bench FigQA tests scientific figure understanding in biology papers, requiring models to interpret charts/diagrams and answer grounded questions about experimental results or relationships. It emphasizes careful visual parsing, domain-aware interpretation, and reasoning over plotted/diagrammatic evidence.","L1: Visual Perception
L2: Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Logical Reasoning, Semantic Understanding & Context Recognition
L3: ",L2
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning evaluates reasoning over scientific charts/figures from arXiv-style papers, often benefiting from precise quantitative extraction and structured interpretation. It tests whether models can connect visual encodings (axes, legends, trends) to correct analytical conclusions.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Logical Reasoning, Working Memory, Attention
L3: ",L2
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU evaluates multimodal understanding and reasoning over videos, requiring integration of information across frames and time. Tasks often depend on tracking events, interpreting visual context, and answering questions that require temporal and causal inference.","L1: Visual Perception
L2: Attention, Working Memory, Scene Understanding & Visual Reasoning, Multisensory Integration
L3: Cognitive Timing & Predictive Modeling",L3
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"FACTS Benchmark Suite systematically evaluates factuality and grounding by testing whether models produce statements consistent with reliable evidence and avoid unsupported claims. It targets failures like hallucinations, misattribution, and overconfident incorrect assertions across diverse settings.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Adaptive Error Correction
L3: Inhibitory Control, Self-reflection",L3
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,Global PIQA evaluates physical commonsense reasoning with broader cultural and linguistic coverage than traditional PIQA-style datasets. It emphasizes choosing plausible actions or explanations in everyday physical scenarios while remaining robust across language variants.,"L1: Language Comprehension
L2: Logical Reasoning, Semantic Understanding & Context Recognition
L3: Cognitive Flexibility",L3
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) tests long-context, multi-round co-reference resolution by embedding multiple similar “needle” interactions in a large “haystack” and asking the model to retrieve the correct referenced response. It primarily probes whether models can maintain and selectively retrieve the right entity/event linkage across very long contexts.","L1: Language Comprehension
L2: Working Memory, Attention, Semantic Understanding & Context Recognition, Episodic Memory
L3: ",L2
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval is a human-judged evaluation of economically valuable, well-specified professional tasks spanning many occupations, where models produce real work artifacts (e.g., spreadsheets, presentations, schedules). It emphasizes end-to-end planning, producing usable outputs, and aligning deliverables to explicit requirements.","L1: Language Production
L2: Planning, Decision-making, Semantic Understanding & Context Recognition, Working Memory
L3: Social Reasoning & Theory of Mind",L3
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks evaluates whether models can correctly follow and reason over graph-structured data (e.g., performing multi-step traversals or answering queries about relationships). It stresses maintaining intermediate state over many hops and avoiding drift as the walk length increases.","L1: 
L2: Spatial Representation & Mapping, Working Memory, Logical Reasoning, Attention, Planning
L3: ",L2
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon benchmarks tool-use competence across diverse tools/APIs, emphasizing correct tool selection, argument construction, sequencing, and recovery from tool failures. It targets agentic reliability in realistic multi-step workflows rather than single-shot question answering.","L1: Language Comprehension
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory
L3: ",L2
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath evaluates expert-level mathematics, including problems intended to be challenging for current frontier models and to resist superficial pattern matching. It emphasizes deep multi-step reasoning, careful symbolic/quantitative manipulation, and rigorous verification of results.","L1: 
L2: Logical Reasoning, Working Memory, Planning, Adaptive Error Correction
L3: ",L2
