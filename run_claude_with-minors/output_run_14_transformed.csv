Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates agentic software engineering by asking a model to produce a patch that fixes real issues in open-source Python repositories, validated by running the project’s tests. The “Verified” subset consists of tasks that have been manually confirmed to be solvable and correctly specified, emphasizing reliable end-to-end bug fixing over toy coding.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Logical Reasoning, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition
L3: ",L2
SWE-bench Multilingual,https://www.swebench.com/multilingual.html,https://arxiv.org/abs/2504.21798,"SWE-bench Multilingual extends SWE-bench-style repository patching to multiple programming languages, assessing whether a model can understand unfamiliar codebases and toolchains beyond Python. It emphasizes robust cross-language debugging, patch generation, and test-driven validation under realistic repo constraints.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Logical Reasoning, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition
L3: Cognitive Flexibility",L3
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro is a larger and more difficult software engineering benchmark designed to be more industrially relevant and contamination-resistant, spanning multiple languages and more complex issues. It tests whether a model can navigate larger repositories, interpret failing tests and logs, and deliver correct patches under stricter evaluation conditions.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Logical Reasoning, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition
L3: Cognitive Flexibility",L3
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 evaluates real-world command-line task completion in isolated environments, requiring models to inspect files, run commands, interpret outputs, and iteratively refine actions. It focuses on practical “computer use” through a terminal, where success depends on correct sequencing, debugging, and state tracking across steps.","L1: 
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition
L3: Inhibitory Control, Cognitive Flexibility",L3
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp evaluates deep research and browsing competence using a controlled document set, where models must search, read, synthesize evidence, and produce grounded answers. It stresses tool-augmented information seeking, long-horizon planning of queries, and integrating multiple sources while avoiding unsupported claims.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Attention, Semantic Understanding & Context Recognition, Episodic Memory, Working Memory
L3: ",L2
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench measures an agent’s ability to handle multi-turn customer-support scenarios while calling tools/APIs and adhering to domain policies (e.g., retail, airline, telecom). It emphasizes policy-following behavior in interactive contexts, including correct tool selection, consistent state updates, and resolving ambiguous user goals.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Working Memory, Adaptive Error Correction, Reward Mechanisms
L3: Social Reasoning & Theory of Mind, Empathy, Inhibitory Control",L3
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld evaluates multimodal computer-use agents on realistic operating system tasks, requiring interaction with GUI elements, application workflows, and multi-step procedures. Models must perceive screens, decide actions, and execute long sequences reliably while tracking progress and recovering from errors.","L1: Visual Perception
L2: Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Planning, Decision-making, Working Memory, Spatial Representation & Mapping, Sensorimotor Coordination, Adaptive Error Correction
L3: ",L2
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,ARC-AGI tests few-shot abstract reasoning on grid-based puzzles where the rule must be inferred from only a handful of input–output examples. It is designed to probe generalization to novel patterns and transformations rather than memorized domain knowledge.,"L1: 
L2: Logical Reasoning, Working Memory, Attention, Spatial Representation & Mapping, Scene Understanding & Visual Reasoning, Planning
L3: Cognitive Flexibility",L3
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon autonomy by having an agent run a simulated vending-machine business over many decisions, optimizing inventory, pricing, procurement, and strategy. Performance depends on maintaining coherent goals across time, adapting to market dynamics, and learning from feedback signals like profit and stockouts.","L1: 
L2: Planning, Decision-making, Working Memory, Episodic Memory, Reward Mechanisms, Adaptive Error Correction
L3: Motivational Drives, Self-reflection",L3
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas evaluates real-world tool use via the Model Context Protocol, requiring models to discover appropriate tools, invoke them correctly, handle errors, and synthesize results into accurate answers. Tasks typically require multi-step workflows across heterogeneous servers/APIs, emphasizing robust orchestration over single-call tool usage.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Working Memory, Semantic Understanding & Context Recognition, Adaptive Error Correction
L3: Cognitive Flexibility",L3
FinanceAgent,https://www.vals.ai/benchmarks/finance_agent,,"FinanceAgent assesses performance on tasks typical of an entry-level financial analyst, such as interpreting financial documents, performing calculations, and producing structured analyses or models. It probes applied reasoning under domain constraints, where correctness depends on grounding in provided data and consistent assumptions.","L1: Language Comprehension, Language Production
L2: Logical Reasoning, Planning, Decision-making, Working Memory, Semantic Understanding & Context Recognition, Adaptive Error Correction
L3: ",L2
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity agent capabilities on large-scale tasks involving finding known vulnerabilities and discovering previously unknown ones in real software projects. It requires interpreting vulnerability descriptions, navigating codebases, reproducing issues, and proposing fixes or exploits under realistic constraints.","L1: 
L2: Logical Reasoning, Planning, Decision-making, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition
L3: Cognitive Flexibility, Inhibitory Control",L3
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"SpreadsheetBench measures the ability to understand, edit, and compute with complex spreadsheets, including multi-sheet dependencies and real-world formatting/logic. Success requires mapping goals to precise operations, tracking intermediate values, and validating outputs against constraints.","L1: 
L2: Working Memory, Planning, Decision-making, Logical Reasoning, Adaptive Error Correction, Semantic Understanding & Context Recognition, Attention
L3: ",L2
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a challenging multimodal benchmark spanning frontier questions across many domains, designed to test advanced reasoning, knowledge integration, and (when enabled) tool-assisted problem solving. It stresses careful interpretation, synthesis, and the ability to avoid hallucinated justifications when evidence is limited.","L1: Language Comprehension, Language Production, Visual Perception
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Attention, Scene Understanding & Visual Reasoning
L3: ",L2
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 evaluates competition-level mathematical problem solving using short, high-difficulty questions that require multi-step derivations and precision. It rewards robust symbolic manipulation, structured reasoning, and checking of intermediate steps under tight problem specifications.","L1: 
L2: Logical Reasoning, Working Memory, Planning, Adaptive Error Correction, Attention
L3: ",L2
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,GPQA Diamond is a subset of GPQA consisting of especially difficult graduate-level multiple-choice science questions designed to be resistant to simple web lookup. It tests deep scientific understanding and careful reasoning under distractors where superficial pattern matching often fails.,"L1: 
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Attention, Decision-making
L3: ",L2
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,MMMLU is a multilingual extension of MMLU that measures broad academic knowledge and reasoning across many subjects in numerous languages. It emphasizes cross-lingual comprehension and consistent conceptual reasoning despite linguistic variation.,"L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Attention
L3: Cognitive Flexibility",L3
MMMU,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2311.16502,"MMMU evaluates multimodal expert-level understanding across many disciplines, requiring models to answer questions that combine text with images such as diagrams, plots, and technical figures. It stresses integrating visual evidence with domain knowledge and reasoning to select or generate correct answers.","L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Working Memory, Attention, Spatial Representation & Mapping
L3: ",L2
LAB-Bench FigQA,https://huggingface.co/datasets/futurehouse/lab-bench,https://arxiv.org/abs/2407.10362,"LAB-Bench FigQA evaluates whether models can correctly interpret complex biology paper figures, including extracting trends, comparing conditions, and drawing mechanistic conclusions from visual evidence. It focuses on scientific visual reasoning grounded in figure content rather than general image captioning.","L1: Visual Perception, Language Comprehension, Language Production
L2: Scene Understanding & Visual Reasoning, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Attention
L3: ",L2
WebArena,https://webarena.dev/,https://arxiv.org/abs/2307.13854,"WebArena evaluates autonomous web agents on realistic tasks across multiple web apps (e-commerce, CMS, forums, code hosting, maps), requiring navigation, form filling, and multi-step workflows. It measures long-horizon planning and robustness to dynamic interfaces, including recovering from mistakes and ambiguities in web state.","L1: Visual Perception
L2: Planning, Decision-making, Working Memory, Adaptive Error Correction, Visual Attention & Eye Movements, Semantic Understanding & Context Recognition
L3: Cognitive Flexibility, Social Reasoning & Theory of Mind",L3
