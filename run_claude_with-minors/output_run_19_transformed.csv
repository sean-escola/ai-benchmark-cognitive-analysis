Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,SWE-bench Verified evaluates software engineering agents by asking them to produce patches that fix real issues in open-source Python repositories. The “Verified” subset consists of tasks confirmed by human engineers to be solvable and is typically scored by whether the patch passes tests and meets the issue requirements.,"L1: Language Comprehension, Language Production
L2: Planning, Logical Reasoning, Adaptive Error Correction, Working Memory
L3: ",L2
SWE-bench Multilingual,https://www.swebench.com/multilingual.html,https://arxiv.org/abs/2504.21798,"SWE-bench Multilingual extends SWE-bench-style repository patching beyond Python, covering multiple programming languages and ecosystems. It tests whether a model can understand problem statements, navigate unfamiliar codebases, and implement correct fixes under varying language/tooling conventions.","L1: Language Comprehension, Language Production
L2: Planning, Logical Reasoning, Adaptive Error Correction, Working Memory
L3: Cognitive Flexibility",L3
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro is a larger and more challenging software engineering benchmark designed to better reflect real industrial tasks across multiple languages and more complex repos. It emphasizes robust end-to-end issue resolution, including locating relevant files, implementing correct changes, and satisfying evaluation tests.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Logical Reasoning, Adaptive Error Correction, Working Memory
L3: Cognitive Flexibility",L3
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 evaluates agentic performance on real command-line tasks in sandboxed environments (e.g., debugging, building, scripting, and system operations). It probes whether agents can iteratively execute commands, interpret outputs/errors, and converge on a working solution under resource and time constraints.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory
L3: ",L2
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp evaluates “deep research” agents that must gather evidence across documents and synthesize answers, typically using browsing/search tools and long-context reasoning. The benchmark targets end-to-end research behavior: query planning, evidence aggregation, and producing grounded final responses.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Episodic Memory, Semantic Understanding & Context Recognition, Working Memory
L3: ",L2
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench (Tau²-bench) measures how well an agent completes multi-turn, tool-using customer support tasks while following domain policies (e.g., retail, airline, telecom). It stresses reliable API/tool calling, policy adherence, and conversational coordination across long interactions with a simulated user.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Working Memory
L3: Inhibitory Control, Social Reasoning & Theory of Mind",L3
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld evaluates multimodal computer-use agents operating in desktop-like environments, requiring screenshot/GUI understanding plus tool actions (mouse/keyboard) to complete tasks. It targets grounded interaction: perceiving UI state, choosing actions, recovering from errors, and completing multi-step workflows.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Sensorimotor Coordination, Planning, Decision-making, Adaptive Error Correction, Working Memory
L3: ",L2
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI tests abstract pattern induction from a few input-output grid examples, requiring the model to infer a latent rule and apply it to a new grid. It is designed to emphasize “fluid” reasoning and generalization rather than memorized knowledge.","L1: Visual Perception
L2: Logical Reasoning, Working Memory, Spatial Representation & Mapping, Scene Understanding & Visual Reasoning
L3: Cognitive Flexibility",L3
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon agent coherence and business planning in a simulated vending-machine company over many sequential decisions (inventory, pricing, supplier negotiation, adaptation). Success depends on maintaining strategy over time, reacting to feedback, and managing resources under uncertainty.","L1: 
L2: Planning, Decision-making, Reward Mechanisms, Working Memory, Adaptive Error Correction
L3: Self-reflection",L3
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas evaluates real-world tool use via the Model Context Protocol (MCP), where agents must discover appropriate tools, call them correctly, and compose multi-step workflows across services. It emphasizes execution reliability, handling tool failures, and synthesizing tool outputs into correct answers.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory
L3: ",L2
FinanceAgent,https://www.vals.ai/benchmarks/finance_agent,,"FinanceAgent measures performance on tasks typical of an entry-level financial analyst, such as interpreting financial documents, performing calculations, and generating structured deliverables. It stresses domain reasoning, consistency, and the ability to integrate quantitative and textual evidence into decisions.","L1: Language Comprehension, Language Production
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Planning, Decision-making, Working Memory
L3: ",L2
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity agents on vulnerability identification and (in some settings) discovery in real-world codebases, using task descriptions and repository context. It targets systematic debugging/analysis, hypothesis-driven investigation, and iterative refinement based on evidence from code and tooling.","L1: Language Comprehension, Language Production
L2: Logical Reasoning, Planning, Decision-making, Adaptive Error Correction, Working Memory
L3: Inhibitory Control",L3
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"SpreadsheetBench tests an agent’s ability to understand, edit, and compute within complex spreadsheets using realistic data and tasks. It stresses structured manipulation (formulas, tables, references), multi-step auditing, and producing correct spreadsheet outputs under tool-driven workflows.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Working Memory, Logical Reasoning, Adaptive Error Correction
L3: ",L2
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a large, frontier-difficulty benchmark spanning many expert domains (often multimodal) and is intended to stress broad reasoning and knowledge integration. It rewards models that can combine deep subject knowledge with careful multi-step inference and, when enabled, tool-grounded verification.","L1: Language Comprehension, Language Production, Visual Perception
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Scene Understanding & Visual Reasoning, Planning
L3: ",L2
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,AIME 2025 uses problems from the American Invitational Mathematics Examination to evaluate competition-style mathematical reasoning. Tasks require constructing multi-step solutions with precise symbolic/quantitative manipulation and minimal ambiguity in final numeric answers.,"L1: 
L2: Logical Reasoning, Working Memory, Planning, Adaptive Error Correction
L3: ",L2
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a high-quality subset of graduate-level, “Google-proof” multiple-choice science questions, intended to be hard for non-experts. It probes whether models can perform disciplined scientific reasoning under distractors rather than relying on superficial pattern matching.","L1: 
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Decision-making
L3: ",L2
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends MMLU-style broad academic testing to multiple languages, covering many subjects with multilingual prompts. It evaluates cross-lingual knowledge access and reasoning consistency when language changes, not just translation fluency.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory
L3: Cognitive Flexibility",L3
MMMU,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2311.16502,"MMMU is a multidisciplinary multimodal benchmark where questions require reasoning over both text and images (e.g., diagrams, charts, scientific visuals). It tests integrated understanding—extracting visual evidence, aligning it with text, and answering with domain-relevant reasoning.","L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Working Memory, Visual Attention & Eye Movements
L3: ",L2
LAB-Bench FigQA,https://huggingface.co/datasets/futurehouse/lab-bench,https://arxiv.org/abs/2407.10362,"LAB-Bench FigQA evaluates whether models can correctly interpret complex scientific figures from biology papers and answer questions that require figure-based reasoning. It targets visual-to-semantic grounding, careful extraction of plotted/diagrammed evidence, and domain-aware inference.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Visual Attention & Eye Movements
L3: ",L2
WebArena,https://webarena.dev/,https://arxiv.org/abs/2307.13854,"WebArena evaluates autonomous web agents completing realistic tasks across multiple web apps (e-commerce, forums, repos, maps) via navigation and form interactions. It stresses robust multi-step planning, UI state tracking, and recovery from mistakes while producing correct end states under a functional grader.","L1: Visual Perception, Language Comprehension, Language Production
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory, Visual Attention & Eye Movements, Sensorimotor Coordination
L3: ",L2
