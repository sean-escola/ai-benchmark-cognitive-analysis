Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates an LLM’s ability to solve real software engineering issues by producing code patches in real repositories and passing test suites. The “Verified” subset consists of tasks that have been confirmed by human review to be solvable as specified, emphasizing reliability over ambiguous or underspecified issues.","L1: Language Comprehension
L2: Planning, Logical Reasoning, Working Memory, Adaptive Error Correction, Decision-making, Semantic Understanding & Context Recognition
L3: ",L2
SWE-bench Multilingual,https://www.swebench.com/multilingual.html,https://arxiv.org/abs/2504.21798,"SWE-bench Multilingual extends SWE-bench-style repository bug-fixing to multiple programming languages, requiring models to understand, edit, and test code across diverse ecosystems. It probes whether agents can transfer debugging and patching skills beyond Python while still satisfying automated test constraints.","L1: Language Comprehension
L2: Planning, Logical Reasoning, Working Memory, Adaptive Error Correction, Decision-making, Semantic Understanding & Context Recognition
L3: Cognitive Flexibility",L3
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro is a larger, more difficult software engineering benchmark designed to be more industrially representative and more resistant to shortcutting via memorization. Tasks typically require deeper repository understanding, longer dependency chains, and more robust end-to-end patch generation.","L1: Language Comprehension
L2: Planning, Logical Reasoning, Working Memory, Adaptive Error Correction, Decision-making, Semantic Understanding & Context Recognition
L3: ",L2
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 measures agent performance on real command-line tasks in sandboxed environments, such as installing tools, manipulating files, running programs, and debugging failures. Success depends on selecting appropriate shell commands, interpreting outputs, and iterating when errors occur under resource constraints.","L1: 
L2: Planning, Decision-making, Working Memory, Adaptive Error Correction, Attention, Semantic Understanding & Context Recognition, Sensorimotor Coordination
L3: ",L2
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp evaluates deep-research agents by requiring them to answer questions using web browsing/search over a controlled document collection, emphasizing reproducibility. The benchmark stresses evidence gathering, cross-checking sources, and synthesizing findings into a final answer rather than relying on parametric memory alone.","L1: Language Comprehension
L2: Planning, Decision-making, Working Memory, Episodic Memory, Semantic Understanding & Context Recognition, Attention
L3: ",L2
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench evaluates interactive tool-using agents in simulated customer-support domains (e.g., retail, airline, telecom) with policies and multi-turn user interactions. Models must follow domain rules, use APIs correctly, and manage conversational state while resolving the user’s issue over time.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Working Memory, Semantic Understanding & Context Recognition
L3: Inhibitory Control, Social Reasoning & Theory of Mind, Empathy",L3
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld measures computer-use agents operating in realistic desktop environments, requiring them to navigate GUIs, interpret screenshots, and execute multi-step tasks (e.g., file management, app interactions). It stresses robust perception-action loops, error recovery, and maintaining task context across long trajectories.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Planning, Decision-making, Working Memory, Adaptive Error Correction, Sensorimotor Coordination
L3: ",L2
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,ARC-AGI tests fluid reasoning using small grid-based tasks where the model must infer latent transformation rules from a few input-output examples and apply them to a new input. It emphasizes systematic generalization to novel patterns rather than domain knowledge or extensive training on similar items.,"L1: Visual Perception
L2: Spatial Representation & Mapping, Logical Reasoning, Working Memory, Attention
L3: Cognitive Flexibility",L3
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon autonomous agency in a simulated business setting where an agent manages a vending machine operation over many steps. The agent must plan inventory and pricing, interact with suppliers/customers via messages, and adapt decisions based on outcomes to maximize final profit.","L1: 
L2: Planning, Decision-making, Working Memory, Episodic Memory, Reward Mechanisms, Adaptive Error Correction
L3: Social Reasoning & Theory of Mind",L3
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas evaluates real-world tool use through the Model Context Protocol, requiring models to discover tools, call them with correct arguments, handle failures, and compose multi-step workflows across services. It targets the reliability of agentic orchestration over authentic APIs rather than single-shot question answering.","L1: Language Comprehension
L2: Planning, Decision-making, Working Memory, Adaptive Error Correction, Semantic Understanding & Context Recognition
L3: ",L2
FinanceAgent,https://www.vals.ai/benchmarks/finance_agent,,"FinanceAgent assesses whether a model can perform tasks typical of an entry-level financial analyst, such as interpreting financial documents, doing structured analysis, and producing decision-relevant outputs. It emphasizes constraint-following, quantitative reasoning in context, and assembling coherent reports from heterogeneous information.","L1: Language Comprehension, Language Production
L2: Logical Reasoning, Planning, Decision-making, Working Memory, Semantic Understanding & Context Recognition
L3: ",L2
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity agents on real-world vulnerability tasks, including identifying known vulnerabilities from descriptions and attempting discovery of new issues in software projects. It stresses careful code reading, hypothesis-driven testing, and iterative debugging under realistic constraints.","L1: Language Comprehension
L2: Logical Reasoning, Planning, Decision-making, Working Memory, Adaptive Error Correction, Attention, Semantic Understanding & Context Recognition
L3: ",L2
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"SpreadsheetBench measures an agent’s ability to navigate, edit, and compute within complex spreadsheets using realistic work tasks (e.g., formulas, tables, formatting, and data transformations). It tests structured manipulation, error checking, and maintaining correctness across interdependent cells and sheets.","L1: 
L2: Planning, Decision-making, Working Memory, Adaptive Error Correction, Attention, Semantic Understanding & Context Recognition, Sensorimotor Coordination
L3: ",L2
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a frontier, multimodal benchmark spanning difficult questions across many expert domains, often requiring multi-step reasoning and (in some setups) tool use such as search or code execution. It emphasizes robust synthesis from provided information and/or retrieved evidence rather than shallow pattern completion.","L1: Language Comprehension, Language Production, Visual Perception
L2: Logical Reasoning, Planning, Decision-making, Working Memory, Semantic Understanding & Context Recognition, Scene Understanding & Visual Reasoning, Multisensory Integration
L3: ",L2
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 evaluates mathematical problem-solving on competition-style questions that require multi-step derivations and careful symbolic reasoning. Performance depends on selecting appropriate strategies, tracking intermediate results, and avoiding algebraic or logical slips under tight problem constraints.","L1: 
L2: Logical Reasoning, Working Memory, Planning, Adaptive Error Correction, Attention
L3: ",L2
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a difficult multiple-choice science benchmark curated to be “Google-proof,” emphasizing questions that require genuine understanding rather than retrieval of memorized facts. The Diamond subset focuses on high-quality items where experts succeed and non-experts often fail, highlighting reasoning over shallow cues.","L1: Language Comprehension
L2: Logical Reasoning, Working Memory, Semantic Understanding & Context Recognition, Attention
L3: ",L2
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends broad academic knowledge and reasoning evaluation across many subjects to multiple languages. It probes whether models can maintain competence under multilingual prompts, varying phrasing, and culturally/linguistically different formulations of similar concepts.","L1: Language Comprehension
L2: Logical Reasoning, Working Memory, Semantic Understanding & Context Recognition
L3: Cognitive Flexibility",L3
MMMU,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2311.16502,"MMMU evaluates expert-level multimodal understanding and reasoning across diverse disciplines by requiring models to answer questions grounded in images and text (e.g., diagrams, charts, photos). It stresses integrating visual evidence with domain knowledge and producing logically consistent explanations or selections.","L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Working Memory, Attention
L3: ",L2
LAB-Bench FigQA,https://huggingface.co/datasets/futurehouse/lab-bench,https://arxiv.org/abs/2407.10362,"LAB-Bench FigQA tests whether models can correctly interpret scientific figures from biology papers (plots, microscopy images, multi-panel figures) and answer questions requiring evidence-based visual analysis. It targets the combination of figure literacy, domain semantics, and reasoning about experimental results and trends.","L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Logical Reasoning, Working Memory, Attention, Semantic Understanding & Context Recognition
L3: ",L2
WebArena,https://webarena.dev/,https://arxiv.org/abs/2307.13854,"WebArena evaluates autonomous web agents performing realistic multi-step tasks across several web applications (e-commerce, CMS, forums, code hosting, maps). Agents must perceive dynamic pages, plan navigation and form-filling actions, recover from mistakes, and complete goals under a step budget.","L1: Visual Perception, Language Comprehension, Language Production
L2: Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Planning, Decision-making, Working Memory, Adaptive Error Correction, Sensorimotor Coordination
L3: ",L2
