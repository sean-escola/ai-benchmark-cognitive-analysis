Benchmark,Website,Paper,Description,Cognitive Functions
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates software engineering agents on real GitHub issues, requiring models to generate code patches that fix bugs or implement small changes and pass repository tests. The “Verified” subset emphasizes tasks confirmed to be solvable and aims to reduce noise from ambiguous or broken test setups.","Language Comprehension, Planning, Decision-making, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition"
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 measures an agent’s ability to complete practical command-line tasks in sandboxed environments (e.g., debugging, file/system operations, installing dependencies, and running programs). It stresses iterative tool use, error recovery from failed commands, and maintaining a coherent plan over multi-step workflows.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Language Comprehension"
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp evaluates “deep research” performance where agents must search within a controlled web-style corpus and synthesize answers to complex questions. It emphasizes long-horizon information gathering, source triangulation, and integrating evidence across many documents.","Language Comprehension, Semantic Understanding & Context Recognition, Planning, Decision-making, Working Memory"
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench evaluates tool-using customer-support agents in simulated domains (e.g., retail/airline/telecom) that require multi-turn dialogue, policy adherence, and API interactions. Success depends on following constraints while still helping the user, including handling edge cases and long interaction histories.","Language Comprehension, Language Production, Social Reasoning & Theory of Mind, Decision-making, Planning, Inhibitory Control, Working Memory"
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld benchmarks computer-use agents that must operate desktop-like GUIs to accomplish tasks (navigation, form filling, app usage, file manipulation) from visual observations. It tests end-to-end perception-to-action competence under long-horizon constraints and frequent interface ambiguity.","Visual Perception, Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Sensorimotor Coordination, Planning, Decision-making, Working Memory"
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI measures fluid, example-efficient abstract reasoning via small grid-based puzzles where the model must infer hidden transformation rules from a few input-output examples. It emphasizes generalization to novel patterns rather than memorized task templates.","Visual Perception, Scene Understanding & Visual Reasoning, Logical Reasoning, Cognitive Flexibility, Working Memory"
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon agent coherence and strategy in a simulated vending-machine business run over many sequential decisions (inventory, pricing, supplier negotiation, and adaptation to market changes). Scores reflect sustained planning, consistent execution, and reacting to feedback over extended time.","Planning, Decision-making, Working Memory, Reward Mechanisms, Adaptive Error Correction"
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas measures real-world tool use through the Model Context Protocol, requiring models to discover tools, execute multi-step workflows across services, and handle tool errors/retries. It focuses on practical API orchestration and correct integration of tool outputs into final answers.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Language Comprehension"
FinanceAgent,https://www.vals.ai/benchmarks/finance_agent,,"FinanceAgent benchmarks agent performance on tasks typical of an entry-level financial analyst, such as analysis, modeling, and report-style outputs grounded in provided materials. It emphasizes structured reasoning with domain constraints and producing professional-grade written artifacts.","Semantic Understanding & Context Recognition, Logical Reasoning, Planning, Decision-making, Working Memory, Language Production"
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity agents on vulnerability identification and discovery in real open-source projects, often requiring codebase navigation and exploit/bug reasoning. It stresses precise technical reasoning, iterative testing, and careful validation under uncertainty.","Logical Reasoning, Planning, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition, Inhibitory Control"
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"SpreadsheetBench evaluates whether models can navigate and manipulate complex spreadsheets to solve real-world-style tasks (data cleaning, formulas, restructuring, and consistency checks). It emphasizes multi-step procedural accuracy and maintaining correct intermediate state across many edits.","Planning, Decision-making, Working Memory, Logical Reasoning, Adaptive Error Correction, Semantic Understanding & Context Recognition"
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a multimodal benchmark designed to probe frontier academic reasoning and knowledge across a wide range of difficult questions. It often rewards careful synthesis, multi-step inference, and (when enabled in a harness) effective use of tools like search or code for verification.","Language Comprehension, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Visual Perception, Scene Understanding & Visual Reasoning"
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 evaluates competition-style mathematics problem solving, typically requiring multi-step derivations and precise final numeric answers. It stresses symbolic manipulation, strategic decomposition, and error-sensitive reasoning under tight correctness criteria.","Logical Reasoning, Working Memory, Planning, Adaptive Error Correction"
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a high-difficulty multiple-choice science benchmark intended to be “Google-proof,” emphasizing reasoning rather than simple retrieval. The Diamond subset focuses on especially high-quality questions where experts succeed and non-experts usually fail.","Language Comprehension, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory"
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends broad academic knowledge testing across many subjects into multiple languages, assessing how well models understand and answer non-English prompts. It probes cross-lingual robustness of reasoning and knowledge application rather than English-only competence.","Language Comprehension, Language Production, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory"
MMMU,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2311.16502,"MMMU is a multimodal, multi-discipline benchmark where questions require jointly reasoning over images and text across expert domains. It evaluates whether models can integrate visual evidence with language-based knowledge and perform multi-step inference.","Visual Perception, Scene Understanding & Visual Reasoning, Multisensory Integration, Language Comprehension, Logical Reasoning, Working Memory"
LAB-Bench FigQA,https://huggingface.co/datasets/futurehouse/lab-bench,https://arxiv.org/abs/2407.10362,"LAB-Bench FigQA tests scientific figure understanding in biology papers, requiring models to interpret charts/diagrams and answer grounded questions about experimental results or relationships. It emphasizes careful visual parsing, domain-aware interpretation, and reasoning over plotted/diagrammatic evidence.","Visual Perception, Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Logical Reasoning, Semantic Understanding & Context Recognition"
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning evaluates reasoning over scientific charts/figures from arXiv-style papers, often benefiting from precise quantitative extraction and structured interpretation. It tests whether models can connect visual encodings (axes, legends, trends) to correct analytical conclusions.","Visual Perception, Scene Understanding & Visual Reasoning, Logical Reasoning, Working Memory, Attention"
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU evaluates multimodal understanding and reasoning over videos, requiring integration of information across frames and time. Tasks often depend on tracking events, interpreting visual context, and answering questions that require temporal and causal inference.","Visual Perception, Cognitive Timing & Predictive Modeling, Attention, Working Memory, Scene Understanding & Visual Reasoning, Multisensory Integration"
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"FACTS Benchmark Suite systematically evaluates factuality and grounding by testing whether models produce statements consistent with reliable evidence and avoid unsupported claims. It targets failures like hallucinations, misattribution, and overconfident incorrect assertions across diverse settings.","Semantic Understanding & Context Recognition, Language Comprehension, Inhibitory Control, Self-reflection, Adaptive Error Correction"
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,Global PIQA evaluates physical commonsense reasoning with broader cultural and linguistic coverage than traditional PIQA-style datasets. It emphasizes choosing plausible actions or explanations in everyday physical scenarios while remaining robust across language variants.,"Logical Reasoning, Semantic Understanding & Context Recognition, Language Comprehension, Cognitive Flexibility"
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) tests long-context, multi-round co-reference resolution by embedding multiple similar “needle” interactions in a large “haystack” and asking the model to retrieve the correct referenced response. It primarily probes whether models can maintain and selectively retrieve the right entity/event linkage across very long contexts.","Working Memory, Attention, Language Comprehension, Semantic Understanding & Context Recognition, Episodic Memory"
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval is a human-judged evaluation of economically valuable, well-specified professional tasks spanning many occupations, where models produce real work artifacts (e.g., spreadsheets, presentations, schedules). It emphasizes end-to-end planning, producing usable outputs, and aligning deliverables to explicit requirements.","Planning, Decision-making, Language Production, Semantic Understanding & Context Recognition, Working Memory, Social Reasoning & Theory of Mind"
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks evaluates whether models can correctly follow and reason over graph-structured data (e.g., performing multi-step traversals or answering queries about relationships). It stresses maintaining intermediate state over many hops and avoiding drift as the walk length increases.","Spatial Representation & Mapping, Working Memory, Logical Reasoning, Attention, Planning"
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon benchmarks tool-use competence across diverse tools/APIs, emphasizing correct tool selection, argument construction, sequencing, and recovery from tool failures. It targets agentic reliability in realistic multi-step workflows rather than single-shot question answering.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Language Comprehension"
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath evaluates expert-level mathematics, including problems intended to be challenging for current frontier models and to resist superficial pattern matching. It emphasizes deep multi-step reasoning, careful symbolic/quantitative manipulation, and rigorous verification of results.","Logical Reasoning, Working Memory, Planning, Adaptive Error Correction"
