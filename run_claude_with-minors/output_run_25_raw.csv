Benchmark,Website,Paper,Description,Cognitive Functions
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates whether a model can solve real GitHub software issues by producing a patch that makes the repository’s tests pass. The Verified subset contains tasks that have been manually confirmed to be solvable, emphasizing end-to-end debugging, code changes, and regression avoidance under realistic constraints.","Planning, Logical Reasoning, Working Memory, Adaptive Error Correction, Language Comprehension"
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 measures an agent’s ability to complete real-world tasks in a command-line environment, such as configuring software, debugging, and running tools. Success depends on choosing correct commands, interpreting outputs/errors, and iterating reliably to reach a goal state.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Language Comprehension"
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp evaluates deep research agents on questions that require browsing and synthesizing information from a controlled document collection (or web-like retrieval setting). It stresses multi-step information seeking, evidence aggregation, and producing a grounded final answer under context limits.","Planning, Decision-making, Attention, Working Memory, Semantic Understanding & Context Recognition, Language Comprehension, Language Production"
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench evaluates interactive tool-using agents in customer-support-style environments (e.g., retail/airline/telecom) with policies and APIs. Agents must follow domain rules while completing multi-turn workflows, handling ambiguous user needs, and using tools correctly and consistently.","Social Reasoning & Theory of Mind, Decision-making, Inhibitory Control, Planning, Language Comprehension, Language Production, Working Memory"
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld measures autonomous computer-use capability in a realistic OS-like GUI environment with screenshots and interactive actions. Agents must navigate interfaces, locate UI elements, execute multi-step procedures, and recover from mistakes under step and time constraints.","Visual Perception, Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Sensorimotor Coordination, Planning, Decision-making, Working Memory, Adaptive Error Correction"
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,ARC-AGI tests fluid reasoning by asking models to infer transformation rules from a few grid input–output examples and apply them to new inputs. It emphasizes novel pattern discovery and generalization rather than memorized knowledge.,"Logical Reasoning, Cognitive Flexibility, Working Memory, Spatial Representation & Mapping, Attention"
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon agent coherence in a simulated business management setting (running a vending machine operation over an extended period). Agents must plan inventory and pricing, negotiate or interact with suppliers, and adapt strategy based on outcomes and constraints.","Planning, Decision-making, Working Memory, Reward Mechanisms, Cognitive Timing & Predictive Modeling, Adaptive Error Correction, Motivational Drives"
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas evaluates real-world tool use via the Model Context Protocol (MCP), requiring models to discover tools, invoke them with correct parameters, handle errors, and compose multi-step workflows. It emphasizes robust orchestration across heterogeneous APIs and tool servers.","Planning, Decision-making, Working Memory, Adaptive Error Correction, Language Comprehension, Semantic Understanding & Context Recognition"
FinanceAgent,https://www.vals.ai/benchmarks/finance_agent,,"FinanceAgent assesses performance on tasks typical of an entry-level financial analyst, such as extracting facts from documents, performing calculations, and producing structured analyses. It stresses correctness, domain reasoning, and workflow execution over realistic finance artifacts.","Logical Reasoning, Semantic Understanding & Context Recognition, Planning, Decision-making, Working Memory, Language Production, Language Comprehension"
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity agent skill on tasks such as identifying known vulnerabilities from descriptions and discovering new issues in real open-source codebases. It stresses investigative reasoning, exploitation/patch reasoning, and careful iteration in complex technical environments.","Logical Reasoning, Planning, Adaptive Error Correction, Working Memory, Language Comprehension, Inhibitory Control"
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"SpreadsheetBench measures the ability to understand, edit, and generate complex spreadsheets using realistic business-like tasks and constraints. Models must track dependencies, apply correct formulas/transformations, and maintain consistent structure and formatting over multi-step edits.","Planning, Working Memory, Logical Reasoning, Adaptive Error Correction, Decision-making, Language Comprehension, Language Production"
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a challenging multimodal benchmark intended to probe frontier knowledge and reasoning across diverse expert domains. Questions often require integrating text and images, handling long contexts, and producing precise answers under high uncertainty.","Language Comprehension, Language Production, Logical Reasoning, Semantic Understanding & Context Recognition, Visual Perception, Working Memory, Attention"
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 evaluates competition-style mathematical problem solving using questions from the American Invitational Mathematics Examination. It emphasizes multi-step symbolic reasoning, careful constraint handling, and exact final answers.","Logical Reasoning, Working Memory, Planning, Adaptive Error Correction"
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a subset of GPQA containing very difficult graduate-level multiple-choice questions in science, designed to be resistant to simple web search. It tests whether models can use deep domain understanding and reasoning to select the correct option.","Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Language Comprehension"
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends MMLU into many non-English languages, testing broad academic knowledge and reasoning across subjects. It evaluates whether models can maintain competence and consistency across multilingual prompts and culturally varied formulations.","Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory"
MMMU,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2311.16502,"MMMU is a multi-discipline multimodal benchmark where questions require jointly reasoning over images and text (e.g., diagrams, charts, and domain visuals). It stresses cross-modal grounding and high-level visual reasoning beyond basic recognition.","Visual Perception, Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Working Memory, Language Comprehension"
LAB-Bench FigQA,https://huggingface.co/datasets/futurehouse/lab-bench,https://arxiv.org/abs/2407.10362,"LAB-Bench FigQA evaluates whether models can answer questions about complex scientific figures from biology papers, requiring extraction of trends, labels, and experimental context. It targets figure-grounded reasoning rather than recalling facts from text alone.","Visual Perception, Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Working Memory, Language Comprehension"
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning evaluates understanding and reasoning over charts/figures and associated scientific context drawn from arXiv-style documents. Tasks emphasize interpreting visual encodings (axes, legends, curves) and mapping them to correct textual inferences.","Visual Perception, Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Working Memory, Language Comprehension"
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU extends multimodal understanding to video, requiring models to integrate information across frames and time to answer questions. It stresses temporal integration, tracking objects/events, and combining visual evidence with language instructions.","Visual Perception, Visual Attention & Eye Movements, Multisensory Integration, Working Memory, Cognitive Timing & Predictive Modeling, Scene Understanding & Visual Reasoning, Language Comprehension"
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"FACTS Benchmark Suite systematically evaluates LLM factuality across multiple settings, focusing on whether outputs are supported by sources or ground truth and how models handle uncertainty. It targets hallucination-like failure modes, calibration, and reliable citation/grounding behavior.","Semantic Understanding & Context Recognition, Language Comprehension, Language Production, Working Memory, Self-reflection, Inhibitory Control"
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA is a multilingual, non-parallel evaluation of physical commonsense reasoning, asking models to choose plausible actions or solutions in everyday physical scenarios. It probes whether models can generalize intuitive physics and practical reasoning across languages and formulations.","Logical Reasoning, Semantic Understanding & Context Recognition, Spatial Representation & Mapping, Language Comprehension, Working Memory"
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,MRCR v2 (8-needle) tests long-context multi-round coreference and retrieval by inserting multiple similar “needle” requests into lengthy “haystack” dialogues/documents and asking the model to reproduce the correct referenced response. It stresses robust tracking of entities/requests over long spans and resistance to distractors.,"Working Memory, Attention, Episodic Memory, Language Comprehension, Semantic Understanding & Context Recognition"
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval evaluates well-specified professional knowledge-work tasks spanning many occupations, where outputs are judged against (or alongside) industry professionals. Tasks require producing real work artifacts (e.g., slides, spreadsheets, plans) with speed, structure, and correctness under constraints.","Planning, Decision-making, Language Production, Language Comprehension, Semantic Understanding & Context Recognition, Social Reasoning & Theory of Mind, Working Memory, Adaptive Error Correction, Self-reflection"
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks evaluates structured reasoning over graph problems expressed in text, such as walking paths, BFS-like traversal, or parent/neighbor queries in large graphs. It stresses maintaining and updating an internal graph state across long contexts and executing algorithmic reasoning steps reliably.","Logical Reasoning, Working Memory, Spatial Representation & Mapping, Attention, Cognitive Flexibility"
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon evaluates generalized tool-use competence across diverse tools and tasks, emphasizing correct tool selection, parameterization, and multi-step orchestration. It probes reliability under execution feedback (tool errors/results) and the ability to adapt plans mid-run.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Language Comprehension, Language Production"
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath evaluates advanced mathematics problem solving intended to be beyond routine competition math, often requiring deep multi-step reasoning and careful verification. It emphasizes sustained abstraction, error checking, and sometimes tool-assisted computation depending on the evaluation setup.","Logical Reasoning, Working Memory, Planning, Cognitive Flexibility, Adaptive Error Correction"
