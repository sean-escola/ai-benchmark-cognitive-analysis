Benchmark,Website,Paper,Description,Cognitive Functions
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates agentic software engineering by asking a model to produce patches that fix real issues in open-source Python repositories, with solutions validated by tests. The “Verified” subset focuses on problems confirmed by human reviewers to be solvable and to have reliable evaluation signals.","Planning, Logical Reasoning, Working Memory, Adaptive Error Correction, Semantic Understanding & Context Recognition"
SWE-bench Multilingual,https://www.swebench.com/multilingual.html,https://arxiv.org/abs/2504.21798,"SWE-bench Multilingual extends SWE-bench-style patch generation beyond Python to multiple programming languages and ecosystems, requiring adaptation to different toolchains and conventions. It evaluates whether models can transfer debugging and code-editing competence across language settings rather than relying on one familiar stack.","Cognitive Flexibility, Planning, Logical Reasoning, Working Memory, Semantic Understanding & Context Recognition, Adaptive Error Correction"
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro is a harder, larger software engineering benchmark aimed at more realistic, contamination-resistant assessment across multiple languages and industrial-style tasks. It typically requires deeper repository understanding, longer multi-step debugging, and more robust patch iteration to satisfy tests and constraints.","Planning, Logical Reasoning, Working Memory, Adaptive Error Correction, Semantic Understanding & Context Recognition, Cognitive Flexibility"
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 evaluates agents that operate a command-line environment to complete real tasks such as installing tools, running programs, manipulating files, and diagnosing failures. Success requires choosing and sequencing shell actions while interpreting tool feedback under resource and environment constraints.","Planning, Decision-making, Working Memory, Adaptive Error Correction, Semantic Understanding & Context Recognition, Motor Skill Learning, Sensorimotor Coordination"
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp measures deep-research web browsing: the agent must search, read, and synthesize information from documents to answer questions that are difficult to solve from parametric memory alone. The benchmark emphasizes navigation strategy, evidence gathering, and consolidation of findings across many sources.","Planning, Attention, Working Memory, Episodic Memory, Semantic Understanding & Context Recognition, Language Comprehension, Language Production, Decision-making"
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench evaluates interactive customer-support agents that must follow domain policies while using tools/APIs across multi-turn conversations in simulated environments (e.g., retail, airline, telecom). It stresses policy adherence under pressure, robust tool use, and maintaining coherent dialogue state over long interactions.","Social Reasoning & Theory of Mind, Language Comprehension, Language Production, Decision-making, Planning, Working Memory, Inhibitory Control, Empathy, Adaptive Error Correction"
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld evaluates multimodal computer-use agents in a desktop OS setting, requiring them to complete tasks by operating real applications through UI interactions. It tests whether models can perceive screens, plan action sequences, and recover from errors while navigating complex interfaces.","Visual Perception, Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Planning, Decision-making, Working Memory, Sensorimotor Coordination, Adaptive Error Correction"
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI tests fluid, few-shot abstract reasoning by presenting grid-based input/output examples with hidden rules and asking the model to infer the correct transformation for new inputs. It is designed to minimize reliance on memorized knowledge and instead emphasize generalization from minimal demonstrations.","Logical Reasoning, Working Memory, Cognitive Flexibility, Scene Understanding & Visual Reasoning, Visual Perception, Attention, Planning"
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon agent coherence and business strategy in a simulated vending-machine operation over many decisions and time steps. Models must manage inventory, pricing, supplier communication, and adaptation to changing conditions to maximize final balance.","Planning, Decision-making, Reward Mechanisms, Episodic Memory, Working Memory, Cognitive Timing & Predictive Modeling, Adaptive Error Correction, Self-reflection"
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas measures real-world tool use through the Model Context Protocol by requiring models to discover tools, invoke them correctly, handle failures, and integrate results across multi-step workflows. It targets practical orchestration skills for production-like API ecosystems rather than single-call function use.","Planning, Decision-making, Working Memory, Adaptive Error Correction, Semantic Understanding & Context Recognition, Language Comprehension, Language Production"
FinanceAgent,https://www.vals.ai/benchmarks/finance_agent,,"FinanceAgent evaluates agent performance on tasks representative of an entry-level financial analyst, often involving documents, calculations, and structured outputs. It emphasizes accurate extraction, reasoning under financial constraints, and producing professional artifacts (e.g., analyses or summaries) with tool assistance when available.","Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Planning, Decision-making, Language Comprehension, Language Production, Adaptive Error Correction"
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity agents on vulnerability identification and discovery using real open-source projects, combining code comprehension with exploit-relevant reasoning. The benchmark stresses precise interpretation of codebases, hypothesis-driven investigation, and iterative refinement based on test results or findings.","Logical Reasoning, Planning, Working Memory, Adaptive Error Correction, Semantic Understanding & Context Recognition, Inhibitory Control"
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"SpreadsheetBench evaluates an agent’s ability to navigate, manipulate, and compute with complex spreadsheets based on realistic tasks, often requiring formula logic, formatting, and multi-sheet consistency. It tests sustained, stepwise work where small mistakes compound and must be detected and corrected.","Working Memory, Planning, Logical Reasoning, Adaptive Error Correction, Decision-making, Semantic Understanding & Context Recognition, Motor Skill Learning"
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a difficult, broad benchmark spanning expert-level questions, including multimodal items, intended to probe frontier reasoning and knowledge at the edge of current models. It rewards careful synthesis and domain reasoning rather than short, pattern-matched answers.","Language Comprehension, Language Production, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Visual Perception, Scene Understanding & Visual Reasoning, Attention"
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 uses problems from a high-level mathematics competition that require multi-step derivations, algebraic manipulation, and careful case handling. It is typically evaluated as exact-answer generation, where partial progress does not count without the correct final result.","Logical Reasoning, Working Memory, Planning, Adaptive Error Correction, Attention"
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a challenging subset of graduate-level science multiple-choice questions designed to be difficult to solve via superficial lookup, emphasizing reasoning and deep conceptual understanding. The Diamond split focuses on high-quality items where experts succeed and non-experts often fail.","Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Attention, Decision-making"
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends broad academic knowledge testing across many subjects into multiple languages, measuring whether competence transfers beyond English. It probes both factual knowledge and reasoning while requiring robust understanding of prompts and answer options in diverse linguistic contexts.","Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Cognitive Flexibility, Attention"
MMMU,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2311.16502,"MMMU is a multimodal benchmark spanning many disciplines where models must combine text with images such as diagrams, charts, and scientific figures to answer questions. It tests integrated reasoning over visual and textual evidence rather than isolated vision or language skills.","Visual Perception, Scene Understanding & Visual Reasoning, Multisensory Integration, Language Comprehension, Language Production, Logical Reasoning, Working Memory, Visual Attention & Eye Movements"
LAB-Bench FigQA,https://huggingface.co/datasets/futurehouse/lab-bench,https://arxiv.org/abs/2407.10362,"LAB-Bench FigQA evaluates whether models can correctly interpret complex scientific figures from biology papers and answer targeted questions about experimental results and visual encodings. It emphasizes careful visual parsing, mapping figure content to scientific meaning, and drawing correct inferences.","Visual Perception, Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Language Comprehension, Language Production"
WebArena,https://webarena.dev/,https://arxiv.org/abs/2307.13854,"WebArena evaluates autonomous web agents completing realistic multi-step tasks across web applications (e-commerce, content management, code hosting, forums), requiring navigation, form filling, and state tracking. Performance depends on planning action sequences, handling UI variability, and recovering from partial failures.","Planning, Decision-making, Working Memory, Adaptive Error Correction, Visual Perception, Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Sensorimotor Coordination, Language Comprehension, Language Production"
