Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates models on real, repository-based software engineering tasks where the goal is to produce a patch that makes previously failing tests pass. The “Verified” subset consists of issues that have been validated by human reviewers as solvable with the provided repository state and tests, making the benchmark more reliable for pass@1 agentic coding comparisons.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory, Logical Reasoning, Semantic Understanding & Context Recognition
L3: ",L2
SWE-bench Multilingual,https://www.swebench.com/multilingual.html,https://arxiv.org/abs/2504.21798,"SWE-bench Multilingual extends the SWE-bench paradigm beyond Python to multiple programming languages and ecosystems, testing whether models can transfer software-engineering skills across diverse tooling and conventions. Tasks still require generating correct code changes in real repos, but add cross-language generalization pressure and varied build/test workflows.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory, Logical Reasoning, Semantic Understanding & Context Recognition
L3: Cognitive Flexibility",L3
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro is a larger, harder suite of repository-level software engineering tasks designed to be more challenging and more resistant to superficial memorization than earlier variants. It emphasizes multi-file changes, realistic debugging and implementation work, and broader language/toolchain coverage to better approximate professional engineering workflows.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory, Logical Reasoning, Semantic Understanding & Context Recognition
L3: Cognitive Flexibility",L3
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 evaluates agentic performance in command-line environments, where models must complete practical tasks by issuing shell commands, editing files, installing dependencies, and iterating based on program output. It stresses end-to-end autonomy: choosing actions, interpreting errors, and converging on a working solution under time/step constraints.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory, Logical Reasoning, Semantic Understanding & Context Recognition
L3: ",L2
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp measures deep-research style question answering with controlled web retrieval, where agents must search, read, and synthesize information from a fixed document collection to produce verifiable answers. It focuses on tool-augmented reasoning over long contexts, including selecting good queries, aggregating evidence, and maintaining coherence across many retrieval steps.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Working Memory, Episodic Memory, Logical Reasoning, Semantic Understanding & Context Recognition, Attention
L3: ",L2
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench evaluates interactive customer-support agents in simulated domains (e.g., retail, airline, telecom) that must use tools/APIs while following policies and handling multi-turn dialogues. Success depends on maintaining state, resolving constraints, and producing compliant outcomes despite ambiguous user goals and evolving context.","L1: Language Comprehension, Language Production
L2: Decision-making, Planning, Working Memory, Reward Mechanisms, Semantic Understanding & Context Recognition, Attention
L3: Social Reasoning & Theory of Mind, Inhibitory Control",L3
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld is a multimodal computer-use benchmark where agents operate within a desktop-like environment to complete tasks across applications, using screenshots and UI interactions. It tests whether models can perceive interfaces, plan action sequences, and recover from mistakes while navigating real software workflows under step limits.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Planning, Decision-making, Working Memory, Adaptive Error Correction, Sensorimotor Coordination, Semantic Understanding & Context Recognition
L3: ",L2
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI tests “fluid” abstract reasoning by asking models to infer transformation rules from a few input–output grid examples and apply them to new grids. Because tasks are intentionally out-of-distribution and compositional, performance reflects pattern induction and rule-like generalization rather than domain knowledge recall.","L1: Visual Perception
L2: Logical Reasoning, Working Memory, Spatial Representation & Mapping, Scene Understanding & Visual Reasoning, Planning, Attention
L3: Cognitive Flexibility",L3
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon autonomous agency in a simulated vending-machine business over many decision steps (e.g., sourcing inventory, pricing, negotiation, and handling stochastic market dynamics). Scores reflect sustained coherence, strategic adaptation, and the ability to manage resources over extended time horizons.","L1: 
L2: Planning, Decision-making, Working Memory, Reward Mechanisms, Adaptive Error Correction, Semantic Understanding & Context Recognition
L3: Motivational Drives",L3
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas measures real-world tool use via the Model Context Protocol by requiring models to discover, invoke, and compose multiple tools across realistic workflows and APIs. Tasks stress correct parameterization, multi-step orchestration, and robust handling of tool errors and partial results.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Working Memory, Adaptive Error Correction, Semantic Understanding & Context Recognition
L3: ",L2
FinanceAgent,https://www.vals.ai/benchmarks/finance_agent,,"FinanceAgent evaluates tool-using agents on tasks typical of an entry-level financial analyst, such as extracting information, building analyses, and producing finance-oriented deliverables under realistic constraints. It emphasizes structured reasoning with domain conventions, iterative refinement, and synthesis into business-ready outputs.","L1: Language Comprehension, Language Production
L2: Logical Reasoning, Planning, Decision-making, Working Memory, Semantic Understanding & Context Recognition, Adaptive Error Correction
L3: ",L2
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity agents on large-scale, real-world vulnerability tasks, including identifying known vulnerability classes in open-source projects and, in some settings, discovering new issues. It measures how well models can reason about code behavior, reproduce exploits conceptually, and navigate complex technical artifacts under constrained attempts.","L1: Language Comprehension, Language Production
L2: Logical Reasoning, Planning, Decision-making, Adaptive Error Correction, Working Memory, Attention
L3: Inhibitory Control",L3
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"SpreadsheetBench tests whether models can correctly navigate, edit, and compute within complex spreadsheets, often requiring multi-step transformations and formula logic consistent with real business artifacts. It stresses precision (small mistakes can cascade), procedural reliability, and the ability to translate instructions into structured spreadsheet operations.","L1: 
L2: Logical Reasoning, Planning, Decision-making, Working Memory, Adaptive Error Correction, Semantic Understanding & Context Recognition, Attention
L3: ",L2
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a difficult, multi-modal benchmark intended to probe frontier-level academic reasoning and knowledge across many subjects, often requiring synthesis rather than lookup. Depending on the evaluation setup, models may be tested with or without tools (e.g., search or code), highlighting both raw reasoning and tool-augmented problem solving.","L1: Language Comprehension, Language Production
L2: Logical Reasoning, Working Memory, Semantic Understanding & Context Recognition, Multisensory Integration, Scene Understanding & Visual Reasoning, Planning, Decision-making
L3: ",L2
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 evaluates mathematical problem solving using competition-style questions that require multi-step derivations, careful casework, and exact numeric answers. It is typically run without tools to assess internal reasoning reliability, though some reports also compare tool-augmented settings.","L1: 
L2: Logical Reasoning, Working Memory, Planning, Attention
L3: ",L2
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a high-quality subset of GPQA consisting of challenging graduate-level multiple-choice questions in science designed to be difficult to answer via superficial recall. It targets rigorous reasoning over domain knowledge, with questions curated so non-experts tend to fail while experts can succeed.","L1: Language Comprehension
L2: Logical Reasoning, Working Memory, Semantic Understanding & Context Recognition, Decision-making, Attention
L3: ",L2
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends broad academic knowledge testing into multiple languages, assessing whether a model’s understanding and reasoning generalize across linguistic contexts and cultural/terminological variation. It is commonly used to evaluate multilingual comprehension, consistency, and robustness of subject-matter knowledge.","L1: Language Comprehension, Language Production
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory
L3: Cognitive Flexibility",L3
MMMU,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2311.16502,"MMMU is a multimodal benchmark spanning many disciplines where models must answer questions that require jointly interpreting images (e.g., diagrams, charts, screenshots) and accompanying text. It probes integrated visual-language reasoning and the ability to ground conclusions in visual evidence rather than purely textual priors.","L1: Visual Perception, Language Comprehension, Language Production
L2: Scene Understanding & Visual Reasoning, Multisensory Integration, Visual Attention & Eye Movements, Logical Reasoning, Working Memory, Semantic Understanding & Context Recognition
L3: ",L2
LAB-Bench FigQA,https://huggingface.co/datasets/futurehouse/lab-bench,https://arxiv.org/abs/2407.10362,"LAB-Bench FigQA evaluates scientific figure question answering, requiring models to extract and reason over information encoded in complex biology research figures (plots, schematics, multi-panel visuals). Performance reflects the ability to read visual scientific evidence, connect it to domain concepts, and answer precisely under ambiguity and noise.","L1: Visual Perception, Language Comprehension, Language Production
L2: Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Multisensory Integration, Logical Reasoning, Working Memory, Semantic Understanding & Context Recognition
L3: ",L2
WebArena,https://webarena.dev/,https://arxiv.org/abs/2307.13854,"WebArena evaluates autonomous web agents on realistic tasks across multiple web apps (e-commerce, forums, code hosting, CMS) requiring navigation, form filling, and multi-step workflows. It stresses perception of dynamic pages, maintaining task state across long interactions, and robust recovery from UI or policy-induced failures.","L1: Visual Perception, Language Comprehension, Language Production
L2: Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Planning, Decision-making, Working Memory, Adaptive Error Correction, Sensorimotor Coordination, Semantic Understanding & Context Recognition
L3: ",L2
