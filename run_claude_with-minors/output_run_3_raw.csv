Benchmark,Website,Paper,Description,Cognitive Functions
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates software engineering agents on real GitHub issues where the model must modify an existing repository to make tests pass. The “Verified” subset uses tasks screened to be solvable and reliably evaluated, emphasizing correct patch generation under realistic codebase constraints.","Planning, Logical Reasoning, Working Memory, Adaptive Error Correction, Decision-making, Language Comprehension, Semantic Understanding & Context Recognition"
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 measures how well an agent can complete real tasks in a command-line environment, typically requiring shell usage, file editing, package/tool invocation, and iterative debugging. Success depends on maintaining state across steps and correcting mistakes based on tool outputs and errors.","Planning, Decision-making, Working Memory, Adaptive Error Correction, Attention, Language Comprehension, Semantic Understanding & Context Recognition"
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp evaluates research-style question answering where the agent must use browsing/search to find information and synthesize a supported answer. It stresses information foraging, source selection, and integrating evidence across multiple documents under time and context constraints.","Planning, Decision-making, Attention, Working Memory, Semantic Understanding & Context Recognition, Language Comprehension, Language Production"
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench tests interactive customer-support agents in simulated domains (e.g., retail/airline/telecom) that must follow policies while using tools/APIs across multi-turn dialogues. It emphasizes reliable tool calling, consistent policy adherence, and managing long conversational trajectories with a simulated user.","Social Reasoning & Theory of Mind, Language Comprehension, Language Production, Planning, Decision-making, Working Memory, Inhibitory Control, Attention"
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld evaluates computer-use agents that operate graphical desktop environments to complete tasks (e.g., navigating apps, settings, web UIs) from pixel/GUI observations. It stresses robust perception-action loops, handling UI variability, and multi-step execution with recovery from mistakes.","Visual Perception, Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Planning, Decision-making, Sensorimotor Coordination, Working Memory, Adaptive Error Correction"
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI measures fluid reasoning on novel grid-based pattern induction tasks: models infer a transformation rule from a few examples and apply it to a new input. It is designed to reduce reliance on memorized knowledge and instead emphasize abstraction, compositional reasoning, and generalization.","Logical Reasoning, Cognitive Flexibility, Working Memory, Spatial Representation & Mapping, Planning, Attention"
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon autonomous agency by running a simulated vending machine business over extended time, requiring procurement, pricing, inventory management, and negotiation-style interactions. The score reflects sustained coherence and strategic adaptation over many sequential decisions.","Planning, Decision-making, Working Memory, Reward Mechanisms, Adaptive Error Correction, Motivational Drives, Semantic Understanding & Context Recognition"
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas evaluates real-world tool use via the Model Context Protocol by requiring multi-step workflows across authentic APIs and tool servers. Tasks probe tool discovery, correct parameterization, error handling/retries, and synthesis of tool outputs into a final response.","Planning, Decision-making, Working Memory, Adaptive Error Correction, Attention, Language Comprehension, Semantic Understanding & Context Recognition"
FinanceAgent,https://www.vals.ai/benchmarks/finance_agent,,"FinanceAgent assesses whether models can perform tasks expected of an entry-level financial analyst, such as analysis, modeling, document-driven reasoning, and generating finance-relevant deliverables. It emphasizes domain-grounded reasoning, structured outputs, and correctness under realistic constraints.","Logical Reasoning, Semantic Understanding & Context Recognition, Language Comprehension, Language Production, Working Memory, Planning, Decision-making"
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity agents on tasks involving identifying and exploiting known vulnerabilities and, in some cases, discovering new ones in real open-source projects. It emphasizes systematic investigation, code/behavior reasoning, and iterative refinement from tool feedback.","Planning, Logical Reasoning, Working Memory, Adaptive Error Correction, Decision-making, Attention, Semantic Understanding & Context Recognition"
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"SpreadsheetBench measures the ability to understand, edit, and compute over complex spreadsheets derived from realistic scenarios. Agents must navigate tables, formulas, formatting constraints, and sometimes use programming tools to transform or validate spreadsheet contents.","Logical Reasoning, Working Memory, Planning, Decision-making, Adaptive Error Correction, Semantic Understanding & Context Recognition, Attention"
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a frontier, multi-domain benchmark intended to probe advanced academic reasoning and knowledge across difficult questions, often spanning modalities and requiring careful synthesis. It stresses deep understanding, multi-step problem solving, and producing justified answers under ambiguity.","Language Comprehension, Language Production, Logical Reasoning, Working Memory, Semantic Understanding & Context Recognition, Scene Understanding & Visual Reasoning, Planning"
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 evaluates competition-style mathematics problem solving on the American Invitational Mathematics Examination questions. It emphasizes multi-step symbolic reasoning, careful constraint handling, and avoiding algebraic/arithmetical errors under time-like pressure.","Logical Reasoning, Working Memory, Planning, Adaptive Error Correction, Attention"
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a hard subset of graduate-level science multiple-choice questions designed to be “Google-proof,” requiring genuine reasoning rather than easy retrieval. It tests whether models can integrate scientific concepts and avoid superficial pattern matching.","Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Attention, Language Comprehension"
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends broad academic knowledge testing across many subjects and languages, evaluating both multilingual understanding and reasoning. It probes whether models can generalize their competence beyond English while maintaining accuracy across domains.","Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Cognitive Flexibility"
MMMU,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2311.16502,"MMMU is a multimodal benchmark requiring models to answer expert-level questions that combine images (e.g., diagrams, charts, screenshots) with text. It stresses extracting visual evidence and integrating it with domain knowledge and reasoning.","Visual Perception, Scene Understanding & Visual Reasoning, Multisensory Integration, Visual Attention & Eye Movements, Language Comprehension, Logical Reasoning, Working Memory"
LAB-Bench FigQA,https://huggingface.co/datasets/futurehouse/lab-bench,https://arxiv.org/abs/2407.10362,"LAB-Bench FigQA tests whether models can interpret complex scientific figures from biology papers and answer questions grounded in the visual evidence. It emphasizes fine-grained chart/figure understanding, cross-referencing captions/labels, and drawing correct scientific inferences from visuals.","Visual Perception, Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Language Comprehension, Logical Reasoning, Working Memory, Semantic Understanding & Context Recognition"
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning evaluates visual-and-text reasoning over scientific paper figures and related context, often requiring quantitative interpretation and stepwise analysis. It stresses figure-grounded inference, extracting structured information, and composing a coherent explanation or answer.","Visual Perception, Scene Understanding & Visual Reasoning, Language Comprehension, Logical Reasoning, Working Memory, Attention, Multisensory Integration"
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU extends multimodal understanding to videos, requiring temporal integration of visual events and accompanying text prompts/questions. It probes whether models can track evolving state, identify key moments, and reason over dynamic scenes.","Visual Perception, Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Working Memory, Cognitive Timing & Predictive Modeling, Multisensory Integration, Language Comprehension"
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"FACTS Benchmark Suite systematically evaluates factuality and grounding by checking whether model outputs remain consistent with available evidence and avoid unsupported claims. It emphasizes calibration, error detection, and resisting hallucination across diverse factuality settings.","Semantic Understanding & Context Recognition, Language Comprehension, Language Production, Inhibitory Control, Adaptive Error Correction, Self-reflection, Attention"
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA evaluates physical commonsense reasoning across many languages, aiming to test whether models can select plausible actions/solutions in everyday situations without relying on English-only cues. It stresses transfer of commonsense reasoning and robust multilingual comprehension.","Language Comprehension, Logical Reasoning, Semantic Understanding & Context Recognition, Cognitive Flexibility, Spatial Representation & Mapping"
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) evaluates multi-round coreference and retrieval over long contexts by embedding multiple similar “needle” turns in a large “haystack” and asking the model to reproduce a specific referenced response. It stresses long-context retention, interference robustness, and precise retrieval under ambiguity.","Working Memory, Episodic Memory, Attention, Semantic Understanding & Context Recognition, Inhibitory Control, Logical Reasoning"
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval evaluates economically valuable, well-specified professional tasks across many occupations, judged by expert humans via comparative scoring. It emphasizes producing real work artifacts (e.g., slides, spreadsheets, plans) with correctness, clarity, and adherence to constraints.","Planning, Decision-making, Language Production, Language Comprehension, Working Memory, Semantic Understanding & Context Recognition, Adaptive Error Correction"
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks evaluates structured reasoning over graph problems (e.g., traversals and parent relationships) often presented in text/sequence form, requiring the model to follow rules across many steps. It stresses maintaining intermediate state, avoiding drift, and executing algorithmic reasoning reliably.","Logical Reasoning, Working Memory, Planning, Spatial Representation & Mapping, Attention, Adaptive Error Correction"
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon measures general tool-using competence across many heterogeneous tools and tasks, requiring the agent to select tools, call them correctly, and compose results. It emphasizes orchestration, robustness to tool errors, and end-to-end task completion rather than isolated QA.","Planning, Decision-making, Working Memory, Adaptive Error Correction, Attention, Semantic Understanding & Context Recognition, Language Comprehension"
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath evaluates extremely difficult, research-level mathematics designed to be challenging even for strong models, often requiring multi-stage derivations and careful checking. It emphasizes deep abstraction, long-horizon proof-like reasoning, and high precision under complex constraints.","Logical Reasoning, Working Memory, Planning, Adaptive Error Correction, Attention, Cognitive Flexibility"
