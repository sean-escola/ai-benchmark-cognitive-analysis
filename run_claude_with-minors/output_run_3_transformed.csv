Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates software engineering agents on real GitHub issues where the model must modify an existing repository to make tests pass. The “Verified” subset uses tasks screened to be solvable and reliably evaluated, emphasizing correct patch generation under realistic codebase constraints.","L1: Language Comprehension
L2: Planning, Logical Reasoning, Working Memory, Adaptive Error Correction, Decision-making, Semantic Understanding & Context Recognition
L3: ",L2
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 measures how well an agent can complete real tasks in a command-line environment, typically requiring shell usage, file editing, package/tool invocation, and iterative debugging. Success depends on maintaining state across steps and correcting mistakes based on tool outputs and errors.","L1: Language Comprehension
L2: Planning, Decision-making, Working Memory, Adaptive Error Correction, Attention, Semantic Understanding & Context Recognition
L3: ",L2
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp evaluates research-style question answering where the agent must use browsing/search to find information and synthesize a supported answer. It stresses information foraging, source selection, and integrating evidence across multiple documents under time and context constraints.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Attention, Working Memory, Semantic Understanding & Context Recognition
L3: ",L2
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench tests interactive customer-support agents in simulated domains (e.g., retail/airline/telecom) that must follow policies while using tools/APIs across multi-turn dialogues. It emphasizes reliable tool calling, consistent policy adherence, and managing long conversational trajectories with a simulated user.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Working Memory, Attention
L3: Social Reasoning & Theory of Mind, Inhibitory Control",L3
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld evaluates computer-use agents that operate graphical desktop environments to complete tasks (e.g., navigating apps, settings, web UIs) from pixel/GUI observations. It stresses robust perception-action loops, handling UI variability, and multi-step execution with recovery from mistakes.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Planning, Decision-making, Sensorimotor Coordination, Working Memory, Adaptive Error Correction
L3: ",L2
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI measures fluid reasoning on novel grid-based pattern induction tasks: models infer a transformation rule from a few examples and apply it to a new input. It is designed to reduce reliance on memorized knowledge and instead emphasize abstraction, compositional reasoning, and generalization.","L1: 
L2: Logical Reasoning, Working Memory, Spatial Representation & Mapping, Planning, Attention
L3: Cognitive Flexibility",L3
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon autonomous agency by running a simulated vending machine business over extended time, requiring procurement, pricing, inventory management, and negotiation-style interactions. The score reflects sustained coherence and strategic adaptation over many sequential decisions.","L1: 
L2: Planning, Decision-making, Working Memory, Reward Mechanisms, Adaptive Error Correction, Semantic Understanding & Context Recognition
L3: Motivational Drives",L3
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas evaluates real-world tool use via the Model Context Protocol by requiring multi-step workflows across authentic APIs and tool servers. Tasks probe tool discovery, correct parameterization, error handling/retries, and synthesis of tool outputs into a final response.","L1: Language Comprehension
L2: Planning, Decision-making, Working Memory, Adaptive Error Correction, Attention, Semantic Understanding & Context Recognition
L3: ",L2
FinanceAgent,https://www.vals.ai/benchmarks/finance_agent,,"FinanceAgent assesses whether models can perform tasks expected of an entry-level financial analyst, such as analysis, modeling, document-driven reasoning, and generating finance-relevant deliverables. It emphasizes domain-grounded reasoning, structured outputs, and correctness under realistic constraints.","L1: Language Comprehension, Language Production
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Planning, Decision-making
L3: ",L2
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity agents on tasks involving identifying and exploiting known vulnerabilities and, in some cases, discovering new ones in real open-source projects. It emphasizes systematic investigation, code/behavior reasoning, and iterative refinement from tool feedback.","L1: 
L2: Planning, Logical Reasoning, Working Memory, Adaptive Error Correction, Decision-making, Attention, Semantic Understanding & Context Recognition
L3: ",L2
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"SpreadsheetBench measures the ability to understand, edit, and compute over complex spreadsheets derived from realistic scenarios. Agents must navigate tables, formulas, formatting constraints, and sometimes use programming tools to transform or validate spreadsheet contents.","L1: 
L2: Logical Reasoning, Working Memory, Planning, Decision-making, Adaptive Error Correction, Semantic Understanding & Context Recognition, Attention
L3: ",L2
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a frontier, multi-domain benchmark intended to probe advanced academic reasoning and knowledge across difficult questions, often spanning modalities and requiring careful synthesis. It stresses deep understanding, multi-step problem solving, and producing justified answers under ambiguity.","L1: Language Comprehension, Language Production
L2: Logical Reasoning, Working Memory, Semantic Understanding & Context Recognition, Scene Understanding & Visual Reasoning, Planning
L3: ",L2
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 evaluates competition-style mathematics problem solving on the American Invitational Mathematics Examination questions. It emphasizes multi-step symbolic reasoning, careful constraint handling, and avoiding algebraic/arithmetical errors under time-like pressure.","L1: 
L2: Logical Reasoning, Working Memory, Planning, Adaptive Error Correction, Attention
L3: ",L2
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a hard subset of graduate-level science multiple-choice questions designed to be “Google-proof,” requiring genuine reasoning rather than easy retrieval. It tests whether models can integrate scientific concepts and avoid superficial pattern matching.","L1: Language Comprehension
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Attention
L3: ",L2
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends broad academic knowledge testing across many subjects and languages, evaluating both multilingual understanding and reasoning. It probes whether models can generalize their competence beyond English while maintaining accuracy across domains.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory
L3: Cognitive Flexibility",L3
MMMU,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2311.16502,"MMMU is a multimodal benchmark requiring models to answer expert-level questions that combine images (e.g., diagrams, charts, screenshots) with text. It stresses extracting visual evidence and integrating it with domain knowledge and reasoning.","L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Multisensory Integration, Visual Attention & Eye Movements, Logical Reasoning, Working Memory
L3: ",L2
LAB-Bench FigQA,https://huggingface.co/datasets/futurehouse/lab-bench,https://arxiv.org/abs/2407.10362,"LAB-Bench FigQA tests whether models can interpret complex scientific figures from biology papers and answer questions grounded in the visual evidence. It emphasizes fine-grained chart/figure understanding, cross-referencing captions/labels, and drawing correct scientific inferences from visuals.","L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Logical Reasoning, Working Memory, Semantic Understanding & Context Recognition
L3: ",L2
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning evaluates visual-and-text reasoning over scientific paper figures and related context, often requiring quantitative interpretation and stepwise analysis. It stresses figure-grounded inference, extracting structured information, and composing a coherent explanation or answer.","L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Logical Reasoning, Working Memory, Attention, Multisensory Integration
L3: ",L2
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU extends multimodal understanding to videos, requiring temporal integration of visual events and accompanying text prompts/questions. It probes whether models can track evolving state, identify key moments, and reason over dynamic scenes.","L1: Visual Perception, Language Comprehension
L2: Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Working Memory, Multisensory Integration
L3: Cognitive Timing & Predictive Modeling",L3
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"FACTS Benchmark Suite systematically evaluates factuality and grounding by checking whether model outputs remain consistent with available evidence and avoid unsupported claims. It emphasizes calibration, error detection, and resisting hallucination across diverse factuality settings.","L1: Language Comprehension, Language Production
L2: Semantic Understanding & Context Recognition, Adaptive Error Correction, Attention
L3: Inhibitory Control, Self-reflection",L3
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA evaluates physical commonsense reasoning across many languages, aiming to test whether models can select plausible actions/solutions in everyday situations without relying on English-only cues. It stresses transfer of commonsense reasoning and robust multilingual comprehension.","L1: Language Comprehension
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Spatial Representation & Mapping
L3: Cognitive Flexibility",L3
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) evaluates multi-round coreference and retrieval over long contexts by embedding multiple similar “needle” turns in a large “haystack” and asking the model to reproduce a specific referenced response. It stresses long-context retention, interference robustness, and precise retrieval under ambiguity.","L1: 
L2: Working Memory, Episodic Memory, Attention, Semantic Understanding & Context Recognition, Logical Reasoning
L3: Inhibitory Control",L3
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval evaluates economically valuable, well-specified professional tasks across many occupations, judged by expert humans via comparative scoring. It emphasizes producing real work artifacts (e.g., slides, spreadsheets, plans) with correctness, clarity, and adherence to constraints.","L1: Language Production, Language Comprehension
L2: Planning, Decision-making, Working Memory, Semantic Understanding & Context Recognition, Adaptive Error Correction
L3: ",L2
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks evaluates structured reasoning over graph problems (e.g., traversals and parent relationships) often presented in text/sequence form, requiring the model to follow rules across many steps. It stresses maintaining intermediate state, avoiding drift, and executing algorithmic reasoning reliably.","L1: 
L2: Logical Reasoning, Working Memory, Planning, Spatial Representation & Mapping, Attention, Adaptive Error Correction
L3: ",L2
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon measures general tool-using competence across many heterogeneous tools and tasks, requiring the agent to select tools, call them correctly, and compose results. It emphasizes orchestration, robustness to tool errors, and end-to-end task completion rather than isolated QA.","L1: Language Comprehension
L2: Planning, Decision-making, Working Memory, Adaptive Error Correction, Attention, Semantic Understanding & Context Recognition
L3: ",L2
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath evaluates extremely difficult, research-level mathematics designed to be challenging even for strong models, often requiring multi-stage derivations and careful checking. It emphasizes deep abstraction, long-horizon proof-like reasoning, and high precision under complex constraints.","L1: 
L2: Logical Reasoning, Working Memory, Planning, Adaptive Error Correction, Attention
L3: Cognitive Flexibility",L3
