Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates autonomous software engineering by asking a model to produce patches that fix real issues in open-source Python repositories, with solutions validated by running tests in a controlled harness. The “Verified” subset emphasizes tasks that have been checked as solvable and reduces noise from ambiguous or broken problem instances.","L1: Language Comprehension, Language Production
L2: Planning, Logical Reasoning, Working Memory, Adaptive Error Correction, Decision-making
L3: ",L2
SWE-bench Multilingual,https://www.swebench.com/multilingual.html,https://arxiv.org/abs/2504.21798,"SWE-bench Multilingual extends the SWE-bench paradigm beyond Python to multiple programming languages, requiring models to understand diverse codebases, ecosystems, and tooling conventions. It tests whether agentic coding competence generalizes across language syntax, libraries, and build/test workflows.","L1: Language Comprehension, Language Production
L2: Planning, Logical Reasoning, Working Memory, Adaptive Error Correction, Decision-making
L3: Cognitive Flexibility",L3
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro is a larger, harder software engineering benchmark aimed at more realistic and contamination-resistant evaluation, spanning multiple languages and more complex engineering workflows. Success typically requires deeper repository understanding, iterative debugging, and robust patch generation under stricter evaluation conditions.","L1: Language Comprehension, Language Production
L2: Planning, Logical Reasoning, Working Memory, Adaptive Error Correction, Decision-making
L3: Cognitive Flexibility",L3
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 measures an agent’s ability to complete real tasks inside a command-line environment (e.g., installing dependencies, running programs, manipulating files, and diagnosing failures). It emphasizes iterative tool use, recovery from errors, and maintaining task state over multi-step execution.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Working Memory, Adaptive Error Correction
L3: Inhibitory Control",L3
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp evaluates “browse-and-answer” capability where an agent must search, read, and synthesize information from a constrained document collection or web-like corpus to answer questions. It stresses evidence gathering, citation-quality grounding, and multi-step information integration across sources.","L1: Language Comprehension, Language Production
L2: Planning, Attention, Semantic Understanding & Context Recognition, Episodic Memory, Working Memory, Decision-making
L3: ",L2
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench evaluates tool-using customer-support agents interacting with simulated users and APIs under domain policies (e.g., retail, airline, telecom). It tests whether the agent can follow constraints, keep coherent dialogue state, and reliably execute multi-turn workflows to resolve user goals.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Working Memory, Reward Mechanisms
L3: Social Reasoning & Theory of Mind, Empathy, Inhibitory Control",L3
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld measures computer-use competence in realistic desktop environments where models must interpret screenshots and perform actions to complete tasks across applications. It assesses end-to-end perception-to-action control, including navigation, error recovery, and multi-step task completion under step limits.","L1: Visual Perception
L2: Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Planning, Decision-making, Working Memory, Sensorimotor Coordination, Adaptive Error Correction
L3: ",L2
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI (Abstraction and Reasoning Corpus) tests few-shot fluid reasoning on novel grid-based pattern transformation problems, where the rule must be inferred from a handful of examples. It emphasizes abstraction, compositional generalization, and flexible strategy selection rather than memorized knowledge.","L1: Visual Perception
L2: Logical Reasoning, Spatial Representation & Mapping, Working Memory, Attention, Planning
L3: Cognitive Flexibility",L3
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon agent coherence and strategy by simulating management of a vending-machine business over extended time, requiring many sequential decisions (pricing, inventory, supplier interactions). The score reflects cumulative outcomes, so early mistakes compound and recovery strategies matter.","L1: 
L2: Planning, Decision-making, Reward Mechanisms, Working Memory, Episodic Memory, Adaptive Error Correction
L3: Self-reflection, Motivational Drives",L3
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas measures real-world tool use through the Model Context Protocol by requiring multi-step workflows that discover, invoke, and coordinate tools across MCP servers. It stresses correct API usage, error handling, and synthesizing tool outputs into accurate final responses.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Working Memory, Adaptive Error Correction, Semantic Understanding & Context Recognition
L3: ",L2
FinanceAgent,https://www.vals.ai/benchmarks/finance_agent,,"FinanceAgent evaluates performance on tasks typical of an entry-level financial analyst, such as extracting information from filings, reasoning about metrics, and producing structured analyses. It often requires combining domain knowledge with quantitative reasoning and producing professional-grade written outputs.","L1: Language Comprehension, Language Production
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Planning, Decision-making
L3: ",L2
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity agent capability on vulnerability identification and discovery tasks grounded in real software projects, sometimes with high-level weakness descriptions. It emphasizes structured debugging, hypothesis-driven investigation, and careful handling of adversarial or ambiguous evidence.","L1: 
L2: Logical Reasoning, Planning, Adaptive Error Correction, Working Memory, Decision-making, Semantic Understanding & Context Recognition
L3: Inhibitory Control",L3
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"SpreadsheetBench measures an agent’s ability to navigate and manipulate complex spreadsheets, including formulas, tables, and multi-sheet dependencies, to solve realistic tasks. It tests careful state tracking and correctness under tool-mediated editing and computation.","L1: Language Comprehension, Language Production
L2: Working Memory, Planning, Decision-making, Logical Reasoning, Attention, Adaptive Error Correction
L3: ",L2
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a high-difficulty, multi-modal benchmark spanning frontier academic and professional topics, designed to probe broad general intelligence under hard, often research-like questions. It evaluates synthesis across text and images and rewards precise, well-justified answers rather than shallow pattern matching.","L1: Language Comprehension, Language Production, Visual Perception
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Multisensory Integration, Working Memory, Planning
L3: ",L2
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 evaluates competition-style mathematics problem solving with concise numeric answers, stressing multi-step derivations and precision. It is commonly used to measure symbolic reasoning reliability and error sensitivity in chained computations.","L1: 
L2: Logical Reasoning, Working Memory, Planning, Adaptive Error Correction, Attention
L3: ",L2
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a subset of GPQA containing especially high-quality, expert-validated graduate-level science multiple-choice questions designed to resist shallow retrieval. It emphasizes deep conceptual understanding, careful elimination of distractors, and multi-step scientific reasoning.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Decision-making, Attention
L3: ",L2
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends MMLU to many non-English languages, testing broad academic knowledge and reasoning across subjects while stressing multilingual comprehension. It evaluates whether knowledge and reasoning transfer robustly across linguistic contexts and scripts.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Working Memory, Logical Reasoning
L3: Cognitive Flexibility",L3
MMMU,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2311.16502,"MMMU evaluates multi-discipline multimodal understanding by combining images (e.g., diagrams, charts, scenes) with text questions across expert domains. It tests integrating visual evidence with textual context to perform domain reasoning and select correct answers.","L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Attention, Working Memory
L3: ",L2
LAB-Bench FigQA,https://huggingface.co/datasets/futurehouse/lab-bench,https://arxiv.org/abs/2407.10362,"LAB-Bench FigQA measures whether models can correctly interpret and reason over complex scientific figures from biology papers (e.g., plots, microscopy panels, multi-part diagrams). It stresses precise extraction of visual evidence, mapping it to scientific claims, and answering figure-grounded questions accurately.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Multisensory Integration, Attention, Logical Reasoning, Working Memory, Semantic Understanding & Context Recognition
L3: ",L2
WebArena,https://webarena.dev/,https://arxiv.org/abs/2307.13854,"WebArena evaluates autonomous web agents performing realistic multi-step tasks across web apps (e-commerce, CMS, forums, code hosting, maps) using browser interaction. It emphasizes robust navigation, form filling, decision-making under partial observability, and maintaining long task context across many UI steps.","L1: Visual Perception, Language Comprehension, Language Production
L2: Planning, Decision-making, Attention, Working Memory, Scene Understanding & Visual Reasoning, Sensorimotor Coordination, Adaptive Error Correction
L3: ",L2
