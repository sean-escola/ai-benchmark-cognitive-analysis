Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates software-engineering agents on real GitHub issues, requiring them to modify a repository so that tests pass. The Verified subset focuses on tasks confirmed solvable and reliably graded, emphasizing end-to-end patch generation rather than isolated code snippets.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Logical Reasoning, Working Memory, Adaptive Error Correction
L3: ",L2
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 measures an agent’s ability to complete real-world tasks in a command-line environment (e.g., debugging, file manipulation, environment setup) under tool and resource constraints. Success depends on iterating through commands, interpreting errors, and converging to a correct final state.","L1: Language Comprehension
L2: Planning, Decision-making, Logical Reasoning, Working Memory, Adaptive Error Correction, Attention
L3: ",L2
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp evaluates deep research and web-browsing competence on questions that require multi-step information gathering and synthesis from documents. It emphasizes search strategy, source triangulation, and producing a grounded final answer from retrieved evidence.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Attention, Working Memory, Semantic Understanding & Context Recognition
L3: ",L2
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench measures interactive tool use in customer-support style domains (e.g., retail, airline, telecom) with simulated users and APIs, where the agent must follow domain policies. It stresses maintaining dialogue state across turns while making correct, policy-compliant tool calls.","L1: Language Comprehension, Language Production
L2: Decision-making, Planning, Working Memory
L3: Social Reasoning & Theory of Mind, Inhibitory Control",L3
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld evaluates multimodal “computer use” by having agents operate within an OS-like desktop environment to accomplish tasks across applications. Agents must perceive UI state (screenshots), plan multi-step procedures, and execute sequences of interactions robustly.","L1: Visual Perception
L2: Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Planning, Decision-making, Working Memory, Sensorimotor Coordination
L3: ",L2
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,ARC-AGI tests fluid pattern discovery by presenting a few input-output grid examples and requiring the model to infer the underlying rule and generate the correct output for a new grid. It is designed to minimize reliance on memorized skills and reward novel abstraction and compositional reasoning.,"L1: 
L2: Logical Reasoning, Working Memory, Scene Understanding & Visual Reasoning, Spatial Representation & Mapping, Attention
L3: Cognitive Flexibility",L3
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon agent coherence and business management in a simulated vending-machine enterprise over many decisions. Agents must handle budgeting, inventory, supplier negotiation, and dynamic adaptation to market conditions to maximize final profit.","L1: 
L2: Planning, Decision-making, Working Memory, Episodic Memory, Reward Mechanisms, Adaptive Error Correction
L3: ",L2
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas measures real-world tool-use performance via the Model Context Protocol, requiring models to discover appropriate tools and execute multi-step workflows across APIs. It emphasizes correct tool selection, argument construction, error handling, and synthesis of tool outputs into final responses.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Working Memory, Adaptive Error Correction
L3: ",L2
FinanceAgent,https://www.vals.ai/benchmarks/finance_agent,,"FinanceAgent evaluates whether a model can perform tasks typical of an entry-level financial analyst, such as interpreting financial statements, performing calculations, and drafting analysis grounded in provided data. It stresses precise numerical reasoning, structured reporting, and consistent assumptions.","L1: Language Production
L2: Logical Reasoning, Working Memory, Semantic Understanding & Context Recognition, Planning, Decision-making
L3: ",L2
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity agent capabilities on tasks involving finding known vulnerabilities and discovering new ones in real open-source projects. It tests the ability to reason about codebases, reproduce issues, and propose correct fixes or exploit explanations under realistic constraints.","L1: 
L2: Logical Reasoning, Planning, Decision-making, Working Memory, Adaptive Error Correction, Semantic Understanding & Context Recognition
L3: ",L2
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"SpreadsheetBench evaluates how well agents can navigate and manipulate complex spreadsheets derived from realistic scenarios, often requiring multi-step transformations and correctness checks. It stresses structured reasoning over tabular data and reliable execution of operations to produce the intended artifact.","L1: 
L2: Working Memory, Planning, Decision-making, Logical Reasoning, Attention, Semantic Understanding & Context Recognition
L3: ",L2
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a high-difficulty, multi-modal benchmark spanning frontier academic and professional knowledge, often requiring multi-step reasoning rather than recall. It evaluates whether models can integrate information from text and images to produce correct, well-justified answers.","L1: Language Comprehension, Language Production, Visual Perception
L2: Logical Reasoning, Working Memory, Semantic Understanding & Context Recognition, Scene Understanding & Visual Reasoning
L3: ",L2
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 uses problems from a prestigious mathematics competition that require symbolic manipulation, proof-style reasoning, and careful case handling. It is typically solved without external tools and rewards correct final numeric answers derived from multi-step reasoning.","L1: 
L2: Logical Reasoning, Working Memory, Attention
L3: Cognitive Flexibility",L3
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,GPQA Diamond is a subset of particularly difficult graduate-level science multiple-choice questions intended to be “Google-proof.” It evaluates deep domain understanding and multi-step scientific reasoning under a constrained answer format.,"L1: Language Comprehension
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory
L3: ",L2
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends MMLU-style academic knowledge testing across many subjects and multiple non-English languages. It evaluates whether models can reason and answer consistently across languages, not just in English.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory
L3: ",L2
MMMU,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2311.16502,"MMMU is a multidisciplinary multimodal benchmark combining text with images (e.g., diagrams, charts, figures) to test expert-level understanding. It emphasizes integrating visual evidence with language-based reasoning across many domains.","L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Logical Reasoning, Working Memory, Multisensory Integration
L3: ",L2
LAB-Bench FigQA,https://huggingface.co/datasets/futurehouse/lab-bench,https://arxiv.org/abs/2407.10362,"LAB-Bench FigQA evaluates scientific figure question answering, requiring models to interpret complex biology figures and answer grounded questions. It probes visual abstraction, linking figure elements to scientific concepts, and avoiding unsupported claims.","L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory
L3: ",L2
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning evaluates reasoning over scientific paper content, often combining technical text with figure- or symbol-heavy contexts and requiring precise, multi-step inference. The benchmark emphasizes faithful interpretation of research-style materials and quantitative/structural reasoning.","L1: Language Comprehension, Visual Perception
L2: Logical Reasoning, Working Memory, Semantic Understanding & Context Recognition, Scene Understanding & Visual Reasoning
L3: ",L2
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU extends multimodal understanding to video, requiring models to answer questions that depend on events unfolding over time and sometimes fine-grained temporal cues. It stresses integrating visual sequences with language prompts and maintaining consistency across frames.","L1: Visual Perception, Language Comprehension
L2: Visual Attention & Eye Movements, Working Memory, Scene Understanding & Visual Reasoning, Multisensory Integration
L3: Cognitive Timing & Predictive Modeling",L3
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"FACTS Benchmark Suite evaluates factuality and grounding by testing whether model outputs remain supported by provided context and avoid fabrications across diverse settings. It emphasizes calibration, abstention when uncertain, and consistency under paraphrase or adversarial prompting.","L1: Language Comprehension, Language Production
L2: Semantic Understanding & Context Recognition, Working Memory, Attention
L3: Inhibitory Control, Self-reflection",L3
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA evaluates pragmatic instruction understanding across many languages and cultural contexts, focusing on whether a model chooses sensible actions or interpretations in everyday situations. It stresses practical commonsense reasoning rather than narrow academic recall.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Decision-making
L3: Social Reasoning & Theory of Mind",L3
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,MRCR v2 (8-needle) tests long-context multi-round coreference resolution by embedding multiple similar “needle” requests inside long “haystacks” and asking models to retrieve the correct referenced content. The 8-needle variant increases interference and demands robust tracking across many distractors.,"L1: 
L2: Working Memory, Attention, Episodic Memory, Semantic Understanding & Context Recognition, Logical Reasoning
L3: ",L2
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval evaluates economically valuable, well-specified knowledge-work tasks across many occupations, with outputs judged by expert humans (e.g., spreadsheets, plans, written deliverables). It stresses producing usable artifacts that meet constraints, communicate clearly, and reflect professional decision quality.","L1: Language Production, Language Comprehension
L2: Planning, Decision-making, Semantic Understanding & Context Recognition, Working Memory
L3: Social Reasoning & Theory of Mind",L3
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks evaluates whether models can follow and reason over graph-structured data described in text, such as traversals, reachability, and parent/neighbor relationships. It stresses maintaining intermediate state and performing correct multi-step navigation without losing track.","L1: 
L2: Spatial Representation & Mapping, Working Memory, Logical Reasoning, Attention
L3: ",L2
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon evaluates general tool-use competence across diverse tools and tasks, measuring whether agents can select tools, call them correctly, and integrate results into accurate final outputs. It emphasizes robustness to tool errors, schema variations, and multi-step workflows.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory
L3: ",L2
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath evaluates advanced, expert-level mathematics beyond standard competition problems, often requiring creative multi-step constructions and rigorous reasoning. It is designed to be challenging and less susceptible to superficial pattern matching, emphasizing sustained correctness on hard instances.","L1: 
L2: Logical Reasoning, Working Memory, Planning, Attention
L3: Cognitive Flexibility",L3
