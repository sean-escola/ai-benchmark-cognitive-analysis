Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates software-engineering agents on real GitHub issues, requiring them to produce code patches that make the repository’s tests pass. The “Verified” subset consists of tasks that have been confirmed by human experts to be solvable and well-specified, emphasizing reliable end-to-end debugging and implementation rather than merely writing standalone code.","L1: Language Comprehension
L2: Planning, Decision-making, Adaptive Error Correction, Logical Reasoning, Working Memory, Semantic Understanding & Context Recognition
L3: ",L2
SWE-bench Multilingual,https://www.swebench.com/multilingual.html,https://arxiv.org/abs/2504.21798,"SWE-bench Multilingual extends SWE-bench-style real-repo issue resolution beyond Python to multiple programming languages and ecosystems. It tests whether an agent can transfer debugging and patching skills across language-specific tooling, conventions, and dependency environments while still meeting test-suite and CI constraints.","L1: Language Comprehension
L2: Planning, Decision-making, Adaptive Error Correction, Logical Reasoning, Working Memory, Semantic Understanding & Context Recognition
L3: Cognitive Flexibility",L3
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro is a larger and more difficult software-engineering benchmark aimed at more contamination-resistant, industry-relevant tasks across multiple languages. It stresses long-horizon problem solving: understanding a bug report, navigating a sizable codebase, making coherent changes, and satisfying regression tests under realistic constraints.","L1: Language Comprehension
L2: Planning, Decision-making, Adaptive Error Correction, Logical Reasoning, Working Memory, Semantic Understanding & Context Recognition
L3: Self-reflection",L3
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 evaluates agent performance on practical command-line tasks in sandboxed environments, such as installing dependencies, manipulating files, running scripts, and debugging failures. Success requires iterative interaction with tools (shell commands), interpreting error output, and adapting actions until a goal state is reached.","L1: Language Comprehension
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition
L3: ",L2
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp evaluates “deep research” agents on questions that require browsing and synthesizing information from a controlled or web-like corpus using search and retrieval tools. The benchmark emphasizes multi-step investigation, source triangulation, and producing grounded answers rather than relying on parametric recall alone.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Attention, Working Memory, Episodic Memory, Semantic Understanding & Context Recognition
L3: ",L2
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench measures an agent’s ability to solve multi-turn customer-support tasks by interacting with simulated users and APIs while adhering to domain policies. It probes robust tool use over long dialogues, handling edge cases, and balancing helpfulness with policy compliance when users request disallowed actions.","L1: Language Comprehension, Language Production
L2: Decision-making, Planning, Working Memory, Semantic Understanding & Context Recognition
L3: Social Reasoning & Theory of Mind, Inhibitory Control",L3
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld is a multimodal computer-use benchmark where agents complete tasks inside an operating-system environment using GUI actions (e.g., clicks, typing) and interpreting screenshots. It tests whether models can ground instructions in visual interfaces, navigate applications, and recover from mistakes over multi-step workflows.","L1: Visual Perception
L2: Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Planning, Decision-making, Sensorimotor Coordination, Working Memory, Adaptive Error Correction
L3: ",L2
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI evaluates fluid reasoning on abstract grid-based tasks where an agent must infer hidden transformation rules from only a few input-output examples. It aims to reduce the advantage of memorization by focusing on novel pattern discovery, compositional rule induction, and generalization to unseen puzzles.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Logical Reasoning, Working Memory, Planning
L3: Cognitive Flexibility",L3
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon autonomy by placing an agent in a simulated vending-machine business that it must operate over an extended period. Strong performance requires strategic planning, budgeting, negotiation and procurement decisions, and adapting to changing demand and constraints across many steps.","L1: 
L2: Planning, Decision-making, Reward Mechanisms, Working Memory, Episodic Memory, Adaptive Error Correction
L3: Motivational Drives",L3
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas assesses real-world tool use via the Model Context Protocol, requiring agents to discover, invoke, and chain multiple API-like tools to complete tasks. It emphasizes workflow execution reliability—correct tool selection, parameterization, error handling, and integrating tool outputs into a final response.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition
L3: ",L2
FinanceAgent,https://www.vals.ai/benchmarks/finance_agent,,"FinanceAgent benchmarks agent performance on tasks typical of an entry-level financial analyst, often requiring multi-step quantitative and document-grounded reasoning. Tasks commonly involve extracting relevant facts, applying finance concepts, building or checking calculations, and producing professional written outputs with justifications.","L1: Language Comprehension, Language Production
L2: Logical Reasoning, Planning, Decision-making, Working Memory, Semantic Understanding & Context Recognition
L3: ",L2
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity agents on tasks involving identifying known vulnerabilities and discovering new ones in real open-source projects, typically from partial descriptions and code context. It stresses iterative hypothesis testing, using tooling and feedback signals, and constructing correct technical fixes or exploit-relevant reasoning paths.","L1: 
L2: Planning, Decision-making, Adaptive Error Correction, Logical Reasoning, Working Memory, Semantic Understanding & Context Recognition
L3: ",L2
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"SpreadsheetBench measures an agent’s ability to understand, edit, and compute over complex spreadsheets in realistic workflows (e.g., formulas, formatting, multi-sheet references). It tests both procedural competence (making correct edits) and analytical competence (deriving correct values and structure under constraints).","L1: 
L2: Planning, Decision-making, Working Memory, Adaptive Error Correction, Logical Reasoning, Semantic Understanding & Context Recognition
L3: ",L2
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a difficult, multimodal benchmark spanning frontier academic and professional knowledge, with questions designed to be hard to answer by rote recall. It often rewards careful reasoning, synthesis across evidence, and—when tools are allowed—competent use of search and code to validate claims.","L1: Language Comprehension, Language Production, Visual Perception
L2: Scene Understanding & Visual Reasoning, Logical Reasoning, Working Memory, Semantic Understanding & Context Recognition, Planning
L3: ",L2
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 is a competition-math evaluation drawn from the American Invitational Mathematics Examination, requiring precise multi-step derivations and error-free arithmetic or algebra. It emphasizes structured reasoning and the ability to maintain intermediate results and constraints without drifting.","L1: Language Comprehension
L2: Logical Reasoning, Working Memory, Planning, Adaptive Error Correction
L3: ",L2
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a high-quality subset of graduate-level, “Google-proof” multiple-choice science questions intended to be challenging for non-experts. It tests deep conceptual understanding and careful elimination under time-limited information, rather than simple fact lookup.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Decision-making
L3: ",L2
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends broad academic knowledge and reasoning evaluation across many subjects into multiple non-English languages. It probes whether a model’s knowledge and problem solving remain robust under multilingual framing, translation ambiguity, and culturally varied phrasing.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory
L3: Cognitive Flexibility",L3
MMMU,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2311.16502,"MMMU evaluates expert-level multimodal understanding by requiring reasoning over images (e.g., diagrams, charts, tables) together with text questions across many disciplines. It stresses grounding: extracting the right visual evidence, aligning it with linguistic constraints, and performing domain reasoning to select an answer.","L1: Visual Perception, Language Comprehension
L2: Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Working Memory, Semantic Understanding & Context Recognition
L3: ",L2
LAB-Bench FigQA,https://huggingface.co/datasets/futurehouse/lab-bench,https://arxiv.org/abs/2407.10362,"LAB-Bench FigQA focuses on answering questions about complex scientific figures from biology papers, including plots and multi-panel visuals. It tests fine-grained figure interpretation, mapping labels and trends to biological claims, and avoiding confusions caused by dense visual layouts.","L1: Visual Perception, Language Comprehension, Language Production
L2: Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory
L3: ",L2
WebArena,https://webarena.dev/,https://arxiv.org/abs/2307.13854,"WebArena evaluates autonomous web agents on realistic tasks across multiple web applications (e-commerce, CMS, forums, repos), requiring navigation and form-filling via a browser interface. It emphasizes long-horizon planning, state tracking across pages, and robust recovery from partial failures in dynamic UIs.","L1: Visual Perception, Language Comprehension, Language Production
L2: Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Planning, Decision-making, Sensorimotor Coordination, Working Memory, Adaptive Error Correction
L3: ",L2
