Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates whether a model can solve real GitHub software engineering issues by generating code patches that make a provided test suite pass. The “Verified” subset consists of tasks that have been manually checked to be solvable and reliably testable, making results more comparable across systems.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Logical Reasoning, Working Memory, Adaptive Error Correction
L3: ",L2
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 measures agentic performance on real command-line tasks in containerized environments (e.g., installing packages, manipulating files, debugging, and running programs). Success depends on iteratively issuing correct shell commands, interpreting outputs, and recovering from errors under resource constraints.","L1: Language Comprehension
L2: Planning, Decision-making, Working Memory, Adaptive Error Correction, Attention
L3: ",L2
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp benchmarks “deep research” behavior where a model must browse a document collection or the web (depending on setup) to answer difficult questions that require multi-hop evidence gathering. It emphasizes search strategy, source selection, and synthesizing a final answer grounded in retrieved evidence.","L1: Language Comprehension
L2: Planning, Decision-making, Working Memory, Attention, Semantic Understanding & Context Recognition
L3: Self-reflection",L3
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench evaluates tool-using customer-support agents interacting with simulated users and APIs in domains like retail, airlines, and telecom. Models must follow policies, take multi-turn actions (e.g., refunds, rebookings, troubleshooting), and maintain consistency across long interactions.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Working Memory
L3: Inhibitory Control, Social Reasoning & Theory of Mind, Empathy",L3
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld evaluates multimodal “computer use” agents that complete tasks by operating a desktop OS through screenshots and UI actions (clicks, typing, navigation). It stresses robust perception of interfaces and long-horizon execution of multi-step workflows with feedback from the environment.","L1: Visual Perception
L2: Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Sensorimotor Coordination, Planning, Decision-making, Working Memory, Adaptive Error Correction
L3: ",L2
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI tests fluid reasoning on abstract grid transformation puzzles: given a few input–output examples, the model must infer the hidden rule and produce the correct output for a new input. The benchmark is designed to minimize reliance on memorized knowledge and reward flexible pattern induction.","L1: Visual Perception
L2: Logical Reasoning, Working Memory, Spatial Representation & Mapping, Attention
L3: Cognitive Flexibility",L3
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon agent coherence by running a simulated vending-machine business over an extended period, tracking outcomes like profit and business health. Agents must plan, negotiate, manage inventory/pricing, and adapt to changing conditions across many steps.","L1: 
L2: Planning, Decision-making, Working Memory, Reward Mechanisms, Adaptive Error Correction
L3: Cognitive Timing & Predictive Modeling, Self-reflection",L3
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas measures real-world tool use through the Model Context Protocol (MCP), requiring models to discover, call, and chain tools across multi-step workflows. Tasks test correct API selection and parameterization, error handling, and synthesis of tool outputs into final responses.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Working Memory, Adaptive Error Correction
L3: ",L2
FinanceAgent,https://www.vals.ai/benchmarks/finance_agent,,"FinanceAgent assesses whether models can perform tasks typical of an entry-level financial analyst (e.g., extracting facts from documents, computing metrics, building simple financial artifacts, and explaining results). It emphasizes faithful analysis, numeric/financial reasoning, and structured reporting.","L1: Language Production
L2: Logical Reasoning, Decision-making, Working Memory, Semantic Understanding & Context Recognition, Attention
L3: ",L2
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity agent skills on real-world vulnerability tasks, including identifying known vulnerabilities from descriptions and discovering new issues in open-source codebases. It typically requires code comprehension, hypothesis testing, and iterative debugging or exploitation reasoning.","L1: 
L2: Logical Reasoning, Planning, Decision-making, Working Memory, Adaptive Error Correction, Attention, Semantic Understanding & Context Recognition
L3: ",L2
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"SpreadsheetBench tests an agent’s ability to understand and manipulate complex spreadsheets using programmatic or tool-based interactions (e.g., editing cells, building formulas, cleaning data, and producing correct outputs). It emphasizes multi-step procedural accuracy and consistency with spreadsheet semantics.","L1: 
L2: Planning, Decision-making, Working Memory, Adaptive Error Correction, Semantic Understanding & Context Recognition, Attention
L3: ",L2
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,Humanity’s Last Exam is a difficult multimodal benchmark intended to probe frontier knowledge and reasoning across many domains. Questions may require integrating text with images/figures and carrying out multi-step inference rather than recalling a single fact.,"L1: Language Comprehension, Visual Perception
L2: Logical Reasoning, Working Memory, Semantic Understanding & Context Recognition, Scene Understanding & Visual Reasoning, Attention
L3: ",L2
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 consists of competition-style math problems from the American Invitational Mathematics Examination, requiring exact numeric answers. It emphasizes multi-step symbolic reasoning, careful constraint handling, and error-free arithmetic/algebraic manipulation.","L1: 
L2: Logical Reasoning, Working Memory, Attention, Adaptive Error Correction
L3: ",L2
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a high-difficulty subset of GPQA containing graduate-level, “Google-proof” multiple-choice science questions. It stresses deep conceptual understanding and reasoning in physics, chemistry, and biology under adversarially hard distractors.","L1: 
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Attention, Decision-making
L3: ",L2
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends MMLU to multiple languages, testing broad academic knowledge and reasoning across many subjects via multilingual multiple-choice questions. It probes whether models can transfer competence across languages while maintaining consistent reasoning quality.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Attention
L3: ",L2
MMMU,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2311.16502,"MMMU is a multidisciplinary multimodal benchmark where questions require joint reasoning over text and images (e.g., diagrams, charts, and visual scenes). It targets expert-level multimodal understanding across STEM, humanities, and applied domains.","L1: Visual Perception, Language Comprehension
L2: Multisensory Integration, Scene Understanding & Visual Reasoning, Logical Reasoning, Working Memory, Attention
L3: ",L2
LAB-Bench FigQA,https://huggingface.co/datasets/futurehouse/lab-bench,https://arxiv.org/abs/2407.10362,"LAB-Bench FigQA evaluates whether models can answer questions about complex biology figures from scientific papers, requiring accurate extraction of visual evidence and domain-relevant interpretation. It stresses figure literacy (axes, legends, panels) and reasoning from experimental results.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Attention
L3: ",L2
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning tests scientific-figure understanding and reasoning using questions derived from research-paper artifacts (e.g., charts/plots) paired with contextual text. Success requires correctly reading graphical encodings and performing chained reasoning, often aided by quantitative calculation.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Logical Reasoning, Working Memory, Attention, Semantic Understanding & Context Recognition
L3: ",L2
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU evaluates multimodal understanding and reasoning over video content paired with questions that may require temporal integration across scenes. It emphasizes tracking events over time, extracting salient visual cues, and using context to answer multi-step queries.","L1: Visual Perception, Language Comprehension
L2: Visual Attention & Eye Movements, Working Memory, Scene Understanding & Visual Reasoning, Multisensory Integration
L3: Cognitive Timing & Predictive Modeling",L3
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"FACTS Benchmark Suite systematically evaluates factuality and grounding, measuring whether model outputs remain accurate, supported, and non-hallucinatory under various prompting and retrieval settings. It focuses on claim correctness, attribution to sources when available, and robustness to misleading contexts.","L1: Language Comprehension, Language Production
L2: Semantic Understanding & Context Recognition, Working Memory, Attention
L3: Inhibitory Control, Self-reflection",L3
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA benchmarks physical commonsense reasoning across many languages and culturally diverse settings, typically via selecting the more plausible solution to a situation. It tests whether models maintain commonsense and pragmatic reasoning beyond English-centric distributions.","L1: Language Comprehension
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Decision-making, Spatial Representation & Mapping
L3: ",L2
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) is a long-context evaluation where multiple similar “needle” interactions are embedded in a long “haystack,” and the model must retrieve and reproduce the correct response for a specified needle. The 8-needle variant increases interference and tests robust in-context retrieval over long documents.","L1: Language Comprehension
L2: Working Memory, Attention, Episodic Memory, Semantic Understanding & Context Recognition
L3: ",L2
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval measures performance on well-specified professional knowledge-work tasks spanning many occupations, judged by expert humans via pairwise comparisons. Tasks often require producing real work artifacts (e.g., spreadsheets, presentations, schedules) under realistic constraints and quality standards.","L1: Language Production
L2: Planning, Decision-making, Semantic Understanding & Context Recognition, Working Memory, Adaptive Error Correction
L3: Self-reflection",L3
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks evaluates structured reasoning over graph-structured data, such as following edges, performing BFS-like traversals, or identifying parent/ancestor relationships. It stresses consistent step-by-step tracking and preventing drift when many similar entities compete for attention.","L1: 
L2: Spatial Representation & Mapping, Working Memory, Logical Reasoning, Planning, Attention, Adaptive Error Correction
L3: ",L2
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon evaluates generalized tool-use competence across diverse tool APIs and multi-step tasks, emphasizing choosing the right tool, composing calls, and integrating results into a correct final output. It also stresses robustness to tool errors, partial failures, and ambiguous requirements.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Working Memory, Adaptive Error Correction, Attention
L3: Inhibitory Control",L3
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath is a high-difficulty mathematics benchmark targeting advanced problem-solving beyond standard contest sets, often requiring creative insight and long chains of derivation. It is designed to reduce contamination and better reflect frontier-level mathematical reasoning.","L1: 
L2: Logical Reasoning, Working Memory, Attention, Planning, Adaptive Error Correction
L3: ",L2
