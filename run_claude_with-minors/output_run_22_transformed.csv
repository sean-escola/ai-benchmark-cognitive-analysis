Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates models on real GitHub issues that require producing a correct code patch in a Python repository. The “Verified” subset consists of tasks that have been confirmed by human review to be solvable and to have reliable, test-based grading, emphasizing end-to-end bug fixing and feature implementation.","L1: Language Comprehension
L2: Planning, Decision-making, Logical Reasoning, Adaptive Error Correction, Working Memory
L3: ",L2
SWE-bench Multilingual,https://www.swebench.com/multilingual.html,https://arxiv.org/abs/2504.21798,"SWE-bench Multilingual extends the SWE-bench format beyond Python to multiple programming languages, testing whether a model can diagnose and repair issues across diverse ecosystems. It stresses robust code reasoning under varying syntax, tooling conventions, and library idioms while still requiring an executable patch that passes tests.","L1: Language Comprehension
L2: Planning, Decision-making, Logical Reasoning, Adaptive Error Correction, Working Memory
L3: Cognitive Flexibility",L3
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro is a larger and more difficult software engineering benchmark designed to be more realistic and contamination-resistant, spanning multiple languages and industrial-style tasks. It emphasizes longer-horizon debugging and implementation, where models must select strategies, manage context, and iteratively converge on a correct solution under strict evaluation.","L1: Language Comprehension
L2: Planning, Decision-making, Logical Reasoning, Adaptive Error Correction, Working Memory
L3: ",L2
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 measures how well an agent can complete real tasks in a command-line environment, such as configuring tools, manipulating files, running programs, and troubleshooting failures. Success depends on issuing correct sequences of terminal commands and adapting when outputs, errors, or environment constraints change.","L1: Language Comprehension
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory
L3: ",L2
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp evaluates deep-research style question answering where a model must use web browsing/search to locate and synthesize evidence for hard queries. It probes the ability to plan an investigation, extract relevant facts from sources, and maintain coherence while integrating information across multiple documents.","L1: Language Comprehension
L2: Planning, Decision-making, Semantic Understanding & Context Recognition, Working Memory, Episodic Memory
L3: ",L2
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench (Tau2-bench) evaluates interactive tool-using agents in multi-turn customer support scenarios with policies, constraints, and simulated users. The agent must follow domain rules, call APIs correctly, and manage conversational state while balancing helpfulness and compliance.","L1: Language Comprehension, Language Production
L2: Decision-making, Planning, Working Memory
L3: Social Reasoning & Theory of Mind, Inhibitory Control",L3
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld tests multimodal “computer use” agents that must complete tasks on a real or realistic operating system desktop using screenshots and action interfaces (e.g., clicks, typing). It measures perception-to-action competence: interpreting UI state, navigating applications, and executing multi-step procedures reliably.","L1: Visual Perception
L2: Visual Attention & Eye Movements, Sensorimotor Coordination, Planning, Decision-making, Working Memory, Adaptive Error Correction
L3: ",L2
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI measures abstract pattern discovery and generalization from a few examples using grid-based input-output tasks. Models must infer latent rules and apply them to novel instances, emphasizing flexible reasoning rather than memorized domain knowledge.","L1: 
L2: Logical Reasoning, Working Memory, Spatial Representation & Mapping, Attention
L3: Cognitive Flexibility",L3
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon autonomous agent performance in a simulated business setting, where the agent manages a vending machine operation over an extended period. It requires sustained strategy, resource management, adaptation to changing conditions, and coherent decision-making across many steps.","L1: 
L2: Planning, Decision-making, Reward Mechanisms, Working Memory, Episodic Memory, Adaptive Error Correction
L3: Self-reflection",L3
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas benchmarks real-world tool use via the Model Context Protocol, requiring models to discover tools, call them with correct arguments, handle errors, and combine outputs into a final answer. It emphasizes multi-step workflow execution across heterogeneous services, closer to production integration than toy tool-use tasks.","L1: Language Comprehension
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory
L3: ",L2
FinanceAgent,https://www.vals.ai/benchmarks/finance_agent,,"FinanceAgent evaluates agent performance on tasks typical of entry-level financial analysis, such as interpreting financial documents, building analyses, and producing structured outputs with appropriate assumptions. It stresses numeracy and domain reasoning while also requiring careful synthesis and consistency across multi-part deliverables.","L1: Language Comprehension
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Planning, Decision-making, Working Memory
L3: ",L2
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates AI agents on cybersecurity tasks at scale, including identifying known vulnerabilities and discovering new ones in real open-source projects. It emphasizes systematic exploration, precise technical reasoning, and iterative debugging in adversarially complex codebases and environments.","L1: Language Comprehension
L2: Logical Reasoning, Planning, Decision-making, Adaptive Error Correction, Working Memory
L3: ",L2
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"SpreadsheetBench measures an agent’s ability to navigate, edit, and compute with complex spreadsheets, often requiring formula creation, data transformation, and multi-sheet consistency. The benchmark stresses executing structured operations correctly while maintaining bookkeeping-like accuracy across many cells and constraints.","L1: 
L2: Planning, Decision-making, Logical Reasoning, Working Memory, Adaptive Error Correction
L3: ",L2
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a difficult, expert-facing benchmark spanning many domains, designed to test frontier reasoning and knowledge and, in some settings, multimodal understanding. Tasks often require integrating scattered evidence, performing multi-step inference, and resisting shallow pattern-matching on unfamiliar questions.","L1: Language Comprehension, Visual Perception
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Scene Understanding & Visual Reasoning
L3: ",L2
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 consists of competition-style math problems requiring multi-step derivations and exact numeric answers. It probes symbolic and quantitative reasoning under tight problem statements, where small logical slips can invalidate the final result.","L1: 
L2: Logical Reasoning, Working Memory, Planning, Attention
L3: ",L2
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a high-quality subset of graduate-level, “Google-proof” science multiple-choice questions curated to be difficult for non-experts. It emphasizes deep conceptual understanding and careful elimination reasoning rather than surface recall.","L1: Language Comprehension
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Decision-making
L3: ",L2
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends MMLU to multiple languages, testing knowledge and reasoning across many subjects under multilingual prompts. It evaluates whether models can transfer conceptual understanding across languages and maintain consistent performance under linguistic variation.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory
L3: Cognitive Flexibility",L3
MMMU,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2311.16502,"MMMU is a multimodal benchmark covering many disciplines where questions require jointly reasoning over images and text (e.g., diagrams, charts, and technical figures). It tests visual interpretation, cross-modal grounding, and multi-step reasoning to arrive at precise answers.","L1: Visual Perception, Language Comprehension
L2: Multisensory Integration, Scene Understanding & Visual Reasoning, Logical Reasoning, Working Memory, Attention
L3: ",L2
LAB-Bench FigQA,https://huggingface.co/datasets/futurehouse/lab-bench,https://arxiv.org/abs/2407.10362,LAB-Bench FigQA evaluates whether models can correctly interpret complex scientific figures from biology papers and answer questions grounded in those visuals. It stresses extracting quantitative/relational information from plots and schematics and mapping it to domain-specific scientific claims.,"L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory
L3: ",L2
WebArena,https://webarena.dev/,https://arxiv.org/abs/2307.13854,"WebArena evaluates autonomous web agents on realistic tasks across multiple web applications (e-commerce, CMS, forums, code hosting, maps), requiring navigation and form-based interactions. It probes end-to-end planning and action selection under dynamic page states, with error recovery when earlier actions lead to dead ends.","L1: Language Comprehension, Visual Perception
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory, Sensorimotor Coordination, Attention
L3: ",L2
