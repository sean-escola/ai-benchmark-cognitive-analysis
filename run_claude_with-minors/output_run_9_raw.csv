Benchmark,Website,Paper,Description,Cognitive Functions
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,SWE-bench Verified evaluates an LLM’s ability to solve real-world software engineering issues by generating patches that fix failing tests in open-source Python repositories. The “Verified” subset contains tasks that have been human-validated to be solvable and are scored by whether the produced patch passes the project’s test suite under a standardized harness.,"Planning, Adaptive Error Correction, Logical Reasoning, Working Memory, Language Comprehension, Language Production, Decision-making, Semantic Understanding & Context Recognition"
SWE-bench Multilingual,https://www.swebench.com/multilingual.html,https://arxiv.org/abs/2504.21798,"SWE-bench Multilingual extends SWE-bench-style patch generation to multiple programming languages, testing whether models can perform repository-level debugging and implementation beyond Python. Tasks require reading issue descriptions and code context, producing correct edits, and passing language-appropriate tests/build checks.","Planning, Adaptive Error Correction, Logical Reasoning, Working Memory, Language Comprehension, Language Production, Decision-making, Cognitive Flexibility, Semantic Understanding & Context Recognition"
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro is a larger, more challenging software engineering benchmark designed to be more contamination-resistant and industrially representative, spanning multiple languages and realistic engineering workflows. Models must propose code changes that satisfy hidden unit/integration checks, emphasizing robust reasoning under repository constraints and ambiguous bug reports.","Planning, Adaptive Error Correction, Logical Reasoning, Working Memory, Language Comprehension, Language Production, Decision-making, Cognitive Flexibility, Semantic Understanding & Context Recognition"
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 evaluates agentic problem-solving in real command-line environments, where models must issue shell commands, inspect files, run programs, and iteratively fix issues to accomplish tasks. Success depends on managing state across many steps (filesystem, tool outputs, errors) and recovering from mistakes under resource constraints.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Language Comprehension, Language Production, Semantic Understanding & Context Recognition, Inhibitory Control"
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp evaluates “deep research” style question answering where models must search and read web documents (or a controlled corpus, in some variants) and synthesize a correct final answer. It emphasizes multi-step information gathering, source triangulation, and maintaining coherence over long contexts while using browsing tools.","Planning, Decision-making, Episodic Memory, Working Memory, Language Comprehension, Language Production, Semantic Understanding & Context Recognition, Self-reflection"
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench measures an agent’s ability to complete multi-turn customer-support tasks by interacting with simulated users and backend APIs while adhering to domain policies (e.g., retail, airline, telecom). The benchmark stresses reliable tool use, policy compliance, and resolving long interaction traces with changing user goals and constraints.","Social Reasoning & Theory of Mind, Language Comprehension, Language Production, Planning, Decision-making, Inhibitory Control, Working Memory, Semantic Understanding & Context Recognition"
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld evaluates multimodal computer-use agents in a realistic desktop operating system, requiring navigation of GUIs, reading screenshots, and executing actions over many steps. Tasks resemble end-user workflows (finding settings, editing files, using applications) and are scored by successful task completion under step limits.","Visual Perception, Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Sensorimotor Coordination, Planning, Decision-making, Working Memory, Adaptive Error Correction"
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI tests abstract pattern induction using small grids where a model must infer a hidden transformation from a few input–output examples and apply it to a new input. It targets generalization to novel rules rather than memorized knowledge, rewarding flexible reasoning over compositional visual structures.","Logical Reasoning, Cognitive Flexibility, Working Memory, Spatial Representation & Mapping, Scene Understanding & Visual Reasoning, Attention"
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon autonomous agency in a simulated vending-machine business over extended time (e.g., a year), where the agent must manage inventory, pricing, suppliers, and communications to maximize final balance. It stresses persistence, strategic planning, and adapting decisions to changing market conditions across many turns.","Planning, Decision-making, Reward Mechanisms, Working Memory, Episodic Memory, Adaptive Error Correction, Motivational Drives, Self-reflection"
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas measures real-world tool-use via Model Context Protocol (MCP) servers, requiring models to discover appropriate tools, invoke APIs correctly, handle errors, and compose multi-step workflows. It emphasizes execution correctness across heterogeneous services and robust recovery when tool calls fail or return unexpected data.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Language Comprehension, Language Production, Semantic Understanding & Context Recognition"
FinanceAgent,https://www.vals.ai/benchmarks/finance_agent,,"FinanceAgent evaluates agent performance on tasks representative of an entry-level financial analyst, such as building or auditing financial models, extracting information, and producing analysis with business context. It stresses accurate quantitative reasoning, structured outputs, and multi-step decomposition of finance workflows.","Logical Reasoning, Planning, Decision-making, Working Memory, Language Comprehension, Language Production, Semantic Understanding & Context Recognition, Adaptive Error Correction"
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity agent capabilities at scale, including identifying known vulnerabilities from high-level descriptions and discovering new vulnerabilities in real open-source projects. Tasks require code comprehension, threat reasoning, and producing actionable findings or fixes under realistic constraints.","Logical Reasoning, Planning, Decision-making, Adaptive Error Correction, Working Memory, Language Comprehension, Language Production, Inhibitory Control"
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"SpreadsheetBench evaluates an agent’s ability to navigate and manipulate complex spreadsheets derived from real-world scenarios, often requiring formula edits, table restructuring, and correct calculations. It stresses stepwise verification, error correction, and maintaining consistency across interdependent cells and sheets.","Working Memory, Planning, Decision-making, Adaptive Error Correction, Logical Reasoning, Semantic Understanding & Context Recognition, Attention"
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam (HLE) is a multi-modal benchmark intended to probe frontier-level knowledge and reasoning across diverse expert domains, using challenging questions that may require synthesis, computation, and interpretation of provided materials. Variants may allow tools (e.g., search or code execution) to measure end-to-end problem solving under realistic workflows.","Language Comprehension, Language Production, Logical Reasoning, Working Memory, Semantic Understanding & Context Recognition, Scene Understanding & Visual Reasoning, Visual Perception, Planning"
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 evaluates mathematical problem solving on competition-style questions from the American Invitational Mathematics Examination. Problems typically require multi-step derivations, precise algebraic/number-theoretic reasoning, and careful constraint management to produce a final numeric answer.","Logical Reasoning, Working Memory, Planning, Attention, Adaptive Error Correction"
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a high-difficulty, multiple-choice science QA benchmark curated to be “Google-proof,” emphasizing questions that require expert-level reasoning rather than lookup. The Diamond subset focuses on the highest-quality items where experts reliably answer correctly and non-experts tend to fail.","Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Language Comprehension, Decision-making, Attention"
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends the MMLU-style broad academic evaluation to multiple languages, testing multilingual knowledge and reasoning across many subjects. It stresses robust comprehension and consistent decision-making under varying linguistic forms and culturally different phrasing.","Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Decision-making, Cognitive Flexibility"
MMMU,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2311.16502,"MMMU is a large-scale multimodal benchmark spanning many disciplines where models must answer questions using both text and images (e.g., diagrams, plots, tables, and natural images). It emphasizes integrating visual evidence with domain knowledge and multi-step reasoning to select or generate correct answers.","Visual Perception, Scene Understanding & Visual Reasoning, Multisensory Integration, Language Comprehension, Logical Reasoning, Working Memory, Attention"
LAB-Bench FigQA,https://huggingface.co/datasets/futurehouse/lab-bench,https://arxiv.org/abs/2407.10362,"LAB-Bench FigQA evaluates whether models can correctly interpret and reason over complex scientific figures from biology papers, such as multi-panel plots and annotated diagrams. It probes figure literacy (reading axes, legends, visual encodings) and translating visual evidence into precise scientific answers.","Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Visual Perception, Language Comprehension, Language Production, Logical Reasoning, Working Memory, Semantic Understanding & Context Recognition"
WebArena,https://webarena.dev/,https://arxiv.org/abs/2307.13854,"WebArena evaluates autonomous web agents in self-hosted, realistic web applications (e-commerce, CMS, forums, code hosting, maps), requiring navigation, form filling, and multi-step task completion. It stresses long-horizon planning, robust interaction with dynamic interfaces, and recovering from navigation or interpretation errors.","Visual Perception, Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Sensorimotor Coordination, Planning, Decision-making, Working Memory, Adaptive Error Correction, Semantic Understanding & Context Recognition"
