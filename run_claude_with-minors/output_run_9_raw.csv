Benchmark,Website,Paper,Description,Cognitive Functions
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates software engineering agents by asking them to produce code patches that fix real issues in open-source repositories, validated by unit tests and human verification of solvability. The “Verified” subset aims to reduce noise (e.g., ambiguous tasks or brittle tests) and better reflect reliable end-to-end bug-fixing and change implementation.","Planning, Decision-making, Logical Reasoning, Working Memory, Adaptive Error Correction, Language Comprehension, Language Production"
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 measures an agent’s ability to complete real tasks in a command-line environment (e.g., inspecting files, running programs, debugging, and configuring systems) using tool calls and iterative feedback. It emphasizes practical autonomy: taking actions, interpreting outputs/errors, and converging on a working solution.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Language Comprehension, Sensorimotor Coordination"
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp evaluates “deep research” behavior by requiring models to browse a document collection or the web to answer questions that typically need multi-step investigation and synthesis. It stresses selecting sources, extracting key evidence, and producing a final answer grounded in retrieved information.","Planning, Decision-making, Attention, Working Memory, Language Comprehension, Semantic Understanding & Context Recognition"
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench (Tau2-bench) evaluates agentic customer-support style interactions where the model must follow domain policies while using tools/APIs across multi-turn dialogues. It tests robustness to policy constraints, conversational dynamics with a simulated user, and reliable completion of procedural workflows.","Planning, Decision-making, Inhibitory Control, Working Memory, Language Comprehension, Language Production, Social Reasoning & Theory of Mind"
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld evaluates multimodal “computer use” by giving tasks that require operating within an OS-like environment (e.g., navigating GUIs, opening apps, editing settings/files) based on screen observations. Success depends on interpreting interface state and executing correct action sequences over many steps.","Visual Perception, Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Planning, Decision-making, Sensorimotor Coordination, Working Memory"
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI measures abstract, few-shot pattern induction using grid-based input-output examples where the rule must be inferred and applied to a new grid. It is intended to isolate fluid reasoning and generalization rather than memorization of domain knowledge.","Logical Reasoning, Cognitive Flexibility, Working Memory, Spatial Representation & Mapping, Attention"
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon autonomous agent performance in a year-long simulated business scenario (operating a vending-machine business) with many interdependent decisions. Models must manage inventory, pricing, supplier negotiation, and adapt to changing conditions to maximize final profit.","Planning, Decision-making, Working Memory, Reward Mechanisms, Cognitive Timing & Predictive Modeling, Adaptive Error Correction"
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas evaluates real-world tool use via the Model Context Protocol, where agents must discover tools, invoke them correctly, handle errors/retries, and integrate tool outputs into coherent solutions. The benchmark emphasizes multi-step workflow execution across diverse tool servers and APIs.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Language Comprehension"
FinanceAgent,https://www.vals.ai/benchmarks/finance_agent,,"FinanceAgent evaluates skills expected of an entry-level financial analyst, such as interpreting financial documents, performing calculations, producing analyses, and drafting structured outputs. Tasks often require combining domain knowledge with careful quantitative reasoning and artifact-quality reporting.","Logical Reasoning, Planning, Decision-making, Working Memory, Language Comprehension, Language Production, Semantic Understanding & Context Recognition"
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity agent capabilities on tasks such as locating known vulnerabilities from descriptions and attempting discovery of unknown issues in real open-source projects. It stresses iterative debugging/investigation, correct interpretation of program behavior, and producing actionable findings.","Logical Reasoning, Planning, Adaptive Error Correction, Working Memory, Decision-making"
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"SpreadsheetBench measures an agent’s ability to understand, edit, and compute over complex spreadsheets resembling real workplace artifacts. It tests whether models can plan multi-step transformations, maintain consistency across cells/sheets, and verify outputs programmatically.","Planning, Working Memory, Logical Reasoning, Adaptive Error Correction, Decision-making, Semantic Understanding & Context Recognition"
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a frontier-level benchmark spanning challenging questions across many academic fields, designed to probe deep reasoning and advanced knowledge. It is multimodal in parts, requiring models to integrate text with visual information and produce precise answers.","Language Comprehension, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Visual Perception"
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 uses problems from a prestigious mathematics competition, emphasizing multi-step symbolic reasoning and careful error-free computation. It is commonly used to evaluate contest-math performance without requiring external tools.","Logical Reasoning, Working Memory, Cognitive Flexibility, Planning"
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a high-difficulty, “Google-proof” set of graduate-level multiple-choice questions in science where superficial pattern matching tends to fail. It probes whether models can apply scientific understanding and reasoning under strong distractors.","Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Language Comprehension"
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends broad academic testing to many non-English languages, measuring whether models retain knowledge and reasoning ability across multilingual contexts. It stresses consistent understanding of questions and options despite linguistic and cultural variation.","Language Comprehension, Semantic Understanding & Context Recognition, Working Memory, Logical Reasoning"
MMMU,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2311.16502,"MMMU evaluates expert-level multimodal understanding across many disciplines, where problems combine images (diagrams, plots, screenshots) with text and multiple-choice reasoning. It targets integrated visual-and-text reasoning rather than vision-only recognition.","Visual Perception, Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Language Comprehension, Logical Reasoning, Working Memory, Multisensory Integration"
LAB-Bench FigQA,https://huggingface.co/datasets/futurehouse/lab-bench,https://arxiv.org/abs/2407.10362,"LAB-Bench FigQA tests whether models can interpret complex scientific figures (common in biology papers) and answer questions requiring reading plotted evidence, labels, and experimental context. It emphasizes faithful extraction of visual details and reasoning from figure-grounded evidence.","Visual Perception, Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory"
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning evaluates reasoning over scientific-paper content (often including charts/figures) where answers require structured interpretation of visual evidence and accompanying text. It emphasizes cross-referencing claims, extracting quantitative/relational information, and multi-step justification.","Visual Perception, Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Language Comprehension, Logical Reasoning, Working Memory, Multisensory Integration"
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU extends multimodal evaluation to video, requiring temporal understanding, event tracking, and reasoning about dynamic scenes. Questions often require integrating information across frames and aligning it with textual prompts or options.","Visual Perception, Visual Attention & Eye Movements, Working Memory, Scene Understanding & Visual Reasoning, Cognitive Timing & Predictive Modeling, Language Comprehension"
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"The FACTS Benchmark Suite systematically evaluates factuality-related behaviors, including whether responses remain grounded, consistent, and resistant to hallucination across varied settings. It targets reliability in knowledge claims rather than raw generation fluency.","Language Comprehension, Language Production, Semantic Understanding & Context Recognition, Inhibitory Control, Working Memory, Self-reflection"
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA evaluates practical, commonsense reasoning across many languages and locales, aiming to test whether models generalize beyond an English-centric worldview. It probes whether models select plausible actions/explanations under diverse everyday contexts.","Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning, Social Reasoning & Theory of Mind"
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) tests long-context multi-round coreference resolution by embedding multiple similar “needle” requests within long “haystack” conversations/documents. Models must retrieve and reproduce the correct referenced response, stressing robust long-range dependency handling.","Working Memory, Attention, Semantic Understanding & Context Recognition, Episodic Memory, Language Comprehension"
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval evaluates economically valuable professional work across many occupations by judging model-produced artifacts (e.g., spreadsheets, presentations, schedules) against expert human outputs. It emphasizes end-to-end task completion quality, including correctness, structure, and usefulness.","Planning, Decision-making, Language Production, Semantic Understanding & Context Recognition, Working Memory, Self-reflection"
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks evaluates algorithmic reasoning over graph-structured data, such as following parent pointers or performing breadth-first style traversal under constraints. It probes whether models can maintain and manipulate structured state across many steps without losing track of dependencies.","Logical Reasoning, Working Memory, Spatial Representation & Mapping, Attention, Planning"
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon evaluates multi-tool competence across diverse tool ecosystems, requiring models to select appropriate tools, compose tool calls, recover from failures, and integrate results into final answers. It focuses on robust agentic execution rather than single-step question answering.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Inhibitory Control, Language Comprehension"
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath measures expert-level mathematical problem solving on difficult, research-adjacent items designed to resist memorization and shallow heuristics. It emphasizes deep multi-step reasoning, precise symbolic manipulation, and sustained focus over long solutions.","Logical Reasoning, Working Memory, Cognitive Flexibility, Planning"
