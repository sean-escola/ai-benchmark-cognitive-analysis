Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates software-engineering agents on real GitHub issues, where the model must produce a patch that makes a project’s tests pass. The Verified subset emphasizes tasks that have been human-checked for solvability and evaluation reliability, reducing noise from ambiguous or flaky issues.","L1: Language Comprehension
L2: Logical Reasoning, Planning, Working Memory, Adaptive Error Correction
L3: ",L2
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 tests agents in command-line environments on multi-step, real-world style tasks (e.g., using shell tools, editing files, running programs, and interpreting outputs). Success requires choosing actions based on tool feedback and recovering from errors under resource and time constraints.","L1: Language Comprehension
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory
L3: ",L2
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp measures deep web-browsing competence on questions that require searching, reading, and synthesizing information from multiple sources. Agent performance depends on decomposing the query, selecting which pages to open, and integrating evidence while avoiding distractors and stale context.","L1: Language Comprehension
L2: Planning, Decision-making, Semantic Understanding & Context Recognition, Working Memory, Attention
L3: ",L2
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench evaluates customer-support agents that must interact with simulated users and APIs while adhering to domain policies (e.g., retail/airline/telecom). It stresses multi-turn dialogue consistency, correct tool use, and policy compliance under adversarial or emotionally charged user requests.","L1: Language Comprehension, Language Production
L2: Decision-making, Planning
L3: Social Reasoning & Theory of Mind, Inhibitory Control",L3
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld benchmarks computer-use agents that complete tasks in realistic desktop operating systems via screenshots and action APIs. Tasks require understanding UI state, navigating multi-step workflows, and executing precise interactions (clicking, typing, switching windows) with feedback-driven correction.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Sensorimotor Coordination, Planning, Adaptive Error Correction
L3: ",L2
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,ARC-AGI is a few-shot abstract reasoning benchmark where models infer transformation rules from small sets of input–output grid examples and apply them to new inputs. It targets fluid reasoning and generalization to novel patterns rather than memorized domain knowledge.,"L1: 
L2: Logical Reasoning, Working Memory, Spatial Representation & Mapping, Attention
L3: Cognitive Flexibility",L3
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon agent coherence in a simulated business setting where an agent runs a vending machine operation over an extended timeline. The agent must plan inventory and pricing decisions, interact with simulated counterparts, and adapt strategy based on outcomes.","L1: 
L2: Planning, Decision-making, Working Memory, Reward Mechanisms, Adaptive Error Correction
L3: ",L2
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas measures real-world tool use through the Model Context Protocol by requiring multi-step workflows across multiple tools/services. It emphasizes selecting appropriate tools, issuing correct calls, handling tool errors, and composing tool outputs into a final answer.","L1: Language Comprehension
L2: Planning, Decision-making, Working Memory, Adaptive Error Correction, Semantic Understanding & Context Recognition
L3: ",L2
FinanceAgent,https://www.vals.ai/benchmarks/finance_agent,,"FinanceAgent assesses whether a model can complete tasks typical of an entry-level financial analyst, often involving multi-document reading, quantitative reasoning, and structured deliverables. It probes the ability to follow finance conventions, maintain assumptions consistently, and produce actionable analyses.","L1: Language Production
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Planning, Decision-making, Working Memory
L3: ",L2
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity agent capabilities on tasks involving finding known vulnerabilities from descriptions and discovering new issues in real open-source codebases. It stresses code comprehension, hypothesis-driven investigation, and precise patch or exploit reasoning under realistic constraints.","L1: 
L2: Logical Reasoning, Planning, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition
L3: ",L2
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"SpreadsheetBench tests agents on complex spreadsheet tasks derived from real workflows, such as editing formulas, cleaning data, building models, and validating outputs. It requires understanding spreadsheet structure, manipulating files via tools, and maintaining consistency across interconnected cells.","L1: 
L2: Logical Reasoning, Working Memory, Planning, Adaptive Error Correction, Semantic Understanding & Context Recognition
L3: ",L2
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a difficult, often multimodal benchmark spanning frontier academic and professional questions that demand synthesis rather than recall. Tasks commonly require combining domain knowledge with careful reasoning, and when tools are enabled, integrating tool outputs into a coherent response.","L1: Language Comprehension, Visual Perception
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Planning
L3: ",L2
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 evaluates competition-style mathematics problem solving, typically requiring multi-step symbolic reasoning and careful algebraic manipulation. The benchmark emphasizes correctness under tight problem specifications and punishes small logical or arithmetic slips.","L1: 
L2: Logical Reasoning, Working Memory, Planning, Adaptive Error Correction
L3: ",L2
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a subset of high-quality, graduate-level science multiple-choice questions designed to be resistant to superficial search or memorization. It tests deep conceptual understanding and multi-step scientific reasoning across physics, chemistry, and biology.","L1: Language Comprehension
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory
L3: ",L2
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends broad academic knowledge testing across many subjects into multiple languages, probing both knowledge and reasoning under multilingual prompts. It measures whether models can maintain performance when linguistic surface form changes while the underlying concepts remain the same.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory
L3: ",L2
MMMU,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2311.16502,MMMU is a multi-discipline multimodal benchmark where models answer expert-level questions using both images and text. It stresses interpreting diagrams/charts and combining visual evidence with textual reasoning across diverse domains.,"L1: Visual Perception, Language Comprehension
L2: Multisensory Integration, Scene Understanding & Visual Reasoning, Logical Reasoning, Working Memory
L3: ",L2
LAB-Bench FigQA,https://huggingface.co/datasets/futurehouse/lab-bench,https://arxiv.org/abs/2407.10362,LAB-Bench FigQA evaluates the ability to interpret scientific figures from biology papers and answer targeted questions about them. It focuses on extracting quantitative/illustrative information from complex plots and linking visual evidence to scientific claims in text.,"L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Attention
L3: ",L2
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,CharXiv Reasoning evaluates scientific figure and document understanding by requiring models to answer reasoning questions grounded in content from research papers. It probes whether a model can connect visual elements (plots/tables/diagrams) with accompanying text and perform multi-step inference.,"L1: Language Comprehension, Visual Perception
L2: Scene Understanding & Visual Reasoning, Logical Reasoning, Working Memory
L3: ",L2
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU benchmarks multimodal understanding and reasoning over videos, requiring models to track events, entities, and temporal relationships. It stresses integrating information across frames and maintaining coherent interpretations over longer sequences.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Working Memory, Attention, Multisensory Integration
L3: Cognitive Timing & Predictive Modeling",L3
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"FACTS Benchmark Suite systematically evaluates factuality and grounding across multiple tasks that probe whether model outputs are supported by provided sources or reliable knowledge. It emphasizes resisting hallucination, maintaining consistency, and appropriately abstaining or correcting when uncertain.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Adaptive Error Correction, Working Memory
L3: Inhibitory Control",L3
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA measures practical commonsense and physical reasoning across many languages, aiming to separate real-world understanding from English-centric artifacts. Items typically require choosing the most plausible action or outcome in everyday scenarios under multilingual phrasing.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning
L3: Cognitive Flexibility",L3
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) tests long-context retrieval and multi-round coreference by embedding multiple similar “needle” requests inside a long “haystack” and asking for the response corresponding to a specific needle. It stresses precise recall, disambiguation among near-duplicates, and robustness as context length scales.","L1: 
L2: Working Memory, Attention, Episodic Memory, Semantic Understanding & Context Recognition
L3: ",L2
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval evaluates economically valuable, well-specified professional tasks (e.g., creating spreadsheets, presentations, schedules, or analyses) judged against human professional outputs. It emphasizes producing correct, usable artifacts and following constraints, often across multi-step workflows.","L1: Language Production
L2: Planning, Decision-making, Semantic Understanding & Context Recognition, Working Memory
L3: ",L2
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks evaluates reasoning over graph-structured data, commonly requiring multi-step traversal, parent/neighbor retrieval, and following specified walk procedures. It stresses maintaining state across steps and accurately executing algorithmic instructions over long sequences.","L1: 
L2: Spatial Representation & Mapping, Logical Reasoning, Working Memory, Attention, Planning
L3: ",L2
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon evaluates general tool-using agents across diverse, multi-step tasks that require selecting among tools, calling them correctly, and stitching results into a final solution. It highlights reliability under tool errors, schema mismatches, and the need for iterative refinement.","L1: Language Comprehension
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory
L3: ",L2
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath measures expert-level mathematics performance on problems intended to be difficult and less susceptible to training contamination. Many tasks require sustained multi-step derivations or proof-like reasoning, often benefiting from careful verification and backtracking.","L1: 
L2: Logical Reasoning, Working Memory, Planning, Adaptive Error Correction
L3: ",L2
