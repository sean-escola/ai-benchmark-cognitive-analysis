Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates agentic software engineering by asking a model to generate patches for real GitHub issues, then running repository tests to verify correctness. The “Verified” subset emphasizes high-quality, human-validated tasks to reduce ambiguous or unsolvable instances and focuses on end-to-end code changes rather than isolated coding questions.","L1: Language Comprehension, Language Production
L2: Planning, Logical Reasoning, Adaptive Error Correction, Working Memory, Decision-making, Semantic Understanding & Context Recognition
L3: ",L2
SWE-bench Multilingual,https://www.swebench.com/multilingual.html,https://arxiv.org/abs/2504.21798,"SWE-bench Multilingual extends SWE-bench-style repository repair to multiple programming languages beyond Python, measuring whether agents generalize debugging and patching behaviors across ecosystems. Tasks require understanding project conventions, applying language-specific tooling, and producing changes that satisfy tests in each language setting.","L1: Language Comprehension, Language Production
L2: Planning, Logical Reasoning, Adaptive Error Correction, Working Memory, Decision-making, Semantic Understanding & Context Recognition
L3: Cognitive Flexibility",L3
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro is a larger and more difficult software engineering benchmark designed to be more contamination-resistant and industrially relevant, with complex repositories and realistic change requests. It emphasizes robust, multi-file modifications, correct dependency/tooling usage, and higher difficulty than SWE-bench Verified.","L1: Language Comprehension, Language Production
L2: Planning, Logical Reasoning, Adaptive Error Correction, Working Memory, Decision-making, Semantic Understanding & Context Recognition
L3: Cognitive Flexibility",L3
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 evaluates agents operating in command-line environments on practical tasks such as installing dependencies, manipulating files, running programs, and debugging failures. Success depends on choosing correct shell commands, interpreting outputs/errors, and iterating toward a working solution under step and resource constraints.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory, Attention, Semantic Understanding & Context Recognition
L3: ",L2
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp measures deep research and information-seeking with browsing/search tools, requiring models to locate evidence in a large document collection and synthesize accurate answers. It tests whether agents can decompose a query, explore sources, track findings across steps, and avoid being misled by irrelevant or noisy documents.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Attention, Working Memory, Episodic Memory, Semantic Understanding & Context Recognition, Adaptive Error Correction
L3: ",L2
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench evaluates interactive tool-using agents in simulated customer-support domains (e.g., retail, airline, telecom) that must follow domain policies while using APIs over multi-turn dialogues. It probes whether an agent can maintain goals, adhere to constraints, and coordinate tool calls while handling user requests and edge cases.","L1: Language Comprehension, Language Production
L2: Decision-making, Planning, Working Memory, Attention, Reward Mechanisms
L3: Social Reasoning & Theory of Mind, Empathy, Inhibitory Control",L3
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld evaluates multimodal “computer use” agents that operate a real or realistic OS GUI to complete tasks across apps and settings. Agents must perceive screens, navigate UI state, execute action sequences (click/type), and recover from mistakes to reach task goals.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Sensorimotor Coordination, Planning, Decision-making, Working Memory, Adaptive Error Correction, Attention
L3: ",L2
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI measures few-shot abstract reasoning on grid-based puzzles where models infer latent transformation rules from a small number of examples. It is designed to emphasize novel pattern induction and rule generalization rather than memorized knowledge, with solutions depending on compositional visual reasoning.","L1: Visual Perception
L2: Logical Reasoning, Working Memory, Scene Understanding & Visual Reasoning, Spatial Representation & Mapping, Attention
L3: Cognitive Flexibility",L3
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon agent coherence by simulating management of a vending-machine business over many time steps, including procurement, pricing, negotiation, and inventory decisions. High scores require sustained planning, adapting strategy to changing conditions, and avoiding compounding errors across a lengthy trajectory.","L1: 
L2: Planning, Decision-making, Working Memory, Episodic Memory, Adaptive Error Correction, Reward Mechanisms
L3: Self-reflection",L3
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas evaluates real-world tool use through the Model Context Protocol, where models must discover tools, call them correctly, and chain multiple calls to complete workflow tasks. It emphasizes practical API literacy, error handling, and orchestration across heterogeneous tool servers and data sources.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition, Attention
L3: ",L2
FinanceAgent,https://www.vals.ai/benchmarks/finance_agent,,"FinanceAgent benchmarks tasks typical of an entry-level financial analyst, such as interpreting financial documents, performing calculations, and producing justified recommendations or reports. It evaluates whether a model can integrate domain knowledge with structured reasoning and produce decision-support artifacts.","L1: Language Comprehension, Language Production
L2: Logical Reasoning, Decision-making, Planning, Working Memory, Semantic Understanding & Context Recognition, Attention
L3: ",L2
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity agents on tasks such as finding known vulnerabilities from descriptions and discovering previously unknown issues in real open-source codebases. It stresses systematic investigation, hypothesis testing, safe exploitation reasoning, and iterative debugging to reach correct vulnerability reports or patches.","L1: 
L2: Logical Reasoning, Planning, Decision-making, Adaptive Error Correction, Working Memory, Attention, Semantic Understanding & Context Recognition
L3: Inhibitory Control",L3
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"SpreadsheetBench evaluates agents’ ability to navigate and manipulate complex spreadsheets drawn from real-world settings, including formula editing, table operations, and data transformations. It tests whether models can maintain consistent structure, apply multi-step changes correctly, and validate outcomes after edits.","L1: 
L2: Planning, Decision-making, Working Memory, Attention, Adaptive Error Correction, Logical Reasoning, Semantic Understanding & Context Recognition
L3: ",L2
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a multimodal benchmark positioned at the frontier of academic and professional knowledge, with questions spanning many disciplines and often requiring synthesis rather than recall. It evaluates advanced reasoning over text and images, and (in tool-enabled settings) robust use of search/code to justify answers.","L1: Language Comprehension, Language Production, Visual Perception
L2: Logical Reasoning, Working Memory, Semantic Understanding & Context Recognition, Scene Understanding & Visual Reasoning, Multisensory Integration, Attention
L3: ",L2
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 evaluates competition-style mathematical problem solving under a fixed set of exam questions, typically requiring multi-step derivations and careful symbolic manipulation. It rewards precise reasoning, correct intermediate constraints, and avoidance of algebraic or arithmetic slips.","L1: 
L2: Logical Reasoning, Working Memory, Planning, Attention, Adaptive Error Correction
L3: ",L2
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a high-difficulty multiple-choice science benchmark designed to be “Google-proof,” with questions that require expert-level reasoning and deep conceptual understanding. The Diamond subset emphasizes quality by selecting items where experts agree on the correct answer and non-experts frequently fail.","L1: Language Comprehension
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Attention, Decision-making
L3: ",L2
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends MMLU to many languages, testing broad academic knowledge and reasoning across subjects in non-English settings. It probes whether models maintain consistent competence under multilingual inputs and varying cultural/linguistic phrasing of the same underlying concepts.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Attention, Decision-making
L3: Cognitive Flexibility",L3
MMMU,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2311.16502,"MMMU is a multidisciplinary multimodal benchmark where questions require reasoning over images (charts, diagrams, tables, scenes) alongside text across many expert domains. It tests whether models can align visual evidence with language, perform compositional reasoning, and select answers based on multimodal cues.","L1: Visual Perception, Language Comprehension, Language Production
L2: Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Working Memory, Attention, Decision-making
L3: ",L2
LAB-Bench FigQA,https://huggingface.co/datasets/futurehouse/lab-bench,https://arxiv.org/abs/2407.10362,"LAB-Bench FigQA evaluates whether models can correctly interpret complex scientific figures from biology papers and answer targeted questions about them. It stresses extracting quantitative/structural information from visuals, mapping figure elements to domain concepts, and reasoning from evidence rather than surface text.","L1: Visual Perception, Language Comprehension, Language Production
L2: Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Semantic Understanding & Context Recognition, Attention, Working Memory
L3: ",L2
WebArena,https://webarena.dev/,https://arxiv.org/abs/2307.13854,"WebArena evaluates autonomous web agents completing realistic multi-step tasks across several web apps (e-commerce, content management, forums, code hosting, maps). Agents must perceive dynamic pages, plan interaction sequences, execute UI actions reliably, and recover from navigation or tool errors to satisfy functional goals.","L1: Visual Perception, Language Comprehension, Language Production
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory, Attention, Scene Understanding & Visual Reasoning, Sensorimotor Coordination
L3: ",L2
