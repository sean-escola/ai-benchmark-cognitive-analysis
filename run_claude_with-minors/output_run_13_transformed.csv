Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates whether a model can solve real GitHub issue tasks by producing a correct patch that passes repository tests. It emphasizes realistic codebase navigation, debugging, and implementing changes under a single-attempt constraint in many reports.","L1: Language Comprehension
L2: Planning, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition, Logical Reasoning
L3: ",L2
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 measures agent performance on end-to-end command-line tasks inside a real terminal environment, where the model must decide commands, inspect outputs, and iteratively fix errors. It stresses tool-mediated problem solving under resource and time constraints.","L1: 
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition
L3: Inhibitory Control",L3
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp evaluates deep research and browsing agents by requiring them to locate and synthesize answers from a fixed or controlled web/document corpus using search and retrieval tools. Success depends on decomposing ambiguous information needs into queries, verifying evidence, and composing a grounded response.","L1: Language Production
L2: Planning, Decision-making, Attention, Working Memory, Semantic Understanding & Context Recognition
L3: ",L2
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench evaluates interactive customer-support agents operating in simulated domains (e.g., retail, airline, telecom) with policies and APIs, requiring multi-turn tool use and adherence to rules. It probes the ability to follow constraints while helping a user through long, branching dialogues.","L1: Language Comprehension, Language Production
L2: Decision-making, Planning
L3: Social Reasoning & Theory of Mind, Inhibitory Control",L3
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld tests multimodal computer-use agents on real operating-system tasks (GUI navigation, app workflows, file operations) with step limits and screen observations. It stresses visual grounding, sequential control, and robust error recovery when UI states change unexpectedly.","L1: Visual Perception
L2: Visual Attention & Eye Movements, Sensorimotor Coordination, Planning, Decision-making, Adaptive Error Correction
L3: ",L2
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,ARC-AGI measures fluid reasoning by asking models to infer abstract transformation rules from a few grid-based input–output examples and apply them to a novel input. It targets generalization to new concepts rather than memorized domain knowledge.,"L1: 
L2: Logical Reasoning, Working Memory, Spatial Representation & Mapping, Planning
L3: Cognitive Flexibility",L3
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon autonomous agency by having a model run a simulated vending-machine business over an extended period, making many interconnected decisions. It requires sustained strategy, adaptation to market dynamics, and coherent bookkeeping over time.","L1: 
L2: Planning, Decision-making, Reward Mechanisms, Working Memory, Adaptive Error Correction
L3: Self-reflection",L3
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas evaluates real-world tool use through the Model Context Protocol by testing whether a model can discover tools, invoke them correctly, and chain multiple calls into a successful workflow. It emphasizes API literacy, multi-step execution, and recovery from tool errors.","L1: 
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition
L3: ",L2
FinanceAgent,https://www.vals.ai/benchmarks/finance_agent,,"FinanceAgent assesses performance on tasks representative of an entry-level financial analyst, such as analysis, modeling, and producing finance-oriented deliverables. It stresses multi-step quantitative reasoning, structured reporting, and consistency with provided data and assumptions.","L1: Language Production
L2: Logical Reasoning, Planning, Decision-making, Working Memory, Semantic Understanding & Context Recognition
L3: ",L2
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity agents on tasks involving finding known vulnerabilities and discovering new ones in real open-source projects from natural-language descriptions and code contexts. It probes secure reasoning, exploit-relevant debugging, and iterative hypothesis testing.","L1: 
L2: Logical Reasoning, Planning, Adaptive Error Correction, Working Memory
L3: Inhibitory Control",L3
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"SpreadsheetBench tests an agent’s ability to understand, manipulate, and generate complex spreadsheets based on realistic tasks (e.g., cleaning, formula editing, restructuring, analysis). It emphasizes precise multi-step operations and verification of outputs against requirements.","L1: 
L2: Working Memory, Planning, Adaptive Error Correction, Logical Reasoning, Semantic Understanding & Context Recognition
L3: ",L2
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a large, multi-modal benchmark designed to probe frontier-level reasoning and knowledge across diverse subjects, often requiring synthesis and careful reading. In tool-enabled settings, it also tests whether agents can search, compute, and ground answers reliably.","L1: Language Comprehension, Visual Perception
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Planning
L3: ",L2
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 evaluates competition-style mathematics problem solving, typically requiring multi-step derivations and exact final answers. It focuses on symbolic reasoning, maintaining intermediate constraints, and avoiding arithmetic slips.","L1: 
L2: Logical Reasoning, Working Memory, Adaptive Error Correction
L3: Cognitive Flexibility",L3
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,GPQA Diamond is a challenging multiple-choice science benchmark curated to be hard for non-experts and resistant to shallow pattern matching. It stresses deep reading of technical questions and disciplined elimination of distractor answers.,"L1: Language Comprehension
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory
L3: Inhibitory Control",L3
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends broad academic knowledge testing to multiple languages, covering many subjects and requiring consistent reasoning across linguistic contexts. It probes multilingual comprehension and cross-domain generalization under standardized formats.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory
L3: ",L2
MMMU,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2311.16502,"MMMU evaluates multimodal expert reasoning across many disciplines by combining images (diagrams, plots, figures) with text questions and multiple-choice answers. It tests whether models can integrate visual evidence with domain knowledge to reach a correct conclusion.","L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Working Memory
L3: ",L2
LAB-Bench FigQA,https://huggingface.co/datasets/futurehouse/lab-bench,https://arxiv.org/abs/2407.10362,LAB-Bench FigQA measures whether models can interpret complex scientific figures (especially in biology) and answer questions that require careful visual reading and scientific reasoning. It targets practical figure-grounded inference rather than generic image captioning.,"L1: Language Comprehension
L2: Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Logical Reasoning, Semantic Understanding & Context Recognition
L3: ",L2
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning tests whether a model can answer questions about scientific paper figures and charts, often requiring extracting quantitative/structural information from visuals. It emphasizes figure-grounded reasoning and precise interpretation of plotted or diagrammed evidence.","L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Logical Reasoning, Working Memory
L3: ",L2
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU extends multimodal understanding to video, requiring models to integrate information across time to answer questions about events, procedures, or visual details. It stresses temporal integration, attention to salient frames, and coherent reasoning from dynamic content.","L1: Visual Perception
L2: Attention, Working Memory, Multisensory Integration
L3: Cognitive Timing & Predictive Modeling",L3
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"FACTS Benchmark Suite systematically evaluates factuality by measuring whether model outputs remain consistent with ground truth or provided sources across varied settings. It focuses on reducing hallucinations, maintaining faithfulness, and resisting unsupported inferences.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Working Memory
L3: Inhibitory Control, Self-reflection",L3
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA evaluates physical commonsense reasoning across many languages and cultures, typically using short scenarios that require selecting the more plausible action or outcome. It probes whether models maintain grounded intuitions about the physical world across linguistic variation.","L1: Language Comprehension
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Spatial Representation & Mapping
L3: ",L2
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,MRCR v2 (8-needle) tests long-context retrieval and multi-round coreference by embedding multiple similar “needle” interactions within long “haystack” dialogues and asking for the response to a specific needle. It stresses maintaining and retrieving the right referenced information amid interference.,"L1: 
L2: Working Memory, Attention, Semantic Understanding & Context Recognition, Episodic Memory
L3: Inhibitory Control",L3
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval evaluates well-specified professional knowledge-work tasks spanning many occupations, judged against outputs from industry professionals. It emphasizes producing usable work artifacts and making correct procedural choices under realistic constraints and rubrics.","L1: Language Production
L2: Planning, Decision-making, Semantic Understanding & Context Recognition, Adaptive Error Correction
L3: Social Reasoning & Theory of Mind",L3
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks evaluates algorithmic reasoning over graphs by requiring models to follow traversal or relational queries that demand step-by-step tracking of nodes and edges. It emphasizes precise state tracking and resistance to distraction in long, structured sequences.","L1: 
L2: Logical Reasoning, Working Memory, Attention, Spatial Representation & Mapping
L3: Inhibitory Control",L3
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon evaluates general tool-using ability across diverse APIs and environments, requiring models to select tools, call them correctly, and compose results into a final answer. It stresses robust orchestration, error handling, and task decomposition under a unified harness.","L1: 
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition
L3: ",L2
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath evaluates expert-level mathematics, including problems intended to be difficult even for strong models and to better reflect research-grade reasoning. It emphasizes long multi-step derivations, careful constraint management, and high precision in intermediate reasoning.","L1: 
L2: Logical Reasoning, Working Memory, Planning, Adaptive Error Correction
L3: Cognitive Flexibility",L3
