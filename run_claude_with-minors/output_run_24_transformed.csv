Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates whether a model can resolve real GitHub issues by producing a code patch that makes repository tests pass on 500 human-verified, solvable tasks. It emphasizes end-to-end software engineering: understanding issue reports, locating relevant code, implementing fixes, and validating via tests under a standardized harness.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Planning, Logical Reasoning, Working Memory, Adaptive Error Correction, Decision-making
L3: ",L2
SWE-bench Multilingual,https://www.swebench.com/multilingual.html,https://arxiv.org/abs/2504.21798,"SWE-bench Multilingual extends the SWE-bench paradigm to multiple programming languages, testing whether models can generalize software engineering behaviors beyond Python across a diverse set of repos and toolchains. It stresses cross-language code understanding, debugging, and patch generation under similar realistic constraints as SWE-bench.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Planning, Logical Reasoning, Working Memory, Adaptive Error Correction, Decision-making
L3: Cognitive Flexibility",L3
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro is a larger, harder, and more industrially oriented software engineering benchmark designed to be more contamination-resistant and to include more diverse, realistic tasks (e.g., multi-language and complex repos). Models must generate correct patches that satisfy tests and constraints, often requiring deeper investigation and stronger robustness than SWE-bench Verified.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Planning, Logical Reasoning, Working Memory, Adaptive Error Correction, Decision-making
L3: Cognitive Flexibility",L3
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 measures agent performance on real-world command-line tasks in sandboxed environments, such as configuring tools, manipulating files, debugging, and running programs. It probes whether an agent can iteratively diagnose failures and use terminal feedback to complete multi-step objectives reliably.","L1: 
L2: Planning, Decision-making, Working Memory, Adaptive Error Correction, Logical Reasoning, Semantic Understanding & Context Recognition
L3: Inhibitory Control",L3
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp evaluates deep-research agents on information-seeking questions that require browsing and synthesizing evidence from a fixed web corpus to enable reproducible search. Success depends on decomposing a query into sub-questions, retrieving supporting documents, and producing a grounded final answer with consistent reasoning.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Working Memory, Semantic Understanding & Context Recognition, Adaptive Error Correction, Episodic Memory
L3: ",L2
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench measures interactive tool-using agents in simulated customer-support domains (e.g., retail, airline, telecom) where the agent must follow policies, call APIs, and manage multi-turn dialogues. It tests whether the agent can resolve user goals while remaining consistent with constraints and handling adversarial or ambiguous conversational dynamics.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Working Memory, Adaptive Error Correction
L3: Social Reasoning & Theory of Mind, Inhibitory Control",L3
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld is a multimodal computer-use benchmark where an agent must operate within a desktop OS environment using screenshots and interface interactions to complete tasks across applications. It tests grounded perception-to-action loops, including navigation, form filling, file management, and multi-step workflows under step limits.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Sensorimotor Coordination, Planning, Decision-making, Working Memory, Adaptive Error Correction
L3: ",L2
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI evaluates few-shot fluid reasoning on grid-transformation puzzles, where models infer abstract rules from a small number of input-output examples and generalize to a new instance. It targets systematic generalization, compositional pattern discovery, and robust rule application under novel task distributions.","L1: 
L2: Logical Reasoning, Working Memory, Planning, Decision-making, Spatial Representation & Mapping, Scene Understanding & Visual Reasoning
L3: Cognitive Flexibility",L3
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 tests long-horizon autonomous agency by having a model manage a simulated vending-machine business over an extended period, optimizing inventory, pricing, supplier negotiation, and operations. Performance reflects sustained coherence, strategic planning, and adaptation to changing market conditions across many decisions.","L1: 
L2: Planning, Decision-making, Working Memory, Episodic Memory, Adaptive Error Correction, Reward Mechanisms
L3: Motivational Drives, Self-reflection",L3
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas evaluates real-world tool use via the Model Context Protocol, requiring models to discover available tools, invoke them correctly, and compose multi-step workflows across authentic API-like environments. It emphasizes robustness to tool errors, correct parameterization, and synthesizing outputs into accurate task completions.","L1: Language Comprehension
L2: Planning, Decision-making, Working Memory, Adaptive Error Correction, Semantic Understanding & Context Recognition
L3: Inhibitory Control",L3
FinanceAgent,https://www.vals.ai/benchmarks/finance_agent,,"FinanceAgent assesses agentic performance on tasks typical of an entry-level financial analyst, such as extracting information, building analyses, and producing finance-relevant deliverables under realistic constraints. It stresses multi-step reasoning, careful handling of numeric and contextual details, and structured communication of results.","L1: Language Production
L2: Logical Reasoning, Planning, Decision-making, Working Memory, Semantic Understanding & Context Recognition, Adaptive Error Correction
L3: ",L2
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity capabilities by asking agents to identify and exploit previously-discovered vulnerabilities from descriptions and to discover new vulnerabilities in real open-source codebases. It tests iterative hypothesis-driven debugging, reading complex code, and adapting strategies based on tool and runtime feedback.","L1: 
L2: Planning, Logical Reasoning, Adaptive Error Correction, Working Memory, Decision-making, Semantic Understanding & Context Recognition
L3: Inhibitory Control",L3
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"SpreadsheetBench measures an agent’s ability to understand, edit, and compute with complex spreadsheets drawn from realistic scenarios, often requiring multi-step transformations and consistent formatting. It tests whether the model can coordinate data manipulation, formula logic, and verification under tool-assisted execution.","L1: 
L2: Logical Reasoning, Working Memory, Planning, Decision-making, Adaptive Error Correction, Semantic Understanding & Context Recognition
L3: Self-reflection",L3
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a large, frontier knowledge benchmark spanning expert-level questions (often multimodal) intended to be difficult even for strong models, with evaluation focused on correctness rather than style. It probes broad academic reasoning and the ability to integrate domain knowledge with careful inference under uncertainty.","L1: Language Comprehension, Language Production
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Scene Understanding & Visual Reasoning, Multisensory Integration
L3: ",L2
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 consists of competition-style mathematics problems that require multi-step symbolic reasoning, careful constraint handling, and exact answers. It tests structured problem solving and error-checking in a setting where small reasoning mistakes typically lead to wrong final results.","L1: 
L2: Logical Reasoning, Working Memory, Planning, Adaptive Error Correction, Decision-making
L3: ",L2
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a curated subset of GPQA featuring high-quality, graduate-level, “Google-proof” multiple-choice science questions that are difficult for non-experts. It evaluates deep scientific reasoning and conceptual understanding rather than surface recall, often requiring elimination and multi-step inference.","L1: Language Comprehension
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Decision-making
L3: ",L2
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends academic knowledge and reasoning evaluation across many subjects and multiple non-English languages, testing whether models can maintain performance under multilingual inputs. It probes multilingual comprehension, cross-lingual transfer, and consistent reasoning across varied cultural and linguistic contexts.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory
L3: Cognitive Flexibility",L3
MMMU,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2311.16502,"MMMU is a multimodal benchmark covering many expert domains where models must answer questions that jointly require interpreting images (e.g., diagrams, charts, figures) and text. It tests visual reasoning integrated with domain knowledge, including reading structured visuals and making grounded inferences.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Working Memory, Semantic Understanding & Context Recognition, Visual Attention & Eye Movements
L3: ",L2
LAB-Bench FigQA,https://huggingface.co/datasets/futurehouse/lab-bench,https://arxiv.org/abs/2407.10362,"LAB-Bench FigQA evaluates whether models can correctly interpret and reason about complex scientific figures from biology papers, including multi-panel plots and experimental schematics. It focuses on extracting relevant visual evidence and mapping it to scientific claims and questions under realistic figure complexity.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Multisensory Integration, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Visual Attention & Eye Movements
L3: ",L2
WebArena,https://webarena.dev/,https://arxiv.org/abs/2307.13854,"WebArena evaluates autonomous web agents on realistic tasks across multiple self-hosted web apps (e.g., e-commerce, CMS, GitLab-like tools), requiring navigation, form interactions, and multi-step workflows. It tests end-to-end planning and robust action selection under dynamic interfaces and partial observability.","L1: Visual Perception
L2: Planning, Decision-making, Working Memory, Adaptive Error Correction, Scene Understanding & Visual Reasoning, Sensorimotor Coordination
L3: Inhibitory Control",L3
