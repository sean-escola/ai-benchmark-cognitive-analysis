Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates whether a model can solve real GitHub issues by producing a correct patch that passes the repository’s tests, using a curated set of tasks verified to be solvable. It measures end-to-end software debugging and implementation skill under realistic constraints (reading code, localizing bugs, editing files, and validating fixes).","L1: Language Comprehension
L2: Logical Reasoning, Planning, Working Memory, Adaptive Error Correction, Decision-making, Semantic Understanding & Context Recognition
L3: ",L2
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 measures an agent’s ability to complete practical tasks in a command-line environment (e.g., installing tools, manipulating files, running programs, and interpreting outputs). It emphasizes iterative diagnosis and recovery from errors, where progress depends on choosing the right shell actions based on feedback from the environment.","L1: Language Comprehension
L2: Planning, Decision-making, Working Memory, Adaptive Error Correction, Semantic Understanding & Context Recognition, Sensorimotor Coordination, Attention
L3: ",L2
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp evaluates deep research and browsing competence by requiring models to answer questions using information found via web browsing/search and multi-step investigation. It stresses decomposing an information need into sub-queries, integrating evidence across sources, and producing a justified final answer under time and context constraints.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Working Memory, Attention, Semantic Understanding & Context Recognition
L3: Self-reflection",L3
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench evaluates interactive tool-using agents in customer-support style domains (e.g., retail, airline, telecom), where the agent must follow policies while calling APIs and conversing over multiple turns. It probes policy compliance, robust dialogue management, and the ability to resolve user problems through correct sequences of actions.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Working Memory, Attention
L3: Inhibitory Control, Social Reasoning & Theory of Mind, Empathy",L3
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld evaluates multimodal “computer use” by asking agents to complete tasks on a real operating system via screenshots and actions (mouse/keyboard), such as configuring settings, navigating apps, or filling forms. Success requires interpreting UI state, planning action sequences, and recovering from misclicks or unexpected interface changes.","L1: Visual Perception
L2: Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Sensorimotor Coordination, Planning, Decision-making, Working Memory, Adaptive Error Correction, Attention
L3: ",L2
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,ARC-AGI measures fluid reasoning by presenting few-shot grid transformation puzzles where the model must infer abstract rules from a handful of examples and apply them to new inputs. It is designed to reduce reliance on memorized knowledge and emphasize generalization to novel pattern-learning tasks.,"L1: 
L2: Logical Reasoning, Working Memory, Attention, Spatial Representation & Mapping, Scene Understanding & Visual Reasoning, Planning
L3: Cognitive Flexibility",L3
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon agent coherence by simulating operation of a vending-machine business over many steps, scoring the final financial outcome. The agent must manage inventory, pricing, procurement, and adaptation to changing conditions while maintaining consistent objectives over extended interactions.","L1: 
L2: Planning, Decision-making, Working Memory, Episodic Memory, Reward Mechanisms, Adaptive Error Correction, Semantic Understanding & Context Recognition
L3: Motivational Drives",L3
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas measures real-world tool use via the Model Context Protocol by requiring models to discover, invoke, and chain calls across production-like tool servers. Tasks emphasize correct API selection and parameterization, error handling, and synthesis of multi-tool results into reliable final outputs.","L1: Language Comprehension
L2: Planning, Decision-making, Working Memory, Adaptive Error Correction, Semantic Understanding & Context Recognition, Sensorimotor Coordination, Attention
L3: ",L2
FinanceAgent,https://www.vals.ai/benchmarks/finance_agent,,"FinanceAgent evaluates whether a model can perform tasks typical of an entry-level financial analyst, such as interpreting financial statements, building analyses, and producing decision-relevant summaries. It targets practical reasoning with domain constraints, often requiring careful handling of assumptions, calculations, and structured outputs.","L1: Language Comprehension, Language Production
L2: Logical Reasoning, Decision-making, Planning, Working Memory, Semantic Understanding & Context Recognition
L3: Self-reflection",L3
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity agent skills on large-scale tasks involving vulnerability identification, reproduction, and in some settings discovery within real open-source codebases. It emphasizes reading and reasoning about code, forming exploitation hypotheses, and iteratively testing/patching based on empirical feedback.","L1: Language Comprehension
L2: Logical Reasoning, Planning, Decision-making, Working Memory, Adaptive Error Correction, Semantic Understanding & Context Recognition
L3: Inhibitory Control",L3
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"SpreadsheetBench evaluates an agent’s ability to understand, edit, and compute over complex spreadsheets that mirror real workplace artifacts. Tasks typically require multi-step transformations, formula reasoning, and validation of results in a structured, high-precision environment.","L1: Language Comprehension
L2: Logical Reasoning, Working Memory, Planning, Decision-making, Adaptive Error Correction, Semantic Understanding & Context Recognition, Attention
L3: ",L2
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a challenging multimodal benchmark spanning frontier academic and professional knowledge, designed to stress models on difficult questions rather than routine recall. It evaluates deep reasoning, cross-domain synthesis, and (when enabled) tool-assisted problem solving across text and images.","L1: Language Comprehension, Language Production, Visual Perception
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Planning, Scene Understanding & Visual Reasoning, Attention
L3: ",L2
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 evaluates competition-style mathematics problem solving using the 2025 AIME questions, typically requiring multi-step derivations and careful algebraic/number-theoretic reasoning. It emphasizes exactness, intermediate-state tracking, and avoidance of subtle logical errors.","L1: 
L2: Logical Reasoning, Working Memory, Planning, Attention, Adaptive Error Correction
L3: ",L2
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a subset of the GPQA science multiple-choice benchmark curated to be especially difficult, where experts succeed and non-experts often fail. It evaluates deep scientific reasoning and the ability to disambiguate plausible distractors under knowledge-intensive constraints.","L1: Language Comprehension
L2: Logical Reasoning, Working Memory, Semantic Understanding & Context Recognition, Attention
L3: ",L2
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU is a multilingual extension of broad academic knowledge testing across many subjects and languages, measuring understanding and reasoning in non-English settings. It probes whether knowledge and problem-solving transfer across languages rather than relying on English-only patterns.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Attention
L3: ",L2
MMMU,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2311.16502,"MMMU evaluates expert-level multimodal understanding across many disciplines, where models must combine text with diagrams, charts, or images to answer questions. It stresses visual reasoning grounded in domain context and the integration of information across modalities.","L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Working Memory, Attention
L3: ",L2
LAB-Bench FigQA,https://huggingface.co/datasets/futurehouse/lab-bench,https://arxiv.org/abs/2407.10362,"LAB-Bench FigQA tests whether models can interpret and reason over scientific figures from biology papers, including plots, schematics, and multi-panel graphics. Success requires extracting relevant visual evidence and mapping it to domain concepts and questions.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Logical Reasoning, Semantic Understanding & Context Recognition, Attention, Working Memory
L3: ",L2
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning evaluates reasoning over charts and figure-like visuals commonly found in scientific documents, often requiring quantitative or relational inference beyond surface recognition. It probes long-form visual-to-text mapping, where correct answers depend on interpreting axes, legends, and plotted relationships.","L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Logical Reasoning, Working Memory, Attention, Semantic Understanding & Context Recognition
L3: ",L2
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU evaluates multimodal understanding and reasoning over videos paired with questions, requiring models to integrate information across time. It stresses tracking events, relationships, and state changes across frames rather than relying on a single static image.","L1: Visual Perception
L2: Attention, Working Memory, Scene Understanding & Visual Reasoning, Multisensory Integration
L3: Cognitive Timing & Predictive Modeling",L3
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"FACTS Benchmark Suite systematically evaluates factuality by testing whether model outputs remain grounded, avoid hallucinations, and preserve correctness under varied prompting and retrieval settings. It emphasizes faithfulness to provided evidence and reliable attribution when generating claims.","L1: Language Comprehension, Language Production
L2: Semantic Understanding & Context Recognition, Working Memory, Attention
L3: Inhibitory Control, Self-reflection",L3
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA evaluates physical commonsense reasoning (how objects and actions interact) across diverse linguistic and cultural settings, aiming to test robustness beyond English-centric distributions. It probes whether models can select plausible solutions to everyday physical interaction scenarios under multilingual variation.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Spatial Representation & Mapping, Scene Understanding & Visual Reasoning, Working Memory
L3: ",L2
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,MRCR v2 (8-needle) tests long-context retrieval and multi-round coreference by embedding multiple similar “needle” interactions within long “haystack” conversations and asking for the correct referenced response. It stresses attention control and accurate retrieval when many distractors are highly confusable.,"L1: Language Comprehension
L2: Working Memory, Episodic Memory, Attention, Semantic Understanding & Context Recognition, Adaptive Error Correction
L3: ",L2
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval evaluates well-specified knowledge-work tasks across many occupations, judged by experts on whether the model’s produced work product wins, ties, or loses against professional baselines. Tasks include creating structured artifacts (e.g., plans, spreadsheets, presentations) where quality depends on correct constraints, clarity, and usefulness.","L1: Language Production, Language Comprehension
L2: Planning, Decision-making, Semantic Understanding & Context Recognition, Working Memory
L3: Social Reasoning & Theory of Mind, Self-reflection",L3
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,Graphwalks evaluates reasoning over graph-structured data by requiring models to follow traversal rules and answer questions that depend on multi-step navigation and relational composition. It stresses consistent state tracking over long sequences and resistance to local distractors.,"L1: 
L2: Logical Reasoning, Working Memory, Attention, Spatial Representation & Mapping, Planning, Semantic Understanding & Context Recognition
L3: ",L2
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon evaluates general tool-using competence across diverse APIs and multi-step workflows, where the model must decide which tools to call, in what order, and how to recover from failures. It emphasizes robust orchestration under realistic tool errors, partial information, and iterative refinement.","L1: Language Comprehension
L2: Planning, Decision-making, Working Memory, Adaptive Error Correction, Semantic Understanding & Context Recognition, Sensorimotor Coordination, Attention
L3: ",L2
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath evaluates expert-level mathematics with problems designed to be difficult and contamination-resistant, often requiring deep multi-step reasoning and precise computation. It probes advanced proof-like reasoning, careful abstraction, and error-checking over long solution chains.","L1: 
L2: Logical Reasoning, Working Memory, Planning, Attention, Adaptive Error Correction
L3: Cognitive Flexibility",L3
