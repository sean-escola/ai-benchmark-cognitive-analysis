Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates whether a model can solve real GitHub software issues by producing a patch that makes the repository’s tests pass. The Verified subset contains tasks that have been manually confirmed to be solvable, emphasizing end-to-end debugging, code changes, and regression avoidance under realistic constraints.","L1: Language Comprehension
L2: Planning, Logical Reasoning, Working Memory, Adaptive Error Correction
L3: ",L2
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 measures an agent’s ability to complete real-world tasks in a command-line environment, such as configuring software, debugging, and running tools. Success depends on choosing correct commands, interpreting outputs/errors, and iterating reliably to reach a goal state.","L1: Language Comprehension
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory
L3: ",L2
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp evaluates deep research agents on questions that require browsing and synthesizing information from a controlled document collection (or web-like retrieval setting). It stresses multi-step information seeking, evidence aggregation, and producing a grounded final answer under context limits.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Attention, Working Memory, Semantic Understanding & Context Recognition
L3: ",L2
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench evaluates interactive tool-using agents in customer-support-style environments (e.g., retail/airline/telecom) with policies and APIs. Agents must follow domain rules while completing multi-turn workflows, handling ambiguous user needs, and using tools correctly and consistently.","L1: Language Comprehension, Language Production
L2: Decision-making, Planning, Working Memory
L3: Social Reasoning & Theory of Mind, Inhibitory Control",L3
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld measures autonomous computer-use capability in a realistic OS-like GUI environment with screenshots and interactive actions. Agents must navigate interfaces, locate UI elements, execute multi-step procedures, and recover from mistakes under step and time constraints.","L1: Visual Perception
L2: Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Sensorimotor Coordination, Planning, Decision-making, Working Memory, Adaptive Error Correction
L3: ",L2
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,ARC-AGI tests fluid reasoning by asking models to infer transformation rules from a few grid input–output examples and apply them to new inputs. It emphasizes novel pattern discovery and generalization rather than memorized knowledge.,"L1: 
L2: Logical Reasoning, Working Memory, Spatial Representation & Mapping, Attention
L3: Cognitive Flexibility",L3
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon agent coherence in a simulated business management setting (running a vending machine operation over an extended period). Agents must plan inventory and pricing, negotiate or interact with suppliers, and adapt strategy based on outcomes and constraints.","L1: 
L2: Planning, Decision-making, Working Memory, Reward Mechanisms, Adaptive Error Correction
L3: Cognitive Timing & Predictive Modeling, Motivational Drives",L3
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas evaluates real-world tool use via the Model Context Protocol (MCP), requiring models to discover tools, invoke them with correct parameters, handle errors, and compose multi-step workflows. It emphasizes robust orchestration across heterogeneous APIs and tool servers.","L1: Language Comprehension
L2: Planning, Decision-making, Working Memory, Adaptive Error Correction, Semantic Understanding & Context Recognition
L3: ",L2
FinanceAgent,https://www.vals.ai/benchmarks/finance_agent,,"FinanceAgent assesses performance on tasks typical of an entry-level financial analyst, such as extracting facts from documents, performing calculations, and producing structured analyses. It stresses correctness, domain reasoning, and workflow execution over realistic finance artifacts.","L1: Language Production, Language Comprehension
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Planning, Decision-making, Working Memory
L3: ",L2
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity agent skill on tasks such as identifying known vulnerabilities from descriptions and discovering new issues in real open-source codebases. It stresses investigative reasoning, exploitation/patch reasoning, and careful iteration in complex technical environments.","L1: Language Comprehension
L2: Logical Reasoning, Planning, Adaptive Error Correction, Working Memory
L3: Inhibitory Control",L3
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"SpreadsheetBench measures the ability to understand, edit, and generate complex spreadsheets using realistic business-like tasks and constraints. Models must track dependencies, apply correct formulas/transformations, and maintain consistent structure and formatting over multi-step edits.","L1: Language Comprehension, Language Production
L2: Planning, Working Memory, Logical Reasoning, Adaptive Error Correction, Decision-making
L3: ",L2
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a challenging multimodal benchmark intended to probe frontier knowledge and reasoning across diverse expert domains. Questions often require integrating text and images, handling long contexts, and producing precise answers under high uncertainty.","L1: Language Comprehension, Language Production, Visual Perception
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Attention
L3: ",L2
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 evaluates competition-style mathematical problem solving using questions from the American Invitational Mathematics Examination. It emphasizes multi-step symbolic reasoning, careful constraint handling, and exact final answers.","L1: 
L2: Logical Reasoning, Working Memory, Planning, Adaptive Error Correction
L3: ",L2
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a subset of GPQA containing very difficult graduate-level multiple-choice questions in science, designed to be resistant to simple web search. It tests whether models can use deep domain understanding and reasoning to select the correct option.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory
L3: ",L2
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends MMLU into many non-English languages, testing broad academic knowledge and reasoning across subjects. It evaluates whether models can maintain competence and consistency across multilingual prompts and culturally varied formulations.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory
L3: ",L2
MMMU,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2311.16502,"MMMU is a multi-discipline multimodal benchmark where questions require jointly reasoning over images and text (e.g., diagrams, charts, and domain visuals). It stresses cross-modal grounding and high-level visual reasoning beyond basic recognition.","L1: Visual Perception, Language Comprehension
L2: Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Working Memory
L3: ",L2
LAB-Bench FigQA,https://huggingface.co/datasets/futurehouse/lab-bench,https://arxiv.org/abs/2407.10362,"LAB-Bench FigQA evaluates whether models can answer questions about complex scientific figures from biology papers, requiring extraction of trends, labels, and experimental context. It targets figure-grounded reasoning rather than recalling facts from text alone.","L1: Visual Perception, Language Comprehension
L2: Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Working Memory
L3: ",L2
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning evaluates understanding and reasoning over charts/figures and associated scientific context drawn from arXiv-style documents. Tasks emphasize interpreting visual encodings (axes, legends, curves) and mapping them to correct textual inferences.","L1: Visual Perception, Language Comprehension
L2: Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Working Memory
L3: ",L2
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU extends multimodal understanding to video, requiring models to integrate information across frames and time to answer questions. It stresses temporal integration, tracking objects/events, and combining visual evidence with language instructions.","L1: Visual Perception, Language Comprehension
L2: Visual Attention & Eye Movements, Multisensory Integration, Working Memory, Scene Understanding & Visual Reasoning
L3: Cognitive Timing & Predictive Modeling",L3
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"FACTS Benchmark Suite systematically evaluates LLM factuality across multiple settings, focusing on whether outputs are supported by sources or ground truth and how models handle uncertainty. It targets hallucination-like failure modes, calibration, and reliable citation/grounding behavior.","L1: Language Comprehension, Language Production
L2: Semantic Understanding & Context Recognition, Working Memory
L3: Self-reflection, Inhibitory Control",L3
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA is a multilingual, non-parallel evaluation of physical commonsense reasoning, asking models to choose plausible actions or solutions in everyday physical scenarios. It probes whether models can generalize intuitive physics and practical reasoning across languages and formulations.","L1: Language Comprehension
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Spatial Representation & Mapping, Working Memory
L3: ",L2
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,MRCR v2 (8-needle) tests long-context multi-round coreference and retrieval by inserting multiple similar “needle” requests into lengthy “haystack” dialogues/documents and asking the model to reproduce the correct referenced response. It stresses robust tracking of entities/requests over long spans and resistance to distractors.,"L1: Language Comprehension
L2: Working Memory, Attention, Episodic Memory, Semantic Understanding & Context Recognition
L3: ",L2
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval evaluates well-specified professional knowledge-work tasks spanning many occupations, where outputs are judged against (or alongside) industry professionals. Tasks require producing real work artifacts (e.g., slides, spreadsheets, plans) with speed, structure, and correctness under constraints.","L1: Language Production, Language Comprehension
L2: Planning, Decision-making, Semantic Understanding & Context Recognition, Working Memory, Adaptive Error Correction
L3: Social Reasoning & Theory of Mind, Self-reflection",L3
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks evaluates structured reasoning over graph problems expressed in text, such as walking paths, BFS-like traversal, or parent/neighbor queries in large graphs. It stresses maintaining and updating an internal graph state across long contexts and executing algorithmic reasoning steps reliably.","L1: 
L2: Logical Reasoning, Working Memory, Spatial Representation & Mapping, Attention
L3: Cognitive Flexibility",L3
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon evaluates generalized tool-use competence across diverse tools and tasks, emphasizing correct tool selection, parameterization, and multi-step orchestration. It probes reliability under execution feedback (tool errors/results) and the ability to adapt plans mid-run.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory
L3: ",L2
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath evaluates advanced mathematics problem solving intended to be beyond routine competition math, often requiring deep multi-step reasoning and careful verification. It emphasizes sustained abstraction, error checking, and sometimes tool-assisted computation depending on the evaluation setup.","L1: 
L2: Logical Reasoning, Working Memory, Planning, Adaptive Error Correction
L3: Cognitive Flexibility",L3
