Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates agentic software engineering by asking a model to produce patches that fix real issues in open-source Python repositories, with solutions validated by tests. The “Verified” subset focuses on problems confirmed by human reviewers to be solvable and to have reliable evaluation signals.","L1: 
L2: Planning, Logical Reasoning, Working Memory, Adaptive Error Correction, Semantic Understanding & Context Recognition
L3: ",L2
SWE-bench Multilingual,https://www.swebench.com/multilingual.html,https://arxiv.org/abs/2504.21798,"SWE-bench Multilingual extends SWE-bench-style patch generation beyond Python to multiple programming languages and ecosystems, requiring adaptation to different toolchains and conventions. It evaluates whether models can transfer debugging and code-editing competence across language settings rather than relying on one familiar stack.","L1: 
L2: Planning, Logical Reasoning, Working Memory, Semantic Understanding & Context Recognition, Adaptive Error Correction
L3: Cognitive Flexibility",L3
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro is a harder, larger software engineering benchmark aimed at more realistic, contamination-resistant assessment across multiple languages and industrial-style tasks. It typically requires deeper repository understanding, longer multi-step debugging, and more robust patch iteration to satisfy tests and constraints.","L1: 
L2: Planning, Logical Reasoning, Working Memory, Adaptive Error Correction, Semantic Understanding & Context Recognition
L3: Cognitive Flexibility",L3
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 evaluates agents that operate a command-line environment to complete real tasks such as installing tools, running programs, manipulating files, and diagnosing failures. Success requires choosing and sequencing shell actions while interpreting tool feedback under resource and environment constraints.","L1: 
L2: Planning, Decision-making, Working Memory, Adaptive Error Correction, Semantic Understanding & Context Recognition, Motor Skill Learning, Sensorimotor Coordination
L3: ",L2
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp measures deep-research web browsing: the agent must search, read, and synthesize information from documents to answer questions that are difficult to solve from parametric memory alone. The benchmark emphasizes navigation strategy, evidence gathering, and consolidation of findings across many sources.","L1: Language Comprehension, Language Production
L2: Planning, Attention, Working Memory, Episodic Memory, Semantic Understanding & Context Recognition, Decision-making
L3: ",L2
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench evaluates interactive customer-support agents that must follow domain policies while using tools/APIs across multi-turn conversations in simulated environments (e.g., retail, airline, telecom). It stresses policy adherence under pressure, robust tool use, and maintaining coherent dialogue state over long interactions.","L1: Language Comprehension, Language Production
L2: Decision-making, Planning, Working Memory, Adaptive Error Correction
L3: Social Reasoning & Theory of Mind, Inhibitory Control, Empathy",L3
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld evaluates multimodal computer-use agents in a desktop OS setting, requiring them to complete tasks by operating real applications through UI interactions. It tests whether models can perceive screens, plan action sequences, and recover from errors while navigating complex interfaces.","L1: Visual Perception
L2: Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Planning, Decision-making, Working Memory, Sensorimotor Coordination, Adaptive Error Correction
L3: ",L2
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI tests fluid, few-shot abstract reasoning by presenting grid-based input/output examples with hidden rules and asking the model to infer the correct transformation for new inputs. It is designed to minimize reliance on memorized knowledge and instead emphasize generalization from minimal demonstrations.","L1: Visual Perception
L2: Logical Reasoning, Working Memory, Scene Understanding & Visual Reasoning, Attention, Planning
L3: Cognitive Flexibility",L3
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon agent coherence and business strategy in a simulated vending-machine operation over many decisions and time steps. Models must manage inventory, pricing, supplier communication, and adaptation to changing conditions to maximize final balance.","L1: 
L2: Planning, Decision-making, Reward Mechanisms, Episodic Memory, Working Memory, Adaptive Error Correction
L3: Cognitive Timing & Predictive Modeling, Self-reflection",L3
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas measures real-world tool use through the Model Context Protocol by requiring models to discover tools, invoke them correctly, handle failures, and integrate results across multi-step workflows. It targets practical orchestration skills for production-like API ecosystems rather than single-call function use.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Working Memory, Adaptive Error Correction, Semantic Understanding & Context Recognition
L3: ",L2
FinanceAgent,https://www.vals.ai/benchmarks/finance_agent,,"FinanceAgent evaluates agent performance on tasks representative of an entry-level financial analyst, often involving documents, calculations, and structured outputs. It emphasizes accurate extraction, reasoning under financial constraints, and producing professional artifacts (e.g., analyses or summaries) with tool assistance when available.","L1: Language Comprehension, Language Production
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Planning, Decision-making, Adaptive Error Correction
L3: ",L2
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity agents on vulnerability identification and discovery using real open-source projects, combining code comprehension with exploit-relevant reasoning. The benchmark stresses precise interpretation of codebases, hypothesis-driven investigation, and iterative refinement based on test results or findings.","L1: 
L2: Logical Reasoning, Planning, Working Memory, Adaptive Error Correction, Semantic Understanding & Context Recognition
L3: Inhibitory Control",L3
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"SpreadsheetBench evaluates an agent’s ability to navigate, manipulate, and compute with complex spreadsheets based on realistic tasks, often requiring formula logic, formatting, and multi-sheet consistency. It tests sustained, stepwise work where small mistakes compound and must be detected and corrected.","L1: 
L2: Working Memory, Planning, Logical Reasoning, Adaptive Error Correction, Decision-making, Semantic Understanding & Context Recognition, Motor Skill Learning
L3: ",L2
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a difficult, broad benchmark spanning expert-level questions, including multimodal items, intended to probe frontier reasoning and knowledge at the edge of current models. It rewards careful synthesis and domain reasoning rather than short, pattern-matched answers.","L1: Language Comprehension, Language Production, Visual Perception
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Scene Understanding & Visual Reasoning, Attention
L3: ",L2
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 uses problems from a high-level mathematics competition that require multi-step derivations, algebraic manipulation, and careful case handling. It is typically evaluated as exact-answer generation, where partial progress does not count without the correct final result.","L1: 
L2: Logical Reasoning, Working Memory, Planning, Adaptive Error Correction, Attention
L3: ",L2
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a challenging subset of graduate-level science multiple-choice questions designed to be difficult to solve via superficial lookup, emphasizing reasoning and deep conceptual understanding. The Diamond split focuses on high-quality items where experts succeed and non-experts often fail.","L1: 
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Attention, Decision-making
L3: ",L2
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends broad academic knowledge testing across many subjects into multiple languages, measuring whether competence transfers beyond English. It probes both factual knowledge and reasoning while requiring robust understanding of prompts and answer options in diverse linguistic contexts.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Attention
L3: Cognitive Flexibility",L3
MMMU,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2311.16502,"MMMU is a multimodal benchmark spanning many disciplines where models must combine text with images such as diagrams, charts, and scientific figures to answer questions. It tests integrated reasoning over visual and textual evidence rather than isolated vision or language skills.","L1: Visual Perception, Language Comprehension, Language Production
L2: Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Working Memory, Visual Attention & Eye Movements
L3: ",L2
LAB-Bench FigQA,https://huggingface.co/datasets/futurehouse/lab-bench,https://arxiv.org/abs/2407.10362,"LAB-Bench FigQA evaluates whether models can correctly interpret complex scientific figures from biology papers and answer targeted questions about experimental results and visual encodings. It emphasizes careful visual parsing, mapping figure content to scientific meaning, and drawing correct inferences.","L1: Visual Perception, Language Comprehension, Language Production
L2: Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory
L3: ",L2
WebArena,https://webarena.dev/,https://arxiv.org/abs/2307.13854,"WebArena evaluates autonomous web agents completing realistic multi-step tasks across web applications (e-commerce, content management, code hosting, forums), requiring navigation, form filling, and state tracking. Performance depends on planning action sequences, handling UI variability, and recovering from partial failures.","L1: Visual Perception, Language Comprehension, Language Production
L2: Planning, Decision-making, Working Memory, Adaptive Error Correction, Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Sensorimotor Coordination
L3: ",L2
