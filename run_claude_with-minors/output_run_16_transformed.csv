Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates models on real GitHub issues where the agent must produce a code patch that makes a failing test suite pass. The “Verified” subset contains tasks confirmed by human annotators to be solvable with the provided repository state and test harness, and is typically scored as pass@1 based on whether the submitted patch passes the evaluation tests.","L1: Language Comprehension, Language Production
L2: Logical Reasoning, Planning, Working Memory, Adaptive Error Correction
L3: ",L2
SWE-bench Multilingual,https://www.swebench.com/multilingual.html,https://arxiv.org/abs/2504.21798,"SWE-bench Multilingual extends the SWE-bench setup beyond Python to a suite of software-engineering tasks spanning multiple programming languages. It measures whether an agent can interpret issue descriptions, localize bugs, and implement correct fixes under language- and ecosystem-specific constraints, usually judged by repository tests.","L1: Language Comprehension, Language Production
L2: Logical Reasoning, Planning, Working Memory, Adaptive Error Correction
L3: Cognitive Flexibility",L3
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro is a larger and more difficult software-engineering benchmark designed to better reflect industrial codebases and increase robustness to contamination. Tasks often require deeper repository understanding, longer dependency chains, and more complex patching than SWE-bench Verified, with scoring based on whether the generated fix passes tests in an evaluation harness.","L1: Language Comprehension, Language Production
L2: Logical Reasoning, Planning, Working Memory, Adaptive Error Correction
L3: Cognitive Flexibility",L3
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 evaluates agentic problem solving in real command-line environments where the model can run shell commands, inspect files, and iteratively debug. Success typically requires choosing appropriate tools/commands, interpreting outputs and errors, and converging to a correct end state under step and resource constraints.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Working Memory, Adaptive Error Correction
L3: ",L2
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp evaluates “deep research” style question answering that requires browsing or retrieving from a fixed document collection (in benchmarked variants) and synthesizing evidence into a final answer. It emphasizes multi-step information seeking, citation-like grounding, and integrating multiple sources rather than single-shot recall.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Episodic Memory, Working Memory, Semantic Understanding & Context Recognition, Logical Reasoning
L3: ",L2
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench measures an agent’s ability to complete multi-turn customer-support tasks by interacting with simulated users and programmatic APIs while following domain-specific policies. Tasks involve policy adherence, tool/API calling, and resolving user goals over extended dialogues, with scoring based on successful resolution and compliance.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Working Memory
L3: Inhibitory Control, Social Reasoning & Theory of Mind",L3
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld evaluates multimodal computer-use agents that must accomplish tasks in operating-system desktop environments using screenshots and UI interactions. Tasks require navigating applications, manipulating files/settings, and adapting to dynamic UI state changes, typically scored by task completion success within a step budget.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Planning, Decision-making, Working Memory, Adaptive Error Correction, Sensorimotor Coordination
L3: ",L2
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI evaluates few-shot abstract reasoning on grid-based transformation puzzles, where models infer latent rules from a small set of input–output examples and generate the correct output for a new input. It targets generalization to novel patterns and compositional rule induction, with accuracy measured by exact-match solutions.","L1: 
L2: Logical Reasoning, Working Memory, Spatial Representation & Mapping, Planning
L3: Cognitive Flexibility",L3
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon agent coherence and strategy in a simulated business setting: running a vending-machine operation over an extended period with many sequential decisions. Models must handle inventory, pricing, supplier negotiation, and changing conditions, and are typically scored by final financial outcome (e.g., ending balance).","L1: 
L2: Planning, Decision-making, Reward Mechanisms, Working Memory, Adaptive Error Correction
L3: Motivational Drives",L3
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas evaluates real-world tool use through the Model Context Protocol (MCP), testing whether models can discover tools, call them with correct schemas/arguments, and compose multi-step workflows. It stresses robustness to tool errors and the ability to synthesize tool outputs into correct user-facing results.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Working Memory, Adaptive Error Correction
L3: ",L2
FinanceAgent,https://www.vals.ai/benchmarks/finance_agent,,"FinanceAgent evaluates agent performance on tasks representative of entry-level financial analysis, such as retrieving/reading financial documents, performing calculations, and producing justified recommendations or reports. It emphasizes tool-augmented analysis, numerical reasoning, and structured communication under domain constraints.","L1: Language Comprehension, Language Production
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Planning, Decision-making, Working Memory
L3: ",L2
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity agents on tasks such as reproducing known vulnerabilities from descriptions and discovering new vulnerabilities in real open-source projects. Agents must reason about code behavior, threat models, and exploit conditions, and are scored by success on pass@1-style task completion over large suites.","L1: Language Comprehension, Language Production
L2: Logical Reasoning, Planning, Working Memory, Adaptive Error Correction
L3: Inhibitory Control",L3
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"SpreadsheetBench evaluates an agent’s ability to navigate, modify, and compute with complex spreadsheets, often using programmatic interfaces (e.g., Python libraries) and file operations. Tasks include cleaning data, editing formulas, building models, and producing correct outputs in the spreadsheet format, with scores based on correctness of resulting artifacts.","L1: Language Comprehension, Language Production
L2: Logical Reasoning, Planning, Decision-making, Working Memory, Adaptive Error Correction
L3: ",L2
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam (HLE) is a multimodal benchmark of very hard, expert-level questions spanning many fields, intended to probe frontier reasoning and knowledge under realistic constraints. It can be evaluated with or without tools (e.g., search or code execution), and scoring is typically accuracy on a large, diverse question set.","L1: Language Comprehension, Language Production
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Scene Understanding & Visual Reasoning, Working Memory, Planning
L3: ",L2
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 evaluates mathematical problem-solving on competition-style questions that require multi-step derivations and careful case handling. Models are typically scored by exact-answer accuracy across the exam set, with or without external tools depending on the evaluation configuration.","L1: 
L2: Logical Reasoning, Working Memory, Planning, Adaptive Error Correction
L3: ",L2
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a subset of the GPQA benchmark containing especially challenging, high-quality graduate-level science multiple-choice questions where non-experts tend to fail. It targets deep scientific understanding and reasoning rather than surface recall, and is scored by multiple-choice accuracy.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory
L3: ",L2
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends the MMLU-style broad academic evaluation into multiple non-English languages, testing cross-lingual knowledge and reasoning across many subjects. It probes whether models maintain capability and calibration across languages and scripts, typically scored by accuracy across languages/subjects.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory
L3: Cognitive Flexibility",L3
MMMU,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2311.16502,"MMMU evaluates expert-level multimodal understanding by requiring models to answer questions that combine text with images such as diagrams, charts, figures, and UI-like visuals across many disciplines. Success requires integrating visual evidence with domain knowledge and performing multi-step reasoning, scored primarily by accuracy.","L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Working Memory
L3: ",L2
LAB-Bench FigQA,https://huggingface.co/datasets/futurehouse/lab-bench,https://arxiv.org/abs/2407.10362,"LAB-Bench FigQA evaluates whether models can interpret complex scientific figures from biology papers and answer detailed questions about experimental results, plots, and figure annotations. It emphasizes precise visual extraction and grounded scientific reasoning, often benefiting from tool assistance like image cropping or analysis pipelines.","L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory
L3: ",L2
WebArena,https://webarena.dev/,https://arxiv.org/abs/2307.13854,"WebArena evaluates autonomous web agents performing realistic multi-step tasks across several interactive websites (e.g., shopping, forums, code hosting, maps). Agents must plan, navigate dynamic interfaces, fill forms, and recover from mistakes, with success graded by task-specific functional outcomes.","L1: Visual Perception, Language Comprehension, Language Production
L2: Planning, Decision-making, Visual Attention & Eye Movements, Working Memory, Adaptive Error Correction
L3: ",L2
