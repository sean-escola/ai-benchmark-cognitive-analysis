Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates an LLM’s ability to solve real software engineering issues by generating patches in real GitHub repositories, typically validated by running the project’s test suite. The “Verified” subset consists of problems confirmed by human annotators to be solvable and to have reliable evaluation signals, improving comparability across systems.","L1: Language Comprehension
L2: Planning, Decision-making, Logical Reasoning, Working Memory, Adaptive Error Correction
L3: ",L2
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 measures agent performance on real-world command-line tasks executed in sandboxed terminal environments (e.g., configuring tools, manipulating files, running programs, debugging). Success depends on choosing correct shell commands, interpreting outputs, and iterating until the objective is met.","L1: Language Comprehension
L2: Planning, Decision-making, Working Memory, Adaptive Error Correction, Semantic Understanding & Context Recognition
L3: ",L2
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp evaluates “deep research” style browsing: models must search a fixed corpus of web documents, identify relevant sources, and synthesize an answer that matches ground truth. It emphasizes multi-step information seeking under constraints (limited retrieval, noisy snippets) and rewards robust citation-like grounding behavior.","L1: Language Comprehension, Language Production
L2: Planning, Attention, Working Memory, Semantic Understanding & Context Recognition, Decision-making
L3: ",L2
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench evaluates interactive agent behavior in simulated customer support domains (e.g., retail, airline, telecom), combining natural language dialogue with API/tool actions. Agents must follow domain policies while completing multi-turn tasks, handling constraints, and resolving user issues without breaking rules.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Working Memory
L3: Social Reasoning & Theory of Mind, Inhibitory Control",L3
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld is a multimodal computer-use benchmark where agents control a desktop-like environment via screenshots and action primitives (mouse/keyboard) to complete realistic tasks across applications. It stresses grounding in visual UI state, long-horizon task completion, and recovery from mistakes under step limits.","L1: Visual Perception
L2: Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Sensorimotor Coordination, Planning, Decision-making, Working Memory, Adaptive Error Correction, Spatial Representation & Mapping
L3: ",L2
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI measures fluid, few-shot abstract reasoning using grid transformation puzzles: models infer latent rules from a handful of input–output examples and must produce the correct output for a new grid. It is designed to reduce reliance on memorized knowledge and emphasize rapid rule induction and generalization.","L1: 
L2: Logical Reasoning, Working Memory, Spatial Representation & Mapping, Attention
L3: Cognitive Flexibility",L3
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon autonomy in a business-management simulation: the agent runs a vending-machine business over many turns (e.g., procurement, pricing, inventory, negotiation) and is scored by final profit/balance. It probes sustained coherence, strategy under uncertainty, and iterative improvement over extended time.","L1: 
L2: Planning, Decision-making, Working Memory, Reward Mechanisms, Adaptive Error Correction
L3: Cognitive Timing & Predictive Modeling",L3
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas evaluates real-world tool use via the Model Context Protocol (MCP): models must discover appropriate tools, call them correctly across multiple steps, handle errors, and integrate results into a final response. It targets robust tool-selection and tool-argument grounding rather than purely text-only reasoning.","L1: Language Comprehension
L2: Planning, Decision-making, Working Memory, Adaptive Error Correction, Semantic Understanding & Context Recognition
L3: ",L2
FinanceAgent,https://www.vals.ai/benchmarks/finance_agent,,"FinanceAgent measures an agent’s ability to perform tasks resembling entry-level financial analysis, such as interpreting financial documents, extracting figures, performing calculations, and producing structured deliverables. Strong performance requires combining domain knowledge with multi-step quantitative reasoning and artifact-focused reporting.","L1: Language Production, Language Comprehension
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Planning, Decision-making
L3: ",L2
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity capabilities by testing whether agents can identify known vulnerabilities from descriptions and also discover previously unknown issues in real open-source projects. It emphasizes code comprehension, hypothesis-driven debugging, and careful iterative validation under a pass@1-style setting.","L1: 
L2: Logical Reasoning, Planning, Working Memory, Adaptive Error Correction, Attention, Semantic Understanding & Context Recognition
L3: ",L2
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"SpreadsheetBench evaluates the ability to understand, edit, and generate complex spreadsheets drawn from realistic workflows (e.g., cleaning data, building formulas, restructuring tables, and producing correct outputs). It tests procedural accuracy and error recovery when manipulating structured artifacts rather than free-form text.","L1: 
L2: Planning, Decision-making, Working Memory, Adaptive Error Correction, Logical Reasoning, Semantic Understanding & Context Recognition
L3: ",L2
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,Humanity’s Last Exam is a large multimodal benchmark intended to probe frontier-level knowledge and reasoning across diverse expert domains. Questions often require integrating text with visual inputs and performing multi-step inference rather than recalling isolated facts.,"L1: Language Comprehension, Visual Perception
L2: Scene Understanding & Visual Reasoning, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory
L3: ",L2
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 evaluates competition-style mathematics problem solving using the 2025 American Invitational Mathematics Examination questions. Problems typically require multi-step derivations, careful algebraic manipulation, and consistency checks under time-like constraints.","L1: 
L2: Logical Reasoning, Working Memory, Planning, Adaptive Error Correction, Attention
L3: ",L2
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a difficult graduate-level science multiple-choice benchmark designed to be resistant to simple web search and pattern matching. The “Diamond” subset focuses on high-quality questions that strongly separate experts from non-experts, requiring deep scientific reasoning and precise reading.","L1: Language Comprehension
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Attention
L3: ",L2
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends MMLU to multiple languages, testing broad academic knowledge and reasoning across many subjects in non-English settings. It stresses cross-lingual generalization and robust comprehension of domain-specific terminology and question styles.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Attention
L3: ",L2
MMMU,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2311.16502,"MMMU is a multimodal, multi-discipline benchmark where questions require jointly using images (diagrams, charts, figures) and text to answer expert-level prompts. It targets visual grounding plus domain reasoning across a broad set of professional and academic areas.","L1: Visual Perception, Language Comprehension
L2: Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Working Memory
L3: ",L2
LAB-Bench FigQA,https://huggingface.co/datasets/futurehouse/lab-bench,https://arxiv.org/abs/2407.10362,"LAB-Bench FigQA evaluates whether models can correctly interpret scientific figures from biology papers, including reading axes/legends, comparing conditions, and drawing conclusions supported by the figure. It focuses on figure-grounded reasoning rather than free-form biological recall.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Attention
L3: ",L2
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning tests chart/figure understanding and quantitative reasoning over figures drawn from arXiv papers, often requiring careful reading of plotted trends and experimental comparisons. Many questions are designed to penalize shallow caption-based guessing and reward figure-grounded inference.","L1: Visual Perception
L2: Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Logical Reasoning, Working Memory, Semantic Understanding & Context Recognition
L3: ",L2
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU extends multimodal understanding to video, requiring models to answer questions grounded in temporal sequences, actions, and state changes. It probes whether an agent can integrate information across frames and maintain coherent context over time.","L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Attention, Working Memory, Multisensory Integration
L3: Cognitive Timing & Predictive Modeling",L3
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"The FACTS Benchmark Suite evaluates factuality and grounding, measuring how reliably models produce statements supported by evidence and how often they hallucinate or confabulate details. It typically combines tasks that test citation-like attribution, conflict handling, and robustness to misleading contexts.","L1: Language Comprehension, Language Production
L2: Semantic Understanding & Context Recognition, Working Memory
L3: Inhibitory Control, Self-reflection",L3
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA evaluates commonsense physical reasoning across many languages using non-parallel variants, reducing the chance that translation artifacts dominate results. Items focus on selecting plausible actions or outcomes in everyday physical situations, testing grounded commonsense rather than specialist knowledge.","L1: Language Comprehension
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Spatial Representation & Mapping, Attention
L3: ",L2
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 is a long-context, multi-round co-reference/recall evaluation where multiple similar “needle” interactions are embedded within long “haystacks,” and the model must reproduce the correct response for a specified needle. The 8-needle setting increases interference and tests robust retrieval under high distractor density.","L1: Language Comprehension
L2: Working Memory, Attention, Episodic Memory, Semantic Understanding & Context Recognition
L3: ",L2
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval measures performance on well-specified professional knowledge-work tasks spanning many occupations, judged by experts via head-to-head comparisons against human outputs. Tasks often require producing structured artifacts (e.g., plans, presentations, spreadsheets) that satisfy constraints and stakeholder needs.","L1: Language Production
L2: Planning, Decision-making, Semantic Understanding & Context Recognition, Working Memory
L3: Social Reasoning & Theory of Mind, Self-reflection",L3
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks evaluates algorithmic reasoning over graph-structured data presented in text, such as following edges, computing paths, or answering node-relation queries at scale. It is commonly used to test systematic traversal behavior and long-context consistency when many nodes and edges are in play.","L1: 
L2: Spatial Representation & Mapping, Working Memory, Logical Reasoning, Attention, Planning
L3: ",L2
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon evaluates general tool-use competence across diverse tools and tasks, emphasizing correct tool selection, parameterization, multi-step orchestration, and robust handling of tool errors. It aims to measure agentic execution quality beyond single-call function invocation.","L1: 
L2: Planning, Decision-making, Working Memory, Adaptive Error Correction, Semantic Understanding & Context Recognition
L3: Inhibitory Control",L3
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath is an expert-level mathematics benchmark designed to be difficult and more resistant to memorization, with tiered problem difficulty and an emphasis on rigorous reasoning. High scores require multi-step derivations, precise symbolic manipulation, and strong verification habits (often aided by computation tools).","L1: 
L2: Logical Reasoning, Working Memory, Planning, Adaptive Error Correction, Attention
L3: Cognitive Flexibility",L3
