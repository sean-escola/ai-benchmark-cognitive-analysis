Benchmark,Website,Paper,Description,Cognitive Functions
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates software engineering agents on real GitHub issues by requiring them to generate code patches that make the project’s tests pass. The Verified subset focuses on tasks that have been human-validated as solvable, aiming to reduce noise from ambiguous or underspecified issues.","Language Comprehension, Logical Reasoning, Planning, Working Memory, Adaptive Error Correction, Decision-making"
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 measures an agent’s ability to complete real-world command-line tasks inside isolated environments, often requiring iterative diagnosis and execution of shell commands. It emphasizes end-to-end problem solving under tool constraints (files, processes, packages, permissions) rather than single-turn answers.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Attention, Language Comprehension"
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,BrowseComp evaluates “deep research” agents that must search a fixed document collection and synthesize answers that require multi-step information gathering. It is designed to be more reproducible than open-web browsing by controlling the retrieval corpus and grading.,"Planning, Decision-making, Attention, Working Memory, Semantic Understanding & Context Recognition, Language Production"
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench evaluates interactive tool-using agents in simulated customer-support domains (e.g., retail, airline, telecom) with policies and APIs. Success requires maintaining conversation state, following constraints, and choosing correct tool actions across multi-turn trajectories.","Language Comprehension, Language Production, Social Reasoning & Theory of Mind, Planning, Decision-making, Inhibitory Control, Working Memory"
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld evaluates multimodal computer-use agents that operate graphical desktop environments to complete tasks across applications and websites. Agents must perceive UI state from screenshots, plan sequences of interactions, and recover from mistakes over many steps.","Visual Perception, Visual Attention & Eye Movements, Sensorimotor Coordination, Planning, Decision-making, Adaptive Error Correction, Working Memory"
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,ARC-AGI is a few-shot abstract reasoning benchmark where models infer hidden transformation rules from a small number of input-output grid examples. It emphasizes generalization to novel patterns and compositional rule discovery rather than memorized knowledge.,"Logical Reasoning, Cognitive Flexibility, Working Memory, Spatial Representation & Mapping, Attention, Visual Perception"
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon autonomous agents managing a simulated vending machine business over an extended period. Agents must make repeated operational decisions (inventory, suppliers, pricing, messaging) and optimize a long-term objective under uncertainty.","Planning, Decision-making, Working Memory, Reward Mechanisms, Adaptive Error Correction, Motivational Drives"
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas evaluates real-world tool use via the Model Context Protocol, requiring models to discover, call, and chain tools across multi-step workflows. Tasks reward robust execution, correct parameterization, and resilient recovery from tool errors.","Planning, Decision-making, Working Memory, Adaptive Error Correction, Language Comprehension"
FinanceAgent,https://www.vals.ai/benchmarks/finance_agent,,"FinanceAgent measures performance on tasks representative of entry-level financial analyst work, such as analysis, modeling, and reporting using provided data and tools. It stresses correctness, structured outputs, and decision-relevant reasoning in business contexts.","Logical Reasoning, Planning, Decision-making, Semantic Understanding & Context Recognition, Working Memory, Language Production"
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity agents on tasks involving vulnerability discovery and exploitation in real-world software contexts, including both known and previously-unknown weaknesses. It requires reading technical artifacts, forming hypotheses, and iterating with tooling to validate findings.","Logical Reasoning, Planning, Adaptive Error Correction, Working Memory, Attention"
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"SpreadsheetBench evaluates agents on complex spreadsheet tasks grounded in realistic workflows, including editing, formula manipulation, and data transformation. It tests whether a model can maintain structured state, execute multi-step operations, and verify results.","Working Memory, Planning, Decision-making, Logical Reasoning, Adaptive Error Correction, Attention"
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam (HLE) is a frontier, multimodal benchmark intended to probe advanced academic reasoning and specialized knowledge across many domains. Problems often require combining textual understanding with rigorous reasoning and, in some settings, tool-assisted verification.","Language Comprehension, Logical Reasoning, Semantic Understanding & Context Recognition, Scene Understanding & Visual Reasoning, Working Memory"
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 consists of competition mathematics problems that require multi-step derivations under time-like constraints and minimal context. It is commonly used to stress symbolic manipulation, careful case analysis, and error-avoidant reasoning.","Logical Reasoning, Working Memory, Attention, Cognitive Flexibility"
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a subset of graduate-level science multiple-choice questions curated to be resistant to simple web lookup and superficial pattern matching. It tests deep domain understanding and the ability to reason through distractors in physics, chemistry, and biology.","Language Comprehension, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Attention"
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends broad academic knowledge testing to multiple non-English languages, probing both factual knowledge and reasoning across subjects. It highlights multilingual competence and robustness to linguistic variation in question framing.","Language Comprehension, Semantic Understanding & Context Recognition, Working Memory, Language Production"
MMMU,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2311.16502,"MMMU is a multidisciplinary multimodal benchmark where models answer questions that require interpreting images (e.g., diagrams, plots, figures) alongside text. It stresses cross-modal grounding and reasoning rather than purely textual recall.","Visual Perception, Multisensory Integration, Scene Understanding & Visual Reasoning, Language Comprehension, Logical Reasoning, Working Memory"
LAB-Bench FigQA,https://huggingface.co/datasets/futurehouse/lab-bench,https://arxiv.org/abs/2407.10362,"LAB-Bench FigQA evaluates whether models can correctly interpret complex scientific figures from biology papers and answer targeted questions about them. It rewards precise visual reading (labels, axes, experimental layouts) and reasoning that connects figure evidence to claims.","Visual Perception, Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Logical Reasoning, Language Comprehension, Working Memory"
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning tests reasoning over scientific visual content (e.g., charts/figures) paired with accompanying paper context, emphasizing faithful extraction and quantitative interpretation. It targets errors common in chart understanding such as misreading axes, legends, and trends.","Scene Understanding & Visual Reasoning, Visual Perception, Language Comprehension, Logical Reasoning, Working Memory, Attention"
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU evaluates multimodal understanding and reasoning over video content, requiring models to integrate information across frames and time. It tests whether the model can track events, infer causality, and answer questions grounded in temporally extended visual evidence.","Visual Perception, Visual Attention & Eye Movements, Cognitive Timing & Predictive Modeling, Working Memory, Multisensory Integration, Scene Understanding & Visual Reasoning, Language Comprehension"
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"The FACTS Benchmark Suite evaluates factuality and faithfulness of model outputs across multiple settings, including grounding to sources and resisting hallucination. It emphasizes whether models can maintain consistent claims, avoid unsupported assertions, and reflect uncertainty appropriately.","Language Comprehension, Semantic Understanding & Context Recognition, Inhibitory Control, Self-reflection, Working Memory"
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA evaluates physical commonsense reasoning across many languages using non-parallel (not direct translations) data, stressing cultural and linguistic robustness. Items typically require selecting plausible actions or outcomes in everyday physical situations.","Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning, Spatial Representation & Mapping"
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) is a long-context benchmark where multiple similar “needle” interactions are embedded within long “haystack” dialogues or documents, and the model must retrieve the correct target response. It probes whether models can maintain accurate cross-reference resolution under distractors and scale.","Working Memory, Attention, Episodic Memory, Semantic Understanding & Context Recognition, Language Comprehension"
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval evaluates well-specified professional knowledge work across many occupations by judging the quality of produced work artifacts (e.g., plans, analyses, presentations, schedules). It is designed to reflect economically valuable end-to-end performance, including adherence to constraints and stakeholder intent.","Planning, Decision-making, Language Production, Semantic Understanding & Context Recognition, Working Memory, Social Reasoning & Theory of Mind"
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks evaluates reasoning over graph-structured data, often requiring multi-step traversal or path-based queries under long-context conditions. It stresses maintaining intermediate state and applying consistent transition rules across many hops.","Logical Reasoning, Working Memory, Attention, Spatial Representation & Mapping, Cognitive Flexibility"
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon evaluates models as tool-using agents across diverse tasks that require selecting appropriate tools, calling them correctly, and composing results into final answers. It emphasizes reliability under tool errors, correct orchestration, and sustained multi-step execution.","Planning, Decision-making, Working Memory, Adaptive Error Correction, Language Comprehension"
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath is a benchmark of expert-level mathematics problems intended to be difficult for both models and many human solvers, often requiring deep multi-step proofs or computations. It targets robust mathematical reasoning under high complexity, including verification and avoidance of subtle logical errors.","Logical Reasoning, Working Memory, Attention, Cognitive Flexibility, Planning"
