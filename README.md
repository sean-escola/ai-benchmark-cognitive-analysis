# AI Benchmark Cognitive Function Analysis

This repository uses GPT-5.2 with high reasoning effort to analyze AI benchmarks and categorize them by cognitive functions and AI tiers, based on the cognitive neuroscience framework from Liu et al.

## Overview

The script analyzes common benchmarks used in evaluation of the latest generation of AI models (Gemini 3 Pro, Claude Opus 4.5, and GPT 5.2) and assigns cognitive functions to each benchmark. It then groups these cognitive functions by AI tier:
- **L1**: Basic perceptual and language processing
- **L2**: Executive functions, planning, reasoning, memory
- **L3**: Higher-order cognition (cognitive flexibility, theory of mind, self-reflection, etc.)

## Quick Start

```bash
# Install dependencies
pip install -r requirements.txt

# Set OpenAI API key
export OPENAI_API_KEY='your-api-key-here'

# Print the prompt to see what will be sent to GPT-5.2
python benchmark_analysis.py --keepers gemini --print-prompt

# Run analysis (25 parallel runs for statistical robustness)
python benchmark_analysis.py --keepers gemini --runs 25
```

## Usage

### Basic Usage

```bash
python benchmark_analysis.py --keepers {gemini|claude|gpt} [OPTIONS]
```

### Required Arguments

- `--keepers {gemini,claude,gpt}`: Select which benchmark set to analyze
  - `gemini`: 18 benchmarks from Gemini 3 Pro's keeper set
  - `claude`: 26 benchmarks from Claude Opus 4.5's keeper set
  - `gpt`: 23 benchmarks from GPT 5.2's keeper set

### Optional Arguments

- `--runs N`: Number of parallel runs (default: 1)
  - Use 25+ runs for statistical analysis
- `--exclude-minors`: Include is_minor flag for cognitive functions
  - Without flag (default): All cognitive functions treated equally
  - With flag: Distinguishes core vs. minimally-probed functions, excludes minor functions from tier calculation
- `--output-dir PATH`: Resume/extend existing run directory
- `--print-prompt`: Print the prompt and exit (useful for debugging)

### Examples

```bash
# Gemini benchmarks, no minor distinction, 25 runs
python benchmark_analysis.py --keepers gemini --runs 25

# Claude benchmarks with minor distinction
python benchmark_analysis.py --keepers claude --exclude-minors --runs 25

# Print GPT prompt without running
python benchmark_analysis.py --keepers gpt --print-prompt

# Resume/extend existing run
python benchmark_analysis.py --keepers gemini --runs 10 --output-dir run_gemini_with-minors
```

## Input Files

The repository includes all necessary input files:

- **`BENCHMARK INFO.csv`**: Master list of 37 benchmarks with website/paper links and keeper flags
  - Union of all benchmarks presented in Gemini 3 Pro, Claude Opus 4.5, and GPT 5.2 launch documents
  - Keeper flags were generated by asking each model separately to select a minimal subset with the same cognitive assessment coverage as the full set
  - Gemini selected 18 benchmarks, Claude selected 26, GPT selected 23
  - Note: The "keepers" for any given model are the subset selected by that model which is not the same subset used to assess that model's capabilities in its release documents
  - These selections are non-deterministic; repeating the process would likely yield different keeper lists
- **`Liu et al., Ch 1.pdf`**: Cognitive neuroscience framework reference
- **`Gemini 3 Pro - eval info.pdf`**: Gemini 3 Pro evaluation document
- **`Claude Opus 4.5 - eval info.pdf`**: Claude Opus 4.5 evaluation document
- **`GPT 5.2 - eval info.pdf`**: GPT 5.2 evaluation document

All PDFs are uploaded to OpenAI on first run and file IDs are cached in `uploaded_file_ids.json` for reuse.

## Output Structure

### Directory Naming

Output directories follow the pattern: `run_{keepers}_{minors-mode}`

Example: `run_gemini_with-minors/`

### Output Files

For each successful run N:
- `output_run_N_raw.csv`: Original analysis from GPT-5.2
  - Columns: Benchmark, Website, Paper, Description, Cognitive Functions
- `output_run_N_transformed.csv`: Grouped by AI tier with max tier
  - Cognitive Functions column shows: "L1: ...\nL2: ...\nL3: ..."
  - Max AI Tier column shows highest tier (L1/L2/L3)

After all runs:
- `tier_variability_summary.csv`: Statistics on tier assignment variability

### Example Output

```
================================================================================
STATISTICS
================================================================================

AI Tier Assignment Statistics:
Tier       Mean       Std Err
------------------------------
L1         0.00       0.00
L2         11.50      0.50
L3         6.50       0.50

================================================================================
L3 ASSIGNMENTS DETAIL
================================================================================

ARC-AGI:
  Run 1: Cognitive Flexibility
  Run 2: Cognitive Flexibility

FACTS Benchmark Suite:
  Run 1: Inhibitory Control, Self-reflection
  Run 2: Inhibitory Control, Self-reflection
```

## Pre-Generated Results

The repository includes six pre-generated result directories for reference:

- `run_gemini_with-minors/`: Gemini benchmarks, all functions counted
- `run_gemini_no-minors/`: Gemini benchmarks, minor functions excluded from tier
- `run_claude_with-minors/`: Claude benchmarks, all functions counted
- `run_claude_no-minors/`: Claude benchmarks, minor functions excluded from tier
- `run_gpt_with-minors/`: GPT benchmarks, all functions counted
- `run_gpt_no-minors/`: GPT benchmarks, minor functions excluded from tier

These can be used directly without regenerating (which requires OpenAI API access and costs).

## How It Works

1. **Benchmark Selection**: Loads benchmarks from `BENCHMARK INFO.csv` based on `--keepers` flag
2. **PDF Upload**: Uploads PDFs to OpenAI (cached after first upload)
3. **GPT-5.2 Call**: Sends prompt with benchmarks + PDFs, uses `reasoning={effort: "high"}`
4. **Structured Output**: Uses JSON schema to enforce valid responses
5. **Validation**: Ensures benchmark order and cognitive function validity
6. **Transformation**: Groups cognitive functions by AI tier (L1, L2, L3)
7. **Tier Assignment**: Determines max AI tier for each benchmark
8. **Statistics**: Aggregates results across multiple runs

## Cognitive Functions

The script uses 34 cognitive functions organized by tier:

**L1 (Basic Processing)**: Visual Perception, Language Comprehension, Language Production, Face Recognition, Auditory Processing, Reflexive Responses

**L2 (Executive Functions)**: Planning, Logical Reasoning, Decision-making, Working Memory, Reward Mechanisms, Multisensory Integration, Spatial Representation & Mapping, Attention, Sensorimotor Coordination, Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Episodic Memory, Semantic Understanding & Context Recognition, Adaptive Error Correction, Motor Skill Learning, Motor Coordination

**L3 (Higher-Order)**: Cognitive Flexibility, Inhibitory Control, Social Reasoning & Theory of Mind, Empathy, Emotional Processing, Self-reflection, Tactile Perception, Lifelong Learning, Cognitive Timing & Predictive Modeling, Autonomic Regulation, Arousal & Attention States, Motivational Drives

## Technical Details

- **Async Execution**: Multiple runs execute in parallel for efficiency
- **Retry Logic**: Up to 3 retries per run if validation fails
- **File Caching**: PDF file IDs are cached to avoid re-uploading
- **JSON Schema**: Enforces exact benchmark names and cognitive function values

## API Costs

Each run uploads 4 PDFs (~20MB total) and uses GPT-5.2 with high reasoning effort. Costs can be significant for multiple runs. Start with `--runs 1` to test.

## License

MIT
