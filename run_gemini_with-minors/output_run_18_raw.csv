Benchmark,Website,Paper,Description,Cognitive Functions
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates software engineering ability by giving a model a real GitHub repository plus an issue description, then scoring whether the model produces a patch that makes the repository’s tests pass. The “Verified” subset uses human-validated tasks intended to be solvable and to reduce noise from ambiguous or broken tasks.","Planning, Logical Reasoning, Adaptive Error Correction, Working Memory, Language Comprehension"
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 measures agentic competence in real command-line environments, where models must use shell commands, inspect files, install/run tools, and iteratively troubleshoot to complete tasks. It emphasizes end-to-end execution under realistic system constraints rather than single-shot question answering.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Attention"
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench evaluates tool-using customer-support agents in multi-turn simulations (e.g., retail, airline, telecom) that require calling APIs, following domain policies, and maintaining coherent dialogue with a simulated user. Scoring rewards both task completion and policy-consistent behavior across long interactions.","Language Comprehension, Language Production, Planning, Decision-making, Inhibitory Control, Social Reasoning & Theory of Mind, Working Memory"
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,ARC-AGI tests “fluid” abstract reasoning using grid-based input–output examples where a model must infer the hidden transformation rule from only a few demonstrations. Success requires generalizing to novel patterns and compositions rather than recalling domain facts.,"Logical Reasoning, Cognitive Flexibility, Working Memory, Spatial Representation & Mapping, Scene Understanding & Visual Reasoning, Visual Perception"
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon autonomous agency by having a model run a simulated vending-machine business over an extended time period, making thousands of decisions about inventory, pricing, supplier negotiation, and cash management. Performance is typically based on final business outcomes (e.g., ending balance), stressing sustained coherence and strategy.","Planning, Decision-making, Reward Mechanisms, Working Memory, Self-reflection"
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam (HLE) is a difficult, broad benchmark intended to probe frontier-level knowledge and reasoning across many domains, with many questions designed to be challenging for both models and non-experts. Variants may be run with or without tools (e.g., search/code), and grading is typically strict due to the long-tail of specialized topics.","Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Visual Perception, Scene Understanding & Visual Reasoning"
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 evaluates competition-style mathematics using problems from the American Invitational Mathematics Examination, emphasizing multi-step derivations and careful symbolic manipulation. Because answers are typically short (often a single integer), the benchmark heavily rewards correct intermediate reasoning under tight constraints.","Logical Reasoning, Working Memory, Planning, Adaptive Error Correction"
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a high-difficulty multiple-choice science benchmark (physics, chemistry, biology) designed to be “Google-proof,” focusing on questions that require expert-level understanding rather than superficial recall. The Diamond subset is curated for quality and difficulty, making it a common stress test for frontier reasoning in scientific domains.","Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Inhibitory Control"
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends the MMLU-style academic knowledge evaluation to multiple languages, probing whether models can maintain subject-matter competence and reasoning across diverse linguistic contexts. It spans many subjects and tests both understanding of the prompt language and robust cross-lingual generalization.","Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Cognitive Flexibility"
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro is a professional-grade multimodal benchmark that tests expert-level understanding and reasoning over text-plus-image questions across many disciplines (e.g., science, engineering, medicine). It emphasizes complex visual interpretation and grounded reasoning rather than simple recognition.","Multisensory Integration, Visual Perception, Scene Understanding & Visual Reasoning, Logical Reasoning, Working Memory, Attention"
MathArena Apex,https://matharena.ai/?view=problem&comp=apex--apex_2025,https://arxiv.org/abs/2505.23281,"MathArena Apex is a challenging mathematics benchmark drawn from harder, research-leaning or Olympiad-adjacent problems and is often used to compare advanced reasoning among frontier models. It stresses long-chain deduction, algebraic/analytic technique selection, and error-prone multi-step computation.","Logical Reasoning, Planning, Working Memory, Adaptive Error Correction"
ScreenShot-Pro,https://gui-agent.github.io/grounding-leaderboard/,https://arxiv.org/abs/2504.07981,"ScreenShot-Pro evaluates GUI understanding from high-resolution screenshots where a model must ground language instructions to on-screen elements (e.g., buttons, fields, menus) and choose the correct interaction. It targets practical “computer use” competence, requiring accurate visual localization and stepwise action selection.","Visual Perception, Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Sensorimotor Coordination, Planning, Decision-making, Working Memory"
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning tests reasoning over figures, charts, and visual elements commonly found in scientific papers, requiring models to extract quantitative/structural information and answer questions that depend on correct interpretation. It is designed to go beyond OCR by emphasizing analytical reasoning grounded in scientific visuals.","Visual Perception, Scene Understanding & Visual Reasoning, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Attention"
OmniDocBench 1.5,https://github.com/opendatalab/OmniDocBench,https://arxiv.org/abs/2412.07626,"OmniDocBench 1.5 evaluates document understanding and OCR-style extraction across heterogeneous document components such as text blocks, tables, formulas, and reading order. It stresses robust parsing of layout and precise transcription/structuring rather than free-form summarization.","Visual Perception, Attention, Language Comprehension, Working Memory, Semantic Understanding & Context Recognition, Scene Understanding & Visual Reasoning"
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,Video-MMMU extends multimodal understanding to the temporal domain by asking questions about video content that require integrating information across frames and scenes. Tasks often require tracking events over time and combining visual evidence with textual reasoning.,"Visual Perception, Attention, Working Memory, Scene Understanding & Visual Reasoning, Multisensory Integration, Cognitive Timing & Predictive Modeling"
LiveCodeBench Pro,https://livecodebenchpro.com/projects/livecodebench-pro/leaderboard,https://arxiv.org/abs/2506.11928,"LiveCodeBench Pro evaluates coding ability on problems designed to reflect realistic programming work, often emphasizing correctness under execution-based testing and contemporary task distributions. Compared to static coding sets, it aims to reduce leakage and better reflect current software development demands.","Language Comprehension, Planning, Logical Reasoning, Adaptive Error Correction, Working Memory, Decision-making"
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"The FACTS Benchmark Suite systematically evaluates factuality by measuring whether a model’s statements are supported, accurate, and appropriately qualified across a variety of settings (including attribution/grounding-style checks). It targets reliability failures such as hallucination, overconfident claims, and unsupported generalizations.","Language Comprehension, Semantic Understanding & Context Recognition, Inhibitory Control, Self-reflection, Working Memory, Logical Reasoning"
SimpleQA Verified,https://www.kaggle.com/benchmarks/deepmind/simpleqa-verified,https://arxiv.org/abs/2509.07968,"SimpleQA Verified is a short-form factual question answering benchmark with verification-oriented curation, aiming to measure whether models can answer precisely and avoid fabricating unsupported facts. It is commonly used as a lightweight indicator of factual reliability under direct questioning.","Language Comprehension, Semantic Understanding & Context Recognition, Inhibitory Control, Working Memory"
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA evaluates physical commonsense and practical reasoning across many languages and cultural contexts, focusing on whether a model can pick plausible actions/outcomes in everyday situations. It probes robustness of commonsense inference beyond English-centric phrasing and datasets.","Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning, Cognitive Flexibility, Spatial Representation & Mapping"
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) is a long-context evaluation where multiple similar “needle” items are inserted into a long “haystack,” and the model must retrieve and reproduce the correct associated content for a specified needle. The 8-needle setting increases interference and tests whether the model can maintain accurate references over very long contexts.","Working Memory, Attention, Episodic Memory, Language Comprehension, Inhibitory Control"
