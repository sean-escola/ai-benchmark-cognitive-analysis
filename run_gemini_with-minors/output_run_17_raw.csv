Benchmark,Website,Paper,Description,Cognitive Functions
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro evaluates LLM software-engineering agents on a large set of real GitHub issues that require producing correct patches in real repositories under test. Tasks typically involve understanding codebases, localizing bugs or implementing features, editing multiple files, and passing project-specific test suites, emphasizing robustness and contamination resistance relative to earlier SWE-bench variants.","Language Comprehension, Logical Reasoning, Planning, Working Memory, Adaptive Error Correction, Decision-making"
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld measures an agent’s ability to use a computer operating system to complete end-to-end tasks (e.g., file management, web actions, app interactions) from pixel-based observations. Success requires interpreting GUI state, choosing sequences of actions, and recovering from mistakes over multi-step episodes.","Visual Perception, Scene Understanding & Visual Reasoning, Attention, Planning, Decision-making, Sensorimotor Coordination, Working Memory, Adaptive Error Correction"
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI tests few-shot abstract reasoning with grid-based puzzles where an agent must infer a hidden rule from a small number of input–output examples and generalize to a new input. It is designed to emphasize novelty and rule induction rather than memorized knowledge, with performance sensitive to flexible pattern discovery.","Logical Reasoning, Cognitive Flexibility, Working Memory, Visual Perception, Scene Understanding & Visual Reasoning, Spatial Representation & Mapping, Attention"
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon agent coherence by simulating operation of a vending-machine business over an extended period (e.g., a year) with many sequential decisions. Agents must manage inventory, pricing, supplier negotiation, and cash flow, requiring strategic adaptation to changing conditions and compounding outcomes.","Planning, Decision-making, Working Memory, Reward Mechanisms, Language Comprehension, Language Production, Adaptive Error Correction, Self-reflection"
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas evaluates real-world tool use via the Model Context Protocol, requiring models to discover tools, call them correctly, and compose multi-step workflows across services. Tasks stress API understanding, argument construction, error handling, and synthesis of tool outputs into correct final responses.","Language Comprehension, Planning, Decision-making, Working Memory, Adaptive Error Correction, Logical Reasoning, Language Production"
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym measures agentic cybersecurity capability at scale, including identifying known vulnerabilities from descriptions and discovering new issues in real open-source projects. The benchmark emphasizes precise code comprehension, vulnerability reasoning, and producing actionable findings under realistic constraints.","Language Comprehension, Logical Reasoning, Planning, Working Memory, Adaptive Error Correction, Decision-making"
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam (HLE) is a frontier benchmark of difficult questions spanning expert knowledge and reasoning, including multimodal items. It targets complex problem solving under uncertainty and often benefits from tool use (e.g., search or code execution) when permitted, mirroring demanding real-world inquiry.","Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Visual Perception, Scene Understanding & Visual Reasoning, Planning"
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a high-quality subset of graduate-level, “Google-proof” multiple-choice science questions where shallow pattern matching performs poorly. It stresses deep scientific reasoning, careful reading, and selecting the best supported answer among plausible distractors.","Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Inhibitory Control"
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro is a multimodal benchmark focused on expert-level understanding and reasoning across many disciplines using text and images. Compared with earlier MMMU settings, it places greater emphasis on robust visual–text grounding, diagram/chart interpretation, and harder reasoning questions.","Visual Perception, Scene Understanding & Visual Reasoning, Multisensory Integration, Language Comprehension, Logical Reasoning, Working Memory, Attention"
OmniDocBench 1.5,https://github.com/opendatalab/OmniDocBench,https://arxiv.org/abs/2412.07626,"OmniDocBench 1.5 evaluates document understanding and OCR across diverse layouts and content types (text, tables, formulas, reading order). It emphasizes faithful extraction and structural reconstruction from visually complex documents, reflecting real-world document digitization and analysis workloads.","Visual Perception, Visual Attention & Eye Movements, Attention, Language Comprehension, Working Memory, Scene Understanding & Visual Reasoning"
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU extends multimodal understanding to video, requiring temporal grounding and reasoning over events, actions, and visual evidence across frames. It tests whether models can integrate information across time rather than relying on single-frame cues.","Visual Perception, Scene Understanding & Visual Reasoning, Multisensory Integration, Working Memory, Attention, Cognitive Timing & Predictive Modeling"
LiveCodeBench Pro,https://livecodebenchpro.com/projects/livecodebench-pro/leaderboard,https://arxiv.org/abs/2506.11928,"LiveCodeBench Pro evaluates coding ability on competitive-programming-style tasks with an emphasis on recency and leakage resistance, typically requiring executable solutions rather than explanations. It probes algorithm design, implementation correctness, and iterative debugging under time/efficiency constraints.","Logical Reasoning, Planning, Working Memory, Adaptive Error Correction, Decision-making, Language Comprehension"
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"FACTS Benchmark Suite measures factuality and grounding behavior across a broad set of tasks designed to stress truthfulness, citation/attribution practices, and resistance to confabulation. It focuses on whether model outputs remain consistent with provided sources or established facts, especially under adversarial or ambiguous prompts.","Semantic Understanding & Context Recognition, Logical Reasoning, Inhibitory Control, Self-reflection, Working Memory, Language Comprehension"
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA evaluates physical commonsense reasoning across many languages and cultural contexts, using non-parallel multilingual items to reduce translation artifacts. It tests whether models can choose plausible actions or explanations grounded in everyday physics and affordances beyond English-centric priors.","Language Comprehension, Logical Reasoning, Semantic Understanding & Context Recognition, Cognitive Flexibility, Working Memory"
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) evaluates long-context multi-round coreference resolution by embedding multiple similar “needle” interactions in long “haystacks” and asking the model to reproduce the correct referenced response. The 8-needle setting stresses sustained context tracking, interference resistance, and precise retrieval under heavy distractors.","Working Memory, Attention, Language Comprehension, Semantic Understanding & Context Recognition, Inhibitory Control, Episodic Memory"
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval evaluates economically meaningful, well-specified professional knowledge-work tasks across many occupations, often requiring creation of polished artifacts (e.g., spreadsheets, presentations, plans). Scoring is based on human-judge comparisons against professional work, emphasizing end-to-end usefulness rather than narrow Q&A accuracy.","Planning, Decision-making, Language Production, Language Comprehension, Working Memory, Self-reflection, Social Reasoning & Theory of Mind"
SWE-Lancer,https://openai.com/index/swe-lancer/,https://arxiv.org/abs/2502.12115,"SWE-Lancer evaluates software-engineering agents on longer-horizon and more autonomous development workflows than typical patch-only benchmarks. It emphasizes maintaining task context across steps, making and validating design choices, and iteratively improving solutions in realistic repo settings.","Planning, Decision-making, Working Memory, Adaptive Error Correction, Logical Reasoning, Language Comprehension, Language Production"
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath evaluates advanced mathematics problem solving at the research frontier, including problems intended to be difficult for current models even with tools. It emphasizes multi-step derivations, rigorous reasoning, and error-sensitive symbolic manipulation over superficial pattern matching.","Logical Reasoning, Working Memory, Planning, Adaptive Error Correction, Attention"
