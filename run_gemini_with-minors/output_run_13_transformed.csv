Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates agentic software engineering by giving a real GitHub issue plus repository context and requiring the model to produce a patch that passes the project’s tests. The Verified subset uses human validation to ensure tasks are solvable and that grading (via unit tests) is reliable, emphasizing end-to-end debugging and code changes rather than isolated coding snippets.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Logical Reasoning, Working Memory, Adaptive Error Correction
L3: ",L2
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 measures an agent’s ability to accomplish practical tasks in a command-line environment using tools like shells, package managers, and interpreters under resource constraints. Success depends on executing multi-step workflows, interpreting tool feedback (errors/logs), and iterating toward a working solution.","L1: Language Comprehension
L2: Planning, Decision-making, Working Memory, Adaptive Error Correction
L3: Inhibitory Control",L3
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench evaluates tool-using customer-support agents in simulated domains (e.g., retail, airline, telecom) where the model must converse with a user, call APIs/tools, and follow domain policies. It stresses reliable multi-turn state tracking and policy adherence while still being helpful in complex edge cases.","L1: Language Comprehension, Language Production
L2: Decision-making, Planning, Working Memory
L3: Social Reasoning & Theory of Mind, Inhibitory Control",L3
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI tests “fluid” pattern induction from a few examples: models infer latent rules mapping input grids to output grids and must generalize to a new grid. The tasks are designed to reduce reliance on memorized knowledge, emphasizing compositional abstraction and novel problem solving.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Spatial Representation & Mapping, Logical Reasoning, Working Memory
L3: Cognitive Flexibility",L3
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon autonomous agency by simulating management of a vending-machine business over an extended timeline with many sequential decisions. Models must plan, manage inventory and pricing, interact with simulated counterparts (e.g., suppliers), and adapt strategies as conditions change.","L1: 
L2: Planning, Decision-making, Working Memory, Reward Mechanisms, Semantic Understanding & Context Recognition
L3: Self-reflection",L3
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a large, frontier-style benchmark spanning difficult questions across many disciplines, often requiring multi-step reasoning and, in some settings, tool use such as search or code execution. It aims to probe the limits of broad academic knowledge and general problem solving, including multimodal items.","L1: Language Comprehension, Language Production, Visual Perception
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Multisensory Integration
L3: ",L2
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 consists of competition mathematics problems that require constructing multi-step solutions under tight constraints, often involving algebra, number theory, combinatorics, or geometry. Performance reflects the ability to maintain intermediate symbolic states and avoid subtle logical slips.","L1: 
L2: Logical Reasoning, Working Memory, Planning
L3: Inhibitory Control",L3
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a difficult multiple-choice benchmark of graduate-level science questions intended to be “Google-proof,” emphasizing reasoning over superficial retrieval. The Diamond subset is curated for high quality and discriminates strongly between expert and non-expert performance.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory
L3: Inhibitory Control",L3
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends broad academic testing to multiple languages, measuring whether models can answer subject-area questions accurately across diverse linguistic settings. It probes both knowledge robustness and the ability to transfer reasoning across languages without losing nuance.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory
L3: Cognitive Flexibility",L3
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro is a multimodal, multi-discipline benchmark that requires combining visual inputs (e.g., diagrams, charts, scenes) with textual prompts to answer expert-level questions. It emphasizes fine-grained visual understanding and cross-modal reasoning rather than text-only knowledge.","L1: Visual Perception, Language Comprehension
L2: Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Working Memory
L3: ",L2
MathArena Apex,https://matharena.ai/?view=problem&comp=apex--apex_2025,https://arxiv.org/abs/2505.23281,"MathArena Apex aggregates challenging mathematics tasks designed to stress advanced reasoning, often with competition-style difficulty and rigorous automatic evaluation. It is used to compare top models’ mathematical problem solving and, in some settings, the benefit of tool assistance such as Python.","L1: 
L2: Logical Reasoning, Working Memory, Planning, Adaptive Error Correction
L3: ",L2
ScreenShot-Pro,https://gui-agent.github.io/grounding-leaderboard/,https://arxiv.org/abs/2504.07981,"ScreenShot-Pro evaluates whether models can understand and ground instructions in high-resolution software screenshots, such as identifying interface elements and answering questions about GUI state. It targets precise visual localization and layout reasoning needed for computer-use agents.","L1: Visual Perception
L2: Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Spatial Representation & Mapping, Decision-making
L3: ",L2
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning tests multimodal scientific reasoning over figures and surrounding context from research papers, requiring models to interpret charts/plots and draw correct conclusions. Many items benefit from quantitative analysis and careful mapping between visual encodings and textual claims.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory
L3: ",L2
OmniDocBench 1.5,https://github.com/opendatalab/OmniDocBench,https://arxiv.org/abs/2412.07626,"OmniDocBench 1.5 evaluates document understanding and OCR across diverse layouts, including text, tables, formulas, and reading order. It measures robustness to complex formatting where correct extraction depends on spatial structure as well as language decoding.","L1: Visual Perception, Language Comprehension
L2: Visual Attention & Eye Movements, Spatial Representation & Mapping, Semantic Understanding & Context Recognition
L3: ",L2
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU extends multimodal understanding to video, requiring models to integrate information across frames and time to answer questions. It stresses temporal reasoning, event understanding, and maintaining cross-frame context under limited observation budgets.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Attention, Working Memory, Multisensory Integration
L3: Cognitive Timing & Predictive Modeling",L3
LiveCodeBench Pro,https://livecodebenchpro.com/projects/livecodebench-pro/leaderboard,https://arxiv.org/abs/2506.11928,"LiveCodeBench Pro evaluates coding ability on regularly updated problems with rigorous execution-based scoring, designed to reduce contamination and better reflect contemporary programming skill. Tasks often require generating correct, efficient code and iterating on failures under realistic constraints.","L1: Language Comprehension, Language Production
L2: Planning, Logical Reasoning, Working Memory, Adaptive Error Correction
L3: ",L2
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"FACTS Benchmark Suite systematically evaluates factuality by testing whether model outputs remain faithful to available evidence and avoid unsupported statements across varied scenarios. It emphasizes calibration and error avoidance, not just producing fluent answers.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Working Memory
L3: Inhibitory Control, Self-reflection",L3
SimpleQA Verified,https://www.kaggle.com/benchmarks/deepmind/simpleqa-verified,https://arxiv.org/abs/2509.07968,SimpleQA Verified measures factual question answering with verified ground truth and evaluation protocols aimed at reducing ambiguous grading. It focuses on precise retrieval-like correctness and discourages plausible-sounding hallucinations.,"L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Working Memory
L3: Inhibitory Control",L3
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA evaluates physical commonsense and everyday causal reasoning across multiple languages, targeting whether models can choose plausible actions or explanations grounded in the physical world. It stresses robustness to linguistic variation while preserving commonsense inference.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Spatial Representation & Mapping
L3: Cognitive Flexibility",L3
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,MRCR v2 (8-needle) evaluates long-context retrieval and multi-round coreference by embedding multiple similar “needle” interactions within long “haystack” contexts and asking the model to reproduce the correct referenced response. The 8-needle setting increases distractors and requires precise attention control over very long inputs.,"L1: 
L2: Working Memory, Attention, Episodic Memory, Semantic Understanding & Context Recognition
L3: Inhibitory Control",L3
