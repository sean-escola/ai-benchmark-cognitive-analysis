Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,SWE-bench Verified evaluates agentic software engineering by giving a real GitHub issue and repository snapshot and requiring the model to produce a patch that passes tests. The “Verified” subset contains tasks validated by humans as solvable and aims to measure end-to-end bug fixing and feature implementation under realistic repo constraints.,"L1: Language Comprehension, Language Production
L2: Planning, Logical Reasoning, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition
L3: ",L2
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 measures an agent’s ability to complete real command-line tasks in sandboxed environments (e.g., debugging, data wrangling, package/tool usage) by issuing shell commands and interpreting outputs. It stresses multi-step execution, iterative troubleshooting, and reliable state tracking across long tool-interaction trajectories.","L1: 
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory, Attention
L3: ",L2
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench evaluates interactive tool-using agents in simulated customer-support domains (e.g., retail, airline, telecom) where the model must follow policies while calling APIs and conversing with a user. Success depends on maintaining dialogue state, choosing compliant actions, and handling multi-turn exceptions (returns, rebookings, troubleshooting).","L1: Language Comprehension, Language Production
L2: Decision-making, Planning, Working Memory
L3: Social Reasoning & Theory of Mind, Inhibitory Control, Empathy",L3
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI is a fluid-reasoning benchmark of small grid-based puzzles where a model must infer the underlying transformation rule from only a few examples and apply it to a new input. It emphasizes out-of-distribution pattern induction, compositional reasoning, and robustness to novel task structures.","L1: Visual Perception
L2: Logical Reasoning, Spatial Representation & Mapping, Working Memory, Scene Understanding & Visual Reasoning, Attention
L3: Cognitive Flexibility",L3
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon agent coherence by simulating a year of running a vending-machine business, including inventory management, pricing, supplier negotiation, and reacting to market changes. The score is typically based on final financial outcomes, rewarding sustained strategy and consistent execution over many steps.","L1: 
L2: Planning, Decision-making, Reward Mechanisms, Working Memory, Episodic Memory
L3: Self-reflection",L3
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a large, frontier-focused benchmark spanning difficult questions across domains (often including multimodal items) intended to probe advanced reasoning at the edge of current model capability. It aims to measure whether models can synthesize knowledge, perform multi-step inference, and avoid shallow pattern-matching on challenging prompts.","L1: Language Comprehension, Visual Perception
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Scene Understanding & Visual Reasoning, Multisensory Integration
L3: ",L2
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 evaluates competition-style mathematics problem solving on the American Invitational Mathematics Examination questions. Items require multi-step derivations, precise symbolic manipulation, and careful constraint tracking to produce exact numerical answers.","L1: 
L2: Logical Reasoning, Working Memory, Attention
L3: ",L2
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a high-difficulty, multiple-choice science benchmark designed to be “Google-proof,” with questions requiring expert-level reasoning rather than direct recall. The Diamond subset targets especially high-quality items where experts agree on the correct answer and non-experts typically fail.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory
L3: ",L2
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends broad academic knowledge and reasoning evaluation into multiple languages, testing whether a model can answer subject-matter questions consistently across linguistic contexts. It probes cross-lingual generalization, domain knowledge access, and reasoning under multilingual prompts.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory
L3: ",L2
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro is a multimodal, expert-oriented benchmark requiring joint reasoning over images (e.g., diagrams, charts, documents) and text across many disciplines. Compared to standard MMMU settings, it is designed to be more challenging and to better differentiate strong multimodal reasoning and perception-grounded inference.","L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Working Memory, Visual Attention & Eye Movements
L3: ",L2
MathArena Apex,https://matharena.ai/?view=problem&comp=apex--apex_2025,https://arxiv.org/abs/2505.23281,"MathArena Apex aggregates difficult math problems intended to stress high-end mathematical reasoning, including multi-step proofs or derivations and careful case analysis. It is often used to compare frontier models under standardized prompting and (optionally) tool settings.","L1: 
L2: Logical Reasoning, Working Memory, Attention
L3: ",L2
ScreenShot-Pro,https://gui-agent.github.io/grounding-leaderboard/,https://arxiv.org/abs/2504.07981,"ScreenShot-Pro evaluates GUI understanding and grounding from screenshots, where the model must locate relevant interface elements and answer questions or take actions based on visual layout. It emphasizes spatially grounded interpretation of UI structure, icons, menus, and text rendered in images.","L1: Visual Perception
L2: Visual Attention & Eye Movements, Spatial Representation & Mapping, Scene Understanding & Visual Reasoning, Sensorimotor Coordination, Planning
L3: ",L2
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning tests understanding of scientific figures and charts (often from research papers) by asking questions that require extracting plotted values, interpreting axes/legends, and reasoning about visual evidence. It targets robust chart/figure comprehension beyond OCR, including quantitative and relational inference.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Visual Attention & Eye Movements
L3: ",L2
OmniDocBench 1.5,https://github.com/opendatalab/OmniDocBench,https://arxiv.org/abs/2412.07626,"OmniDocBench 1.5 evaluates document AI on complex pages containing mixed content such as text blocks, tables, formulas, and reading order. It measures whether models can accurately extract and structure information from documents with challenging layouts and formatting.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Semantic Understanding & Context Recognition, Working Memory
L3: ",L2
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU evaluates multimodal understanding over videos, requiring models to integrate information across time to answer questions about events, actions, and causal relationships. It probes temporal coherence and the ability to maintain and use context from multiple frames (and associated text) to reason correctly.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Working Memory, Attention, Multisensory Integration
L3: Cognitive Timing & Predictive Modeling",L3
LiveCodeBench Pro,https://livecodebenchpro.com/projects/livecodebench-pro/leaderboard,https://arxiv.org/abs/2506.11928,"LiveCodeBench Pro evaluates coding ability on contemporary programming tasks, typically emphasizing practical implementation, debugging, and reasoning under realistic constraints. It aims to reduce contamination effects by using time-based splits and focuses on code that must compile/run against tests rather than just producing plausible snippets.","L1: Language Comprehension, Language Production
L2: Logical Reasoning, Planning, Adaptive Error Correction, Working Memory
L3: ",L2
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"FACTS Benchmark Suite systematically evaluates factuality and groundedness of model outputs across multiple sub-benchmarks, targeting both knowledge accuracy and faithfulness to provided sources when applicable. It is designed to expose hallucinations, unsupported claims, and brittle retrieval-grounding behavior.","L1: Language Comprehension, Language Production
L2: Semantic Understanding & Context Recognition, Adaptive Error Correction
L3: Inhibitory Control, Self-reflection",L3
SimpleQA Verified,https://www.kaggle.com/benchmarks/deepmind/simpleqa-verified,https://arxiv.org/abs/2509.07968,"SimpleQA Verified is a short-form question answering benchmark focused on verifiable factual correctness, typically with careful curation and validation of ground truth. It is used to measure whether models can answer succinctly without introducing fabricated details or confident errors.","L1: Language Comprehension, Language Production
L2: Semantic Understanding & Context Recognition
L3: Inhibitory Control",L3
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA evaluates physical commonsense and practical reasoning across a broad set of languages and cultural contexts, aiming to test whether models generalize “what would happen” intuition beyond English. Items typically require choosing the more plausible outcome/action in everyday physical scenarios.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Spatial Representation & Mapping
L3: ",L2
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) is a long-context evaluation where multiple similar “needle” requests are embedded within a long multi-round conversation or document, and the model must retrieve the correct referenced response for a specified needle. It targets robustness to interference and distractors, requiring stable cross-reference tracking at long context lengths.","L1: Language Comprehension
L2: Working Memory, Attention, Episodic Memory, Semantic Understanding & Context Recognition
L3: ",L2
