Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro is a large, hard software engineering benchmark where a model must produce patches that fix real issues in open-source repositories and satisfy hidden tests. Compared with SWE-bench Verified, it broadens difficulty and (typically) language and task diversity, stressing end-to-end code understanding, editing, and debugging under realistic constraints.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory, Logical Reasoning
L3: ",L2
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld evaluates computer-use agents that operate a real desktop environment to complete multi-step tasks across applications (e.g., browsing, file operations, productivity tools). Agents must interpret screenshots and UI state, choose actions (click/type/shortcut), and recover from errors within a step budget.","L1: Visual Perception
L2: Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Sensorimotor Coordination, Spatial Representation & Mapping, Planning, Decision-making, Working Memory, Adaptive Error Correction
L3: ",L2
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,ARC-AGI measures few-shot abstract reasoning on grid-based puzzles where the system must infer a hidden rule from a handful of input–output examples and apply it to a new input. The benchmark is designed to emphasize generalization to novel patterns rather than memorization of task types.,"L1: Visual Perception
L2: Logical Reasoning, Working Memory, Spatial Representation & Mapping, Planning, Attention
L3: Cognitive Flexibility",L3
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon autonomous agency in a simulated vending-machine business over an extended time period. The agent must manage inventory and cash flow, interact with suppliers/customers via messages, adapt to changing conditions, and maintain coherent strategy across many decisions.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Reward Mechanisms, Working Memory, Adaptive Error Correction
L3: Cognitive Timing & Predictive Modeling",L3
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas assesses real-world tool use via the Model Context Protocol by requiring models to discover, call, and compose tools across multi-step workflows. Tasks emphasize correct parameterization, handling tool errors, and integrating tool outputs into a final answer.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition
L3: ",L2
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity agent capabilities on tasks such as reproducing known vulnerabilities in real projects and, in some settings, discovering new vulnerabilities. It stresses understanding codebases and vulnerability descriptions, forming hypotheses, and iteratively testing and refining exploits or patches.","L1: Language Comprehension
L2: Logical Reasoning, Planning, Adaptive Error Correction, Working Memory, Decision-making, Semantic Understanding & Context Recognition
L3: ",L2
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam (HLE) is a multimodal benchmark intended to probe frontier academic knowledge and reasoning across many domains, often at expert difficulty. Questions can require synthesizing information, performing multi-step reasoning, and (in some configurations) using tools like search or code to verify results.","L1: Language Comprehension, Visual Perception
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Planning, Decision-making
L3: ",L2
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a high-quality subset of GPQA composed of very difficult multiple-choice science questions designed to be resistant to superficial retrieval. It emphasizes deep domain understanding, careful elimination among options, and robust reasoning under uncertainty.","L1: Language Comprehension
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Decision-making
L3: ",L2
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro is a challenging multimodal benchmark spanning many disciplines, requiring models to answer questions grounded in images (figures, diagrams, charts) and text. It stresses integrated visual–text reasoning and precise interpretation of scientific/technical visuals.","L1: Visual Perception, Language Comprehension
L2: Multisensory Integration, Scene Understanding & Visual Reasoning, Logical Reasoning, Working Memory, Attention
L3: ",L2
OmniDocBench 1.5,https://github.com/opendatalab/OmniDocBench,https://arxiv.org/abs/2412.07626,"OmniDocBench 1.5 evaluates document understanding and OCR-centric extraction across heterogeneous layouts (text, formulas, tables, reading order). Systems must recognize content and structure from document images and reconstruct or extract it with high fidelity.","L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Attention, Working Memory
L3: ",L2
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU extends multimodal reasoning to video, requiring temporal understanding of events, actions, and visual cues across multiple frames. It targets higher-level comprehension beyond per-frame recognition, including reasoning about changes over time and context.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Working Memory, Attention, Multisensory Integration
L3: Cognitive Timing & Predictive Modeling",L3
LiveCodeBench Pro,https://livecodebenchpro.com/projects/livecodebench-pro/leaderboard,https://arxiv.org/abs/2506.11928,"LiveCodeBench Pro evaluates coding ability using up-to-date, competitive-programming-style and practical coding tasks with a focus on realistic performance measurement (e.g., pass@k/Elo-style reporting). It stresses writing correct executable code, handling edge cases, and iterating when initial attempts fail.","L1: Language Comprehension, Language Production
L2: Logical Reasoning, Planning, Working Memory, Adaptive Error Correction, Decision-making
L3: ",L2
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"FACTS Benchmark Suite systematically evaluates factuality: whether model outputs are supported by provided sources/context and whether they avoid introducing unsupported claims. It probes grounded generation, calibration, and consistency across diverse factuality subtests.","L1: Language Comprehension, Language Production
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Attention
L3: Self-reflection, Inhibitory Control",L3
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA evaluates pragmatic/common-sense inference across many languages and cultural contexts using non-parallel data, aiming to reduce English-centric bias. Tasks probe whether a model can choose or generate plausible actions/interpretations in everyday situations across linguistic settings.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Decision-making
L3: Social Reasoning & Theory of Mind",L3
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 is a long-context evaluation where multiple similar ‘needle’ queries are embedded within long ‘haystack’ conversations/documents and the model must retrieve the correct referenced response (here, the 8-needle variant). It emphasizes robust context tracking and resistance to interference from near-duplicates.","L1: Language Comprehension
L2: Working Memory, Attention, Semantic Understanding & Context Recognition, Episodic Memory
L3: Inhibitory Control",L3
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval measures performance on well-specified professional knowledge-work tasks across many occupations, judged by expert humans via head-to-head comparisons against professional outputs. It emphasizes producing complete artifacts (e.g., plans, analyses, spreadsheets/slides) with correct structure, assumptions, and actionable recommendations.","L1: Language Production, Language Comprehension
L2: Planning, Decision-making, Semantic Understanding & Context Recognition, Working Memory
L3: Social Reasoning & Theory of Mind, Self-reflection",L3
SWE-Lancer,https://openai.com/index/swe-lancer/,https://arxiv.org/abs/2502.12115,"SWE-Lancer evaluates agentic software engineering on realistic repository tasks that often require deeper investigation, coordination of changes, and higher autonomy than short single-file edits. It targets end-to-end workflows like debugging, implementing fixes, and validating changes against project expectations.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory, Logical Reasoning
L3: ",L2
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath is a benchmark of expert-level mathematics problems designed to be difficult for current models and more resistant to contamination, often requiring long chains of derivation. It probes mathematical reasoning accuracy, strategic problem decomposition, and rigorous handling of constraints and edge cases.","L1: 
L2: Logical Reasoning, Planning, Working Memory, Adaptive Error Correction, Semantic Understanding & Context Recognition, Attention
L3: ",L2
