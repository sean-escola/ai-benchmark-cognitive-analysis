Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates software engineering ability by giving a model a real GitHub repository plus an issue description, then scoring whether the model produces a patch that makes the repository’s tests pass. The “Verified” subset uses human-validated tasks intended to be solvable and to reduce noise from ambiguous or broken tasks.","L1: Language Comprehension
L2: Planning, Logical Reasoning, Adaptive Error Correction, Working Memory
L3: ",L2
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 measures agentic competence in real command-line environments, where models must use shell commands, inspect files, install/run tools, and iteratively troubleshoot to complete tasks. It emphasizes end-to-end execution under realistic system constraints rather than single-shot question answering.","L1: 
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory, Attention
L3: ",L2
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench evaluates tool-using customer-support agents in multi-turn simulations (e.g., retail, airline, telecom) that require calling APIs, following domain policies, and maintaining coherent dialogue with a simulated user. Scoring rewards both task completion and policy-consistent behavior across long interactions.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Working Memory
L3: Inhibitory Control, Social Reasoning & Theory of Mind",L3
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,ARC-AGI tests “fluid” abstract reasoning using grid-based input–output examples where a model must infer the hidden transformation rule from only a few demonstrations. Success requires generalizing to novel patterns and compositions rather than recalling domain facts.,"L1: Visual Perception
L2: Logical Reasoning, Working Memory, Spatial Representation & Mapping, Scene Understanding & Visual Reasoning
L3: Cognitive Flexibility",L3
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon autonomous agency by having a model run a simulated vending-machine business over an extended time period, making thousands of decisions about inventory, pricing, supplier negotiation, and cash management. Performance is typically based on final business outcomes (e.g., ending balance), stressing sustained coherence and strategy.","L1: 
L2: Planning, Decision-making, Reward Mechanisms, Working Memory
L3: Self-reflection",L3
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam (HLE) is a difficult, broad benchmark intended to probe frontier-level knowledge and reasoning across many domains, with many questions designed to be challenging for both models and non-experts. Variants may be run with or without tools (e.g., search/code), and grading is typically strict due to the long-tail of specialized topics.","L1: Language Comprehension, Visual Perception
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Scene Understanding & Visual Reasoning
L3: ",L2
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 evaluates competition-style mathematics using problems from the American Invitational Mathematics Examination, emphasizing multi-step derivations and careful symbolic manipulation. Because answers are typically short (often a single integer), the benchmark heavily rewards correct intermediate reasoning under tight constraints.","L1: 
L2: Logical Reasoning, Working Memory, Planning, Adaptive Error Correction
L3: ",L2
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a high-difficulty multiple-choice science benchmark (physics, chemistry, biology) designed to be “Google-proof,” focusing on questions that require expert-level understanding rather than superficial recall. The Diamond subset is curated for quality and difficulty, making it a common stress test for frontier reasoning in scientific domains.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory
L3: Inhibitory Control",L3
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends the MMLU-style academic knowledge evaluation to multiple languages, probing whether models can maintain subject-matter competence and reasoning across diverse linguistic contexts. It spans many subjects and tests both understanding of the prompt language and robust cross-lingual generalization.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory
L3: Cognitive Flexibility",L3
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro is a professional-grade multimodal benchmark that tests expert-level understanding and reasoning over text-plus-image questions across many disciplines (e.g., science, engineering, medicine). It emphasizes complex visual interpretation and grounded reasoning rather than simple recognition.","L1: Visual Perception
L2: Multisensory Integration, Scene Understanding & Visual Reasoning, Logical Reasoning, Working Memory, Attention
L3: ",L2
MathArena Apex,https://matharena.ai/?view=problem&comp=apex--apex_2025,https://arxiv.org/abs/2505.23281,"MathArena Apex is a challenging mathematics benchmark drawn from harder, research-leaning or Olympiad-adjacent problems and is often used to compare advanced reasoning among frontier models. It stresses long-chain deduction, algebraic/analytic technique selection, and error-prone multi-step computation.","L1: 
L2: Logical Reasoning, Planning, Working Memory, Adaptive Error Correction
L3: ",L2
ScreenShot-Pro,https://gui-agent.github.io/grounding-leaderboard/,https://arxiv.org/abs/2504.07981,"ScreenShot-Pro evaluates GUI understanding from high-resolution screenshots where a model must ground language instructions to on-screen elements (e.g., buttons, fields, menus) and choose the correct interaction. It targets practical “computer use” competence, requiring accurate visual localization and stepwise action selection.","L1: Visual Perception
L2: Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Sensorimotor Coordination, Planning, Decision-making, Working Memory
L3: ",L2
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning tests reasoning over figures, charts, and visual elements commonly found in scientific papers, requiring models to extract quantitative/structural information and answer questions that depend on correct interpretation. It is designed to go beyond OCR by emphasizing analytical reasoning grounded in scientific visuals.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Attention
L3: ",L2
OmniDocBench 1.5,https://github.com/opendatalab/OmniDocBench,https://arxiv.org/abs/2412.07626,"OmniDocBench 1.5 evaluates document understanding and OCR-style extraction across heterogeneous document components such as text blocks, tables, formulas, and reading order. It stresses robust parsing of layout and precise transcription/structuring rather than free-form summarization.","L1: Visual Perception, Language Comprehension
L2: Attention, Working Memory, Semantic Understanding & Context Recognition, Scene Understanding & Visual Reasoning
L3: ",L2
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,Video-MMMU extends multimodal understanding to the temporal domain by asking questions about video content that require integrating information across frames and scenes. Tasks often require tracking events over time and combining visual evidence with textual reasoning.,"L1: Visual Perception
L2: Attention, Working Memory, Scene Understanding & Visual Reasoning, Multisensory Integration
L3: Cognitive Timing & Predictive Modeling",L3
LiveCodeBench Pro,https://livecodebenchpro.com/projects/livecodebench-pro/leaderboard,https://arxiv.org/abs/2506.11928,"LiveCodeBench Pro evaluates coding ability on problems designed to reflect realistic programming work, often emphasizing correctness under execution-based testing and contemporary task distributions. Compared to static coding sets, it aims to reduce leakage and better reflect current software development demands.","L1: Language Comprehension
L2: Planning, Logical Reasoning, Adaptive Error Correction, Working Memory, Decision-making
L3: ",L2
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"The FACTS Benchmark Suite systematically evaluates factuality by measuring whether a model’s statements are supported, accurate, and appropriately qualified across a variety of settings (including attribution/grounding-style checks). It targets reliability failures such as hallucination, overconfident claims, and unsupported generalizations.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Working Memory, Logical Reasoning
L3: Inhibitory Control, Self-reflection",L3
SimpleQA Verified,https://www.kaggle.com/benchmarks/deepmind/simpleqa-verified,https://arxiv.org/abs/2509.07968,"SimpleQA Verified is a short-form factual question answering benchmark with verification-oriented curation, aiming to measure whether models can answer precisely and avoid fabricating unsupported facts. It is commonly used as a lightweight indicator of factual reliability under direct questioning.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Working Memory
L3: Inhibitory Control",L3
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA evaluates physical commonsense and practical reasoning across many languages and cultural contexts, focusing on whether a model can pick plausible actions/outcomes in everyday situations. It probes robustness of commonsense inference beyond English-centric phrasing and datasets.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Spatial Representation & Mapping
L3: Cognitive Flexibility",L3
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) is a long-context evaluation where multiple similar “needle” items are inserted into a long “haystack,” and the model must retrieve and reproduce the correct associated content for a specified needle. The 8-needle setting increases interference and tests whether the model can maintain accurate references over very long contexts.","L1: Language Comprehension
L2: Working Memory, Attention, Episodic Memory
L3: Inhibitory Control",L3
