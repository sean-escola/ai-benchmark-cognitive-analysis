Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro evaluates models on realistic software engineering issues drawn from real repositories, where the model must produce patches that satisfy hidden tests. Compared with easier coding evals, it emphasizes longer problem statements, multi-file changes, and robustness against superficial pattern matching, making it a stronger proxy for professional coding work.","L1: Language Comprehension, Language Production
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Planning, Decision-making, Working Memory, Adaptive Error Correction
L3: ",L2
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld evaluates multimodal “computer use” agents that must complete tasks by interacting with a desktop operating system (e.g., navigating GUIs, opening apps, editing files) based on visual observations. It tests whether models can translate natural-language goals into sequences of grounded UI actions while coping with interface variability and errors.","L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Planning, Decision-making, Sensorimotor Coordination, Spatial Representation & Mapping, Working Memory, Adaptive Error Correction
L3: ",L2
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI measures fluid, few-shot abstract reasoning on grid-based puzzles: given a small set of input–output examples, models must infer the underlying rule and generate the correct output for a new input. It is designed to reward compositional rule induction and generalization rather than memorization of domain knowledge.","L1: 
L2: Logical Reasoning, Working Memory, Spatial Representation & Mapping, Scene Understanding & Visual Reasoning, Planning, Attention
L3: Cognitive Flexibility",L3
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon agent performance in a simulated vending-machine business over an extended period, requiring many sequential decisions (pricing, inventory, supplier negotiation) to maximize final balance. The benchmark stresses sustained coherence, strategic adaptation to changing conditions, and avoiding compounding errors over time.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Working Memory, Reward Mechanisms, Adaptive Error Correction, Semantic Understanding & Context Recognition
L3: ",L2
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas evaluates real-world tool use through the Model Context Protocol (MCP), where models must discover, invoke, and chain API-like tools to complete tasks in production-like environments. It emphasizes correct tool selection, parameterization, multi-step workflow execution, and recovery from tool or environment errors.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Semantic Understanding & Context Recognition, Working Memory, Adaptive Error Correction
L3: Inhibitory Control",L3
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates agentic cybersecurity capability across large-scale tasks including identifying known vulnerabilities from descriptions and, in some settings, discovering new ones in real open-source codebases. It stresses code comprehension, hypothesis-driven debugging, and iterative testing with real tooling constraints.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Planning, Decision-making, Working Memory, Adaptive Error Correction
L3: ",L2
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a multimodal benchmark at the frontier of academic and professional knowledge, spanning difficult questions that often require multi-step reasoning and careful interpretation. Variants may allow tools (e.g., search or code) to test end-to-end problem solving and verification behaviors under realistic conditions.","L1: Language Comprehension, Visual Perception
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Scene Understanding & Visual Reasoning, Planning, Decision-making
L3: ",L2
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a high-quality subset of extremely challenging, graduate-level multiple-choice science questions intended to be resistant to shallow lookup strategies. It primarily tests deep conceptual understanding and reasoning in physics, chemistry, and biology under tight answer constraints.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Decision-making
L3: ",L2
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro is a difficult multimodal benchmark covering many disciplines where models must answer questions that combine text with images such as diagrams, plots, and complex visual layouts. It targets higher-level multimodal reasoning rather than simple recognition, including interpreting figures and integrating multiple evidence sources.","L1: Visual Perception, Language Comprehension
L2: Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Working Memory
L3: ",L2
OmniDocBench 1.5,https://github.com/opendatalab/OmniDocBench,https://arxiv.org/abs/2412.07626,"OmniDocBench 1.5 evaluates document understanding and OCR-centric capabilities across heterogeneous content, including text, tables, formulas, and reading order. It tests whether a model can accurately extract and structure information from realistic documents with challenging layouts.","L1: Visual Perception, Language Comprehension
L2: Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Semantic Understanding & Context Recognition, Working Memory
L3: ",L2
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU evaluates multimodal understanding and reasoning over videos, requiring models to integrate information across time and answer questions about events, actions, and context. The benchmark stresses temporal integration and maintaining coherence over long multimodal sequences.","L1: Visual Perception, Language Comprehension
L2: Visual Attention & Eye Movements, Working Memory, Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning
L3: Cognitive Timing & Predictive Modeling",L3
LiveCodeBench Pro,https://livecodebenchpro.com/projects/livecodebench-pro/leaderboard,https://arxiv.org/abs/2506.11928,"LiveCodeBench Pro evaluates coding ability using competitive-programming–style tasks with up-to-date, contamination-resistant curation and automated execution-based grading. It emphasizes correct algorithm selection, implementation quality, and iterative debugging against failing tests.","L1: Language Comprehension, Language Production
L2: Logical Reasoning, Planning, Decision-making, Working Memory, Adaptive Error Correction
L3: ",L2
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"The FACTS Benchmark Suite systematically evaluates LLM factuality across multiple settings, focusing on whether generated claims are supported, consistent, and appropriately qualified. It is designed to probe hallucination tendencies, calibration, and the ability to avoid confidently stating unsupported information.","L1: Language Comprehension, Language Production
L2: Semantic Understanding & Context Recognition, Working Memory
L3: Inhibitory Control, Self-reflection",L3
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA measures physical commonsense and practical reasoning across many languages and cultures, focusing on selecting plausible actions or solutions in everyday scenarios. It stresses transferring intuitive physical and procedural knowledge across linguistic contexts rather than relying on English-only cues.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Spatial Representation & Mapping
L3: Cognitive Flexibility",L3
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,MRCR v2 (8-needle) evaluates long-context multi-round co-reference resolution by embedding multiple similar “needle” interactions inside long “haystack” transcripts and asking the model to retrieve the correct referenced response. It tests robustness of retrieval under interference and the ability to attend to the right instance among many near-duplicates.,"L1: Language Comprehension
L2: Working Memory, Episodic Memory, Attention, Semantic Understanding & Context Recognition
L3: ",L2
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval evaluates well-specified professional knowledge work across many occupations by comparing model outputs against industry professionals, often using expert human judging and artifact-quality criteria. Tasks include producing real work products (e.g., spreadsheets, slides, plans), testing end-to-end execution quality rather than isolated question answering.","L1: Language Production, Language Comprehension
L2: Planning, Decision-making, Semantic Understanding & Context Recognition, Working Memory
L3: Self-reflection, Social Reasoning & Theory of Mind",L3
SWE-Lancer,https://openai.com/index/swe-lancer/,https://arxiv.org/abs/2502.12115,"SWE-Lancer evaluates software engineering agents on realistic development tasks that require producing correct patches and navigating larger, messier codebases than typical coding puzzles. It emphasizes reliability, change management across files, and aligning implementation details with written requirements and repository conventions.","L1: Language Comprehension, Language Production
L2: Semantic Understanding & Context Recognition, Planning, Decision-making, Working Memory, Adaptive Error Correction, Logical Reasoning
L3: ",L2
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath evaluates expert-level mathematics, including problems that require deep multi-step reasoning and, in some settings, tool-assisted computation. It is designed to be difficult for both memorization-based approaches and shallow pattern matching, highlighting true mathematical problem-solving ability.","L1: 
L2: Logical Reasoning, Working Memory, Planning, Semantic Understanding & Context Recognition, Attention
L3: Cognitive Flexibility",L3
