Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates software-engineering ability by asking a model to generate patches that fix real issues in open-source Python repositories, judged by whether unit tests pass. The “Verified” subset uses human-validated tasks intended to be solvable and to reduce ambiguity in problem statements and grading.","L1: 
L2: Planning, Logical Reasoning, Working Memory, Adaptive Error Correction, Semantic Understanding & Context Recognition
L3: ",L2
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 measures agentic performance on real command-line tasks, where the model must interact with a terminal environment to accomplish objectives (e.g., debugging, installing tools, manipulating files, running programs). Success depends on executing correct sequences of actions and recovering from errors under realistic constraints.","L1: Language Comprehension
L2: Planning, Decision-making, Working Memory, Adaptive Error Correction
L3: ",L2
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench evaluates interactive tool-using agents in customer-support style scenarios (e.g., retail, airline, telecom) that require multi-turn dialogue plus API/tool calls while following domain policies. It probes whether an agent can consistently adhere to constraints, maintain state across turns, and resolve user goals through correct procedures.","L1: Language Comprehension
L2: Planning, Decision-making, Working Memory
L3: Social Reasoning & Theory of Mind, Empathy, Inhibitory Control",L3
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI tests “fluid intelligence” via few-shot abstract pattern induction on grid transformation puzzles, where the system must infer rules from a small set of input-output examples. It is designed to emphasize generalization to novel tasks over memorization, with performance reflecting the ability to discover latent structure and apply it reliably.","L1: Visual Perception
L2: Logical Reasoning, Working Memory, Spatial Representation & Mapping, Scene Understanding & Visual Reasoning
L3: Cognitive Flexibility",L3
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 measures long-horizon agent coherence by having a model run a simulated vending-machine business over many sequential decisions, optimizing for outcomes like final balance. The agent must plan, manage inventory and pricing, negotiate with simulated counterparts, and adapt strategy as conditions change over time.","L1: 
L2: Planning, Decision-making, Reward Mechanisms, Working Memory
L3: Cognitive Timing & Predictive Modeling",L3
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a large, challenging benchmark spanning expert-level questions across many domains, with an emphasis on difficult reasoning and broad knowledge under realistic, sometimes multimodal prompts. It is often used to compare frontier models’ ability to solve complex problems with and without tool assistance (e.g., search or code).","L1: Language Comprehension, Visual Perception
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Multisensory Integration
L3: ",L2
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 evaluates competition-style mathematics problem solving, typically requiring multi-step derivations and careful symbolic manipulation. It stresses correctness under time-like constraints and sensitivity to small logical errors that can derail a solution.","L1: 
L2: Logical Reasoning, Working Memory, Planning
L3: ",L2
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a difficult multiple-choice science benchmark intended to be “Google-proof,” focusing on questions where superficial retrieval is insufficient and reasoning matters. The Diamond subset emphasizes higher-quality items that better discriminate expert-level understanding.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory
L3: ",L2
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends MMLU to multiple languages and subjects, testing whether models can answer academic and professional questions across diverse linguistic contexts. It probes robustness of knowledge and reasoning when the same underlying skills must be expressed and understood in different languages.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory
L3: ",L2
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro is a multimodal benchmark covering many disciplines where questions require integrating text with images such as diagrams, charts, and figures. It targets expert-level multimodal reasoning, stressing accurate perception plus domain knowledge and structured inference.","L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Working Memory
L3: ",L2
MathArena Apex,https://matharena.ai/?view=problem&comp=apex--apex_2025,https://arxiv.org/abs/2505.23281,"MathArena Apex aggregates harder mathematics problems intended to stress frontier-level mathematical reasoning and solution robustness beyond standard contest items. It is commonly used to measure how well models sustain long, correct reasoning chains and avoid subtle algebraic or logical mistakes.","L1: 
L2: Logical Reasoning, Working Memory, Planning, Adaptive Error Correction
L3: ",L2
ScreenShot-Pro,https://gui-agent.github.io/grounding-leaderboard/,https://arxiv.org/abs/2504.07981,"ScreenShot-Pro evaluates GUI understanding from screenshots, requiring models or agents to interpret interface layouts and identify relevant UI elements for tasks. It emphasizes precise visual grounding—mapping instructions to the correct on-screen targets under varied, high-resolution interfaces.","L1: Visual Perception
L2: Visual Attention & Eye Movements, Spatial Representation & Mapping, Scene Understanding & Visual Reasoning, Sensorimotor Coordination, Planning
L3: ",L2
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning focuses on questions grounded in scientific figures from research papers, testing whether models can extract and reason over visual evidence with technical context. Performance depends on accurately reading plots/diagrams and integrating them with scientific language and quantitative reasoning.","L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory
L3: ",L2
OmniDocBench 1.5,https://github.com/opendatalab/OmniDocBench,https://arxiv.org/abs/2412.07626,"OmniDocBench 1.5 evaluates document understanding and OCR across heterogeneous elements such as text, formulas, tables, and reading order. It measures whether systems can faithfully reconstruct structured content from complex document images and preserve layout-dependent meaning.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Spatial Representation & Mapping, Working Memory, Semantic Understanding & Context Recognition
L3: ",L2
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU tests multimodal reasoning over videos, where answers require integrating information across time rather than from a single frame. It stresses temporal event understanding, long-range dependencies, and coherent summarization of visual evidence.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Working Memory, Attention
L3: Cognitive Timing & Predictive Modeling",L3
LiveCodeBench Pro,https://livecodebenchpro.com/projects/livecodebench-pro/leaderboard,https://arxiv.org/abs/2506.11928,"LiveCodeBench Pro evaluates coding ability on programming tasks designed to better reflect contemporary, non-stale coding requirements, often scored with functional tests and competitive-style metrics. It probes algorithmic reasoning, implementation accuracy, and iterative debugging under realistic constraints.","L1: Language Comprehension
L2: Logical Reasoning, Planning, Working Memory, Adaptive Error Correction
L3: ",L2
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,The FACTS Benchmark Suite systematically evaluates factuality by measuring whether models produce accurate claims and appropriately avoid unsupported assertions across diverse settings. It targets both knowledge correctness and reliability behaviors such as resisting hallucination when evidence is insufficient.,"L1: Language Production
L2: Semantic Understanding & Context Recognition, Working Memory
L3: Inhibitory Control, Self-reflection",L3
SimpleQA Verified,https://www.kaggle.com/benchmarks/deepmind/simpleqa-verified,https://arxiv.org/abs/2509.07968,"SimpleQA Verified is a short-form factual question answering benchmark with verification-oriented grading to reduce ambiguity and reward correctness. It is designed to test whether models can provide concise, accurate answers and refrain from confident errors on simple-looking queries.","L1: Language Comprehension, Language Production
L2: Semantic Understanding & Context Recognition
L3: Inhibitory Control",L3
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA evaluates physical commonsense reasoning across many languages, typically using short scenarios that require choosing the more plausible action or outcome. It targets grounded everyday reasoning about objects, forces, affordances, and simple causal consequences under multilingual variation.","L1: Language Comprehension
L2: Spatial Representation & Mapping, Sensorimotor Coordination, Logical Reasoning, Semantic Understanding & Context Recognition
L3: ",L2
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) evaluates long-context, multi-round coreference and retrieval by inserting multiple similar “needle” requests into a long “haystack” dialogue and asking the model to reproduce the correct response for a specific needle. It stresses maintaining attention over very long contexts and resolving confusable references accurately.","L1: Language Comprehension
L2: Working Memory, Attention, Episodic Memory, Semantic Understanding & Context Recognition
L3: ",L2
