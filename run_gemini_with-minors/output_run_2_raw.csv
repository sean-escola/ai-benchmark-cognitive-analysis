Benchmark,Website,Paper,Description,Cognitive Functions
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro evaluates software engineering agents on a large set of real-world bug-fix and feature tasks drawn from open-source repositories, requiring producing patches that pass tests. It is designed to be harder and more contamination-resistant than SWE-bench Verified, emphasizing realistic engineering workflows and robustness under constrained attempts.","Language Comprehension, Language Production, Logical Reasoning, Planning, Working Memory, Adaptive Error Correction"
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld measures an agent’s ability to operate a computer-like operating system via a multimodal interface, completing tasks that require navigating GUIs, using apps, and executing multi-step procedures. Success depends on reliably mapping visual observations to action sequences under step limits, similar to practical “computer use” assistants.","Visual Perception, Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Sensorimotor Coordination, Planning, Decision-making, Working Memory, Attention"
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI tests “fluid” reasoning by presenting a few input–output grid examples generated by an underlying rule, and asking models to infer the rule to transform new inputs. It emphasizes generalization to novel patterns with minimal examples rather than recall of domain knowledge.","Logical Reasoning, Cognitive Flexibility, Spatial Representation & Mapping, Working Memory, Planning, Attention"
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon agent coherence and business strategy in a simulated year-long vending-machine operation, where the agent must manage inventory, pricing, supplier interaction, and finances. The score reflects whether decisions compound into sustained success across many steps rather than short, isolated wins.","Planning, Decision-making, Reward Mechanisms, Working Memory, Semantic Understanding & Context Recognition, Self-reflection"
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas assesses real-world tool use through Model Context Protocol servers, testing whether models can discover appropriate tools, call them correctly, recover from errors, and integrate results across multi-step workflows. It targets production-like API interaction and orchestration rather than single-call function use.","Planning, Decision-making, Language Comprehension, Working Memory, Adaptive Error Correction, Attention"
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity agent skills on tasks involving finding known vulnerabilities from descriptions and discovering previously unknown vulnerabilities in real open-source projects. It emphasizes practical reasoning over codebases, vulnerability patterns, and iterative debugging-like refinement under pass@1 scoring.","Logical Reasoning, Planning, Adaptive Error Correction, Language Comprehension, Working Memory, Attention"
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a large, frontier-difficulty benchmark spanning expert-level questions across many disciplines, often including multimodal inputs. It aims to probe deep reasoning and broad academic knowledge, and can be run with or without tools such as search and code execution.","Language Comprehension, Visual Perception, Multisensory Integration, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory"
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a high-quality subset of extremely challenging graduate-level multiple-choice science questions designed to be “Google-proof.” It focuses on rigorous scientific reasoning and conceptual understanding, with reduced benefit from surface pattern matching.","Language Comprehension, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Attention"
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro is a multimodal benchmark measuring expert-level understanding and reasoning over images plus text across many disciplines. It stresses interpreting diagrams, plots, and specialized visuals, requiring multi-step reasoning rather than straightforward recognition.","Visual Perception, Scene Understanding & Visual Reasoning, Multisensory Integration, Language Comprehension, Logical Reasoning, Working Memory, Attention"
OmniDocBench 1.5,https://github.com/opendatalab/OmniDocBench,https://arxiv.org/abs/2412.07626,"OmniDocBench 1.5 evaluates document understanding and OCR-centric capabilities across complex layouts, including text, formulas, tables, and reading order. It targets end-to-end document parsing where spatial structure and formatting are essential for correctness.","Visual Perception, Scene Understanding & Visual Reasoning, Spatial Representation & Mapping, Language Comprehension, Attention, Working Memory"
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU extends multimodal reasoning to the video setting, requiring models to integrate information across frames and time to answer questions. It stresses temporal comprehension, tracking, and synthesis rather than single-image interpretation.","Visual Perception, Scene Understanding & Visual Reasoning, Cognitive Timing & Predictive Modeling, Working Memory, Attention, Multisensory Integration"
LiveCodeBench Pro,https://livecodebenchpro.com/projects/livecodebench-pro/leaderboard,https://arxiv.org/abs/2506.11928,"LiveCodeBench Pro measures competitive coding performance under conditions intended to reflect modern programming tasks and reduce leakage, often emphasizing solution quality and reliability. It evaluates whether models can write correct programs and adapt solutions under realistic constraints.","Language Comprehension, Language Production, Logical Reasoning, Planning, Working Memory, Adaptive Error Correction"
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"The FACTS Benchmark Suite systematically evaluates factuality by testing whether models produce correct, well-grounded statements across diverse factuality stressors. It emphasizes resisting hallucination and maintaining consistency with provided evidence or known facts.","Language Comprehension, Semantic Understanding & Context Recognition, Working Memory, Inhibitory Control, Logical Reasoning, Self-reflection"
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA tests practical reasoning about physical situations and everyday interactions across many languages, aiming to measure commonsense understanding beyond English-only datasets. It focuses on whether models can infer plausible actions/outcomes given real-world constraints described in text.","Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Cognitive Flexibility"
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) is a long-context multi-round coreference evaluation where multiple similar “needle” interactions are embedded in long “haystacks,” and models must reproduce the correct response for a specified needle. It stresses maintaining and retrieving precise contextual bindings amid distractors over long token spans.","Working Memory, Attention, Semantic Understanding & Context Recognition, Episodic Memory, Inhibitory Control"
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval evaluates economically valuable, well-specified knowledge work across many occupations, using expert human judgments on produced work artifacts (e.g., analyses, plans, presentations, spreadsheets). It measures end-to-end task execution quality, including following constraints and producing usable deliverables.","Planning, Decision-making, Language Production, Language Comprehension, Semantic Understanding & Context Recognition, Working Memory, Social Reasoning & Theory of Mind, Self-reflection"
SWE-Lancer,https://openai.com/index/swe-lancer/,https://arxiv.org/abs/2502.12115,"SWE-Lancer evaluates software engineering agents on tasks that more closely resemble real engineering “work orders,” emphasizing end-to-end patch production and reliability. It is aimed at measuring sustained engineering competence beyond short code-generation prompts.","Planning, Decision-making, Language Production, Language Comprehension, Working Memory, Adaptive Error Correction, Logical Reasoning"
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath is an expert-level mathematics benchmark designed to probe advanced problem solving that is difficult to answer by memorization or simple heuristics. It emphasizes multi-step derivations, careful constraint handling, and robustness on novel mathematical problems.","Logical Reasoning, Working Memory, Planning, Cognitive Flexibility, Adaptive Error Correction, Attention"
