Benchmark,Website,Paper,Description,Cognitive Functions
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates software-engineering capability by asking a model to produce patches in real GitHub repositories so that previously failing tests pass. The “Verified” split consists of tasks that have been validated as solvable and are typically scored by running the repo’s test suite to check functional correctness, emphasizing end-to-end debugging and code editing rather than isolated coding puzzles.","Planning, Logical Reasoning, Adaptive Error Correction, Working Memory, Language Comprehension, Language Production"
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 measures how well an agent can complete real tasks in a command-line environment, such as installing dependencies, manipulating files, running programs, and diagnosing failures. Success depends on choosing effective command sequences, interpreting tool outputs, and iteratively correcting mistakes under environment constraints.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Language Comprehension, Language Production, Inhibitory Control"
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench evaluates interactive agent performance in multi-turn customer-support style settings (e.g., retail, airline, telecom) where the agent must use tools/APIs and follow domain policies. It stresses consistent policy adherence while responding helpfully to a simulated user over long dialogs with state changes and external actions.","Social Reasoning & Theory of Mind, Decision-making, Planning, Inhibitory Control, Working Memory, Language Comprehension, Language Production"
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,ARC-AGI is a fluid-intelligence benchmark of novel pattern induction on small grid-based tasks: the model sees a few input–output examples and must infer the hidden transformation rule for a new input. It is designed to reward generalization and compositional reasoning under minimal supervision and few-shot evidence.,"Logical Reasoning, Cognitive Flexibility, Working Memory, Spatial Representation & Mapping, Visual Perception, Scene Understanding & Visual Reasoning, Attention"
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 tests long-horizon agent coherence and strategy by simulating a year of running a vending-machine business, requiring repeated decisions about purchasing, pricing, inventory, and negotiations. The benchmark rewards sustained planning, adapting to market dynamics, and maintaining consistent goals across many steps.","Planning, Decision-making, Reward Mechanisms, Working Memory, Episodic Memory, Cognitive Timing & Predictive Modeling, Language Comprehension, Language Production"
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam (HLE) is a large, frontier-level benchmark of difficult questions spanning advanced academic and professional knowledge, often including multimodal items. It aims to measure robust reasoning and synthesis rather than rote recall, and is commonly evaluated both with and without tool assistance (e.g., code or search) depending on the setup.","Language Comprehension, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Scene Understanding & Visual Reasoning, Visual Perception"
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 consists of competition-style mathematics problems that require multi-step derivations and careful symbolic manipulation. Performance reflects the model’s ability to plan solution strategies, execute precise intermediate steps, and avoid or correct algebraic/logical slips.","Logical Reasoning, Planning, Working Memory, Adaptive Error Correction"
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a high-quality subset of graduate-level multiple-choice science questions designed to be difficult to answer via shallow pattern matching. It emphasizes deep domain understanding and reasoning over plausible distractors across physics, chemistry, and biology.","Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Inhibitory Control, Language Comprehension"
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends broad subject-area testing into multiple languages, evaluating whether a model’s knowledge and reasoning transfer beyond English across many academic topics. It probes multilingual comprehension and the ability to map questions in diverse languages onto correct conceptual and factual knowledge.","Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory"
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro is a multimodal benchmark of expert-level questions that combine text with images (e.g., diagrams, charts, scientific figures) and require multi-step reasoning. It tests whether models can integrate visual evidence with language instructions and domain knowledge to select or produce correct answers.","Visual Perception, Scene Understanding & Visual Reasoning, Multisensory Integration, Language Comprehension, Logical Reasoning, Working Memory, Visual Attention & Eye Movements"
MathArena Apex,https://matharena.ai/?view=problem&comp=apex--apex_2025,https://arxiv.org/abs/2505.23281,"MathArena Apex aggregates challenging math problems to compare advanced mathematical reasoning across models under standardized prompting and scoring. It targets non-trivial problem solving (often beyond basic contest items), where success depends on sustained chains of reasoning and error-resistant computation.","Logical Reasoning, Planning, Working Memory, Adaptive Error Correction"
ScreenShot-Pro,https://gui-agent.github.io/grounding-leaderboard/,https://arxiv.org/abs/2504.07981,"ScreenShot-Pro evaluates GUI understanding and grounding from screenshots, typically requiring the model to identify interface elements, interpret layout, and answer questions tied to precise visual targets. It stresses spatially grounded visual reasoning and robust attention to small but task-critical UI details.","Visual Perception, Visual Attention & Eye Movements, Spatial Representation & Mapping, Scene Understanding & Visual Reasoning, Decision-making, Working Memory"
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning measures a model’s ability to reason over scientific documents and figures (e.g., charts/plots and accompanying context) as found in research papers. It emphasizes extracting quantitative/structural information from visuals and integrating it with textual cues to answer higher-level questions.","Visual Perception, Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Attention"
OmniDocBench 1.5,https://github.com/opendatalab/OmniDocBench,https://arxiv.org/abs/2412.07626,"OmniDocBench 1.5 evaluates document understanding and OCR-style extraction across diverse layouts, including text, tables, formulas, and reading order. It focuses on accurate reconstruction of structured content from complex page images rather than only plain text transcription.","Visual Perception, Attention, Scene Understanding & Visual Reasoning, Language Comprehension, Working Memory, Visual Attention & Eye Movements"
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU extends multimodal reasoning to the temporal domain by asking questions that require understanding events, interactions, and state changes across video clips. It probes whether models can integrate information across frames and align it with language questions to produce coherent answers.","Visual Perception, Cognitive Timing & Predictive Modeling, Working Memory, Scene Understanding & Visual Reasoning, Multisensory Integration, Language Comprehension, Attention"
LiveCodeBench Pro,https://livecodebenchpro.com/projects/livecodebench-pro/leaderboard,https://arxiv.org/abs/2506.11928,"LiveCodeBench Pro evaluates code generation and software problem solving on a curated set of programming tasks with standardized execution-based grading. It emphasizes producing runnable, correct solutions (often under realistic constraints) and reflects iterative debugging skill when models must reason about failing outputs.","Planning, Logical Reasoning, Adaptive Error Correction, Working Memory, Language Comprehension, Language Production"
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"FACTS Benchmark Suite systematically evaluates factuality by testing whether model outputs remain consistent with evidence and avoid unsupported claims across multiple factuality-related tasks. It targets reliability behaviors such as resisting hallucination, tracking what is known versus uncertain, and maintaining grounded responses under varied prompts.","Semantic Understanding & Context Recognition, Logical Reasoning, Inhibitory Control, Self-reflection, Working Memory, Language Comprehension, Language Production"
SimpleQA Verified,https://www.kaggle.com/benchmarks/deepmind/simpleqa-verified,https://arxiv.org/abs/2509.07968,"SimpleQA Verified is a factual question-answering benchmark with verified ground truth intended to measure whether models can answer succinctly and correctly without embellishing or hallucinating. It is often used to assess precision on short, unambiguous queries and robustness against confident falsehoods.","Semantic Understanding & Context Recognition, Language Comprehension, Language Production, Inhibitory Control"
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA evaluates physical commonsense reasoning across languages, focusing on everyday interactions with objects and environments (e.g., what actions are plausible or effective in a physical situation). It tests whether models can generalize intuitive physics and affordance-like knowledge beyond English wording and cultural framing.","Semantic Understanding & Context Recognition, Logical Reasoning, Spatial Representation & Mapping, Language Comprehension"
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) is a long-context evaluation where multiple similar “needle” mentions are embedded in a long “haystack” of text, and the model must retrieve or reproduce the correct associated content for a specified needle. It stresses robust attention control, interference resistance among similar spans, and maintaining coherence across extended contexts.","Working Memory, Attention, Episodic Memory, Semantic Understanding & Context Recognition, Inhibitory Control"
