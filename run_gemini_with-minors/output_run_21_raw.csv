Benchmark,Website,Paper,Description,Cognitive Functions
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates software engineering agents on real GitHub issues by requiring them to generate patches that make a project’s tests pass. The “Verified” subset adds stronger task validity checks (e.g., solvable by humans, reliable evaluation) to reduce spurious passes and better reflect end-to-end debugging and code-change skill.","Planning, Logical Reasoning, Working Memory, Adaptive Error Correction, Language Comprehension, Language Production"
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 measures an agent’s ability to solve realistic tasks in a command-line environment, typically involving multi-step operations such as file manipulation, package/tool usage, debugging, and system inspection. It stresses reliable action sequencing under partial feedback, where intermediate mistakes must be detected and corrected to reach a working final state.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Language Comprehension"
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench evaluates interactive, tool-using customer-support agents in simulated domains (e.g., retail, airline, telecom) where the agent must follow policies while helping a user through multi-turn problems. It emphasizes robust dialogue state tracking, correct API usage, and handling adversarial or ambiguous user behavior without violating constraints.","Language Comprehension, Language Production, Planning, Decision-making, Inhibitory Control, Social Reasoning & Theory of Mind, Working Memory"
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI tests “fluid” abstraction and generalization by presenting a few input–output grid examples and asking models to infer the underlying rule to transform a new input grid. The tasks are designed to be novel and compositional, discouraging memorization and rewarding flexible hypothesis formation and rule induction.","Logical Reasoning, Cognitive Flexibility, Working Memory, Spatial Representation & Mapping, Visual Perception, Scene Understanding & Visual Reasoning"
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon autonomous performance in a simulated business setting where an agent manages a vending-machine operation over an extended period. Success requires maintaining coherent goals, optimizing inventory/pricing/supplier interactions, and adapting strategies as conditions change across many decisions.","Planning, Decision-making, Reward Mechanisms, Working Memory, Episodic Memory, Semantic Understanding & Context Recognition"
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a challenging, broad benchmark intended to probe frontier reasoning and knowledge across disciplines, often including multimodal questions and tasks requiring deep synthesis. It aims to reduce shortcut solutions by using difficult, diverse items that stress careful reasoning rather than shallow pattern matching.","Language Comprehension, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Visual Perception, Scene Understanding & Visual Reasoning, Multisensory Integration"
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 consists of competition-style mathematics problems that require multi-step symbolic reasoning, algebraic manipulation, and careful constraint tracking. Performance is sensitive to logical consistency and error checking across several intermediate steps rather than single-fact recall.","Logical Reasoning, Working Memory, Adaptive Error Correction, Planning"
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a difficult multiple-choice science benchmark curated to be “Google-proof,” emphasizing reasoning over rote retrieval. The Diamond subset targets high-quality questions where experts reliably agree on the correct answer and non-experts tend to fail, increasing the need for deep conceptual understanding.","Language Comprehension, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory"
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends broad academic knowledge testing to multiple languages, covering many subjects and requiring consistent performance across diverse linguistic contexts. It probes whether models preserve reasoning and domain knowledge when comprehension and expression shift across languages.","Language Comprehension, Language Production, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory"
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro evaluates multimodal expert-level understanding by combining images (e.g., diagrams, charts, figures) with questions spanning multiple disciplines. It stresses integrating visual evidence with text instructions and domain knowledge to perform high-level reasoning.","Visual Perception, Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Working Memory, Language Comprehension"
MathArena Apex,https://matharena.ai/?view=problem&comp=apex--apex_2025,https://arxiv.org/abs/2505.23281,"MathArena Apex is a hard mathematics evaluation suite designed to measure advanced problem-solving under competition-like conditions, often emphasizing complex, multi-stage derivations. It is used to compare models’ mathematical reasoning robustness across a curated set of difficult items.","Logical Reasoning, Working Memory, Planning, Adaptive Error Correction"
ScreenShot-Pro,https://gui-agent.github.io/grounding-leaderboard/,https://arxiv.org/abs/2504.07981,"ScreenShot-Pro evaluates how well models understand and act on information embedded in UI screenshots, such as locating relevant widgets, interpreting interface state, and answering grounded questions. It targets vision-language grounding and spatial layout understanding common in computer-use agents.","Visual Perception, Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Spatial Representation & Mapping, Language Comprehension, Working Memory"
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning focuses on scientific figure and document-based visual reasoning, requiring models to interpret plots, equations, and structured layouts typical of research papers. Tasks emphasize extracting correct evidence from visuals and performing reasoning steps that link figure content to the question.","Visual Perception, Scene Understanding & Visual Reasoning, Logical Reasoning, Language Comprehension, Working Memory, Semantic Understanding & Context Recognition"
OmniDocBench 1.5,https://github.com/opendatalab/OmniDocBench,https://arxiv.org/abs/2412.07626,"OmniDocBench 1.5 evaluates document understanding and OCR-style extraction across heterogeneous elements such as text blocks, formulas, tables, and reading order. It measures whether a model can accurately parse structured documents and preserve semantics when converting from visual layouts to machine-readable text.","Visual Perception, Scene Understanding & Visual Reasoning, Attention, Working Memory, Language Production, Language Comprehension, Semantic Understanding & Context Recognition"
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU extends multimodal understanding to the temporal domain by asking questions about videos that require integrating cues across frames. It stresses tracking events, actions, and state changes over time rather than relying on a single keyframe.","Visual Perception, Cognitive Timing & Predictive Modeling, Working Memory, Scene Understanding & Visual Reasoning, Multisensory Integration, Attention"
LiveCodeBench Pro,https://livecodebenchpro.com/projects/livecodebench-pro/leaderboard,https://arxiv.org/abs/2506.11928,"LiveCodeBench Pro evaluates coding ability on time-sensitive, execution-checked programming tasks intended to reduce contamination and reflect realistic development constraints. It rewards producing correct, runnable solutions and iteratively fixing errors under a single-attempt or limited-attempt evaluation regime.","Logical Reasoning, Planning, Adaptive Error Correction, Working Memory, Language Comprehension, Language Production"
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"FACTS Benchmark Suite systematically evaluates LLM factuality, including whether models produce statements supported by evidence and avoid hallucinations across different settings and prompt styles. It targets reliability in knowledge-intensive responses, emphasizing calibration and resisting unsupported generation.","Semantic Understanding & Context Recognition, Inhibitory Control, Logical Reasoning, Working Memory, Language Comprehension, Language Production"
SimpleQA Verified,https://www.kaggle.com/benchmarks/deepmind/simpleqa-verified,https://arxiv.org/abs/2509.07968,SimpleQA Verified measures short-form factual question answering with verification-oriented ground truth and scoring designed to penalize unsupported claims. It focuses on precision in direct answers and the ability to refrain from guessing when uncertain.,"Language Comprehension, Semantic Understanding & Context Recognition, Inhibitory Control, Working Memory, Language Production"
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,Global PIQA evaluates commonsense physical reasoning across languages and cultural contexts by testing which actions or explanations are plausible in everyday physical situations. It probes whether models maintain practical intuitive physics and commonsense priors when the linguistic surface form varies.,"Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning, Cognitive Flexibility"
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) is a long-context retrieval and multi-round coreference task where multiple similar “needle” requests are embedded in a large “haystack,” and the model must recover the correct referenced response. It stresses sustained attention, interference resistance, and accurate retrieval across long spans of context.","Working Memory, Attention, Episodic Memory, Language Comprehension, Inhibitory Control"
