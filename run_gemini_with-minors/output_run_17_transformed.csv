Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates software-engineering agents by asking them to fix real issues in open-source repositories and submit a patch that passes the project’s tests. The “Verified” split contains tasks validated by human review to be solvable and correctly specified, emphasizing end-to-end debugging and codebase navigation rather than isolated coding puzzles.","L1: Language Comprehension
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory, Logical Reasoning
L3: ",L2
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 measures an agent’s ability to accomplish real command-line tasks in a sandboxed terminal, typically requiring iterative tool use (shell commands), environment inspection, and corrective retries. It stresses reliability under action–observation loops where mistakes have concrete consequences and must be recovered from.","L1: 
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory
L3: Inhibitory Control",L3
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench evaluates interactive customer-support agents operating in simulated domains (e.g., retail, airline, telecom) that must follow policies while using tools/APIs over multi-turn dialogues. Success requires maintaining conversation state, handling constraints and exceptions, and coordinating tool calls to resolve user requests.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Working Memory
L3: Social Reasoning & Theory of Mind, Inhibitory Control",L3
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,ARC-AGI tests “fluid” abstract reasoning via grid-based pattern transformation problems where only a few input–output examples are provided and the model must infer the rule for a new input. It is designed to reduce reliance on memorized content and emphasize generalization to novel structures.,"L1: Visual Perception
L2: Logical Reasoning, Spatial Representation & Mapping, Scene Understanding & Visual Reasoning, Working Memory
L3: Cognitive Flexibility",L3
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon agent coherence in a simulated vending-machine business over an extended period, with goals like maximizing profit while managing inventory, suppliers, and pricing. It captures strategic tradeoffs, delayed consequences, and the need to maintain consistent plans across many steps.","L1: 
L2: Planning, Decision-making, Reward Mechanisms, Working Memory, Adaptive Error Correction
L3: Motivational Drives",L3
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam (HLE) is a difficult multimodal benchmark intended to probe frontier-level knowledge and reasoning across diverse expert domains. Questions often require multi-step inference, careful interpretation of provided context (including images), and robust handling of uncertainty.","L1: Language Comprehension, Visual Perception
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Scene Understanding & Visual Reasoning
L3: ",L2
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 evaluates competition-style mathematics problem solving, typically requiring multi-step derivations, algebraic manipulation, and careful case analysis. The benchmark emphasizes correctness under time-like constraints (single-shot solving) rather than interactive tool use.","L1: 
L2: Logical Reasoning, Working Memory, Planning
L3: Inhibitory Control",L3
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a high-quality subset of graduate-level, “Google-proof” multiple-choice science questions designed to be hard for non-experts and to discourage shallow pattern matching. It stresses rigorous scientific reasoning and precise understanding of technical language.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory
L3: Inhibitory Control",L3
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends broad academic knowledge testing into multiple non-English languages, covering many subjects in a multiple-choice format. It emphasizes multilingual comprehension and the ability to apply stored knowledge and reasoning consistently across languages.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory
L3: Cognitive Flexibility",L3
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,MMMU-Pro evaluates expert-level multimodal understanding across many disciplines using images paired with questions that require reasoning (often beyond literal perception). It targets stronger visual reasoning and reduced shortcutting compared with earlier multimodal multiple-choice settings.,"L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Working Memory
L3: ",L2
MathArena Apex,https://matharena.ai/?view=problem&comp=apex--apex_2025,https://arxiv.org/abs/2505.23281,MathArena Apex is a challenging math-reasoning evaluation focused on difficult problems that require long chains of deduction and robust intermediate correctness. It is designed to differentiate top models on advanced mathematical reasoning rather than basic computation.,"L1: 
L2: Logical Reasoning, Working Memory, Planning, Adaptive Error Correction
L3: Inhibitory Control",L3
ScreenShot-Pro,https://gui-agent.github.io/grounding-leaderboard/,https://arxiv.org/abs/2504.07981,"ScreenShot-Pro evaluates screenshot-based GUI understanding, where a model must interpret high-resolution interface images and answer questions or select actions grounded in the visual layout. It probes fine-grained spatial grounding (buttons/menus/fields) and the ability to map visual evidence to a correct response.","L1: Visual Perception
L2: Visual Attention & Eye Movements, Spatial Representation & Mapping, Scene Understanding & Visual Reasoning, Decision-making, Working Memory
L3: ",L2
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning tests scientific figure understanding and reasoning over charts/figures drawn from research contexts, often requiring reading plotted relationships and integrating them with textual prompts. It rewards accurate interpretation of visual evidence and quantitative/causal reasoning based on that evidence.","L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Logical Reasoning, Working Memory
L3: ",L2
OmniDocBench 1.5,https://github.com/opendatalab/OmniDocBench,https://arxiv.org/abs/2412.07626,"OmniDocBench 1.5 evaluates document understanding and OCR-centric extraction over diverse layouts (text, tables, formulas, and reading order). It targets robustness to real-world document formatting where correct structure and sequence matter, not just raw text recognition.","L1: Visual Perception, Language Comprehension
L2: Attention, Scene Understanding & Visual Reasoning, Working Memory
L3: ",L2
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU evaluates multimodal understanding and reasoning over video, requiring models to integrate information across frames and answer questions that depend on temporal context. It stresses tracking entities/events over time and combining visual cues with textual reasoning.","L1: Visual Perception
L2: Working Memory, Scene Understanding & Visual Reasoning, Multisensory Integration, Attention
L3: Cognitive Timing & Predictive Modeling",L3
LiveCodeBench Pro,https://livecodebenchpro.com/projects/livecodebench-pro/leaderboard,https://arxiv.org/abs/2506.11928,"LiveCodeBench Pro evaluates coding ability on curated programming tasks with an emphasis on realistic, contemporary problems and reduced contamination. It typically measures single-attempt solution quality and can reflect an agent’s capacity to reason about specs, implement correct algorithms, and debug logic.","L1: Language Comprehension
L2: Planning, Logical Reasoning, Working Memory, Adaptive Error Correction
L3: Inhibitory Control",L3
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"FACTS Benchmark Suite systematically evaluates factuality by probing whether model outputs remain grounded and correct across varied query types and failure modes. It emphasizes resisting hallucinations, maintaining consistency under pressure, and producing verifiable statements rather than plausible-sounding text.","L1: Language Comprehension, Language Production
L2: Semantic Understanding & Context Recognition, Working Memory
L3: Inhibitory Control, Self-reflection",L3
SimpleQA Verified,https://www.kaggle.com/benchmarks/deepmind/simpleqa-verified,https://arxiv.org/abs/2509.07968,"SimpleQA Verified is a short-form factual question answering benchmark with verification/ground-truth checks, designed to measure whether models can produce correct, concise answers without confabulation. It is used to quantify factual accuracy under minimal prompting and limited room for hedging.","L1: Language Comprehension, Language Production
L2: Semantic Understanding & Context Recognition, Episodic Memory
L3: Inhibitory Control",L3
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA evaluates pragmatic and commonsense reasoning across many languages and cultural contexts, aiming to test whether models generalize everyday physical and social intuitions beyond English-centric distributions. It probes robustness to linguistic variation while keeping the underlying scenario constant.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning
L3: Cognitive Flexibility, Social Reasoning & Theory of Mind",L3
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) tests long-context, multi-round coreference and retrieval by inserting multiple similar “needle” interactions into long conversational or document “haystacks,” then asking for the response tied to a specific needle. It stresses maintaining and selecting the correct reference under heavy distraction and long-range dependencies.","L1: Language Comprehension
L2: Working Memory, Attention, Episodic Memory, Logical Reasoning
L3: Inhibitory Control",L3
