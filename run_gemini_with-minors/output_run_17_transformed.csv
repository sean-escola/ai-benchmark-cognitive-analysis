Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro evaluates LLM software-engineering agents on a large set of real GitHub issues that require producing correct patches in real repositories under test. Tasks typically involve understanding codebases, localizing bugs or implementing features, editing multiple files, and passing project-specific test suites, emphasizing robustness and contamination resistance relative to earlier SWE-bench variants.","L1: Language Comprehension
L2: Logical Reasoning, Planning, Working Memory, Adaptive Error Correction, Decision-making
L3: ",L2
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld measures an agent’s ability to use a computer operating system to complete end-to-end tasks (e.g., file management, web actions, app interactions) from pixel-based observations. Success requires interpreting GUI state, choosing sequences of actions, and recovering from mistakes over multi-step episodes.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Attention, Planning, Decision-making, Sensorimotor Coordination, Working Memory, Adaptive Error Correction
L3: ",L2
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI tests few-shot abstract reasoning with grid-based puzzles where an agent must infer a hidden rule from a small number of input–output examples and generalize to a new input. It is designed to emphasize novelty and rule induction rather than memorized knowledge, with performance sensitive to flexible pattern discovery.","L1: Visual Perception
L2: Logical Reasoning, Working Memory, Scene Understanding & Visual Reasoning, Spatial Representation & Mapping, Attention
L3: Cognitive Flexibility",L3
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon agent coherence by simulating operation of a vending-machine business over an extended period (e.g., a year) with many sequential decisions. Agents must manage inventory, pricing, supplier negotiation, and cash flow, requiring strategic adaptation to changing conditions and compounding outcomes.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Working Memory, Reward Mechanisms, Adaptive Error Correction
L3: Self-reflection",L3
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas evaluates real-world tool use via the Model Context Protocol, requiring models to discover tools, call them correctly, and compose multi-step workflows across services. Tasks stress API understanding, argument construction, error handling, and synthesis of tool outputs into correct final responses.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Working Memory, Adaptive Error Correction, Logical Reasoning
L3: ",L2
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym measures agentic cybersecurity capability at scale, including identifying known vulnerabilities from descriptions and discovering new issues in real open-source projects. The benchmark emphasizes precise code comprehension, vulnerability reasoning, and producing actionable findings under realistic constraints.","L1: Language Comprehension
L2: Logical Reasoning, Planning, Working Memory, Adaptive Error Correction, Decision-making
L3: ",L2
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam (HLE) is a frontier benchmark of difficult questions spanning expert knowledge and reasoning, including multimodal items. It targets complex problem solving under uncertainty and often benefits from tool use (e.g., search or code execution) when permitted, mirroring demanding real-world inquiry.","L1: Language Comprehension, Visual Perception
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Scene Understanding & Visual Reasoning, Planning
L3: ",L2
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a high-quality subset of graduate-level, “Google-proof” multiple-choice science questions where shallow pattern matching performs poorly. It stresses deep scientific reasoning, careful reading, and selecting the best supported answer among plausible distractors.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory
L3: Inhibitory Control",L3
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro is a multimodal benchmark focused on expert-level understanding and reasoning across many disciplines using text and images. Compared with earlier MMMU settings, it places greater emphasis on robust visual–text grounding, diagram/chart interpretation, and harder reasoning questions.","L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Working Memory, Attention
L3: ",L2
OmniDocBench 1.5,https://github.com/opendatalab/OmniDocBench,https://arxiv.org/abs/2412.07626,"OmniDocBench 1.5 evaluates document understanding and OCR across diverse layouts and content types (text, tables, formulas, reading order). It emphasizes faithful extraction and structural reconstruction from visually complex documents, reflecting real-world document digitization and analysis workloads.","L1: Visual Perception, Language Comprehension
L2: Visual Attention & Eye Movements, Attention, Working Memory, Scene Understanding & Visual Reasoning
L3: ",L2
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU extends multimodal understanding to video, requiring temporal grounding and reasoning over events, actions, and visual evidence across frames. It tests whether models can integrate information across time rather than relying on single-frame cues.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Multisensory Integration, Working Memory, Attention
L3: Cognitive Timing & Predictive Modeling",L3
LiveCodeBench Pro,https://livecodebenchpro.com/projects/livecodebench-pro/leaderboard,https://arxiv.org/abs/2506.11928,"LiveCodeBench Pro evaluates coding ability on competitive-programming-style tasks with an emphasis on recency and leakage resistance, typically requiring executable solutions rather than explanations. It probes algorithm design, implementation correctness, and iterative debugging under time/efficiency constraints.","L1: Language Comprehension
L2: Logical Reasoning, Planning, Working Memory, Adaptive Error Correction, Decision-making
L3: ",L2
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"FACTS Benchmark Suite measures factuality and grounding behavior across a broad set of tasks designed to stress truthfulness, citation/attribution practices, and resistance to confabulation. It focuses on whether model outputs remain consistent with provided sources or established facts, especially under adversarial or ambiguous prompts.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory
L3: Inhibitory Control, Self-reflection",L3
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA evaluates physical commonsense reasoning across many languages and cultural contexts, using non-parallel multilingual items to reduce translation artifacts. It tests whether models can choose plausible actions or explanations grounded in everyday physics and affordances beyond English-centric priors.","L1: Language Comprehension
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory
L3: Cognitive Flexibility",L3
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) evaluates long-context multi-round coreference resolution by embedding multiple similar “needle” interactions in long “haystacks” and asking the model to reproduce the correct referenced response. The 8-needle setting stresses sustained context tracking, interference resistance, and precise retrieval under heavy distractors.","L1: Language Comprehension
L2: Working Memory, Attention, Semantic Understanding & Context Recognition, Episodic Memory
L3: Inhibitory Control",L3
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval evaluates economically meaningful, well-specified professional knowledge-work tasks across many occupations, often requiring creation of polished artifacts (e.g., spreadsheets, presentations, plans). Scoring is based on human-judge comparisons against professional work, emphasizing end-to-end usefulness rather than narrow Q&A accuracy.","L1: Language Production, Language Comprehension
L2: Planning, Decision-making, Working Memory
L3: Self-reflection, Social Reasoning & Theory of Mind",L3
SWE-Lancer,https://openai.com/index/swe-lancer/,https://arxiv.org/abs/2502.12115,"SWE-Lancer evaluates software-engineering agents on longer-horizon and more autonomous development workflows than typical patch-only benchmarks. It emphasizes maintaining task context across steps, making and validating design choices, and iteratively improving solutions in realistic repo settings.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Working Memory, Adaptive Error Correction, Logical Reasoning
L3: ",L2
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath evaluates advanced mathematics problem solving at the research frontier, including problems intended to be difficult for current models even with tools. It emphasizes multi-step derivations, rigorous reasoning, and error-sensitive symbolic manipulation over superficial pattern matching.","L1: 
L2: Logical Reasoning, Working Memory, Planning, Adaptive Error Correction, Attention
L3: ",L2
