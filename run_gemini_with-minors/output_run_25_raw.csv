Benchmark,Website,Paper,Description,Cognitive Functions
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro evaluates real-world software engineering by giving a model a repository, an issue description, and requiring it to produce a correct patch that passes tests. Compared to earlier SWE-bench variants, it is designed to be more challenging and more representative of professional, multi-language engineering work under realistic constraints.","Language Comprehension, Language Production, Logical Reasoning, Planning, Working Memory, Adaptive Error Correction, Decision-making"
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld evaluates computer-use agents on end-to-end tasks inside a real operating system environment (e.g., browsing, file operations, app interactions) using screenshots and GUI actions. Success requires interpreting interface state, selecting correct UI operations over many steps, and recovering from action or perception errors.","Visual Perception, Scene Understanding & Visual Reasoning, Attention, Spatial Representation & Mapping, Planning, Decision-making, Sensorimotor Coordination, Working Memory"
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,ARC-AGI measures fluid reasoning via novel grid transformation puzzles where only a few input-output examples are provided and the model must infer the hidden rule. It is designed to emphasize generalization to unfamiliar tasks rather than memorization of domain knowledge.,"Logical Reasoning, Working Memory, Cognitive Flexibility, Attention, Planning, Scene Understanding & Visual Reasoning, Spatial Representation & Mapping"
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon autonomous agent behavior by having a model run a simulated vending machine business over an extended period, making thousands of operational and strategic decisions. It rewards sustained coherence, adaptive strategy, and effective interaction with simulated stakeholders (e.g., suppliers) to maximize final balance.","Planning, Decision-making, Working Memory, Episodic Memory, Reward Mechanisms, Motivational Drives, Language Comprehension, Language Production, Social Reasoning & Theory of Mind"
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas evaluates real-world tool use through the Model Context Protocol (MCP), requiring models to discover tools, call them with correct arguments, and compose multi-step workflows. Tasks stress robustness to tool errors, retries, and cross-tool integration in production-like environments.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Attention, Language Comprehension, Language Production"
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym benchmarks agentic cybersecurity skill by testing whether models can locate known vulnerabilities in real open-source projects from a weakness description and, in some settings, discover new vulnerabilities. It emphasizes structured investigation across codebases and precise reasoning about program behavior and security properties.","Logical Reasoning, Planning, Adaptive Error Correction, Working Memory, Attention, Language Comprehension, Decision-making"
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a difficult, frontier-level benchmark spanning broad academic and professional knowledge, including multimodal questions, often intended to stress test models near the limits of current capability. Variants may allow tool use (e.g., search, code execution), testing whether models can orchestrate reasoning and tools without losing reliability.","Language Comprehension, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Visual Perception, Scene Understanding & Visual Reasoning, Planning, Decision-making"
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,GPQA Diamond is a high-quality subset of extremely challenging graduate-level science multiple-choice questions designed to be resistant to shallow pattern matching and simple web search. It probes whether a model can integrate scientific knowledge with multi-step reasoning to select the best answer under distractors.,"Language Comprehension, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Attention"
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro evaluates expert-level multimodal understanding and reasoning across many disciplines using images (e.g., diagrams, plots, scientific figures) paired with text questions. It stresses grounded visual reasoning and the ability to combine information from multiple modalities to reach a correct choice or explanation.","Visual Perception, Scene Understanding & Visual Reasoning, Multisensory Integration, Language Comprehension, Logical Reasoning, Working Memory, Attention"
OmniDocBench 1.5,https://github.com/opendatalab/OmniDocBench,https://arxiv.org/abs/2412.07626,"OmniDocBench 1.5 evaluates document AI systems on end-to-end understanding of complex documents, including OCR text accuracy, table structure, formula recognition, and reading order. It tests whether a model can preserve layout and structure while converting visual documents into faithful, usable representations.","Visual Perception, Scene Understanding & Visual Reasoning, Spatial Representation & Mapping, Attention, Language Comprehension, Working Memory"
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU extends multimodal evaluation to video, requiring models to answer questions that depend on temporal events, actions, and context across frames. It probes whether a model can integrate information over time and maintain a coherent representation of evolving scenes.","Visual Perception, Scene Understanding & Visual Reasoning, Working Memory, Attention, Cognitive Timing & Predictive Modeling, Multisensory Integration"
LiveCodeBench Pro,https://livecodebenchpro.com/projects/livecodebench-pro/leaderboard,https://arxiv.org/abs/2506.11928,"LiveCodeBench Pro evaluates competitive-programming-style code generation and problem solving under conditions meant to reflect modern, continuously-updated coding challenges. It emphasizes algorithmic reasoning, producing correct runnable code, and correcting failures when initial approaches break on hidden tests.","Logical Reasoning, Planning, Working Memory, Adaptive Error Correction, Language Comprehension, Language Production, Decision-making"
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"FACTS Benchmark Suite systematically evaluates LLM factuality across multiple subtests, focusing on whether outputs remain grounded, accurate, and appropriately qualified when knowledge is uncertain. It targets hallucination-related failures by checking fine-grained factual consistency and calibration behaviors.","Language Comprehension, Semantic Understanding & Context Recognition, Inhibitory Control, Self-reflection, Working Memory, Attention"
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA tests physical commonsense and everyday practical reasoning across many languages, using non-parallel multilingual data to reduce direct translation artifacts. The benchmark probes whether models can infer plausible actions and outcomes in the physical world using culturally and linguistically varied prompts.","Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory"
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) is a long-context evaluation where multiple similar “needle” requests are embedded within a long “haystack” of dialogue, and the model must retrieve the correct response for a specified needle. The 8-needle setting stresses attention control and interference resistance across long documents or conversations.","Working Memory, Attention, Language Comprehension, Episodic Memory, Semantic Understanding & Context Recognition"
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval evaluates economically meaningful, well-specified knowledge work across many occupations by judging the quality of produced work artifacts (e.g., analyses, spreadsheets, plans) against expert human performance. It emphasizes end-to-end task execution quality, including following constraints, producing usable outputs, and making correct tradeoffs.","Planning, Decision-making, Language Production, Language Comprehension, Working Memory, Self-reflection, Adaptive Error Correction, Social Reasoning & Theory of Mind"
SWE-Lancer,https://openai.com/index/swe-lancer/,https://arxiv.org/abs/2502.12115,"SWE-Lancer evaluates agentic software engineering on realistic tasks that require interacting with a codebase over multiple steps to deliver high-quality changes (often beyond single-file edits). It stresses sustained problem solving, iterative debugging, and aligning solutions with project conventions and requirements.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Language Comprehension, Language Production, Logical Reasoning, Attention"
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath is an expert-level mathematics benchmark designed to test difficult problem solving beyond standard contest-style datasets, often requiring deeper multi-step derivations and careful verification. It probes whether models can maintain long chains of reasoning and avoid subtle logical or arithmetic errors under high complexity.","Logical Reasoning, Working Memory, Planning, Adaptive Error Correction, Attention, Decision-making"
