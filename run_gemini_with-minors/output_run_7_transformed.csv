Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro is a large-scale software engineering benchmark where a model is given a real code repository plus an issue description and must generate a patch that fixes the problem. It emphasizes realistic debugging and code changes that pass tests across multiple projects and (in the Pro variant) multiple programming languages, with an emphasis on higher difficulty and contamination resistance.","L1: Language Comprehension, Language Production
L2: Logical Reasoning, Planning, Working Memory, Adaptive Error Correction
L3: ",L2
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld evaluates multimodal “computer use” agents on completing tasks inside a real operating-system desktop environment by interacting with GUIs (e.g., clicking, typing, navigating apps). Success requires perceiving screen state, choosing multi-step actions, and recovering from errors under step and time constraints.","L1: Visual Perception
L2: Visual Attention & Eye Movements, Attention, Planning, Decision-making, Sensorimotor Coordination, Working Memory, Adaptive Error Correction
L3: ",L2
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI measures fluid, novel pattern reasoning using small grid-based input–output examples where the rule must be inferred from only a few demonstrations. It is designed to emphasize generalization to new abstract transformations rather than memorized domain knowledge.","L1: 
L2: Logical Reasoning, Working Memory, Planning, Spatial Representation & Mapping, Scene Understanding & Visual Reasoning, Attention
L3: Cognitive Flexibility",L3
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon agent coherence and strategy in a simulated vending-machine business run over many steps (e.g., pricing, inventory, supplier negotiation, responding to events). The metric is typically profit/final balance, rewarding sustained planning, adaptation, and consistent execution across time.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Reward Mechanisms, Working Memory, Episodic Memory, Adaptive Error Correction
L3: Cognitive Flexibility",L3
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas measures real-world tool use through the Model Context Protocol (MCP), where models must discover tools, invoke them correctly, and chain multi-step workflows across services. Tasks stress reliable API interaction, argument formation, error handling, and synthesis of tool outputs into a final response.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Working Memory, Adaptive Error Correction, Semantic Understanding & Context Recognition
L3: ",L2
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity agent capabilities on tasks that include identifying known vulnerabilities from descriptions and discovering new vulnerabilities in real open-source codebases. It probes an agent’s ability to reason about program behavior, execute investigative steps, and produce correct exploit-relevant findings or patches under constraints.","L1: Language Comprehension, Language Production
L2: Logical Reasoning, Planning, Decision-making, Working Memory, Adaptive Error Correction
L3: ",L2
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam (HLE) is a broad, frontier-oriented benchmark spanning advanced academic and professional questions, often including multimodal items. It aims to measure integrated reasoning and knowledge under challenging, long-form question styles, sometimes in tool-assisted settings depending on the evaluation protocol.","L1: Language Comprehension, Language Production, Visual Perception
L2: Logical Reasoning, Working Memory, Semantic Understanding & Context Recognition, Scene Understanding & Visual Reasoning
L3: ",L2
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a curated subset of very difficult multiple-choice science questions intended to be “Google-proof,” emphasizing reasoning rather than straightforward lookup. The Diamond split focuses on the highest-quality items where experts reliably answer correctly while non-experts often fail.","L1: Language Comprehension
L2: Logical Reasoning, Working Memory, Semantic Understanding & Context Recognition, Decision-making
L3: ",L2
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro is a challenging multimodal understanding and reasoning benchmark across many disciplines, where problems require combining text with images such as diagrams, charts, and scientific figures. The Pro variant is designed to be harder and more diagnostic of expert-level multimodal reasoning than earlier MMMU settings.","L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Working Memory, Attention
L3: ",L2
OmniDocBench 1.5,https://github.com/opendatalab/OmniDocBench,https://arxiv.org/abs/2412.07626,"OmniDocBench 1.5 evaluates document understanding/OCR systems on complex documents that include mixed text, tables, formulas, and layout/reading-order structure. It tests whether models can faithfully extract and structure information from visually rich documents rather than only recognizing plain text.","L1: Visual Perception, Language Comprehension, Language Production
L2: Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Attention, Working Memory, Spatial Representation & Mapping
L3: ",L2
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU evaluates multimodal reasoning over videos paired with text questions, requiring understanding of events, temporal dependencies, and visual details across frames. It targets robust video understanding beyond static images, including multi-step inference grounded in what occurs over time.","L1: Visual Perception, Language Comprehension
L2: Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Multisensory Integration, Working Memory, Attention, Logical Reasoning
L3: Cognitive Timing & Predictive Modeling",L3
LiveCodeBench Pro,https://livecodebenchpro.com/projects/livecodebench-pro/leaderboard,https://arxiv.org/abs/2506.11928,"LiveCodeBench Pro benchmarks coding ability on recently created programming tasks with executable tests, aiming to reduce contamination and better reflect contemporary developer work. It emphasizes producing correct, runnable code under realistic constraints and is often reported via pass rates or an Elo-style score.","L1: Language Comprehension, Language Production
L2: Logical Reasoning, Planning, Working Memory, Adaptive Error Correction
L3: ",L2
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"The FACTS Benchmark Suite evaluates factuality-related behaviors such as making supported claims, resisting hallucination, and maintaining consistency under prompts that can induce errors. It aggregates multiple factuality tests to provide a more systematic view of truthfulness and grounding than a single QA metric.","L1: Language Comprehension, Language Production
L2: Semantic Understanding & Context Recognition, Working Memory, Logical Reasoning, Attention
L3: Inhibitory Control, Self-reflection",L3
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA evaluates physical commonsense reasoning across many languages, focusing on whether models can select plausible actions/solutions in everyday scenarios without relying on English-only cues. The non-parallel multilingual design aims to measure robust commonsense generalization across linguistic and cultural contexts.","L1: Language Comprehension
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Decision-making, Working Memory
L3: ",L2
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) is a long-context retrieval and multi-round coreference test where multiple similar “needle” interactions are embedded in a long “haystack,” and the model must reproduce the correct referenced content. The 8-needle setting stresses maintaining and selecting the right context among many confusable candidates.","L1: 
L2: Working Memory, Episodic Memory, Attention, Semantic Understanding & Context Recognition, Logical Reasoning
L3: Inhibitory Control",L3
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval evaluates well-specified professional knowledge work across many occupations by having models produce real deliverables (e.g., spreadsheets, slide decks, plans) and comparing them against expert human work via judging. It is intended to measure economically relevant end-to-end task performance rather than isolated QA.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Working Memory, Semantic Understanding & Context Recognition, Adaptive Error Correction
L3: Self-reflection",L3
SWE-Lancer,https://openai.com/index/swe-lancer/,https://arxiv.org/abs/2502.12115,"SWE-Lancer evaluates software engineering agents on longer, more open-ended tasks that resemble real developer workflows, often requiring navigating large codebases, making multi-file changes, and validating results. It stresses end-to-end execution quality, including choosing what to change, implementing it correctly, and iterating when tests or checks fail.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Logical Reasoning, Working Memory, Adaptive Error Correction
L3: ",L2
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath is an advanced mathematics benchmark targeting difficult, research-adjacent problems with careful curation to reduce memorization and emphasize genuine reasoning. It is often analyzed by tiers of difficulty and may be run with or without computational tools, highlighting the boundary between symbolic reasoning and computation-assisted solving.","L1: 
L2: Logical Reasoning, Working Memory, Planning, Adaptive Error Correction, Semantic Understanding & Context Recognition
L3: Cognitive Flexibility",L3
