Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates LLM-based coding agents on real GitHub issues by requiring them to generate patches that make a repository’s tests pass. The “Verified” variant filters to tasks confirmed by human annotators to be solvable and to have reliable evaluation signals, reducing noise from flaky tasks and underspecified bugs.","L1: Language Comprehension, Language Production
L2: Planning, Logical Reasoning, Working Memory, Adaptive Error Correction
L3: ",L2
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 measures an agent’s ability to complete practical tasks in a command-line environment (e.g., installing dependencies, manipulating files, running programs, debugging failures). It emphasizes iterative interaction, tool use via shell commands, and recovering from errors under realistic resource and environment constraints.","L1: Language Comprehension
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory
L3: ",L2
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench evaluates interactive agent performance in multi-turn, tool-using customer support scenarios (e.g., retail, airline, telecom) where models must follow domain policies while using APIs and conversation to resolve user needs. Success requires balancing helpfulness with constraint adherence over long dialogues with simulated users and systems.","L1: Language Comprehension, Language Production
L2: Decision-making, Planning, Working Memory
L3: Social Reasoning & Theory of Mind, Inhibitory Control",L3
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,ARC-AGI tests fluid reasoning by presenting a few input–output grid examples that follow an unknown transformation rule and asking the model to infer the correct output for a new input. It is designed to minimize reliance on memorized knowledge and instead probe novel rule induction from sparse demonstrations.,"L1: Visual Perception
L2: Logical Reasoning, Working Memory, Spatial Representation & Mapping, Scene Understanding & Visual Reasoning
L3: Cognitive Flexibility",L3
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 assesses long-horizon autonomous agency by simulating operation of a vending machine business over an extended period, including inventory, pricing, supplier interactions, and adapting to market dynamics. Performance is typically scored by business outcomes (e.g., final balance) that depend on sustained coherent strategy and execution.","L1: 
L2: Planning, Decision-making, Reward Mechanisms, Working Memory, Semantic Understanding & Context Recognition
L3: Social Reasoning & Theory of Mind",L3
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam (HLE) is a multimodal benchmark intended to probe frontier-level academic reasoning and knowledge across many subjects, often requiring multi-step inference rather than recall. It includes difficult questions that may combine text with figures or other modalities and is commonly evaluated with strict grading protocols to reduce spurious correctness.","L1: Language Comprehension, Visual Perception
L2: Logical Reasoning, Working Memory, Semantic Understanding & Context Recognition, Scene Understanding & Visual Reasoning
L3: ",L2
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 comprises competition-style mathematics problems that require constructing and executing multi-step solution paths under tight correctness criteria. It is frequently used to evaluate symbolic manipulation, quantitative reasoning, and robustness of stepwise derivations without tool assistance.","L1: 
L2: Logical Reasoning, Working Memory, Planning, Adaptive Error Correction
L3: ",L2
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a high-quality subset of graduate-level, “Google-proof” multiple-choice science questions designed to be difficult for non-experts while solvable by domain experts. It targets deep conceptual understanding and careful reasoning under distractors rather than surface-level pattern matching.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory
L3: ",L2
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends broad academic knowledge testing to multiple languages, evaluating whether models can understand questions and answer choices across diverse subjects beyond English. It probes cross-lingual robustness and whether reasoning and knowledge transfer remain stable under multilingual phrasing and cultural context.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory
L3: ",L2
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro is a professional-grade multimodal benchmark that tests expert-level understanding and reasoning over images paired with text questions across many disciplines (e.g., charts, diagrams, scientific visuals). It emphasizes fine-grained perception plus multi-step reasoning to integrate visual evidence with domain knowledge.","L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Working Memory, Visual Attention & Eye Movements
L3: ",L2
MathArena Apex,https://matharena.ai/?view=problem&comp=apex--apex_2025,https://arxiv.org/abs/2505.23281,"MathArena Apex aggregates very challenging mathematics problems intended to stress test advanced mathematical reasoning beyond standard contest sets. It is used to compare models’ ability to sustain long derivations, avoid subtle logical slips, and maintain correctness under higher difficulty distributions.","L1: 
L2: Logical Reasoning, Planning, Working Memory, Adaptive Error Correction
L3: ",L2
ScreenShot-Pro,https://gui-agent.github.io/grounding-leaderboard/,https://arxiv.org/abs/2504.07981,"ScreenShot-Pro evaluates GUI understanding from screenshots by asking models to identify interface elements, interpret layouts, and (in agentic settings) decide where to click/type to accomplish goals. It targets precise visual grounding—mapping textual instructions to pixel-level UI affordances and spatial relationships.","L1: Visual Perception
L2: Visual Attention & Eye Movements, Spatial Representation & Mapping, Scene Understanding & Visual Reasoning, Planning, Sensorimotor Coordination
L3: ",L2
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning focuses on scientific figure and chart reasoning, often requiring extracting quantitative and qualitative signals from plots, tables, and diagrams and then answering reasoning-heavy questions. It tests whether a model can correctly interpret visual evidence and connect it to scientific context and constraints.","L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory
L3: ",L2
OmniDocBench 1.5,https://github.com/opendatalab/OmniDocBench,https://arxiv.org/abs/2412.07626,"OmniDocBench 1.5 evaluates document understanding and OCR robustness across heterogeneous document components such as text blocks, formulas, tables, and reading order. It stresses accurate recognition plus structural reconstruction, where small perception errors can cascade into large semantic mistakes.","L1: Visual Perception, Language Comprehension
L2: Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Working Memory
L3: ",L2
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU measures multimodal understanding and reasoning over videos, typically requiring tracking entities and events across time and integrating them with text questions. It emphasizes temporal coherence and the ability to retain and use information that is only briefly visible in earlier frames.","L1: Visual Perception
L2: Working Memory, Scene Understanding & Visual Reasoning, Multisensory Integration, Attention
L3: Cognitive Timing & Predictive Modeling",L3
LiveCodeBench Pro,https://livecodebenchpro.com/projects/livecodebench-pro/leaderboard,https://arxiv.org/abs/2506.11928,"LiveCodeBench Pro evaluates coding ability on programming tasks with execution-based checking, emphasizing whether produced code actually runs and solves the specified problem. It is designed to reflect practical coding performance, including debugging and iterative refinement when initial attempts fail.","L1: Language Comprehension, Language Production
L2: Logical Reasoning, Planning, Working Memory, Adaptive Error Correction
L3: ",L2
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"The FACTS Benchmark Suite systematically measures factuality and grounding behavior across a collection of tests that probe when models make incorrect claims, fabricate details, or fail to attribute sources appropriately. It is intended to be more robust than single-dataset factuality checks by covering multiple failure modes and domains.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Working Memory
L3: Inhibitory Control, Self-reflection",L3
SimpleQA Verified,https://www.kaggle.com/benchmarks/deepmind/simpleqa-verified,https://arxiv.org/abs/2509.07968,SimpleQA Verified is a short-form question answering benchmark with verified ground truth that targets factual correctness under concise prompts. It is often used to quantify hallucination propensity and precision on atomic facts where partial credit is not appropriate.,"L1: Language Comprehension
L2: Semantic Understanding & Context Recognition
L3: Inhibitory Control",L3
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA evaluates physical commonsense reasoning across many languages and culturally diverse contexts, focusing on selecting or generating plausible actions/outcomes in everyday physical situations. It probes whether models can generalize intuitive physics and affordance reasoning beyond English-centric priors.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Spatial Representation & Mapping
L3: ",L2
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) is a long-context evaluation where multiple similar “needle” queries are embedded throughout a large context (“haystack”), and the model must retrieve the correct referenced answer for a specified needle. It tests sustained attention and accurate retrieval under high interference from near-duplicate distractors over very long documents.","L1: Language Comprehension
L2: Working Memory, Attention, Episodic Memory
L3: Inhibitory Control",L3
