Benchmark,Website,Paper,Description,Cognitive Functions
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates software engineering agents on real GitHub issues by requiring them to produce a patch that passes the repository’s tests. The “Verified” subset uses problems that have been manually checked to be solvable and to have reliable evaluation via test execution, emphasizing end-to-end debugging and codebase modification.","Planning, Logical Reasoning, Working Memory, Adaptive Error Correction, Decision-making"
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 measures an agent’s ability to complete real tasks inside a command-line environment (e.g., installing tools, manipulating files, running programs, and troubleshooting). It emphasizes sequential tool use under constraints, where success depends on choosing correct commands, interpreting outputs, and iterating when errors occur.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition"
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench evaluates customer-support style agents interacting with simulated users and APIs across domains (e.g., retail, airline, telecom), with policy compliance requirements. It tests whether models can follow domain rules, manage multi-turn state, and reliably use tools to resolve user goals without violating constraints.","Social Reasoning & Theory of Mind, Decision-making, Planning, Inhibitory Control, Language Comprehension, Working Memory, Empathy"
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI tests fluid intelligence via few-shot grid transformation puzzles: models infer latent rules from a small set of input-output examples and produce the correct output for a new input. The tasks are designed to minimize reliance on memorized knowledge and instead emphasize abstraction, compositional pattern discovery, and generalization to novel rules.","Logical Reasoning, Cognitive Flexibility, Working Memory, Spatial Representation & Mapping, Scene Understanding & Visual Reasoning, Attention"
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon autonomous agent performance by simulating the operation of a vending machine business over an extended period, scoring by final financial outcome. Agents must plan, manage inventory and pricing, negotiate with suppliers, and adapt to changing conditions while maintaining coherence across many decisions.","Planning, Decision-making, Reward Mechanisms, Working Memory, Self-reflection, Adaptive Error Correction"
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a large, frontier-difficulty benchmark spanning many expert domains and often incorporating multimodal inputs, intended to probe broad reasoning and knowledge under challenging conditions. It is designed to stress-test complex problem solving, synthesis, and (in some reported setups) tool-enabled research workflows.","Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Scene Understanding & Visual Reasoning, Planning"
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 evaluates competition-style mathematics problem solving using questions from the American Invitational Mathematics Examination. Problems typically require multi-step derivations, algebraic manipulation, and careful control of intermediate results, rewarding precise symbolic reasoning over rote recall.","Logical Reasoning, Working Memory, Attention, Planning"
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a curated subset of very difficult, “Google-proof” multiple-choice science questions (physics, chemistry, biology) designed to resist shallow retrieval and reward genuine understanding. The Diamond subset aims for particularly high-quality items where experts reliably answer correctly while non-experts struggle.","Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Attention"
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends MMLU-style academic testing to multiple languages, measuring breadth of knowledge and reasoning across many subjects under multilingual prompts. It probes whether a model can maintain comparable competence across languages rather than relying on English-only priors.","Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory"
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro is an advanced multimodal benchmark that evaluates expert-level understanding and reasoning over images and text across many disciplines. Tasks often require integrating visual evidence (diagrams, tables, plots, scenes) with textual instructions to select or construct correct answers.","Visual Perception, Scene Understanding & Visual Reasoning, Multisensory Integration, Language Comprehension, Logical Reasoning, Attention"
MathArena Apex,https://matharena.ai/?view=problem&comp=apex--apex_2025,https://arxiv.org/abs/2505.23281,"MathArena Apex is a high-difficulty mathematics evaluation set that emphasizes complex, multi-step problem solving and robustness across problem types. It is used to compare frontier models’ mathematical reasoning and error rates under standardized scoring setups.","Logical Reasoning, Working Memory, Attention, Planning"
ScreenShot-Pro,https://gui-agent.github.io/grounding-leaderboard/,https://arxiv.org/abs/2504.07981,"ScreenShot-Pro evaluates models on understanding and reasoning over screenshots of software interfaces, often requiring identification of UI elements and interpretation of layout-dependent cues. It targets practical visual grounding for “computer use” scenarios, where success depends on reading screens and mapping them to correct actions or answers.","Visual Perception, Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Spatial Representation & Mapping, Attention, Sensorimotor Coordination"
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning evaluates scientific figure and chart reasoning drawn from arXiv-style papers, testing whether models can interpret plots, axes, legends, and figure captions to answer questions. It emphasizes quantitative and relational understanding from visual scientific artifacts rather than surface text alone.","Visual Perception, Scene Understanding & Visual Reasoning, Logical Reasoning, Semantic Understanding & Context Recognition, Attention"
OmniDocBench 1.5,https://github.com/opendatalab/OmniDocBench,https://arxiv.org/abs/2412.07626,"OmniDocBench 1.5 evaluates document understanding and OCR-centric extraction across heterogeneous elements such as text blocks, formulas, tables, and reading order. It measures whether models can convert complex page layouts into faithful structured text and maintain correctness across multiple document modalities.","Visual Perception, Visual Attention & Eye Movements, Attention, Semantic Understanding & Context Recognition, Working Memory"
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU evaluates multimodal reasoning over videos, requiring models to integrate information across time (and often across modalities) to answer questions about events, actions, and causal relations. It stresses temporal integration and consistency when relevant evidence is distributed across multiple frames.","Visual Perception, Scene Understanding & Visual Reasoning, Working Memory, Attention, Multisensory Integration, Cognitive Timing & Predictive Modeling"
LiveCodeBench Pro,https://livecodebenchpro.com/projects/livecodebench-pro/leaderboard,https://arxiv.org/abs/2506.11928,"LiveCodeBench Pro evaluates coding ability on fresh, competition-like programming tasks, often emphasizing generalization and resistance to benchmark contamination. It measures solution correctness under execution-based grading, highlighting algorithmic reasoning, implementation accuracy, and iterative debugging behavior.","Logical Reasoning, Planning, Working Memory, Adaptive Error Correction, Decision-making"
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"FACTS Benchmark Suite systematically evaluates factuality and grounding behavior across multiple tasks and conditions, focusing on whether models make accurate, verifiable claims. It probes robustness to hallucination and the ability to refrain from unsupported assertions under uncertainty.","Semantic Understanding & Context Recognition, Inhibitory Control, Self-reflection, Logical Reasoning, Attention"
SimpleQA Verified,https://www.kaggle.com/benchmarks/deepmind/simpleqa-verified,https://arxiv.org/abs/2509.07968,"SimpleQA Verified is a factual question-answering benchmark with verification-oriented scoring designed to emphasize correctness and reduce ambiguity in evaluation. It targets reliable retrieval-like knowledge and penalizes confident but incorrect outputs, highlighting calibration and precision.","Semantic Understanding & Context Recognition, Language Comprehension, Inhibitory Control, Attention"
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA extends physical commonsense and everyday reasoning evaluation across many languages and cultural contexts, using non-parallel multilingual data to reduce translation artifacts. It tests whether models can apply practical commonsense consistently across languages rather than relying on English-centric cues.","Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory"
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) evaluates long-context multi-round coreference resolution by inserting multiple similar “needle” queries within very long conversational or document “haystacks,” then asking for the response associated with a particular needle. It stresses whether models can maintain and retrieve the correct reference among many near-duplicates as context length grows.","Working Memory, Attention, Language Comprehension, Semantic Understanding & Context Recognition, Episodic Memory"
