Benchmark,Website,Paper,Description,Cognitive Functions
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro is a large, hard software engineering benchmark where a model must produce patches that fix real issues in open-source repositories and satisfy hidden tests. Compared with SWE-bench Verified, it broadens difficulty and (typically) language and task diversity, stressing end-to-end code understanding, editing, and debugging under realistic constraints.","Language Comprehension, Language Production, Planning, Decision-making, Adaptive Error Correction, Working Memory, Logical Reasoning"
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld evaluates computer-use agents that operate a real desktop environment to complete multi-step tasks across applications (e.g., browsing, file operations, productivity tools). Agents must interpret screenshots and UI state, choose actions (click/type/shortcut), and recover from errors within a step budget.","Visual Perception, Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Sensorimotor Coordination, Spatial Representation & Mapping, Planning, Decision-making, Working Memory, Adaptive Error Correction"
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,ARC-AGI measures few-shot abstract reasoning on grid-based puzzles where the system must infer a hidden rule from a handful of input–output examples and apply it to a new input. The benchmark is designed to emphasize generalization to novel patterns rather than memorization of task types.,"Logical Reasoning, Cognitive Flexibility, Working Memory, Visual Perception, Spatial Representation & Mapping, Planning, Attention"
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon autonomous agency in a simulated vending-machine business over an extended time period. The agent must manage inventory and cash flow, interact with suppliers/customers via messages, adapt to changing conditions, and maintain coherent strategy across many decisions.","Planning, Decision-making, Reward Mechanisms, Cognitive Timing & Predictive Modeling, Working Memory, Language Comprehension, Language Production, Adaptive Error Correction"
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas assesses real-world tool use via the Model Context Protocol by requiring models to discover, call, and compose tools across multi-step workflows. Tasks emphasize correct parameterization, handling tool errors, and integrating tool outputs into a final answer.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Language Comprehension, Language Production, Semantic Understanding & Context Recognition"
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity agent capabilities on tasks such as reproducing known vulnerabilities in real projects and, in some settings, discovering new vulnerabilities. It stresses understanding codebases and vulnerability descriptions, forming hypotheses, and iteratively testing and refining exploits or patches.","Logical Reasoning, Planning, Adaptive Error Correction, Working Memory, Language Comprehension, Decision-making, Semantic Understanding & Context Recognition"
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam (HLE) is a multimodal benchmark intended to probe frontier academic knowledge and reasoning across many domains, often at expert difficulty. Questions can require synthesizing information, performing multi-step reasoning, and (in some configurations) using tools like search or code to verify results.","Language Comprehension, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Visual Perception, Planning, Decision-making"
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a high-quality subset of GPQA composed of very difficult multiple-choice science questions designed to be resistant to superficial retrieval. It emphasizes deep domain understanding, careful elimination among options, and robust reasoning under uncertainty.","Language Comprehension, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Decision-making"
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro is a challenging multimodal benchmark spanning many disciplines, requiring models to answer questions grounded in images (figures, diagrams, charts) and text. It stresses integrated visual–text reasoning and precise interpretation of scientific/technical visuals.","Multisensory Integration, Visual Perception, Scene Understanding & Visual Reasoning, Language Comprehension, Logical Reasoning, Working Memory, Attention"
OmniDocBench 1.5,https://github.com/opendatalab/OmniDocBench,https://arxiv.org/abs/2412.07626,"OmniDocBench 1.5 evaluates document understanding and OCR-centric extraction across heterogeneous layouts (text, formulas, tables, reading order). Systems must recognize content and structure from document images and reconstruct or extract it with high fidelity.","Visual Perception, Scene Understanding & Visual Reasoning, Attention, Language Comprehension, Working Memory"
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU extends multimodal reasoning to video, requiring temporal understanding of events, actions, and visual cues across multiple frames. It targets higher-level comprehension beyond per-frame recognition, including reasoning about changes over time and context.","Visual Perception, Scene Understanding & Visual Reasoning, Cognitive Timing & Predictive Modeling, Working Memory, Attention, Multisensory Integration"
LiveCodeBench Pro,https://livecodebenchpro.com/projects/livecodebench-pro/leaderboard,https://arxiv.org/abs/2506.11928,"LiveCodeBench Pro evaluates coding ability using up-to-date, competitive-programming-style and practical coding tasks with a focus on realistic performance measurement (e.g., pass@k/Elo-style reporting). It stresses writing correct executable code, handling edge cases, and iterating when initial attempts fail.","Language Comprehension, Language Production, Logical Reasoning, Planning, Working Memory, Adaptive Error Correction, Decision-making"
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"FACTS Benchmark Suite systematically evaluates factuality: whether model outputs are supported by provided sources/context and whether they avoid introducing unsupported claims. It probes grounded generation, calibration, and consistency across diverse factuality subtests.","Semantic Understanding & Context Recognition, Language Comprehension, Language Production, Self-reflection, Inhibitory Control, Logical Reasoning, Attention"
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA evaluates pragmatic/common-sense inference across many languages and cultural contexts using non-parallel data, aiming to reduce English-centric bias. Tasks probe whether a model can choose or generate plausible actions/interpretations in everyday situations across linguistic settings.","Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning, Social Reasoning & Theory of Mind, Decision-making"
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 is a long-context evaluation where multiple similar ‘needle’ queries are embedded within long ‘haystack’ conversations/documents and the model must retrieve the correct referenced response (here, the 8-needle variant). It emphasizes robust context tracking and resistance to interference from near-duplicates.","Working Memory, Attention, Language Comprehension, Semantic Understanding & Context Recognition, Episodic Memory, Inhibitory Control"
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval measures performance on well-specified professional knowledge-work tasks across many occupations, judged by expert humans via head-to-head comparisons against professional outputs. It emphasizes producing complete artifacts (e.g., plans, analyses, spreadsheets/slides) with correct structure, assumptions, and actionable recommendations.","Planning, Decision-making, Language Production, Language Comprehension, Semantic Understanding & Context Recognition, Working Memory, Social Reasoning & Theory of Mind, Self-reflection"
SWE-Lancer,https://openai.com/index/swe-lancer/,https://arxiv.org/abs/2502.12115,"SWE-Lancer evaluates agentic software engineering on realistic repository tasks that often require deeper investigation, coordination of changes, and higher autonomy than short single-file edits. It targets end-to-end workflows like debugging, implementing fixes, and validating changes against project expectations.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Logical Reasoning, Language Comprehension, Language Production"
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath is a benchmark of expert-level mathematics problems designed to be difficult for current models and more resistant to contamination, often requiring long chains of derivation. It probes mathematical reasoning accuracy, strategic problem decomposition, and rigorous handling of constraints and edge cases.","Logical Reasoning, Planning, Working Memory, Adaptive Error Correction, Semantic Understanding & Context Recognition, Attention"
