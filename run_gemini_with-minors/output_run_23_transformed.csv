Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates an agent’s ability to solve real, human-validated software engineering issues by producing a correct patch in a real GitHub repository and passing the project’s tests. The “Verified” subset focuses on tasks that have been confirmed solvable and aims to reduce noise from ambiguous or underspecified issues, emphasizing end-to-end debugging and codebase navigation rather than isolated coding questions.","L1: Language Comprehension, Language Production
L2: Planning, Logical Reasoning, Adaptive Error Correction, Working Memory
L3: ",L2
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 measures how well an agent can complete practical tasks in a command-line environment, typically involving file manipulation, package/tool usage, scripting, and system-level troubleshooting. Success requires iteratively interpreting errors, issuing correct shell commands, and maintaining state across multi-step workflows under realistic constraints.","L1: Language Comprehension
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory
L3: ",L2
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench evaluates agentic tool use in multi-turn customer-support-style environments (e.g., retail, airline, telecom) where the model must follow policies while calling APIs/tools to complete tasks. It emphasizes robust interaction, policy adherence, and recovering from partial failures or user-provided complications over extended dialogues.","L1: Language Comprehension
L2: Decision-making, Planning, Working Memory
L3: Social Reasoning & Theory of Mind, Inhibitory Control",L3
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI tests fluid reasoning on novel grid-based transformation problems where a model must infer the rule from a few input–output examples and apply it to a new input. It is designed to minimize reliance on memorized domain knowledge and instead probe abstraction, compositional pattern discovery, and generalization from sparse data.","L1: 
L2: Logical Reasoning, Spatial Representation & Mapping, Working Memory, Scene Understanding & Visual Reasoning
L3: Cognitive Flexibility",L3
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon agent coherence and business decision-making in a simulated vending-machine enterprise over many steps (e.g., procurement, pricing, negotiation, and inventory management). The score typically reflects cumulative outcomes (like final balance), requiring sustained strategy, adaptation to changing conditions, and error recovery over extended time.","L1: 
L2: Planning, Decision-making, Reward Mechanisms, Working Memory, Adaptive Error Correction
L3: Cognitive Timing & Predictive Modeling",L3
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam (HLE) is a frontier benchmark of difficult, often multimodal questions intended to stress advanced reasoning and expert-level knowledge under test conditions that may include tool use (e.g., search/code) depending on the evaluation setup. It targets breadth across domains and emphasizes correct synthesis, not just retrieval of isolated facts.","L1: Language Comprehension
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Scene Understanding & Visual Reasoning
L3: ",L2
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 evaluates mathematical problem-solving on competition-style questions that require multi-step derivations, algebraic manipulation, and careful case analysis. Because answers are typically short numeric outputs, performance reflects reasoning accuracy and intermediate-step reliability rather than verbose explanation quality.","L1: 
L2: Logical Reasoning, Working Memory, Planning
L3: ",L2
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a high-difficulty multiple-choice science QA benchmark aimed at being “Google-proof,” emphasizing reasoning with specialized scientific concepts rather than lookup. The Diamond subset is curated for quality and difficulty, often requiring the model to integrate multiple pieces of domain knowledge and avoid plausible distractors.","L1: Language Comprehension
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory
L3: Inhibitory Control",L3
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends broad academic knowledge testing into multiple languages, measuring whether models can understand and answer subject-matter questions consistently across linguistic contexts. It probes both factual/semantic competence and reasoning under translation and cultural-linguistic variation.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning
L3: Cognitive Flexibility",L3
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro is a multimodal benchmark covering many disciplines where questions require reasoning over images (e.g., charts, diagrams, documents) alongside text. The “Pro” variant is designed to be more challenging and evaluation-oriented for frontier models, stressing cross-modal grounding and visual reasoning rather than text-only knowledge.","L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Visual Attention & Eye Movements
L3: ",L2
MathArena Apex,https://matharena.ai/?view=problem&comp=apex--apex_2025,https://arxiv.org/abs/2505.23281,"MathArena Apex aggregates difficult mathematics problems intended to differentiate top models with strong multi-step reasoning, often exceeding typical competition-question difficulty. It is used to compare advanced mathematical capability under standardized evaluation, sometimes with tool-optional settings depending on the reported protocol.","L1: 
L2: Logical Reasoning, Working Memory, Planning, Adaptive Error Correction
L3: ",L2
ScreenShot-Pro,https://gui-agent.github.io/grounding-leaderboard/,https://arxiv.org/abs/2504.07981,"ScreenShot-Pro evaluates GUI grounding and screenshot-based task understanding, where a model must interpret interface elements and produce correct actions/answers tied to what is visually present. It targets fine-grained visual localization, layout understanding, and reliable mapping from visual state to an intended action or decision.","L1: Visual Perception
L2: Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Sensorimotor Coordination, Decision-making, Planning
L3: ",L2
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning evaluates reasoning over figures, tables, and scientific content from research-paper-style sources, requiring models to extract and synthesize information from visual artifacts and accompanying text. The benchmark stresses faithful interpretation (e.g., reading plots/axes) and multi-step inference rather than surface-level caption matching.","L1: Language Comprehension, Visual Perception
L2: Scene Understanding & Visual Reasoning, Semantic Understanding & Context Recognition, Working Memory, Logical Reasoning
L3: ",L2
OmniDocBench 1.5,https://github.com/opendatalab/OmniDocBench,https://arxiv.org/abs/2412.07626,"OmniDocBench 1.5 evaluates document understanding and OCR-centric extraction across heterogeneous page content such as text blocks, formulas, tables, and reading order. It emphasizes structured extraction and layout-aware interpretation, penalizing errors that break document fidelity (e.g., incorrect ordering or malformed table reconstruction).","L1: Visual Perception, Language Comprehension
L2: Attention, Scene Understanding & Visual Reasoning, Working Memory
L3: ",L2
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU extends multimodal understanding to videos, requiring models to answer questions that depend on temporal context, events, and multi-frame evidence rather than a single image. It probes whether a model can integrate information over time and maintain coherence across longer audiovisual narratives (often evaluated primarily from frames and associated text).","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Working Memory, Multisensory Integration, Attention
L3: Cognitive Timing & Predictive Modeling",L3
LiveCodeBench Pro,https://livecodebenchpro.com/projects/livecodebench-pro/leaderboard,https://arxiv.org/abs/2506.11928,"LiveCodeBench Pro evaluates coding ability on competitive-programming-like tasks designed for robust, up-to-date assessment, typically scored with strict execution-based correctness. It emphasizes algorithm selection, implementation correctness under constraints, and iterative debugging when initial attempts fail.","L1: Language Production
L2: Logical Reasoning, Planning, Adaptive Error Correction, Working Memory
L3: ",L2
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"The FACTS Benchmark Suite systematically evaluates factuality-related behaviors, including producing correct statements, resisting hallucination, and (in some settings) grounding claims in provided context. It is intended to measure not just knowledge, but whether the model can regulate uncertainty and avoid confidently generating unsupported content.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Working Memory
L3: Inhibitory Control, Self-reflection",L3
SimpleQA Verified,https://www.kaggle.com/benchmarks/deepmind/simpleqa-verified,https://arxiv.org/abs/2509.07968,"SimpleQA Verified is a short-form factual QA benchmark with verification-focused evaluation designed to reduce ambiguity about what constitutes a correct answer. It targets precision on straightforward queries where hallucination, overgeneralization, or hedging can still lead to wrong outputs.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition
L3: Inhibitory Control",L3
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA tests practical, everyday reasoning across many languages and locales, focusing on whether models can choose sensible actions/answers in common situations beyond narrow academic domains. It emphasizes cross-linguistic robustness and “grounded” commonsense-style inference sensitive to context and intent.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning
L3: Cognitive Flexibility",L3
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) evaluates long-context retrieval and multi-round coreference by inserting repeated, similar “needle” requests across a long “haystack” and asking the model to reproduce the correct response to a specific instance. The 8-needle variant stresses attention control under interference and the ability to retain and select the correct target across substantial context lengths.","L1: Language Comprehension
L2: Working Memory, Attention, Episodic Memory
L3: Inhibitory Control",L3
