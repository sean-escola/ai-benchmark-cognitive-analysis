Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro evaluates software engineering agents on a large set of real-world GitHub issues that require making correct code changes in a repository and passing the project’s tests. Compared with SWE-bench Verified, it is designed to be harder and more contamination-resistant, spanning multiple languages and more diverse engineering workflows.","L1: Language Comprehension, Language Production
L2: Logical Reasoning, Planning, Working Memory, Adaptive Error Correction, Decision-making
L3: ",L2
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld measures an agent’s ability to operate a full computer operating system through a GUI to complete multi-step tasks (e.g., using apps, browsers, and system utilities). It emphasizes visual grounding, long-horizon interaction, and robust tool/GUI manipulation under partial observability and interface variability.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Planning, Decision-making, Working Memory, Sensorimotor Coordination, Attention
L3: ",L2
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI is a “fluid intelligence” benchmark where models infer hidden rules from a few input–output grid examples and must produce the correct output grid for a new input. It targets generalization to novel, compositional patterns with minimal demonstrations, rather than memorization of domain knowledge.","L1: 
L2: Logical Reasoning, Working Memory, Attention, Spatial Representation & Mapping, Scene Understanding & Visual Reasoning
L3: Cognitive Flexibility",L3
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon agent coherence and strategy by having an agent run a simulated vending-machine business over an extended period, making thousands of decisions (procurement, pricing, inventory, negotiation). Performance is scored by the final business outcome (e.g., ending balance), requiring sustained planning and adaptation to changing conditions.","L1: 
L2: Planning, Decision-making, Reward Mechanisms, Working Memory, Episodic Memory, Adaptive Error Correction
L3: Social Reasoning & Theory of Mind",L3
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas measures real-world tool-use competence via the Model Context Protocol (MCP), where tasks require discovering appropriate tools, issuing correct calls, handling errors, and composing multi-step workflows across services. It emphasizes reliable action sequencing and robust interaction with production-like APIs and tool schemas.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Working Memory, Adaptive Error Correction
L3: Inhibitory Control",L3
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity agents on tasks involving finding known vulnerabilities from descriptions and discovering new vulnerabilities in real open-source projects. It stresses codebase understanding, exploit-relevant reasoning, and the ability to iteratively test hypotheses and fixes in realistic security workflows.","L1: Language Comprehension
L2: Logical Reasoning, Planning, Working Memory, Adaptive Error Correction, Semantic Understanding & Context Recognition, Decision-making
L3: ",L2
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a frontier academic benchmark spanning many expert-level questions (often multimodal) intended to probe broad knowledge and reasoning at the edge of human expertise. It includes tasks where tool use (e.g., search or code) may be evaluated separately from tool-free reasoning, highlighting both problem solving and information synthesis.","L1: Language Comprehension, Visual Perception, Language Production
L2: Logical Reasoning, Working Memory, Semantic Understanding & Context Recognition, Scene Understanding & Visual Reasoning
L3: ",L2
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,GPQA Diamond is a high-quality subset of GPQA consisting of very challenging multiple-choice science questions curated to be “Google-proof” and to differentiate experts from non-experts. It probes deep scientific reasoning and careful reading under strong distractors rather than shallow recall.,"L1: Language Comprehension
L2: Logical Reasoning, Working Memory, Semantic Understanding & Context Recognition
L3: Inhibitory Control",L3
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro is an expert-level multimodal benchmark that extends MMMU-style evaluation with harder questions requiring joint reasoning over images (figures, diagrams, screenshots) and text across many disciplines. It targets fine-grained visual understanding, cross-modal grounding, and multi-step reasoning under multiple-choice style constraints.","L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Multisensory Integration, Working Memory, Visual Attention & Eye Movements, Logical Reasoning
L3: ",L2
OmniDocBench 1.5,https://github.com/opendatalab/OmniDocBench,https://arxiv.org/abs/2412.07626,"OmniDocBench 1.5 evaluates document understanding and OCR across diverse real-world layouts, including text, tables, formulas, and reading order. It measures whether models can accurately extract and structure information from visually complex documents rather than only recognizing isolated text.","L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Semantic Understanding & Context Recognition, Visual Attention & Eye Movements, Working Memory
L3: ",L2
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU benchmarks multimodal reasoning over videos, requiring models to answer questions that depend on events unfolding across time and visual context. It stresses temporal integration, tracking entities/actions across frames, and combining visual evidence with language understanding.","L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Working Memory, Attention, Multisensory Integration
L3: Cognitive Timing & Predictive Modeling",L3
LiveCodeBench Pro,https://livecodebenchpro.com/projects/livecodebench-pro/leaderboard,https://arxiv.org/abs/2506.11928,"LiveCodeBench Pro evaluates coding ability on competitively sourced programming tasks with strong controls against test leakage, often emphasizing “fresh” problems and robust grading. It measures end-to-end program synthesis and debugging, not just code completion, under realistic constraints.","L1: Language Comprehension, Language Production
L2: Logical Reasoning, Planning, Working Memory, Adaptive Error Correction, Decision-making
L3: ",L2
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"FACTS Benchmark Suite systematically evaluates LLM factuality across multiple settings (e.g., open-domain claims, grounded contexts, and robustness to misleading prompts), focusing on whether outputs stay faithful to evidence. It emphasizes calibration, resisting hallucination, and maintaining consistent truthfulness across formats.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Working Memory, Attention
L3: Inhibitory Control, Self-reflection",L3
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA tests physical commonsense reasoning across many languages and cultures, using non-parallel items to reduce translation artifacts and better reflect local context. It probes whether models can select plausible actions or explanations grounded in everyday physical interaction and practical intuition.","L1: Language Comprehension
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Spatial Representation & Mapping, Sensorimotor Coordination, Working Memory
L3: ",L2
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) evaluates long-context multi-round co-reference resolution by embedding repeated, similar “needle” requests inside long “haystack” conversations/documents, then asking for the response corresponding to a particular needle. It stresses precise retrieval amid distractors and maintaining fidelity across very long contexts.","L1: Language Comprehension
L2: Working Memory, Attention, Episodic Memory
L3: Inhibitory Control",L3
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval measures performance on well-specified professional knowledge work tasks spanning many occupations, judged by expert humans via head-to-head comparisons (wins/ties). Tasks often require producing realistic artifacts (e.g., spreadsheets, presentations, plans), emphasizing end-to-end usefulness under real workplace constraints.","L1: Language Production, Language Comprehension
L2: Planning, Decision-making, Working Memory
L3: Social Reasoning & Theory of Mind, Self-reflection",L3
SWE-Lancer,https://openai.com/index/swe-lancer/,https://arxiv.org/abs/2502.12115,"SWE-Lancer evaluates advanced software engineering capability on realistic tasks that resemble contracted engineering work, emphasizing reliability on longer, more complex implementation and maintenance jobs. It tests whether models can execute multi-step development work with strong correctness requirements in real codebases.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory, Logical Reasoning
L3: ",L2
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath evaluates expert-level mathematics with problems designed to be difficult for current models and to require genuine reasoning rather than memorized patterns. It often benefits from tool-assisted verification (e.g., computation) while still primarily measuring the model’s ability to construct correct solution strategies.","L1: 
L2: Logical Reasoning, Working Memory, Planning, Attention, Adaptive Error Correction
L3: Cognitive Flexibility",L3
