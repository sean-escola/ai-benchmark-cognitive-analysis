Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates an agent’s ability to solve real-world software engineering issues by producing patches that make a repository’s tests pass. The Verified subset emphasizes tasks confirmed solvable by humans and uses standardized evaluation to measure end-to-end debugging, code editing, and integration behavior.","L1: 
L2: Planning, Decision-making, Logical Reasoning, Working Memory, Adaptive Error Correction
L3: Inhibitory Control",L3
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 measures how well an agent can complete practical tasks in a command-line environment (e.g., using shells, tools, and files) under realistic constraints. Success depends on executing correct sequences of actions, recovering from errors, and managing state over multi-step workflows.","L1: 
L2: Planning, Decision-making, Working Memory, Adaptive Error Correction, Attention
L3: Inhibitory Control",L3
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench evaluates interactive, multi-turn tool-using agents in customer-service-like simulations (e.g., retail/airline/telecom) with policy constraints. It tests whether an agent can follow rules, ask clarifying questions, use APIs correctly, and resolve a user’s request across many turns.","L1: Language Comprehension, Language Production
L2: Decision-making, Planning, Working Memory
L3: Social Reasoning & Theory of Mind, Inhibitory Control",L3
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI tests generalization to novel abstract reasoning tasks defined by small numbers of input-output grid examples, where the model must infer the underlying transformation rule. It is designed to emphasize fluid reasoning and systematic generalization rather than memorization of known task templates.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Spatial Representation & Mapping, Logical Reasoning, Working Memory, Attention
L3: Cognitive Flexibility",L3
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon autonomy by having an agent run a simulated vending-machine business over extended time, making repeated decisions about purchasing, pricing, inventory, and negotiation. Performance depends on maintaining coherent strategy, adapting to changing conditions, and optimizing outcomes over many steps.","L1: 
L2: Planning, Decision-making, Reward Mechanisms, Working Memory, Episodic Memory
L3: Cognitive Flexibility, Self-reflection",L3
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam (HLE) is a frontier, multidisciplinary benchmark intended to probe difficult reasoning and expert-level knowledge, often in multimodal settings. It aims to measure performance on challenging questions that require synthesis, careful justification, and avoiding hallucinations under uncertainty.","L1: Language Comprehension, Language Production, Visual Perception
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Scene Understanding & Visual Reasoning
L3: Inhibitory Control",L3
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 consists of competition mathematics problems that require multi-step derivations, algebraic manipulation, and careful case analysis. It stresses exactness and consistency across a full solution path rather than surface pattern matching.","L1: 
L2: Logical Reasoning, Working Memory, Planning, Adaptive Error Correction, Attention
L3: ",L2
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a hard subset of graduate-level, “Google-proof” multiple-choice science questions designed to resist shallow retrieval and reward deep understanding. It evaluates domain reasoning and the ability to discriminate between closely related scientific alternatives.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory
L3: Inhibitory Control",L3
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends broad academic knowledge evaluation to many languages, testing whether models can answer questions across subjects while handling multilingual context. It probes how well knowledge and reasoning transfer across linguistic settings and prompts.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory
L3: Cognitive Flexibility",L3
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,MMMU-Pro evaluates multimodal understanding and reasoning across diverse disciplines using images paired with text questions (often at higher difficulty and with more complex visuals than earlier variants). It measures whether a model can integrate visual evidence with domain knowledge to select or produce correct answers.,"L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Multisensory Integration, Logical Reasoning, Working Memory
L3: ",L2
MathArena Apex,https://matharena.ai/?view=problem&comp=apex--apex_2025,https://arxiv.org/abs/2505.23281,"MathArena Apex is a competitive-math-style benchmark focused on difficult problems that require long reasoning chains, formal manipulation, and robustness to distractors. It is often used to compare high-end mathematical reasoning and tool-assisted solving ability across frontier models.","L1: 
L2: Logical Reasoning, Working Memory, Planning, Adaptive Error Correction, Attention
L3: ",L2
ScreenShot-Pro,https://gui-agent.github.io/grounding-leaderboard/,https://arxiv.org/abs/2504.07981,"ScreenShot-Pro evaluates GUI understanding from screenshots, requiring models to ground instructions in visual interface elements and reason about layout, affordances, and targets. It reflects real agentic computer-use settings where accurate localization and stepwise interaction planning are critical.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Spatial Representation & Mapping, Planning, Sensorimotor Coordination, Decision-making
L3: ",L2
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning focuses on answering questions that require interpreting scientific paper figures/charts and linking them to textual context. It emphasizes quantitative and relational reasoning over visual encodings, and careful extraction of evidence from complex academic visuals.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Attention
L3: ",L2
OmniDocBench 1.5,https://github.com/opendatalab/OmniDocBench,https://arxiv.org/abs/2412.07626,"OmniDocBench 1.5 evaluates document understanding and OCR across heterogeneous layouts, including text, tables, formulas, and reading order. It measures robust perception-to-structure conversion and the ability to preserve document semantics in extracted outputs.","L1: Visual Perception, Language Comprehension
L2: Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Working Memory, Attention
L3: ",L2
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU tests multimodal understanding over videos, requiring temporal integration of events and reasoning about actions, objects, and context across frames. It evaluates whether models can maintain coherence over time and answer questions grounded in dynamic visual evidence.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Working Memory, Attention, Multisensory Integration, Logical Reasoning
L3: Cognitive Timing & Predictive Modeling",L3
LiveCodeBench Pro,https://livecodebenchpro.com/projects/livecodebench-pro/leaderboard,https://arxiv.org/abs/2506.11928,"LiveCodeBench Pro evaluates coding capability on problems curated to reduce contamination and to reflect realistic programming tasks, often scored via competitive-style metrics. It tests generating correct code under time/attempt constraints and handling edge cases through iterative debugging behavior.","L1: 
L2: Planning, Logical Reasoning, Working Memory, Adaptive Error Correction, Attention
L3: Inhibitory Control",L3
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"FACTS Benchmark Suite systematically evaluates factuality, including whether model outputs are supported by provided context and whether they avoid introducing unsupported claims. It targets reliability in knowledge-intensive generation and measures groundedness across varied settings.","L1: Language Comprehension, Language Production
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory
L3: Inhibitory Control, Self-reflection",L3
SimpleQA Verified,https://www.kaggle.com/benchmarks/deepmind/simpleqa-verified,https://arxiv.org/abs/2509.07968,SimpleQA Verified evaluates short-form factual question answering with verification-oriented scoring to emphasize correctness over fluent but unsupported responses. It is intended to reveal hallucination tendencies and calibration failures on seemingly straightforward queries.,"L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning
L3: Inhibitory Control, Self-reflection",L3
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA evaluates practical, everyday physical commonsense reasoning across many languages and culturally diverse contexts. It tests whether models can choose plausible actions or outcomes in real-world scenarios, emphasizing transfer beyond English-centric priors.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Spatial Representation & Mapping
L3: Cognitive Flexibility",L3
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) measures long-context multi-round coreference resolution by placing multiple similar “needles” in a long “haystack” and requiring retrieval of the correct referenced response. It stresses robustness to interference, maintaining distinctions across repeated patterns, and accurate recall over long contexts.","L1: Language Comprehension
L2: Working Memory, Attention, Episodic Memory, Semantic Understanding & Context Recognition
L3: Inhibitory Control",L3
