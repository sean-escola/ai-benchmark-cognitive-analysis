Benchmark,Website,Paper,Description,Cognitive Functions
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro evaluates software engineering agents on a large set of real GitHub issues that require producing correct patches in repository context and passing tests. It is designed to be more challenging and contamination-resistant than SWE-bench Verified, with broader language coverage and more realistic engineering workflows.","Language Comprehension, Language Production, Planning, Decision-making, Working Memory, Adaptive Error Correction, Logical Reasoning, Semantic Understanding & Context Recognition"
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld measures an agent’s ability to use a graphical operating system to complete user-specified tasks (e.g., navigating apps, editing files, changing settings) through multimodal perception and tool-like actions. It emphasizes end-to-end computer-use competence under step limits and interface variability.","Visual Perception, Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Attention, Planning, Decision-making, Working Memory, Spatial Representation & Mapping, Sensorimotor Coordination"
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,ARC-AGI tests abstract pattern induction using small grid-based input–output examples where the rule must be inferred from only a few demonstrations. Success requires composing novel transformations rather than relying on memorized domain knowledge.,"Visual Perception, Scene Understanding & Visual Reasoning, Logical Reasoning, Cognitive Flexibility, Working Memory, Attention"
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon agent coherence and strategy by simulating operation of a vending-machine business over an extended period with many sequential decisions. Agents must manage inventory, pricing, supplier interactions, and cash flow to maximize final balance under changing conditions.","Planning, Decision-making, Reward Mechanisms, Working Memory, Episodic Memory, Semantic Understanding & Context Recognition, Adaptive Error Correction"
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas evaluates real-world tool use through the Model Context Protocol (MCP), where models must discover tools, call them correctly, handle failures, and synthesize results across multi-step workflows. Tasks resemble production integrations with authentic APIs and structured tool schemas.","Language Comprehension, Language Production, Planning, Decision-making, Working Memory, Adaptive Error Correction, Semantic Understanding & Context Recognition"
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity agent capability on large-scale tasks involving identifying known vulnerabilities from descriptions and, in some settings, discovering new issues in real open-source code. It typically rewards correct exploitation/patching behavior under realistic constraints and single-attempt scoring regimes.","Language Comprehension, Logical Reasoning, Planning, Decision-making, Working Memory, Adaptive Error Correction, Semantic Understanding & Context Recognition"
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a multimodal benchmark intended to probe frontier academic knowledge and reasoning across a wide range of difficult questions, sometimes involving diagrams, figures, or specialized domains. It is often evaluated both with and without external tools (e.g., search or code execution) to distinguish internal reasoning from tool-augmented performance.","Language Comprehension, Visual Perception, Scene Understanding & Visual Reasoning, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory"
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a hard subset of graduate-level, “Google-proof” multiple-choice science questions curated to be high quality and resistant to superficial pattern matching. It targets deep scientific reasoning and domain knowledge under constrained answer formats.","Language Comprehension, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory"
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro is an advanced multimodal understanding and reasoning benchmark spanning many disciplines, with questions grounded in images (e.g., charts, diagrams, scenes) and accompanying text. It stresses expert-level visual reasoning, cross-domain knowledge, and precise option selection.","Visual Perception, Scene Understanding & Visual Reasoning, Multisensory Integration, Visual Attention & Eye Movements, Language Comprehension, Logical Reasoning, Working Memory"
OmniDocBench 1.5,https://github.com/opendatalab/OmniDocBench,https://arxiv.org/abs/2412.07626,"OmniDocBench 1.5 evaluates document understanding and OCR in complex, real-world layouts, including text blocks, tables, formulas, and reading order. It emphasizes structured extraction fidelity and layout-aware reconstruction rather than only plain-text recognition.","Visual Perception, Visual Attention & Eye Movements, Spatial Representation & Mapping, Scene Understanding & Visual Reasoning, Language Comprehension, Working Memory"
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU evaluates multimodal reasoning over videos, requiring models to integrate information across frames and time to answer questions about events, actions, and visual details. Performance depends on tracking temporal context and maintaining coherence over long video inputs.","Visual Perception, Attention, Working Memory, Cognitive Timing & Predictive Modeling, Scene Understanding & Visual Reasoning, Multisensory Integration"
LiveCodeBench Pro,https://livecodebenchpro.com/projects/livecodebench-pro/leaderboard,https://arxiv.org/abs/2506.11928,"LiveCodeBench Pro measures code-generation and debugging ability on competitively styled programming tasks designed for robust, up-to-date evaluation. It typically emphasizes producing correct, executable solutions under time- and reasoning-like constraints, and is often summarized with rating-style metrics (e.g., ELO).","Language Comprehension, Language Production, Logical Reasoning, Planning, Working Memory, Adaptive Error Correction"
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"The FACTS Benchmark Suite systematically evaluates factuality-related behaviors, including grounding, resisting hallucinations, and maintaining consistency under varied prompts and contexts. It aims to capture reliability across multiple factuality subtests rather than a single QA score.","Language Comprehension, Semantic Understanding & Context Recognition, Inhibitory Control, Working Memory, Logical Reasoning"
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA evaluates commonsense and physical reasoning across many languages and culturally diverse contexts, emphasizing robustness beyond English-only benchmarks. Questions target practical understanding of everyday interactions, constraints, and outcomes rather than specialized academic knowledge.","Language Comprehension, Logical Reasoning, Semantic Understanding & Context Recognition, Cognitive Flexibility, Working Memory"
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,MRCR v2 (8-needle) tests long-context multi-round co-reference resolution by embedding multiple similar “needle” requests in long “haystack” conversations and asking the model to reproduce a specific needle’s answer. It stresses reliable retrieval and disambiguation as context length grows.,"Working Memory, Attention, Episodic Memory, Semantic Understanding & Context Recognition, Language Comprehension"
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval evaluates economically meaningful, well-specified professional knowledge work across many occupations by comparing model outputs to expert human work products. Tasks often require producing artifacts (e.g., slides, spreadsheets, plans) judged by humans for quality and correctness.","Planning, Decision-making, Language Production, Language Comprehension, Working Memory, Self-reflection, Semantic Understanding & Context Recognition"
SWE-Lancer,https://openai.com/index/swe-lancer/,https://arxiv.org/abs/2502.12115,"SWE-Lancer evaluates agentic software engineering on realistic tasks that go beyond isolated coding puzzles, often requiring iterative debugging, repository navigation, and integration-quality fixes. It is designed to reflect end-to-end engineering competence under practical constraints and evaluation harnesses.","Language Comprehension, Language Production, Planning, Decision-making, Working Memory, Adaptive Error Correction, Logical Reasoning, Semantic Understanding & Context Recognition"
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath measures high-end mathematical problem solving, including questions intended to be difficult even for strong models and to resist memorization. It emphasizes rigorous multi-step reasoning and error-free symbolic/quantitative manipulation, sometimes with tool-enabled variants for computation.","Logical Reasoning, Working Memory, Planning, Adaptive Error Correction, Semantic Understanding & Context Recognition"
