Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates software engineering agents on real GitHub issues by requiring them to generate a patch that fixes the bug or implements the requested change and passes the repository’s tests. The ""Verified"" split uses tasks that have been manually validated as solvable with the provided repo state and tests, emphasizing end-to-end debugging and codebase navigation under realistic constraints.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Logical Reasoning, Working Memory, Adaptive Error Correction, Semantic Understanding & Context Recognition
L3: ",L2
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 measures how well an agent can complete real-world tasks in a command-line environment, such as installing tools, manipulating files, running programs, and interpreting logs and outputs. It stresses reliable tool use over multiple steps, recovery from errors, and maintaining task state across long action sequences.","L1: 
L2: Planning, Decision-making, Working Memory, Adaptive Error Correction, Logical Reasoning, Semantic Understanding & Context Recognition
L3: Inhibitory Control",L3
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench evaluates interactive, policy-constrained customer support agents that must communicate with a simulated user and call domain APIs to resolve multi-turn tasks (e.g., retail orders, airline changes, telecom troubleshooting). It probes whether the agent can follow rules consistently while still being helpful under conversational pressure and ambiguous user intent.","L1: Language Comprehension, Language Production
L2: Decision-making, Planning, Working Memory
L3: Social Reasoning & Theory of Mind, Inhibitory Control, Empathy",L3
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,ARC-AGI is a few-shot abstract reasoning benchmark in which models infer hidden transformations from a small number of input-output grid examples and produce the correct output grid for a new input. The tasks are designed to emphasize novelty and compositional pattern learning rather than memorization of domain facts.,"L1: Visual Perception
L2: Logical Reasoning, Working Memory, Spatial Representation & Mapping, Scene Understanding & Visual Reasoning, Attention
L3: Cognitive Flexibility",L3
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon agent coherence by simulating a year of operating a vending machine business, starting from a small budget. The agent must make repeated decisions about inventory, pricing, supplier negotiation, and adapting to market dynamics, with success measured by final financial outcomes.","L1: 
L2: Planning, Decision-making, Reward Mechanisms, Working Memory, Semantic Understanding & Context Recognition, Adaptive Error Correction
L3: Self-reflection",L3
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam (HLE) is a frontier, multi-domain benchmark intended to stress advanced academic reasoning, specialized knowledge, and (in some settings) multimodal understanding across a large set of difficult questions. Evaluations often compare tool-free reasoning versus tool-enabled configurations (e.g., web search or code) to assess end-to-end problem solving.","L1: Language Comprehension, Language Production
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Multisensory Integration, Decision-making
L3: ",L2
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 benchmarks competition-style mathematics problem solving, typically requiring multi-step symbolic reasoning, careful case analysis, and exact numeric answers. It is often used to measure reliability of chain-of-thought reasoning (tool-free) and the marginal benefit of external computation tools.","L1: 
L2: Logical Reasoning, Working Memory, Planning, Adaptive Error Correction
L3: Cognitive Flexibility",L3
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a hard subset of GPQA consisting of high-quality, graduate-level multiple-choice questions in sciences where non-experts tend to fail but experts succeed. It targets deep conceptual understanding and multi-step reasoning under a fixed set of answer options, discouraging shallow pattern matching.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Decision-making
L3: Inhibitory Control",L3
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends MMLU-style evaluation to multiple languages, testing broad academic knowledge and reasoning across many subjects via multiple-choice questions. It is used to assess multilingual generalization, robustness of learned concepts across languages, and consistency of knowledge retrieval.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory
L3: Cognitive Flexibility",L3
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro is a multimodal benchmark that tests expert-level understanding and reasoning over images paired with text (e.g., diagrams, charts, scientific figures) across many disciplines. Compared with earlier multimodal benchmarks, it emphasizes harder questions, reduced shortcutting, and stronger evaluation protocols for vision-language reasoning.","L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Visual Attention & Eye Movements, Working Memory
L3: ",L2
MathArena Apex,https://matharena.ai/?view=problem&comp=apex--apex_2025,https://arxiv.org/abs/2505.23281,"MathArena Apex aggregates difficult math problems and evaluates models under standardized settings, often focusing on robust multi-step reasoning rather than single-trick questions. It is commonly used to compare frontier math capability across models and prompting/tool configurations.","L1: 
L2: Logical Reasoning, Working Memory, Planning, Adaptive Error Correction
L3: Cognitive Flexibility",L3
ScreenShot-Pro,https://gui-agent.github.io/grounding-leaderboard/,https://arxiv.org/abs/2504.07981,"ScreenShot-Pro evaluates GUI understanding and interaction from screenshots, typically requiring the model to identify UI elements, interpret layouts, and produce grounded actions or answers (sometimes with tool support like screenshot capture). It targets practical screen-based reasoning relevant to computer-use agents operating across professional software interfaces.","L1: Visual Perception
L2: Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Sensorimotor Coordination, Planning, Decision-making, Working Memory
L3: ",L2
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning tests whether models can answer questions that require interpreting scientific paper figures (often charts/plots) and performing quantitative or logical reasoning over them. It is designed to probe grounded, visual-to-symbolic reasoning rather than surface caption matching, and is frequently evaluated with optional Python tool use.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Logical Reasoning, Working Memory, Multisensory Integration, Attention, Semantic Understanding & Context Recognition
L3: ",L2
OmniDocBench 1.5,https://github.com/opendatalab/OmniDocBench,https://arxiv.org/abs/2412.07626,"OmniDocBench 1.5 evaluates document understanding/OCR across complex layouts, including text, formulas, tables, and reading order. It measures whether a system can correctly perceive and structure information from heterogeneous document elements, which is critical for downstream reasoning and extraction.","L1: Visual Perception
L2: Attention, Semantic Understanding & Context Recognition, Working Memory, Scene Understanding & Visual Reasoning
L3: ",L2
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU benchmarks multimodal reasoning over videos, requiring models to integrate information across frames and align it with language queries. Tasks often require temporal understanding (events, causality, and changes over time) in addition to visual recognition.","L1: Visual Perception, Language Comprehension
L2: Multisensory Integration, Attention, Working Memory, Scene Understanding & Visual Reasoning
L3: Cognitive Timing & Predictive Modeling",L3
LiveCodeBench Pro,https://livecodebenchpro.com/projects/livecodebench-pro/leaderboard,https://arxiv.org/abs/2506.11928,"LiveCodeBench Pro evaluates coding ability on curated programming tasks with strong controls for temporal leakage and contamination, typically using pass@k or Elo-style summaries. It targets practical code synthesis and debugging skills that require iterative reasoning about program behavior and edge cases.","L1: Language Comprehension, Language Production
L2: Planning, Logical Reasoning, Working Memory, Adaptive Error Correction, Decision-making
L3: ",L2
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"FACTS Benchmark Suite systematically evaluates factuality and grounding behaviors across multiple tasks designed to detect hallucinations, unsupported claims, and citation/attribution failures. It emphasizes faithfulness to provided evidence and careful handling of uncertainty rather than purely fluent generation.","L1: Language Comprehension, Language Production
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory
L3: Inhibitory Control, Self-reflection",L3
SimpleQA Verified,https://www.kaggle.com/benchmarks/deepmind/simpleqa-verified,https://arxiv.org/abs/2509.07968,"SimpleQA Verified measures factual question answering with verified ground truth and evaluation procedures intended to reduce noise and ambiguous labeling. It is commonly used to track whether models answer accurately, abstain appropriately, and avoid fabricating details when uncertain.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Working Memory
L3: Inhibitory Control, Self-reflection",L3
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,Global PIQA evaluates commonsense and practical reasoning about physical interactions and everyday situations across many languages and culturally diverse contexts. It stresses whether models can select or generate plausible actions/outcomes grounded in intuitive physics and real-world constraints rather than language priors alone.,"L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Spatial Representation & Mapping, Working Memory
L3: Cognitive Flexibility",L3
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) is a long-context, multi-round coreference/recall evaluation where multiple similar “needle” requests are embedded throughout long conversational haystacks and the model must reproduce the correct response to a specified needle. The 8-needle variant increases interference and demands robust retrieval, disambiguation, and sustained context integration over long inputs.","L1: 
L2: Working Memory, Attention, Episodic Memory, Semantic Understanding & Context Recognition, Logical Reasoning
L3: Inhibitory Control",L3
