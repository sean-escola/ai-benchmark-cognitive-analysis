Benchmark,Website,Paper,Description,Cognitive Functions
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro evaluates software engineering agents on a large set of real-world GitHub issues that require making correct code changes in a repository and passing the project’s tests. Compared with SWE-bench Verified, it is designed to be harder and more contamination-resistant, spanning multiple languages and more diverse engineering workflows.","Language Comprehension, Language Production, Logical Reasoning, Planning, Working Memory, Adaptive Error Correction, Decision-making"
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld measures an agent’s ability to operate a full computer operating system through a GUI to complete multi-step tasks (e.g., using apps, browsers, and system utilities). It emphasizes visual grounding, long-horizon interaction, and robust tool/GUI manipulation under partial observability and interface variability.","Visual Perception, Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Planning, Decision-making, Working Memory, Sensorimotor Coordination, Attention"
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI is a “fluid intelligence” benchmark where models infer hidden rules from a few input–output grid examples and must produce the correct output grid for a new input. It targets generalization to novel, compositional patterns with minimal demonstrations, rather than memorization of domain knowledge.","Logical Reasoning, Cognitive Flexibility, Working Memory, Attention, Spatial Representation & Mapping, Scene Understanding & Visual Reasoning"
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon agent coherence and strategy by having an agent run a simulated vending-machine business over an extended period, making thousands of decisions (procurement, pricing, inventory, negotiation). Performance is scored by the final business outcome (e.g., ending balance), requiring sustained planning and adaptation to changing conditions.","Planning, Decision-making, Reward Mechanisms, Working Memory, Episodic Memory, Social Reasoning & Theory of Mind, Adaptive Error Correction"
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas measures real-world tool-use competence via the Model Context Protocol (MCP), where tasks require discovering appropriate tools, issuing correct calls, handling errors, and composing multi-step workflows across services. It emphasizes reliable action sequencing and robust interaction with production-like APIs and tool schemas.","Planning, Decision-making, Working Memory, Language Comprehension, Language Production, Adaptive Error Correction, Inhibitory Control"
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity agents on tasks involving finding known vulnerabilities from descriptions and discovering new vulnerabilities in real open-source projects. It stresses codebase understanding, exploit-relevant reasoning, and the ability to iteratively test hypotheses and fixes in realistic security workflows.","Logical Reasoning, Planning, Working Memory, Adaptive Error Correction, Language Comprehension, Semantic Understanding & Context Recognition, Decision-making"
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a frontier academic benchmark spanning many expert-level questions (often multimodal) intended to probe broad knowledge and reasoning at the edge of human expertise. It includes tasks where tool use (e.g., search or code) may be evaluated separately from tool-free reasoning, highlighting both problem solving and information synthesis.","Language Comprehension, Logical Reasoning, Working Memory, Semantic Understanding & Context Recognition, Visual Perception, Scene Understanding & Visual Reasoning, Language Production"
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,GPQA Diamond is a high-quality subset of GPQA consisting of very challenging multiple-choice science questions curated to be “Google-proof” and to differentiate experts from non-experts. It probes deep scientific reasoning and careful reading under strong distractors rather than shallow recall.,"Language Comprehension, Logical Reasoning, Working Memory, Semantic Understanding & Context Recognition, Inhibitory Control"
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro is an expert-level multimodal benchmark that extends MMMU-style evaluation with harder questions requiring joint reasoning over images (figures, diagrams, screenshots) and text across many disciplines. It targets fine-grained visual understanding, cross-modal grounding, and multi-step reasoning under multiple-choice style constraints.","Visual Perception, Scene Understanding & Visual Reasoning, Multisensory Integration, Language Comprehension, Working Memory, Visual Attention & Eye Movements, Logical Reasoning"
OmniDocBench 1.5,https://github.com/opendatalab/OmniDocBench,https://arxiv.org/abs/2412.07626,"OmniDocBench 1.5 evaluates document understanding and OCR across diverse real-world layouts, including text, tables, formulas, and reading order. It measures whether models can accurately extract and structure information from visually complex documents rather than only recognizing isolated text.","Visual Perception, Scene Understanding & Visual Reasoning, Language Comprehension, Semantic Understanding & Context Recognition, Visual Attention & Eye Movements, Working Memory"
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU benchmarks multimodal reasoning over videos, requiring models to answer questions that depend on events unfolding across time and visual context. It stresses temporal integration, tracking entities/actions across frames, and combining visual evidence with language understanding.","Visual Perception, Scene Understanding & Visual Reasoning, Working Memory, Attention, Cognitive Timing & Predictive Modeling, Multisensory Integration, Language Comprehension"
LiveCodeBench Pro,https://livecodebenchpro.com/projects/livecodebench-pro/leaderboard,https://arxiv.org/abs/2506.11928,"LiveCodeBench Pro evaluates coding ability on competitively sourced programming tasks with strong controls against test leakage, often emphasizing “fresh” problems and robust grading. It measures end-to-end program synthesis and debugging, not just code completion, under realistic constraints.","Logical Reasoning, Planning, Working Memory, Adaptive Error Correction, Language Comprehension, Language Production, Decision-making"
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"FACTS Benchmark Suite systematically evaluates LLM factuality across multiple settings (e.g., open-domain claims, grounded contexts, and robustness to misleading prompts), focusing on whether outputs stay faithful to evidence. It emphasizes calibration, resisting hallucination, and maintaining consistent truthfulness across formats.","Semantic Understanding & Context Recognition, Language Comprehension, Inhibitory Control, Self-reflection, Working Memory, Attention"
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA tests physical commonsense reasoning across many languages and cultures, using non-parallel items to reduce translation artifacts and better reflect local context. It probes whether models can select plausible actions or explanations grounded in everyday physical interaction and practical intuition.","Logical Reasoning, Semantic Understanding & Context Recognition, Spatial Representation & Mapping, Sensorimotor Coordination, Language Comprehension, Working Memory"
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) evaluates long-context multi-round co-reference resolution by embedding repeated, similar “needle” requests inside long “haystack” conversations/documents, then asking for the response corresponding to a particular needle. It stresses precise retrieval amid distractors and maintaining fidelity across very long contexts.","Working Memory, Attention, Episodic Memory, Language Comprehension, Inhibitory Control"
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval measures performance on well-specified professional knowledge work tasks spanning many occupations, judged by expert humans via head-to-head comparisons (wins/ties). Tasks often require producing realistic artifacts (e.g., spreadsheets, presentations, plans), emphasizing end-to-end usefulness under real workplace constraints.","Planning, Decision-making, Language Production, Language Comprehension, Working Memory, Social Reasoning & Theory of Mind, Self-reflection"
SWE-Lancer,https://openai.com/index/swe-lancer/,https://arxiv.org/abs/2502.12115,"SWE-Lancer evaluates advanced software engineering capability on realistic tasks that resemble contracted engineering work, emphasizing reliability on longer, more complex implementation and maintenance jobs. It tests whether models can execute multi-step development work with strong correctness requirements in real codebases.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Language Comprehension, Language Production, Logical Reasoning"
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath evaluates expert-level mathematics with problems designed to be difficult for current models and to require genuine reasoning rather than memorized patterns. It often benefits from tool-assisted verification (e.g., computation) while still primarily measuring the model’s ability to construct correct solution strategies.","Logical Reasoning, Working Memory, Planning, Cognitive Flexibility, Attention, Adaptive Error Correction"
