Benchmark,Website,Paper,Description,Cognitive Functions
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates an agent’s ability to solve real, human-validated software engineering issues by producing a correct patch in a real GitHub repository and passing the project’s tests. The “Verified” subset focuses on tasks that have been confirmed solvable and aims to reduce noise from ambiguous or underspecified issues, emphasizing end-to-end debugging and codebase navigation rather than isolated coding questions.","Planning, Logical Reasoning, Adaptive Error Correction, Working Memory, Language Comprehension, Language Production"
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 measures how well an agent can complete practical tasks in a command-line environment, typically involving file manipulation, package/tool usage, scripting, and system-level troubleshooting. Success requires iteratively interpreting errors, issuing correct shell commands, and maintaining state across multi-step workflows under realistic constraints.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Language Comprehension"
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench evaluates agentic tool use in multi-turn customer-support-style environments (e.g., retail, airline, telecom) where the model must follow policies while calling APIs/tools to complete tasks. It emphasizes robust interaction, policy adherence, and recovering from partial failures or user-provided complications over extended dialogues.","Social Reasoning & Theory of Mind, Decision-making, Planning, Inhibitory Control, Working Memory, Language Comprehension"
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI tests fluid reasoning on novel grid-based transformation problems where a model must infer the rule from a few input–output examples and apply it to a new input. It is designed to minimize reliance on memorized domain knowledge and instead probe abstraction, compositional pattern discovery, and generalization from sparse data.","Logical Reasoning, Cognitive Flexibility, Spatial Representation & Mapping, Working Memory, Scene Understanding & Visual Reasoning"
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon agent coherence and business decision-making in a simulated vending-machine enterprise over many steps (e.g., procurement, pricing, negotiation, and inventory management). The score typically reflects cumulative outcomes (like final balance), requiring sustained strategy, adaptation to changing conditions, and error recovery over extended time.","Planning, Decision-making, Reward Mechanisms, Working Memory, Cognitive Timing & Predictive Modeling, Adaptive Error Correction"
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam (HLE) is a frontier benchmark of difficult, often multimodal questions intended to stress advanced reasoning and expert-level knowledge under test conditions that may include tool use (e.g., search/code) depending on the evaluation setup. It targets breadth across domains and emphasizes correct synthesis, not just retrieval of isolated facts.","Language Comprehension, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Scene Understanding & Visual Reasoning"
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 evaluates mathematical problem-solving on competition-style questions that require multi-step derivations, algebraic manipulation, and careful case analysis. Because answers are typically short numeric outputs, performance reflects reasoning accuracy and intermediate-step reliability rather than verbose explanation quality.","Logical Reasoning, Working Memory, Planning"
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a high-difficulty multiple-choice science QA benchmark aimed at being “Google-proof,” emphasizing reasoning with specialized scientific concepts rather than lookup. The Diamond subset is curated for quality and difficulty, often requiring the model to integrate multiple pieces of domain knowledge and avoid plausible distractors.","Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Language Comprehension, Inhibitory Control"
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends broad academic knowledge testing into multiple languages, measuring whether models can understand and answer subject-matter questions consistently across linguistic contexts. It probes both factual/semantic competence and reasoning under translation and cultural-linguistic variation.","Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning, Cognitive Flexibility"
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro is a multimodal benchmark covering many disciplines where questions require reasoning over images (e.g., charts, diagrams, documents) alongside text. The “Pro” variant is designed to be more challenging and evaluation-oriented for frontier models, stressing cross-modal grounding and visual reasoning rather than text-only knowledge.","Visual Perception, Scene Understanding & Visual Reasoning, Multisensory Integration, Language Comprehension, Logical Reasoning, Visual Attention & Eye Movements"
MathArena Apex,https://matharena.ai/?view=problem&comp=apex--apex_2025,https://arxiv.org/abs/2505.23281,"MathArena Apex aggregates difficult mathematics problems intended to differentiate top models with strong multi-step reasoning, often exceeding typical competition-question difficulty. It is used to compare advanced mathematical capability under standardized evaluation, sometimes with tool-optional settings depending on the reported protocol.","Logical Reasoning, Working Memory, Planning, Adaptive Error Correction"
ScreenShot-Pro,https://gui-agent.github.io/grounding-leaderboard/,https://arxiv.org/abs/2504.07981,"ScreenShot-Pro evaluates GUI grounding and screenshot-based task understanding, where a model must interpret interface elements and produce correct actions/answers tied to what is visually present. It targets fine-grained visual localization, layout understanding, and reliable mapping from visual state to an intended action or decision.","Visual Perception, Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Sensorimotor Coordination, Decision-making, Planning"
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning evaluates reasoning over figures, tables, and scientific content from research-paper-style sources, requiring models to extract and synthesize information from visual artifacts and accompanying text. The benchmark stresses faithful interpretation (e.g., reading plots/axes) and multi-step inference rather than surface-level caption matching.","Language Comprehension, Visual Perception, Scene Understanding & Visual Reasoning, Semantic Understanding & Context Recognition, Working Memory, Logical Reasoning"
OmniDocBench 1.5,https://github.com/opendatalab/OmniDocBench,https://arxiv.org/abs/2412.07626,"OmniDocBench 1.5 evaluates document understanding and OCR-centric extraction across heterogeneous page content such as text blocks, formulas, tables, and reading order. It emphasizes structured extraction and layout-aware interpretation, penalizing errors that break document fidelity (e.g., incorrect ordering or malformed table reconstruction).","Visual Perception, Attention, Scene Understanding & Visual Reasoning, Language Comprehension, Working Memory"
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU extends multimodal understanding to videos, requiring models to answer questions that depend on temporal context, events, and multi-frame evidence rather than a single image. It probes whether a model can integrate information over time and maintain coherence across longer audiovisual narratives (often evaluated primarily from frames and associated text).","Visual Perception, Scene Understanding & Visual Reasoning, Working Memory, Cognitive Timing & Predictive Modeling, Multisensory Integration, Attention"
LiveCodeBench Pro,https://livecodebenchpro.com/projects/livecodebench-pro/leaderboard,https://arxiv.org/abs/2506.11928,"LiveCodeBench Pro evaluates coding ability on competitive-programming-like tasks designed for robust, up-to-date assessment, typically scored with strict execution-based correctness. It emphasizes algorithm selection, implementation correctness under constraints, and iterative debugging when initial attempts fail.","Logical Reasoning, Planning, Adaptive Error Correction, Working Memory, Language Production"
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"The FACTS Benchmark Suite systematically evaluates factuality-related behaviors, including producing correct statements, resisting hallucination, and (in some settings) grounding claims in provided context. It is intended to measure not just knowledge, but whether the model can regulate uncertainty and avoid confidently generating unsupported content.","Language Comprehension, Semantic Understanding & Context Recognition, Inhibitory Control, Self-reflection, Working Memory"
SimpleQA Verified,https://www.kaggle.com/benchmarks/deepmind/simpleqa-verified,https://arxiv.org/abs/2509.07968,"SimpleQA Verified is a short-form factual QA benchmark with verification-focused evaluation designed to reduce ambiguity about what constitutes a correct answer. It targets precision on straightforward queries where hallucination, overgeneralization, or hedging can still lead to wrong outputs.","Language Comprehension, Semantic Understanding & Context Recognition, Inhibitory Control"
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA tests practical, everyday reasoning across many languages and locales, focusing on whether models can choose sensible actions/answers in common situations beyond narrow academic domains. It emphasizes cross-linguistic robustness and “grounded” commonsense-style inference sensitive to context and intent.","Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning, Cognitive Flexibility"
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) evaluates long-context retrieval and multi-round coreference by inserting repeated, similar “needle” requests across a long “haystack” and asking the model to reproduce the correct response to a specific instance. The 8-needle variant stresses attention control under interference and the ability to retain and select the correct target across substantial context lengths.","Working Memory, Attention, Episodic Memory, Inhibitory Control, Language Comprehension"
