Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,SWE-bench Verified evaluates LLM-based software engineering agents on real GitHub issues by asking them to produce code changes that make a failing test suite pass. The “Verified” subset uses human-validated tasks and standardized evaluation harnesses to ensure the issues are solvable and that patches are graded reliably by running tests.,"L1: Language Comprehension, Language Production
L2: Logical Reasoning, Planning, Working Memory, Adaptive Error Correction, Semantic Understanding & Context Recognition
L3: ",L2
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 tests agents in real command-line environments where they must complete multi-step tasks by issuing shell commands, editing files, and interpreting program outputs. Success depends on correctly navigating stateful environments, debugging failures, and iterating toward a working solution under resource constraints.","L1: 
L2: Planning, Decision-making, Working Memory, Adaptive Error Correction, Logical Reasoning, Semantic Understanding & Context Recognition
L3: Inhibitory Control",L3
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench evaluates tool-using conversational agents in simulated customer-support domains (e.g., retail, airline, telecom) where the agent must follow policies while using APIs over multiple turns. It stresses robust interaction with a user simulator, correct tool invocation, and adherence to constraints despite ambiguity and adversarial or distracting context.","L1: Language Comprehension, Language Production
L2: Decision-making, Planning, Working Memory, Attention
L3: Social Reasoning & Theory of Mind, Inhibitory Control",L3
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI measures “fluid” pattern generalization by presenting a few input–output grid examples and asking the model to infer the hidden transformation for a new grid. Tasks are designed to be novel and low-data, emphasizing abstraction, compositional reasoning, and systematic generalization rather than memorization.","L1: Visual Perception
L2: Logical Reasoning, Working Memory, Spatial Representation & Mapping, Scene Understanding & Visual Reasoning, Attention
L3: Cognitive Flexibility",L3
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon autonomy by having an agent run a simulated vending machine business across many decisions (inventory, pricing, suppliers, budgeting) with delayed consequences. The score reflects sustained coherence and strategy over extended interaction sequences rather than isolated short tasks.","L1: 
L2: Planning, Decision-making, Working Memory, Reward Mechanisms, Semantic Understanding & Context Recognition, Adaptive Error Correction
L3: Self-reflection",L3
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a challenging multi-domain (often multimodal) benchmark intended to probe frontier knowledge and reasoning on difficult questions across many subjects. It emphasizes complex problem solving, careful reading, and synthesis, and is frequently reported both with and without tool use (e.g., search or code).","L1: Language Comprehension, Visual Perception
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Scene Understanding & Visual Reasoning, Multisensory Integration
L3: ",L2
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 is a competition-math benchmark drawn from the American Invitational Mathematics Examination, focusing on nontrivial algebra, geometry, number theory, and counting problems. It rewards precise multi-step derivations and error-free symbolic reasoning under strict answer formats.","L1: 
L2: Logical Reasoning, Working Memory, Adaptive Error Correction, Attention
L3: ",L2
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a graduate-level, “Google-proof” multiple-choice science benchmark designed to resist simple retrieval and reward genuine reasoning. The Diamond subset is curated for high quality, with questions that require deep understanding across physics, chemistry, and biology.","L1: Language Comprehension
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory
L3: Inhibitory Control",L3
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends broad academic knowledge testing to multiple languages, evaluating whether models can answer questions across many subjects beyond English. It probes both multilingual comprehension and whether reasoning and knowledge transfer remain robust under language shifts.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory
L3: Cognitive Flexibility",L3
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro is a harder multimodal benchmark that tests expert-level understanding and reasoning over images plus text across many disciplines (e.g., charts, diagrams, scientific figures). It emphasizes grounded interpretation of visual evidence and integrating it with domain knowledge to select or construct correct answers.","L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Working Memory, Visual Attention & Eye Movements
L3: ",L2
MathArena Apex,https://matharena.ai/?view=problem&comp=apex--apex_2025,https://arxiv.org/abs/2505.23281,"MathArena Apex aggregates difficult mathematics problems and evaluates models on high-end quantitative reasoning, often tracking performance under different settings (e.g., with/without tools). It targets robust, stepwise problem solving rather than template matching, with broad coverage across advanced math topics.","L1: 
L2: Logical Reasoning, Working Memory, Adaptive Error Correction, Attention, Planning
L3: ",L2
ScreenShot-Pro,https://gui-agent.github.io/grounding-leaderboard/,https://arxiv.org/abs/2504.07981,"ScreenShot-Pro evaluates GUI understanding and grounding by asking models to answer questions or take actions based on high-resolution screenshots of software interfaces. It stresses precise localization of relevant UI elements and reliable interpretation of layout, icons, and text embedded in images.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Spatial Representation & Mapping, Sensorimotor Coordination, Decision-making, Working Memory
L3: ",L2
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning focuses on reasoning over scientific-paper figures and plots, often requiring extracting quantitative relationships, trends, or comparisons from visualizations. Many tasks benefit from combining visual interpretation with structured analysis (sometimes with optional code tools).","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Logical Reasoning, Working Memory, Attention, Semantic Understanding & Context Recognition
L3: ",L2
OmniDocBench 1.5,https://github.com/opendatalab/OmniDocBench,https://arxiv.org/abs/2412.07626,"OmniDocBench 1.5 evaluates document understanding and OCR robustness across mixed-content pages (text, tables, formulas, and reading order). It measures whether systems can faithfully transcribe and structure information from complex layouts, not just recognize isolated text.","L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Attention, Working Memory, Semantic Understanding & Context Recognition
L3: ",L2
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU extends multimodal reasoning to videos, requiring models to integrate information across frames and time to answer questions about events, actions, and causal structure. It probes temporal grounding and cross-modal understanding rather than static image captioning.","L1: Visual Perception
L2: Multisensory Integration, Scene Understanding & Visual Reasoning, Working Memory, Attention
L3: Cognitive Timing & Predictive Modeling",L3
LiveCodeBench Pro,https://livecodebenchpro.com/projects/livecodebench-pro/leaderboard,https://arxiv.org/abs/2506.11928,"LiveCodeBench Pro measures coding ability on continuously updated, harder programming tasks intended to reduce training-set leakage, often reported with an Elo-style rating. It evaluates producing correct code under realistic constraints and iterative problem solving across diverse algorithmic and software tasks.","L1: Language Comprehension, Language Production
L2: Logical Reasoning, Planning, Working Memory, Adaptive Error Correction
L3: Inhibitory Control",L3
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"FACTS Benchmark Suite systematically evaluates factuality and grounding behavior across multiple complementary tests, focusing on whether model outputs are supported, accurate, and appropriately calibrated. It targets failure modes like hallucination, misattribution, and overconfident incorrect assertions across varied contexts.","L1: Language Comprehension, Language Production
L2: Semantic Understanding & Context Recognition, Working Memory, Attention
L3: Inhibitory Control, Self-reflection",L3
SimpleQA Verified,https://www.kaggle.com/benchmarks/deepmind/simpleqa-verified,https://arxiv.org/abs/2509.07968,"SimpleQA Verified measures factual question answering with an emphasis on verified ground truth and careful grading, aiming to quantify reliability on straightforward but failure-prone queries. It highlights whether a model can answer accurately (or abstain/qualify appropriately) when uncertainty is high.","L1: Language Comprehension, Language Production
L2: Semantic Understanding & Context Recognition
L3: Inhibitory Control, Self-reflection",L3
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA evaluates physical commonsense and practical reasoning across many languages and cultures using non-parallel multilingual data, testing whether models generalize beyond English-centric priors. Items typically require choosing the more plausible action or outcome in everyday physical scenarios.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory
L3: Cognitive Flexibility",L3
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) tests long-context, multi-round coreference and retrieval by inserting multiple similar “needle” requests into long “haystacks” and asking the model to reproduce the correct referenced content. The 8-needle setting increases interference, stressing robustness to distractors and sustained context tracking.","L1: Language Comprehension
L2: Working Memory, Attention, Episodic Memory, Semantic Understanding & Context Recognition
L3: Inhibitory Control",L3
