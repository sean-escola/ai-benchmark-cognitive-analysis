Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro evaluates software engineering agents on a large set of real-world bug-fix and feature tasks drawn from open-source repositories, requiring producing patches that pass tests. It is designed to be harder and more contamination-resistant than SWE-bench Verified, emphasizing realistic engineering workflows and robustness under constrained attempts.","L1: Language Comprehension, Language Production
L2: Logical Reasoning, Planning, Working Memory, Adaptive Error Correction
L3: ",L2
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld measures an agent’s ability to operate a computer-like operating system via a multimodal interface, completing tasks that require navigating GUIs, using apps, and executing multi-step procedures. Success depends on reliably mapping visual observations to action sequences under step limits, similar to practical “computer use” assistants.","L1: Visual Perception
L2: Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Sensorimotor Coordination, Planning, Decision-making, Working Memory, Attention
L3: ",L2
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI tests “fluid” reasoning by presenting a few input–output grid examples generated by an underlying rule, and asking models to infer the rule to transform new inputs. It emphasizes generalization to novel patterns with minimal examples rather than recall of domain knowledge.","L1: 
L2: Logical Reasoning, Spatial Representation & Mapping, Working Memory, Planning, Attention
L3: Cognitive Flexibility",L3
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon agent coherence and business strategy in a simulated year-long vending-machine operation, where the agent must manage inventory, pricing, supplier interaction, and finances. The score reflects whether decisions compound into sustained success across many steps rather than short, isolated wins.","L1: 
L2: Planning, Decision-making, Reward Mechanisms, Working Memory, Semantic Understanding & Context Recognition
L3: Self-reflection",L3
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas assesses real-world tool use through Model Context Protocol servers, testing whether models can discover appropriate tools, call them correctly, recover from errors, and integrate results across multi-step workflows. It targets production-like API interaction and orchestration rather than single-call function use.","L1: Language Comprehension
L2: Planning, Decision-making, Working Memory, Adaptive Error Correction, Attention
L3: ",L2
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity agent skills on tasks involving finding known vulnerabilities from descriptions and discovering previously unknown vulnerabilities in real open-source projects. It emphasizes practical reasoning over codebases, vulnerability patterns, and iterative debugging-like refinement under pass@1 scoring.","L1: Language Comprehension
L2: Logical Reasoning, Planning, Adaptive Error Correction, Working Memory, Attention
L3: ",L2
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a large, frontier-difficulty benchmark spanning expert-level questions across many disciplines, often including multimodal inputs. It aims to probe deep reasoning and broad academic knowledge, and can be run with or without tools such as search and code execution.","L1: Language Comprehension, Visual Perception
L2: Multisensory Integration, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory
L3: ",L2
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a high-quality subset of extremely challenging graduate-level multiple-choice science questions designed to be “Google-proof.” It focuses on rigorous scientific reasoning and conceptual understanding, with reduced benefit from surface pattern matching.","L1: Language Comprehension
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Attention
L3: ",L2
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro is a multimodal benchmark measuring expert-level understanding and reasoning over images plus text across many disciplines. It stresses interpreting diagrams, plots, and specialized visuals, requiring multi-step reasoning rather than straightforward recognition.","L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Working Memory, Attention
L3: ",L2
OmniDocBench 1.5,https://github.com/opendatalab/OmniDocBench,https://arxiv.org/abs/2412.07626,"OmniDocBench 1.5 evaluates document understanding and OCR-centric capabilities across complex layouts, including text, formulas, tables, and reading order. It targets end-to-end document parsing where spatial structure and formatting are essential for correctness.","L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Spatial Representation & Mapping, Attention, Working Memory
L3: ",L2
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU extends multimodal reasoning to the video setting, requiring models to integrate information across frames and time to answer questions. It stresses temporal comprehension, tracking, and synthesis rather than single-image interpretation.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Working Memory, Attention, Multisensory Integration
L3: Cognitive Timing & Predictive Modeling",L3
LiveCodeBench Pro,https://livecodebenchpro.com/projects/livecodebench-pro/leaderboard,https://arxiv.org/abs/2506.11928,"LiveCodeBench Pro measures competitive coding performance under conditions intended to reflect modern programming tasks and reduce leakage, often emphasizing solution quality and reliability. It evaluates whether models can write correct programs and adapt solutions under realistic constraints.","L1: Language Comprehension, Language Production
L2: Logical Reasoning, Planning, Working Memory, Adaptive Error Correction
L3: ",L2
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"The FACTS Benchmark Suite systematically evaluates factuality by testing whether models produce correct, well-grounded statements across diverse factuality stressors. It emphasizes resisting hallucination and maintaining consistency with provided evidence or known facts.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Working Memory, Logical Reasoning
L3: Inhibitory Control, Self-reflection",L3
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA tests practical reasoning about physical situations and everyday interactions across many languages, aiming to measure commonsense understanding beyond English-only datasets. It focuses on whether models can infer plausible actions/outcomes given real-world constraints described in text.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory
L3: Cognitive Flexibility",L3
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) is a long-context multi-round coreference evaluation where multiple similar “needle” interactions are embedded in long “haystacks,” and models must reproduce the correct response for a specified needle. It stresses maintaining and retrieving precise contextual bindings amid distractors over long token spans.","L1: 
L2: Working Memory, Attention, Semantic Understanding & Context Recognition, Episodic Memory
L3: Inhibitory Control",L3
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval evaluates economically valuable, well-specified knowledge work across many occupations, using expert human judgments on produced work artifacts (e.g., analyses, plans, presentations, spreadsheets). It measures end-to-end task execution quality, including following constraints and producing usable deliverables.","L1: Language Production, Language Comprehension
L2: Planning, Decision-making, Semantic Understanding & Context Recognition, Working Memory
L3: Social Reasoning & Theory of Mind, Self-reflection",L3
SWE-Lancer,https://openai.com/index/swe-lancer/,https://arxiv.org/abs/2502.12115,"SWE-Lancer evaluates software engineering agents on tasks that more closely resemble real engineering “work orders,” emphasizing end-to-end patch production and reliability. It is aimed at measuring sustained engineering competence beyond short code-generation prompts.","L1: Language Production, Language Comprehension
L2: Planning, Decision-making, Working Memory, Adaptive Error Correction, Logical Reasoning
L3: ",L2
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath is an expert-level mathematics benchmark designed to probe advanced problem solving that is difficult to answer by memorization or simple heuristics. It emphasizes multi-step derivations, careful constraint handling, and robustness on novel mathematical problems.","L1: 
L2: Logical Reasoning, Working Memory, Planning, Adaptive Error Correction, Attention
L3: Cognitive Flexibility",L3
