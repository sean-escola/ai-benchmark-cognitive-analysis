Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates whether a model can solve real GitHub issues by producing a code patch that passes the project’s tests in a standardized harness. The “Verified” split consists of tasks validated to be solvable with the provided repository state, reducing noise from ambiguous or unsound issues.","L1: Language Comprehension, Language Production
L2: Planning, Logical Reasoning, Adaptive Error Correction, Working Memory, Decision-making
L3: ",L2
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 measures an agent’s ability to complete realistic command-line tasks inside sandboxed environments (e.g., debugging, configuration, data processing), using shell commands and files as the primary interface. Success depends on executing correct tool actions over many steps while handling errors, environment state changes, and resource constraints.","L1: Language Comprehension
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory
L3: ",L2
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench evaluates interactive customer-support agents operating in simulated domains (e.g., retail, airline, telecom) that must use tools/APIs while following domain policies. It emphasizes multi-turn dialogue consistency, correct tool invocation, and adherence to constraints even under user pressure or ambiguous requests.","L1: Language Comprehension, Language Production
L2: Decision-making, Planning, Working Memory
L3: Social Reasoning & Theory of Mind, Inhibitory Control",L3
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI tests “fluid” reasoning via small grid-based puzzles where models must infer a hidden transformation rule from a few input–output examples and apply it to a new input. The benchmark is designed to reward abstraction, generalization, and systematic reasoning rather than memorized knowledge.","L1: Visual Perception
L2: Logical Reasoning, Working Memory, Spatial Representation & Mapping, Scene Understanding & Visual Reasoning
L3: Cognitive Flexibility",L3
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon autonomous decision-making in a simulated vending-machine business over many time steps. Agents must manage inventory, pricing, supplier interactions, and budgeting to maximize end-of-year balance, requiring sustained coherence and adaptation to changing conditions.","L1: 
L2: Planning, Decision-making, Reward Mechanisms, Working Memory, Episodic Memory, Semantic Understanding & Context Recognition
L3: ",L2
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a broad, frontier-style benchmark spanning difficult questions across many domains, often requiring multi-step reasoning and sometimes multimodal understanding. It is intended to stress both depth of knowledge and the ability to synthesize and reason under novel, expert-level prompts.","L1: Language Comprehension, Language Production, Visual Perception
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Multisensory Integration
L3: ",L2
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 is a competition-math benchmark drawn from the American Invitational Mathematics Examination, focusing on problems that require creative algebraic/number-theoretic reasoning. Performance reflects the ability to plan solution strategies, manipulate symbolic relationships, and avoid arithmetic or logical slips.","L1: 
L2: Logical Reasoning, Planning, Working Memory, Adaptive Error Correction
L3: ",L2
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a subset of GPQA containing particularly challenging, high-quality graduate-level multiple-choice questions in physics, chemistry, and biology. It aims to be resistant to superficial pattern-matching by emphasizing expert-only difficulty and requiring careful reasoning over technical content.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory
L3: ",L2
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU (Multilingual MMLU) evaluates knowledge and reasoning across many academic subjects and multiple languages, extending the classic MMLU setting beyond English. It probes whether models can preserve understanding and reasoning quality under multilingual variation and culturally diverse phrasing.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory
L3: ",L2
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro is a multimodal benchmark emphasizing expert-level understanding and reasoning over images paired with text (e.g., diagrams, charts, scientific figures) across diverse disciplines. It evaluates whether models can integrate visual evidence with textual constraints to select or produce correct answers.","L1: Visual Perception
L2: Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Working Memory
L3: ",L2
MathArena Apex,https://matharena.ai/?view=problem&comp=apex--apex_2025,https://arxiv.org/abs/2505.23281,"MathArena Apex is a high-difficulty mathematics evaluation suite used to compare frontier models on complex problem solving, often under standardized settings and aggregation protocols. It emphasizes sustained, multi-step derivations and correctness under competition-like constraints rather than short factual recall.","L1: 
L2: Logical Reasoning, Planning, Working Memory, Adaptive Error Correction
L3: ",L2
ScreenShot-Pro,https://gui-agent.github.io/grounding-leaderboard/,https://arxiv.org/abs/2504.07981,"ScreenShot-Pro evaluates GUI grounding and interaction understanding from screenshots, where a model must identify interface elements, interpret layout, and (in some setups) choose actions or references consistent with the screen state. It stresses precise visual grounding, spatial reasoning over UI structure, and action-oriented interpretation of what is visible.","L1: Visual Perception
L2: Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Spatial Representation & Mapping, Decision-making, Planning, Working Memory
L3: ",L2
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning evaluates reasoning over scientific charts/figures (often derived from arXiv-style papers), including reading plotted values, comparing trends, and combining figure evidence with textual questions. It targets robust visual-to-semantic extraction and multi-step inference from graphical representations.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Attention
L3: ",L2
OmniDocBench 1.5,https://github.com/opendatalab/OmniDocBench,https://arxiv.org/abs/2412.07626,"OmniDocBench 1.5 measures document AI capabilities such as OCR and structured understanding across heterogeneous layouts (text blocks, tables, formulas, and reading order). It emphasizes faithful extraction and reconstruction of document content where spatial layout and symbol accuracy are critical.","L1: Visual Perception, Language Comprehension
L2: Attention, Spatial Representation & Mapping, Working Memory, Scene Understanding & Visual Reasoning
L3: ",L2
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU extends multimodal understanding to video, requiring models to answer questions that depend on temporal context, events, and visual details across frames. It probes whether models can maintain and integrate information over time rather than relying on a single image snapshot.","L1: Visual Perception
L2: Working Memory, Scene Understanding & Visual Reasoning, Multisensory Integration, Attention
L3: Cognitive Timing & Predictive Modeling",L3
LiveCodeBench Pro,https://livecodebenchpro.com/projects/livecodebench-pro/leaderboard,https://arxiv.org/abs/2506.11928,"LiveCodeBench Pro evaluates coding ability using contemporary programming tasks with strong anti-contamination practices and standardized scoring (often via automated tests and leaderboard-style aggregation). It targets practical software development skills such as implementing correct algorithms, debugging, and iterating toward passing solutions.","L1: Language Comprehension, Language Production
L2: Planning, Logical Reasoning, Adaptive Error Correction, Working Memory, Decision-making
L3: ",L2
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"FACTS Benchmark Suite systematically evaluates LLM factuality across multiple tasks that check whether generated statements are supported by evidence or known ground truth. It is designed to measure hallucination-related failure modes, including overconfident fabrication and unsupported extrapolation, under controlled evaluation protocols.","L1: Language Comprehension, Language Production
L2: Semantic Understanding & Context Recognition, Working Memory
L3: Inhibitory Control",L3
SimpleQA Verified,https://www.kaggle.com/benchmarks/deepmind/simpleqa-verified,https://arxiv.org/abs/2509.07968,"SimpleQA Verified is a factual question-answering benchmark with verification aimed at reliable scoring and reduced ambiguity in acceptable answers. It focuses on whether a model can provide concise, correct responses without introducing unsupported details.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition
L3: Inhibitory Control",L3
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA extends physical commonsense/affordance-style reasoning to multilingual or cross-lingual settings, probing whether models can infer plausible actions and outcomes in everyday environments. It emphasizes grounded, intuitive physics and practical causality rather than purely encyclopedic recall.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Spatial Representation & Mapping
L3: ",L2
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) evaluates long-context multi-round coreference resolution by inserting repeated, similar “needle” requests into a long “haystack” of dialogue or documents and asking for the correct referenced response. It primarily measures whether a model can attend to and retrieve the correct instance among many distractors over long contexts.","L1: Language Comprehension
L2: Working Memory, Attention, Episodic Memory, Semantic Understanding & Context Recognition
L3: ",L2
