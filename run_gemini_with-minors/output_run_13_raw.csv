Benchmark,Website,Paper,Description,Cognitive Functions
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates agentic software engineering by giving a real GitHub issue plus repository context and requiring the model to produce a patch that passes the project’s tests. The Verified subset uses human validation to ensure tasks are solvable and that grading (via unit tests) is reliable, emphasizing end-to-end debugging and code changes rather than isolated coding snippets.","Planning, Decision-making, Logical Reasoning, Working Memory, Adaptive Error Correction, Language Comprehension, Language Production"
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 measures an agent’s ability to accomplish practical tasks in a command-line environment using tools like shells, package managers, and interpreters under resource constraints. Success depends on executing multi-step workflows, interpreting tool feedback (errors/logs), and iterating toward a working solution.","Planning, Decision-making, Working Memory, Adaptive Error Correction, Language Comprehension, Inhibitory Control"
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench evaluates tool-using customer-support agents in simulated domains (e.g., retail, airline, telecom) where the model must converse with a user, call APIs/tools, and follow domain policies. It stresses reliable multi-turn state tracking and policy adherence while still being helpful in complex edge cases.","Social Reasoning & Theory of Mind, Decision-making, Planning, Working Memory, Inhibitory Control, Language Comprehension, Language Production"
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI tests “fluid” pattern induction from a few examples: models infer latent rules mapping input grids to output grids and must generalize to a new grid. The tasks are designed to reduce reliance on memorized knowledge, emphasizing compositional abstraction and novel problem solving.","Visual Perception, Scene Understanding & Visual Reasoning, Spatial Representation & Mapping, Logical Reasoning, Cognitive Flexibility, Working Memory"
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon autonomous agency by simulating management of a vending-machine business over an extended timeline with many sequential decisions. Models must plan, manage inventory and pricing, interact with simulated counterparts (e.g., suppliers), and adapt strategies as conditions change.","Planning, Decision-making, Working Memory, Reward Mechanisms, Self-reflection, Semantic Understanding & Context Recognition"
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a large, frontier-style benchmark spanning difficult questions across many disciplines, often requiring multi-step reasoning and, in some settings, tool use such as search or code execution. It aims to probe the limits of broad academic knowledge and general problem solving, including multimodal items.","Language Comprehension, Language Production, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Multisensory Integration, Visual Perception"
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 consists of competition mathematics problems that require constructing multi-step solutions under tight constraints, often involving algebra, number theory, combinatorics, or geometry. Performance reflects the ability to maintain intermediate symbolic states and avoid subtle logical slips.","Logical Reasoning, Working Memory, Planning, Inhibitory Control"
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a difficult multiple-choice benchmark of graduate-level science questions intended to be “Google-proof,” emphasizing reasoning over superficial retrieval. The Diamond subset is curated for high quality and discriminates strongly between expert and non-expert performance.","Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Inhibitory Control"
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends broad academic testing to multiple languages, measuring whether models can answer subject-area questions accurately across diverse linguistic settings. It probes both knowledge robustness and the ability to transfer reasoning across languages without losing nuance.","Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Cognitive Flexibility"
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro is a multimodal, multi-discipline benchmark that requires combining visual inputs (e.g., diagrams, charts, scenes) with textual prompts to answer expert-level questions. It emphasizes fine-grained visual understanding and cross-modal reasoning rather than text-only knowledge.","Visual Perception, Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Language Comprehension, Working Memory"
MathArena Apex,https://matharena.ai/?view=problem&comp=apex--apex_2025,https://arxiv.org/abs/2505.23281,"MathArena Apex aggregates challenging mathematics tasks designed to stress advanced reasoning, often with competition-style difficulty and rigorous automatic evaluation. It is used to compare top models’ mathematical problem solving and, in some settings, the benefit of tool assistance such as Python.","Logical Reasoning, Working Memory, Planning, Adaptive Error Correction"
ScreenShot-Pro,https://gui-agent.github.io/grounding-leaderboard/,https://arxiv.org/abs/2504.07981,"ScreenShot-Pro evaluates whether models can understand and ground instructions in high-resolution software screenshots, such as identifying interface elements and answering questions about GUI state. It targets precise visual localization and layout reasoning needed for computer-use agents.","Visual Perception, Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Spatial Representation & Mapping, Decision-making"
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning tests multimodal scientific reasoning over figures and surrounding context from research papers, requiring models to interpret charts/plots and draw correct conclusions. Many items benefit from quantitative analysis and careful mapping between visual encodings and textual claims.","Visual Perception, Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory"
OmniDocBench 1.5,https://github.com/opendatalab/OmniDocBench,https://arxiv.org/abs/2412.07626,"OmniDocBench 1.5 evaluates document understanding and OCR across diverse layouts, including text, tables, formulas, and reading order. It measures robustness to complex formatting where correct extraction depends on spatial structure as well as language decoding.","Visual Perception, Visual Attention & Eye Movements, Spatial Representation & Mapping, Language Comprehension, Semantic Understanding & Context Recognition"
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU extends multimodal understanding to video, requiring models to integrate information across frames and time to answer questions. It stresses temporal reasoning, event understanding, and maintaining cross-frame context under limited observation budgets.","Visual Perception, Scene Understanding & Visual Reasoning, Cognitive Timing & Predictive Modeling, Attention, Working Memory, Multisensory Integration"
LiveCodeBench Pro,https://livecodebenchpro.com/projects/livecodebench-pro/leaderboard,https://arxiv.org/abs/2506.11928,"LiveCodeBench Pro evaluates coding ability on regularly updated problems with rigorous execution-based scoring, designed to reduce contamination and better reflect contemporary programming skill. Tasks often require generating correct, efficient code and iterating on failures under realistic constraints.","Planning, Logical Reasoning, Working Memory, Adaptive Error Correction, Language Comprehension, Language Production"
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"FACTS Benchmark Suite systematically evaluates factuality by testing whether model outputs remain faithful to available evidence and avoid unsupported statements across varied scenarios. It emphasizes calibration and error avoidance, not just producing fluent answers.","Semantic Understanding & Context Recognition, Language Comprehension, Inhibitory Control, Self-reflection, Working Memory"
SimpleQA Verified,https://www.kaggle.com/benchmarks/deepmind/simpleqa-verified,https://arxiv.org/abs/2509.07968,SimpleQA Verified measures factual question answering with verified ground truth and evaluation protocols aimed at reducing ambiguous grading. It focuses on precise retrieval-like correctness and discourages plausible-sounding hallucinations.,"Language Comprehension, Semantic Understanding & Context Recognition, Inhibitory Control, Working Memory"
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA evaluates physical commonsense and everyday causal reasoning across multiple languages, targeting whether models can choose plausible actions or explanations grounded in the physical world. It stresses robustness to linguistic variation while preserving commonsense inference.","Language Comprehension, Cognitive Flexibility, Semantic Understanding & Context Recognition, Logical Reasoning, Spatial Representation & Mapping"
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,MRCR v2 (8-needle) evaluates long-context retrieval and multi-round coreference by embedding multiple similar “needle” interactions within long “haystack” contexts and asking the model to reproduce the correct referenced response. The 8-needle setting increases distractors and requires precise attention control over very long inputs.,"Working Memory, Attention, Episodic Memory, Semantic Understanding & Context Recognition, Inhibitory Control"
