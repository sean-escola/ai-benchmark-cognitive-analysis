Benchmark,Website,Paper,Description,Cognitive Functions
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro evaluates software engineering agents on realistic repository-level tasks where a model must understand a large codebase, implement or fix functionality, and submit a patch that passes tests. Compared with SWE-bench Verified, Pro is larger and harder, covering multiple languages and aiming to be more contamination-resistant and industry-relevant.","Language Comprehension, Language Production, Logical Reasoning, Planning, Decision-making, Working Memory, Adaptive Error Correction, Semantic Understanding & Context Recognition"
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld tests “computer use” by having an agent operate a real desktop-like operating system to complete end-to-end tasks (e.g., using apps, navigating settings, filling forms). It emphasizes multimodal perception of GUIs, long-horizon action sequences, and robust interaction under noisy or changing interface states.","Visual Perception, Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Attention, Planning, Decision-making, Working Memory, Sensorimotor Coordination, Spatial Representation & Mapping, Adaptive Error Correction"
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,ARC-AGI measures fluid reasoning by asking models to infer abstract transformations from a few input–output grid examples and apply them to a new grid. The tasks are designed to resist shortcut memorization and require discovering novel rules from minimal supervision.,"Logical Reasoning, Cognitive Flexibility, Working Memory, Attention, Scene Understanding & Visual Reasoning, Spatial Representation & Mapping"
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon autonomous agency in a simulated vending machine business over an extended period (e.g., a full simulated year). Agents must manage inventory, pricing, supplier negotiation, and changing market conditions to maximize final profit while staying coherent across thousands of steps.","Planning, Decision-making, Reward Mechanisms, Working Memory, Episodic Memory, Semantic Understanding & Context Recognition, Adaptive Error Correction"
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas benchmarks real-world tool use through the Model Context Protocol by testing whether a model can discover tools, call them correctly, handle errors, and compose multi-step workflows across multiple MCP servers. Tasks resemble production integrations where correct API sequencing and robust recovery matter as much as raw reasoning.","Planning, Decision-making, Language Comprehension, Language Production, Working Memory, Adaptive Error Correction, Semantic Understanding & Context Recognition"
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity agency by having models find known vulnerabilities in real open-source projects (given high-level weakness descriptions) and also attempt to discover new vulnerabilities. Success depends on reading code, forming hypotheses about exploit paths, and producing precise, testable vulnerability reports or patches.","Language Comprehension, Logical Reasoning, Planning, Decision-making, Working Memory, Adaptive Error Correction, Semantic Understanding & Context Recognition, Attention"
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam (HLE) is a difficult, often multimodal benchmark intended to probe frontier knowledge and reasoning at (and beyond) typical expert-human levels across many domains. Questions often require integrating specialized concepts, careful reasoning under uncertainty, and sometimes interpreting figures or diagrams.","Language Comprehension, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Visual Perception, Scene Understanding & Visual Reasoning"
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a high-quality subset of graduate-level, “Google-proof” multiple-choice science questions where non-experts tend to fail while experts succeed. It stresses deep conceptual understanding and multi-step reasoning rather than retrieval of easily searchable facts.","Language Comprehension, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Attention"
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro evaluates expert-level multimodal understanding and reasoning across many disciplines, requiring models to answer questions grounded in images, charts, and technical visuals along with text. It tests whether models can connect visual evidence to domain knowledge and perform multi-step inference beyond simple recognition.","Visual Perception, Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Language Comprehension, Logical Reasoning, Working Memory, Multisensory Integration, Attention"
OmniDocBench 1.5,https://github.com/opendatalab/OmniDocBench,https://arxiv.org/abs/2412.07626,"OmniDocBench 1.5 evaluates document understanding/OCR systems on complex, real-world documents including text, tables, formulas, and reading-order structure. Models must extract or reconstruct content accurately while preserving layout and logical structure under diverse formatting and noise.","Visual Perception, Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Language Comprehension, Working Memory, Semantic Understanding & Context Recognition, Attention"
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU extends multimodal reasoning to video, requiring models to answer questions that depend on events unfolding over time rather than a single frame. It tests temporal integration (tracking entities and actions), understanding visual scenes, and aligning video evidence with textual questions and answers.","Visual Perception, Scene Understanding & Visual Reasoning, Working Memory, Attention, Cognitive Timing & Predictive Modeling, Multisensory Integration, Language Comprehension"
LiveCodeBench Pro,https://livecodebenchpro.com/projects/livecodebench-pro/leaderboard,https://arxiv.org/abs/2506.11928,"LiveCodeBench Pro evaluates coding capability on competitive-programming-style problems curated to reduce leakage and reflect contemporary difficulty. Models must derive correct algorithms, implement them reliably, and handle edge cases that often break superficially plausible solutions.","Logical Reasoning, Planning, Working Memory, Adaptive Error Correction, Language Production, Language Comprehension, Decision-making"
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"The FACTS Benchmark Suite systematically measures factuality and grounding behavior across a collection of factuality-related tests, targeting when models produce incorrect statements, unsupported claims, or inconsistencies. It is designed to characterize reliability under realistic prompting conditions rather than only measuring knowledge recall.","Semantic Understanding & Context Recognition, Language Comprehension, Language Production, Inhibitory Control, Self-reflection, Working Memory, Attention"
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA evaluates commonsense reasoning about physical interactions and everyday actions across many languages, emphasizing robustness beyond English-only settings. It tests whether models can apply intuitive physics and practical knowledge consistently under multilingual phrasing and cultural variation.","Language Comprehension, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Cognitive Flexibility"
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) is a long-context, multi-round coreference evaluation where multiple similar “needle” requests are embedded in a long “haystack” of dialogue and the model must reproduce the correct referenced response. It stresses precise retrieval and disambiguation under heavy context interference and long-range dependencies.","Working Memory, Attention, Episodic Memory, Language Comprehension, Semantic Understanding & Context Recognition, Inhibitory Control"
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval evaluates well-specified professional knowledge-work tasks spanning many occupations by comparing model-produced work products (e.g., slides, spreadsheets, plans) against expert human outputs using human judging. It emphasizes end-to-end execution quality, instruction following, and producing usable artifacts rather than just answering questions.","Planning, Decision-making, Language Production, Language Comprehension, Working Memory, Semantic Understanding & Context Recognition, Adaptive Error Correction, Social Reasoning & Theory of Mind"
SWE-Lancer,https://openai.com/index/swe-lancer/,https://arxiv.org/abs/2502.12115,"SWE-Lancer evaluates real-world software engineering work in a “freelance-like” setting, where models must deliver correct, integrated changes across a codebase and satisfy task requirements that resemble paid engineering tickets. It stresses reliability in larger, messier engineering workflows (understanding requirements, implementing, validating, and iterating).","Planning, Decision-making, Language Comprehension, Language Production, Working Memory, Adaptive Error Correction, Logical Reasoning, Semantic Understanding & Context Recognition"
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath is a difficult mathematics benchmark targeting problems at or beyond typical graduate/research-level difficulty, designed to probe genuine mathematical reasoning under strong anti-contamination practices. It often requires long chains of deduction, careful abstraction, and error-prone intermediate steps where verification matters.","Logical Reasoning, Working Memory, Planning, Adaptive Error Correction, Attention, Semantic Understanding & Context Recognition"
