Benchmark,Website,Paper,Description,Cognitive Functions
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,SWE-bench Verified evaluates software engineering agents by giving them a real GitHub issue plus repository context and asking them to produce a patch that makes the test suite pass. The Verified split adds additional human verification to ensure tasks are solvable and that the evaluation reliably reflects end-to-end debugging and code change competence.,"Planning, Logical Reasoning, Working Memory, Adaptive Error Correction, Decision-making"
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 measures an agent’s ability to complete practical tasks in a command-line environment (e.g., installing tools, manipulating files, running programs, and troubleshooting). Success depends on choosing correct shell commands, interpreting outputs, and iterating when errors occur under realistic constraints.","Planning, Decision-making, Working Memory, Adaptive Error Correction, Language Comprehension, Inhibitory Control"
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench evaluates tool-using, policy-following customer support agents in simulated domains (e.g., retail, airline, telecom) that require multi-turn interaction with a user and APIs. It tests whether an agent can achieve user goals while adhering to domain rules and maintaining consistent behavior across long dialogues.","Language Comprehension, Language Production, Social Reasoning & Theory of Mind, Decision-making, Planning, Inhibitory Control, Working Memory, Empathy"
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,ARC-AGI is a few-shot abstract reasoning benchmark where models infer the hidden transformation rule from a small number of input–output grid examples and apply it to a new grid. It targets generalization to novel concepts and compositional rules rather than memorized domain knowledge.,"Logical Reasoning, Cognitive Flexibility, Working Memory, Spatial Representation & Mapping, Attention, Cognitive Timing & Predictive Modeling"
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon agent coherence and strategy by simulating operation of a vending machine business over an extended period. The agent must manage inventory, pricing, supplier interactions, and cash flow to maximize final balance while adapting to changing conditions.","Planning, Decision-making, Working Memory, Reward Mechanisms, Cognitive Timing & Predictive Modeling, Adaptive Error Correction"
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a broad, frontier-level benchmark spanning difficult questions across many academic and professional domains (often including multimodal items). It emphasizes deep reasoning, precise knowledge application, and (in tool-enabled settings) effective use of search/code tools without over-relying on spurious sources.","Language Comprehension, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Planning, Adaptive Error Correction, Visual Perception, Scene Understanding & Visual Reasoning"
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 consists of competition mathematics problems designed to test multi-step mathematical reasoning under tight answer formats. It rewards structured derivations, careful error-checking, and the ability to maintain intermediate constraints across long solution chains.","Logical Reasoning, Working Memory, Adaptive Error Correction, Planning"
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,GPQA Diamond is a challenging multiple-choice science QA benchmark curated to be difficult for non-experts and resistant to simple web lookup. It measures whether models can apply graduate-level scientific knowledge and reasoning to distinguish among close distractors.,"Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Decision-making"
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends broad subject-matter testing into multiple languages, probing whether models can answer questions across many academic domains beyond English. It highlights multilingual understanding, cross-lingual transfer, and robustness of knowledge and reasoning under language variation.","Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory"
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro is a multimodal benchmark requiring models to answer expert-level questions grounded in images (diagrams, charts, figures) alongside text. It stresses integrated visual understanding and textual reasoning, often requiring interpretation of fine-grained visual evidence.","Visual Perception, Scene Understanding & Visual Reasoning, Multisensory Integration, Visual Attention & Eye Movements, Logical Reasoning, Working Memory"
MathArena Apex,https://matharena.ai/?view=problem&comp=apex--apex_2025,https://arxiv.org/abs/2505.23281,"MathArena Apex aggregates high-difficulty math problems intended to stress advanced mathematical reasoning, including complex multi-step derivations and proof-like thinking. It is often used to compare frontier models’ reliability on challenging quantitative tasks and sensitivity to subtle constraints.","Logical Reasoning, Working Memory, Adaptive Error Correction, Planning"
ScreenShot-Pro,https://gui-agent.github.io/grounding-leaderboard/,https://arxiv.org/abs/2504.07981,"ScreenShot-Pro evaluates GUI understanding by asking models to answer questions or take actions grounded in high-resolution screenshots of software interfaces. It tests whether an agent can localize relevant UI elements, reason about layout, and map visual evidence to correct tool/actions or answers.","Visual Perception, Visual Attention & Eye Movements, Spatial Representation & Mapping, Scene Understanding & Visual Reasoning, Planning, Decision-making, Sensorimotor Coordination"
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning focuses on reasoning over scientific figures from research papers, such as plots, tables, and diagrammatic illustrations. It measures whether models can extract quantitative/structural information from visuals and combine it with scientific context to answer reasoning questions.","Visual Perception, Scene Understanding & Visual Reasoning, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Visual Attention & Eye Movements"
OmniDocBench 1.5,https://github.com/opendatalab/OmniDocBench,https://arxiv.org/abs/2412.07626,"OmniDocBench 1.5 evaluates document understanding and OCR on complex documents containing text, formulas, tables, and reading order structure. Scores reflect how well models recover faithful content and layout-dependent structure from heterogeneous document inputs.","Visual Perception, Attention, Spatial Representation & Mapping, Language Comprehension, Working Memory"
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU measures multimodal understanding and reasoning over videos, where answers require integrating information across frames and sometimes aligning it with textual prompts. It emphasizes temporal integration, event understanding, and maintaining context over long visual sequences.","Visual Perception, Scene Understanding & Visual Reasoning, Working Memory, Attention, Cognitive Timing & Predictive Modeling, Multisensory Integration"
LiveCodeBench Pro,https://livecodebenchpro.com/projects/livecodebench-pro/leaderboard,https://arxiv.org/abs/2506.11928,"LiveCodeBench Pro evaluates coding ability on up-to-date programming tasks, often emphasizing realistic constraints and solution verification via execution or tests. It aims to reflect interactive, practical software development skills rather than purely synthetic puzzle solving.","Logical Reasoning, Planning, Working Memory, Adaptive Error Correction, Decision-making"
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"FACTS Benchmark Suite systematically evaluates factuality across a range of tasks designed to surface hallucinations, incorrect attributions, and unsupported statements. It emphasizes truthfulness under pressure from ambiguous prompts, long contexts, and retrieval-like settings where models may be tempted to guess.","Semantic Understanding & Context Recognition, Language Comprehension, Inhibitory Control, Adaptive Error Correction, Self-reflection"
SimpleQA Verified,https://www.kaggle.com/benchmarks/deepmind/simpleqa-verified,https://arxiv.org/abs/2509.07968,"SimpleQA Verified is a short-form question answering benchmark focused on factual correctness, with verification procedures intended to improve label reliability. It tests whether a model can provide accurate, concise answers and avoid fabricating details when unsure.","Language Comprehension, Semantic Understanding & Context Recognition, Inhibitory Control, Adaptive Error Correction"
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA extends physical commonsense reasoning to many languages, asking models to choose plausible solutions or interpretations grounded in everyday physical interactions. It stresses whether commonsense knowledge and reasoning remain consistent across linguistic and cultural variation.","Language Comprehension, Logical Reasoning, Semantic Understanding & Context Recognition, Spatial Representation & Mapping"
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,MRCR v2 (8-needle) evaluates long-context multi-round coreference and retrieval by embedding multiple similar ‘needles’ within long ‘haystacks’ of text and requiring the model to recover the correct referenced content. The 8-needle setting stresses robust tracking and disambiguation across many competing mentions in very long contexts.,"Working Memory, Attention, Language Comprehension, Semantic Understanding & Context Recognition, Episodic Memory, Inhibitory Control"
