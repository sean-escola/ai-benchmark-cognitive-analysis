Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates software engineering agents on real GitHub issues by requiring them to produce a patch that makes a failing test suite pass. The “Verified” subset uses tasks that have been validated as solvable and is commonly reported as pass@1 under a fixed scaffold (repo checkout, edit, run tests, submit).","L1: Language Comprehension, Language Production
L2: Planning, Logical Reasoning, Working Memory, Adaptive Error Correction
L3: ",L2
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 measures an agent’s ability to solve real command-line tasks in sandboxed environments (e.g., debugging, building, scripting, and environment management). Success depends on choosing correct shell commands, interpreting tool outputs, and iteratively repairing mistakes under resource/time constraints.","L1: Language Comprehension
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory
L3: ",L2
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench evaluates tool-using customer-support agents interacting with simulated users and backend APIs across domains (e.g., retail, airline, telecom) while adhering to domain policies. It stresses multi-turn state tracking, policy compliance, and reliable API/tool invocation to reach correct resolutions.","L1: Language Comprehension, Language Production
L2: Working Memory, Decision-making, Planning
L3: Inhibitory Control, Social Reasoning & Theory of Mind",L3
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI is a “fluid intelligence” benchmark where models infer hidden rules from a few input-output grid examples and must generate the correct output grid for a new input. It emphasizes novel pattern induction with minimal training priors, rather than memorized domain knowledge.","L1: Visual Perception
L2: Logical Reasoning, Working Memory, Spatial Representation & Mapping, Scene Understanding & Visual Reasoning
L3: Cognitive Flexibility",L3
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon agent coherence by having an agent run a simulated vending-machine business over an extended period, making thousands of decisions (pricing, inventory, supplier outreach/negotiation). Performance is typically measured by final balance/profit, reflecting sustained planning and adaptation.","L1: Language Production
L2: Planning, Decision-making, Working Memory, Episodic Memory, Reward Mechanisms
L3: Social Reasoning & Theory of Mind",L3
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a difficult, frontier-knowledge benchmark spanning many expert domains (and often multimodal items), designed to probe advanced reasoning beyond routine test-taking. Systems are commonly evaluated both without tools and with tool access (e.g., search/code) to assess end-to-end problem solving and retrieval-augmented reasoning.","L1: Language Comprehension, Visual Perception
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Planning, Multisensory Integration
L3: ",L2
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 consists of competition-style mathematics problems requiring multi-step derivations, careful algebra/number theory, and exact final answers. It is frequently used to test structured reasoning and error-prone symbolic manipulation under pass@1 settings (with or without tool assistance).","L1: 
L2: Logical Reasoning, Working Memory, Planning, Adaptive Error Correction
L3: ",L2
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a high-difficulty multiple-choice science Q&A subset intended to be “Google-proof,” emphasizing reasoning over expert-level physics, chemistry, and biology questions. The Diamond split is curated for quality and difficulty, often used to compare frontier-model scientific reasoning without tool use.","L1: Language Comprehension
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory
L3: ",L2
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends MMLU-style academic questions to multiple languages, testing whether models retain knowledge and reasoning competence across diverse linguistic contexts. It is used to assess multilingual generalization, translation robustness, and cross-lingual consistency in subject-matter understanding.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory
L3: ",L2
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro is an advanced multimodal benchmark with expert-level questions that require jointly interpreting images (e.g., diagrams, plots, figures) and text. It is designed to be more challenging than earlier MMMU variants, stressing fine-grained visual understanding and multimodal reasoning.","L1: Visual Perception
L2: Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Working Memory
L3: ",L2
MathArena Apex,https://matharena.ai/?view=problem&comp=apex--apex_2025,https://arxiv.org/abs/2505.23281,"MathArena Apex aggregates very challenging math problems (beyond typical competition difficulty) aimed at separating top reasoning models. It is commonly used to probe long-chain mathematical reasoning, robustness to traps, and the ability to maintain correctness over many dependent steps.","L1: 
L2: Logical Reasoning, Working Memory, Planning, Adaptive Error Correction
L3: ",L2
ScreenShot-Pro,https://gui-agent.github.io/grounding-leaderboard/,https://arxiv.org/abs/2504.07981,"ScreenShot-Pro evaluates understanding of graphical user interfaces from high-resolution screenshots, often requiring identifying UI elements, reading small text, and grounding answers to specific on-screen regions. It targets GUI perception and spatial grounding that are critical for computer-use agents.","L1: Visual Perception
L2: Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Spatial Representation & Mapping, Sensorimotor Coordination, Working Memory
L3: ",L2
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning evaluates reasoning over scientific charts/figures drawn from arXiv-style papers, requiring extraction of quantitative/relational information and answering nontrivial questions about plots. It is used to measure how well models integrate visual evidence with domain context and multi-step inference (often with optional Python tooling).","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Multisensory Integration
L3: ",L2
OmniDocBench 1.5,https://github.com/opendatalab/OmniDocBench,https://arxiv.org/abs/2412.07626,"OmniDocBench 1.5 evaluates end-to-end document understanding across formats such as text, tables, formulas, and reading order, often using OCR-like extraction metrics and structure fidelity. It probes whether models can accurately parse, reconstruct, and reason over complex document layouts.","L1: Visual Perception, Language Comprehension
L2: Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Working Memory, Semantic Understanding & Context Recognition
L3: ",L2
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU extends multimodal understanding to video, requiring temporal integration of visual events and contextual text to answer questions. It assesses whether models can track evolving scenes, recall earlier frames, and use temporal cues for correct reasoning.","L1: Visual Perception
L2: Attention, Working Memory, Episodic Memory, Multisensory Integration, Scene Understanding & Visual Reasoning
L3: Cognitive Timing & Predictive Modeling",L3
LiveCodeBench Pro,https://livecodebenchpro.com/projects/livecodebench-pro/leaderboard,https://arxiv.org/abs/2506.11928,"LiveCodeBench Pro evaluates coding ability on fresh, time-sensitive programming problems designed to reduce contamination, typically scored in pass@1 or ELO-style aggregates. It stresses algorithm design, implementation correctness, and iterative debugging under realistic constraints.","L1: Language Comprehension, Language Production
L2: Planning, Logical Reasoning, Working Memory, Adaptive Error Correction
L3: ",L2
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"FACTS Benchmark Suite systematically evaluates LLM factuality across a collection of tasks that measure correctness, grounding, and resistance to hallucination under different prompts and conditions. It targets reliability in knowledge-intensive generation rather than pure fluency or style.","L1: Language Production
L2: Semantic Understanding & Context Recognition, Working Memory
L3: Inhibitory Control, Self-reflection",L3
SimpleQA Verified,https://www.kaggle.com/benchmarks/deepmind/simpleqa-verified,https://arxiv.org/abs/2509.07968,"SimpleQA Verified is a factual question-answering benchmark with verification-focused evaluation aimed at measuring whether models provide correct, non-hallucinated answers to short queries. It is commonly used as a lightweight factuality check under strict scoring and low-latency settings.","L1: Language Comprehension, Language Production
L2: Semantic Understanding & Context Recognition
L3: Inhibitory Control",L3
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA evaluates physical commonsense and everyday reasoning across many languages and cultural contexts, focusing on whether models can select plausible actions/explanations in real-world scenarios. It is used to test cross-lingual robustness of grounded commonsense reasoning beyond English-only datasets.","L1: Language Comprehension
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Spatial Representation & Mapping
L3: ",L2
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) is a long-context evaluation where multiple similar “needle” queries are embedded in long conversational/document “haystacks,” and the model must retrieve and reproduce the correct referenced content for a specified needle. It measures long-range retrieval, interference resistance, and multi-round co-reference resolution over very long inputs.","L1: Language Comprehension
L2: Working Memory, Attention, Episodic Memory, Semantic Understanding & Context Recognition
L3: ",L2
