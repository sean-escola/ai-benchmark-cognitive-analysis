Benchmark,Website,Paper,Description,Cognitive Functions
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,SWE-bench Verified evaluates whether a model can solve real GitHub issues by producing a code patch that makes the project’s tests pass. The “Verified” subset uses tasks that have been manually validated as solvable and includes stronger checks to reduce evaluation noise and reward overfitting to brittle setups.,"Planning, Logical Reasoning, Working Memory, Adaptive Error Correction, Language Comprehension"
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 measures agentic performance in command-line environments, where the model must accomplish real tasks using shell commands and filesystem operations. Success requires iterating based on tool outputs (errors, logs, test results) and maintaining task state over multi-step workflows.","Planning, Decision-making, Working Memory, Adaptive Error Correction, Language Comprehension"
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench evaluates tool-using agents in multi-turn customer-support style environments (e.g., retail, airline, telecom) with simulated users and programmatic APIs. Models must follow domain policies while completing user goals, handling exceptions, and making consistent decisions across long interactions.","Language Comprehension, Language Production, Planning, Decision-making, Social Reasoning & Theory of Mind, Inhibitory Control, Working Memory"
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI measures abstract pattern induction from a few examples using grid-based input–output tasks. Models must infer the latent transformation rules and generalize to new grids, emphasizing novel problem solving over memorized knowledge.","Logical Reasoning, Cognitive Flexibility, Working Memory, Spatial Representation & Mapping, Scene Understanding & Visual Reasoning, Attention"
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon autonomous agency by having a model run a simulated vending-machine business over extended time (many decisions across a “year”). It tests sustained coherence: managing inventory, pricing, supplier negotiation, and adapting to changing conditions for profit.","Planning, Decision-making, Working Memory, Reward Mechanisms, Adaptive Error Correction, Semantic Understanding & Context Recognition"
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a large multimodal benchmark intended to probe frontier-level academic reasoning and expert knowledge across diverse fields. Questions often require multi-step inference and, in some settings, effective tool use (e.g., search/code) without leaking from online answer keys.","Language Comprehension, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Visual Perception, Multisensory Integration"
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 evaluates competition-style mathematics problem solving, typically requiring multi-step derivations and careful symbolic manipulation. It is commonly used to gauge quantitative reasoning accuracy and robustness under time-limited, single-attempt conditions.","Logical Reasoning, Working Memory, Attention, Adaptive Error Correction"
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a high-difficulty, graduate-level multiple-choice science benchmark designed to be “Google-proof” and resistant to shallow retrieval. The Diamond subset focuses on the highest-quality items where experts agree on the correct answer and non-experts struggle.","Language Comprehension, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Attention"
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends broad academic knowledge evaluation to multiple languages, testing whether models can answer subject-matter questions beyond English. It stresses cross-lingual understanding and consistent reasoning across diverse linguistic and cultural contexts.","Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory"
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro evaluates multimodal understanding and reasoning across many disciplines with image-grounded questions (e.g., diagrams, charts, scientific figures). It is designed to better differentiate strong vision-language reasoning systems under harder settings and stricter evaluation protocols.","Multisensory Integration, Visual Perception, Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Language Comprehension, Logical Reasoning, Working Memory"
MathArena Apex,https://matharena.ai/?view=problem&comp=apex--apex_2025,https://arxiv.org/abs/2505.23281,"MathArena Apex aggregates challenging math problems to compare advanced reasoning systems, often emphasizing hard, multi-step solutions and error-prone algebraic or combinatorial reasoning. It is used to benchmark reliability and depth of mathematical problem solving across models.","Logical Reasoning, Working Memory, Attention, Adaptive Error Correction"
ScreenShot-Pro,https://gui-agent.github.io/grounding-leaderboard/,https://arxiv.org/abs/2504.07981,"ScreenShot-Pro evaluates GUI grounding: a model must interpret screenshots of real software interfaces and answer questions or identify targets relevant to completing tasks. It stresses precise perception of layout, icons, and text, and requires mapping visual evidence to actionable decisions.","Visual Perception, Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Spatial Representation & Mapping, Language Comprehension, Decision-making, Sensorimotor Coordination"
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning measures reasoning over scientific figures (e.g., plots and diagrams) from arXiv-style papers, often requiring interpreting axes, trends, and visual encodings. Many items demand quantitative or causal inference grounded in the visual evidence rather than text alone.","Visual Perception, Scene Understanding & Visual Reasoning, Logical Reasoning, Language Comprehension, Working Memory, Attention"
OmniDocBench 1.5,https://github.com/opendatalab/OmniDocBench,https://arxiv.org/abs/2412.07626,"OmniDocBench 1.5 evaluates document understanding and OCR across complex layouts, including text, formulas, tables, and reading order. It tests whether models can accurately transcribe and structure content from heterogeneous documents under realistic noise and formatting variation.","Visual Perception, Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Semantic Understanding & Context Recognition, Working Memory, Language Comprehension"
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU evaluates multimodal reasoning over videos, requiring models to integrate information across frames and time to answer questions. It probes temporal understanding, event reasoning, and the ability to maintain coherent interpretations as visual context evolves.","Multisensory Integration, Visual Perception, Scene Understanding & Visual Reasoning, Working Memory, Attention, Cognitive Timing & Predictive Modeling, Language Comprehension"
LiveCodeBench Pro,https://livecodebenchpro.com/projects/livecodebench-pro/leaderboard,https://arxiv.org/abs/2506.11928,"LiveCodeBench Pro evaluates coding ability on fresh, execution-validated programming tasks, often emphasizing realistic constraints and robustness against contamination. Models must write correct programs (or patches) and handle edge cases that are exposed by automated tests.","Planning, Logical Reasoning, Working Memory, Adaptive Error Correction, Language Comprehension"
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"FACTS Benchmark Suite evaluates factuality and grounding, testing whether a model’s claims are supported by provided sources or verifiable evidence and whether it avoids hallucinating details. It typically measures precision of attribution, correctness of statements, and stability under adversarially tempting prompts.","Language Comprehension, Semantic Understanding & Context Recognition, Inhibitory Control, Working Memory, Self-reflection"
SimpleQA Verified,https://www.kaggle.com/benchmarks/deepmind/simpleqa-verified,https://arxiv.org/abs/2509.07968,SimpleQA Verified evaluates short-form factual question answering with a “verified” process intended to improve label quality and reduce ambiguity. It is designed to measure whether models can answer directly and correctly without adding unsupported details.,"Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning, Inhibitory Control"
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA evaluates physical commonsense reasoning across languages and cultures, focusing on whether a model can pick plausible actions/solutions in everyday physical scenarios. It emphasizes robust commonsense generalization rather than narrow domain knowledge.","Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning, Spatial Representation & Mapping"
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) tests long-context retrieval and multi-round coreference by embedding multiple similar “needle” requests inside long “haystack” conversations or documents. Models must identify the correct referenced instance and reproduce the appropriate response, stressing precision under heavy context load.","Working Memory, Attention, Episodic Memory, Language Comprehension, Semantic Understanding & Context Recognition, Inhibitory Control"
