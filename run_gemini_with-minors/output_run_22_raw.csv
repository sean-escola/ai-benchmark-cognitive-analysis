Benchmark,Website,Paper,Description,Cognitive Functions
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro evaluates software engineering agents on a large set of real GitHub issues that require understanding a codebase, implementing a correct patch, and passing tests. Compared to SWE-bench Verified, it emphasizes harder, more diverse, and more contamination-resistant tasks across multiple languages and realistic developer workflows.","Language Comprehension, Language Production, Logical Reasoning, Planning, Working Memory, Adaptive Error Correction, Decision-making"
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld measures how well an agent can use a real (or realistic) operating system by perceiving GUI state (screenshots) and executing sequences of computer actions to complete tasks. It stresses end-to-end autonomy across multi-step goals, including navigation, form filling, file operations, and error recovery in a dynamic interface.","Visual Perception, Visual Attention & Eye Movements, Attention, Planning, Decision-making, Sensorimotor Coordination, Working Memory, Adaptive Error Correction"
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,ARC-AGI tests few-shot generalization on abstract grid transformation puzzles where the model must infer hidden rules from a handful of input–output examples and apply them to a new input. It is designed to probe fluid reasoning and systematic generalization rather than recall of learned patterns.,"Logical Reasoning, Cognitive Flexibility, Working Memory, Spatial Representation & Mapping, Scene Understanding & Visual Reasoning, Attention"
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon agent performance in a simulated vending-machine business over many decisions, tracking whether the agent can maintain coherence and improve outcomes over time. Success requires strategic planning, adapting to changing conditions, and managing limited resources while interacting with tools and simulated counterparts.","Planning, Decision-making, Working Memory, Reward Mechanisms, Adaptive Error Correction, Semantic Understanding & Context Recognition, Lifelong Learning"
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas evaluates real-world tool use via the Model Context Protocol (MCP), where models must discover appropriate tools, call them with correct arguments, handle failures, and synthesize results. Tasks emphasize multi-step workflows and reliable execution in production-like tool environments rather than pure text QA.","Planning, Decision-making, Working Memory, Language Comprehension, Language Production, Adaptive Error Correction, Inhibitory Control"
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity agent capabilities on large-scale tasks involving identifying and exploiting (or fixing) vulnerabilities in real open-source projects, often under realistic constraints. It tests whether agents can interpret vulnerability descriptions, navigate code, reproduce issues, and produce correct security-relevant patches or findings.","Logical Reasoning, Planning, Working Memory, Semantic Understanding & Context Recognition, Adaptive Error Correction, Decision-making, Inhibitory Control"
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam (HLE) is a frontier academic benchmark spanning many difficult questions across domains, including multimodal items, intended to stress advanced reasoning and knowledge integration. Evaluations often compare tool-free performance to tool-assisted settings (e.g., search and code) to assess end-to-end problem solving.","Language Comprehension, Language Production, Logical Reasoning, Working Memory, Semantic Understanding & Context Recognition, Multisensory Integration, Visual Perception"
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a high-quality subset of GPQA comprising very difficult graduate-level multiple-choice science questions designed to be “Google-proof.” It probes deep scientific reasoning and careful reading, with strong penalties for shallow pattern-matching or relying on memorized trivia.","Language Comprehension, Logical Reasoning, Working Memory, Semantic Understanding & Context Recognition, Inhibitory Control"
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro is a challenging multimodal benchmark for expert-level understanding across many disciplines, combining images (e.g., charts, diagrams, figures) with textual questions. It evaluates whether models can ground reasoning in visual evidence and integrate it with domain knowledge to choose correct answers.","Visual Perception, Scene Understanding & Visual Reasoning, Language Comprehension, Multisensory Integration, Working Memory, Attention, Logical Reasoning"
OmniDocBench 1.5,https://github.com/opendatalab/OmniDocBench,https://arxiv.org/abs/2412.07626,"OmniDocBench 1.5 evaluates document understanding and OCR-style extraction from complex, real-world documents containing text, tables, formulas, and layout structure. It emphasizes faithful reconstruction and reading order/layout correctness rather than only recognizing isolated text.","Visual Perception, Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Attention, Language Comprehension, Working Memory"
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU extends multimodal understanding to video, requiring models to reason over sequences of frames and temporal context to answer questions. It tests whether an agent can integrate visual evidence across time and maintain consistent interpretations for multi-step video reasoning.","Visual Perception, Scene Understanding & Visual Reasoning, Multisensory Integration, Working Memory, Attention, Cognitive Timing & Predictive Modeling, Decision-making"
LiveCodeBench Pro,https://livecodebenchpro.com/projects/livecodebench-pro/leaderboard,https://arxiv.org/abs/2506.11928,"LiveCodeBench Pro evaluates coding ability on timed, competitive-programming-style and practical coding tasks, typically emphasizing correctness under single-attempt constraints. It probes whether models can translate natural language requirements into working code, debug failures, and manage intermediate reasoning reliably.","Language Comprehension, Language Production, Logical Reasoning, Planning, Working Memory, Adaptive Error Correction, Decision-making"
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"The FACTS Benchmark Suite systematically evaluates factuality, including whether models make accurate claims, appropriately express uncertainty, and avoid unsupported hallucinations. It focuses on truthfulness and calibration across varied query styles and contexts, often separating grounded answers from persuasive but incorrect ones.","Language Comprehension, Language Production, Semantic Understanding & Context Recognition, Working Memory, Inhibitory Control, Self-reflection"
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,Global PIQA evaluates physical commonsense and practical reasoning using a globally oriented (non-parallel) dataset intended to reduce cultural and linguistic bias in everyday physical-interaction questions. It tests whether models choose plausible actions/outcomes in real-world scenarios given context and constraints.,"Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning, Decision-making, Spatial Representation & Mapping"
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) is a long-context evaluation where multiple similar “needle” interactions are embedded within a long “haystack,” and the model must retrieve the correct target response for a specified needle. It stresses robust tracking of entities and references under heavy distractors across very long contexts.","Working Memory, Attention, Language Comprehension, Semantic Understanding & Context Recognition, Inhibitory Control"
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval evaluates well-specified professional knowledge-work tasks across many occupations, judged by expert humans via pairwise comparisons (wins/ties). It emphasizes producing usable work artifacts and decisions (e.g., plans, analyses, documents) rather than only answering questions.","Planning, Decision-making, Language Production, Language Comprehension, Working Memory, Self-reflection, Social Reasoning & Theory of Mind"
SWE-Lancer,https://openai.com/index/swe-lancer/,https://arxiv.org/abs/2502.12115,"SWE-Lancer evaluates agentic software engineering in settings closer to real development cycles, where models must carry tasks from understanding requirements through implementation and verification. It typically emphasizes reliability, tool use, and end-to-end delivery over multi-step codebase modifications.","Language Comprehension, Language Production, Planning, Decision-making, Working Memory, Adaptive Error Correction, Logical Reasoning"
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath evaluates very difficult mathematics problems intended to be beyond standard benchmark difficulty, often requiring multi-step reasoning and careful formal manipulation. It is used to measure progress on advanced mathematical problem solving, especially when paired with tools like Python for verification.","Logical Reasoning, Working Memory, Planning, Adaptive Error Correction, Decision-making"
