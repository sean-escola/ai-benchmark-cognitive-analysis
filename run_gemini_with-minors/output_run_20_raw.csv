Benchmark,Website,Paper,Description,Cognitive Functions
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates whether a model can solve real GitHub issues by producing a code patch that passes the project’s tests in a standardized harness. The “Verified” split consists of tasks validated to be solvable with the provided repository state, reducing noise from ambiguous or unsound issues.","Planning, Logical Reasoning, Adaptive Error Correction, Working Memory, Language Comprehension, Language Production, Decision-making"
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 measures an agent’s ability to complete realistic command-line tasks inside sandboxed environments (e.g., debugging, configuration, data processing), using shell commands and files as the primary interface. Success depends on executing correct tool actions over many steps while handling errors, environment state changes, and resource constraints.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Language Comprehension"
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench evaluates interactive customer-support agents operating in simulated domains (e.g., retail, airline, telecom) that must use tools/APIs while following domain policies. It emphasizes multi-turn dialogue consistency, correct tool invocation, and adherence to constraints even under user pressure or ambiguous requests.","Language Comprehension, Language Production, Social Reasoning & Theory of Mind, Decision-making, Planning, Inhibitory Control, Working Memory"
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI tests “fluid” reasoning via small grid-based puzzles where models must infer a hidden transformation rule from a few input–output examples and apply it to a new input. The benchmark is designed to reward abstraction, generalization, and systematic reasoning rather than memorized knowledge.","Logical Reasoning, Cognitive Flexibility, Working Memory, Spatial Representation & Mapping, Visual Perception, Scene Understanding & Visual Reasoning"
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon autonomous decision-making in a simulated vending-machine business over many time steps. Agents must manage inventory, pricing, supplier interactions, and budgeting to maximize end-of-year balance, requiring sustained coherence and adaptation to changing conditions.","Planning, Decision-making, Reward Mechanisms, Working Memory, Episodic Memory, Semantic Understanding & Context Recognition"
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a broad, frontier-style benchmark spanning difficult questions across many domains, often requiring multi-step reasoning and sometimes multimodal understanding. It is intended to stress both depth of knowledge and the ability to synthesize and reason under novel, expert-level prompts.","Language Comprehension, Language Production, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Multisensory Integration, Visual Perception"
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 is a competition-math benchmark drawn from the American Invitational Mathematics Examination, focusing on problems that require creative algebraic/number-theoretic reasoning. Performance reflects the ability to plan solution strategies, manipulate symbolic relationships, and avoid arithmetic or logical slips.","Logical Reasoning, Planning, Working Memory, Adaptive Error Correction"
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a subset of GPQA containing particularly challenging, high-quality graduate-level multiple-choice questions in physics, chemistry, and biology. It aims to be resistant to superficial pattern-matching by emphasizing expert-only difficulty and requiring careful reasoning over technical content.","Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory"
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU (Multilingual MMLU) evaluates knowledge and reasoning across many academic subjects and multiple languages, extending the classic MMLU setting beyond English. It probes whether models can preserve understanding and reasoning quality under multilingual variation and culturally diverse phrasing.","Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory"
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro is a multimodal benchmark emphasizing expert-level understanding and reasoning over images paired with text (e.g., diagrams, charts, scientific figures) across diverse disciplines. It evaluates whether models can integrate visual evidence with textual constraints to select or produce correct answers.","Visual Perception, Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Working Memory"
MathArena Apex,https://matharena.ai/?view=problem&comp=apex--apex_2025,https://arxiv.org/abs/2505.23281,"MathArena Apex is a high-difficulty mathematics evaluation suite used to compare frontier models on complex problem solving, often under standardized settings and aggregation protocols. It emphasizes sustained, multi-step derivations and correctness under competition-like constraints rather than short factual recall.","Logical Reasoning, Planning, Working Memory, Adaptive Error Correction"
ScreenShot-Pro,https://gui-agent.github.io/grounding-leaderboard/,https://arxiv.org/abs/2504.07981,"ScreenShot-Pro evaluates GUI grounding and interaction understanding from screenshots, where a model must identify interface elements, interpret layout, and (in some setups) choose actions or references consistent with the screen state. It stresses precise visual grounding, spatial reasoning over UI structure, and action-oriented interpretation of what is visible.","Visual Perception, Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Spatial Representation & Mapping, Decision-making, Planning, Working Memory"
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning evaluates reasoning over scientific charts/figures (often derived from arXiv-style papers), including reading plotted values, comparing trends, and combining figure evidence with textual questions. It targets robust visual-to-semantic extraction and multi-step inference from graphical representations.","Visual Perception, Scene Understanding & Visual Reasoning, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Attention"
OmniDocBench 1.5,https://github.com/opendatalab/OmniDocBench,https://arxiv.org/abs/2412.07626,"OmniDocBench 1.5 measures document AI capabilities such as OCR and structured understanding across heterogeneous layouts (text blocks, tables, formulas, and reading order). It emphasizes faithful extraction and reconstruction of document content where spatial layout and symbol accuracy are critical.","Visual Perception, Attention, Spatial Representation & Mapping, Language Comprehension, Working Memory, Scene Understanding & Visual Reasoning"
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU extends multimodal understanding to video, requiring models to answer questions that depend on temporal context, events, and visual details across frames. It probes whether models can maintain and integrate information over time rather than relying on a single image snapshot.","Visual Perception, Cognitive Timing & Predictive Modeling, Working Memory, Scene Understanding & Visual Reasoning, Multisensory Integration, Attention"
LiveCodeBench Pro,https://livecodebenchpro.com/projects/livecodebench-pro/leaderboard,https://arxiv.org/abs/2506.11928,"LiveCodeBench Pro evaluates coding ability using contemporary programming tasks with strong anti-contamination practices and standardized scoring (often via automated tests and leaderboard-style aggregation). It targets practical software development skills such as implementing correct algorithms, debugging, and iterating toward passing solutions.","Planning, Logical Reasoning, Adaptive Error Correction, Working Memory, Language Comprehension, Language Production, Decision-making"
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"FACTS Benchmark Suite systematically evaluates LLM factuality across multiple tasks that check whether generated statements are supported by evidence or known ground truth. It is designed to measure hallucination-related failure modes, including overconfident fabrication and unsupported extrapolation, under controlled evaluation protocols.","Language Comprehension, Language Production, Semantic Understanding & Context Recognition, Inhibitory Control, Working Memory"
SimpleQA Verified,https://www.kaggle.com/benchmarks/deepmind/simpleqa-verified,https://arxiv.org/abs/2509.07968,"SimpleQA Verified is a factual question-answering benchmark with verification aimed at reliable scoring and reduced ambiguity in acceptable answers. It focuses on whether a model can provide concise, correct responses without introducing unsupported details.","Language Comprehension, Semantic Understanding & Context Recognition, Inhibitory Control"
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA extends physical commonsense/affordance-style reasoning to multilingual or cross-lingual settings, probing whether models can infer plausible actions and outcomes in everyday environments. It emphasizes grounded, intuitive physics and practical causality rather than purely encyclopedic recall.","Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning, Spatial Representation & Mapping"
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) evaluates long-context multi-round coreference resolution by inserting repeated, similar “needle” requests into a long “haystack” of dialogue or documents and asking for the correct referenced response. It primarily measures whether a model can attend to and retrieve the correct instance among many distractors over long contexts.","Working Memory, Attention, Episodic Memory, Language Comprehension, Semantic Understanding & Context Recognition"
