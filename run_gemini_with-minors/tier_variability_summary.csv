Benchmark,L1_count,L2_count,L3_count,Total_runs,Mode_tier,Distinct_tiers
SWE-bench Verified,0,24,1,25,L2,2
Terminal-Bench 2.0,0,18,7,25,L2,2
τ2-bench,0,0,25,25,L3,1
ARC-AGI,0,0,25,25,L3,1
Vending-Bench 2,0,7,18,25,L3,2
Humanity’s Last Exam,0,24,1,25,L2,2
AIME 2025,0,22,3,25,L2,2
GPQA Diamond,0,16,9,25,L2,2
MMMLU,0,17,8,25,L2,2
MMMU-Pro,0,25,0,25,L2,1
MathArena Apex,0,23,2,25,L2,2
ScreenShot-Pro,0,25,0,25,L2,1
CharXiv Reasoning,0,25,0,25,L2,1
OmniDocBench 1.5,0,25,0,25,L2,1
Video-MMMU,0,0,25,25,L3,1
LiveCodeBench Pro,0,22,3,25,L2,2
FACTS Benchmark Suite,0,0,25,25,L3,1
SimpleQA Verified,0,0,25,25,L3,1
Global PIQA,0,14,11,25,L2,2
MRCR v2 (8-needle),0,8,17,25,L3,2
