Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro is a large-scale software engineering benchmark where a model must modify real repositories to resolve issues, producing a patch that passes the project’s tests and task-specific checks. Compared with earlier SWE-bench variants, it emphasizes harder, more realistic and contamination-resistant tasks across multiple languages and codebases, stressing end-to-end debugging and implementation.","L1: Language Comprehension, Language Production
L2: Logical Reasoning, Planning, Working Memory, Adaptive Error Correction
L3: ",L2
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld evaluates computer-use agents on completing real tasks in a full desktop operating system environment (e.g., using apps, settings, files, and browsers) under step and time constraints. It tests whether models can perceive GUI state, plan multi-step actions, recover from mistakes, and robustly interact with dynamic interfaces.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Planning, Decision-making, Sensorimotor Coordination, Working Memory, Adaptive Error Correction
L3: ",L2
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI is a fluid intelligence benchmark where models infer latent rules from a few input–output grid examples and produce the correct output grid for a new input. It is designed to minimize reliance on memorized knowledge and instead measure abstraction, rule induction, and generalization to novel tasks.","L1: 
L2: Logical Reasoning, Working Memory, Attention, Spatial Representation & Mapping, Scene Understanding & Visual Reasoning
L3: Cognitive Flexibility",L3
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 measures long-horizon agent coherence and strategy by having a model run a simulated vending-machine business over extended time, making thousands of decisions (inventory, pricing, supplier negotiation, budgeting). High scores require sustained planning, adaptation to changing conditions, and consistent execution of business workflows without drifting goals or state.","L1: 
L2: Planning, Decision-making, Working Memory, Reward Mechanisms, Semantic Understanding & Context Recognition, Adaptive Error Correction
L3: Cognitive Timing & Predictive Modeling",L3
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas evaluates real-world tool use through the Model Context Protocol (MCP), where models must discover tools, call them with correct arguments, and compose multi-step workflows across services. It stresses robust API interaction, error handling, and synthesis of tool outputs into correct final answers.","L1: Language Comprehension
L2: Planning, Decision-making, Working Memory, Adaptive Error Correction, Semantic Understanding & Context Recognition
L3: ",L2
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates agents on cybersecurity tasks including finding known vulnerabilities in real open-source projects from high-level weakness descriptions and, in some settings, discovering previously unknown issues. It emphasizes repository navigation, exploit/bug reasoning, and iterative debugging of hypotheses under realistic constraints.","L1: Language Comprehension
L2: Logical Reasoning, Planning, Adaptive Error Correction, Working Memory, Decision-making, Attention
L3: ",L2
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam (HLE) is a challenging benchmark intended to probe frontier-level academic and professional reasoning, often spanning multiple disciplines and modalities. Questions are designed to be difficult to answer via shallow pattern matching, requiring careful interpretation, multi-step reasoning, and (in some evaluation setups) effective tool use.","L1: Language Comprehension, Language Production
L2: Logical Reasoning, Working Memory, Semantic Understanding & Context Recognition, Multisensory Integration, Scene Understanding & Visual Reasoning
L3: ",L2
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a high-quality subset of graduate-level, “Google-proof” multiple-choice science questions where non-experts tend to fail but domain experts succeed. It targets deep scientific understanding and reasoning rather than retrieval of common facts.","L1: Language Comprehension
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Attention
L3: ",L2
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro is a difficult multimodal benchmark spanning many disciplines, where models answer questions requiring joint understanding of images (e.g., diagrams, charts, figures) and text. It focuses on expert-level visual reasoning, quantitative interpretation, and multi-step inference over multimodal evidence.","L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Spatial Representation & Mapping, Working Memory, Attention
L3: ",L2
OmniDocBench 1.5,https://github.com/opendatalab/OmniDocBench,https://arxiv.org/abs/2412.07626,"OmniDocBench 1.5 evaluates document understanding and OCR-centric capabilities over complex layouts that can include text, tables, formulas, and reading order. It stresses faithful extraction and structured interpretation of visually-presented information, including layout-sensitive reconstruction.","L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Working Memory, Attention, Spatial Representation & Mapping
L3: ",L2
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU extends multimodal understanding to video, requiring models to answer questions grounded in sequences of frames and temporal events. It tests whether a model can integrate visual evidence over time, track state changes, and reason about actions and causality in dynamic scenes.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Multisensory Integration, Working Memory, Attention, Logical Reasoning
L3: Cognitive Timing & Predictive Modeling",L3
LiveCodeBench Pro,https://livecodebenchpro.com/projects/livecodebench-pro/leaderboard,https://arxiv.org/abs/2506.11928,"LiveCodeBench Pro is a competitive coding benchmark that evaluates a model’s ability to solve programming problems under realistic constraints, typically emphasizing correctness, robustness, and timeliness. It targets not just writing code, but also iterative debugging and aligning implementations with precise specifications.","L1: Language Comprehension, Language Production
L2: Logical Reasoning, Planning, Working Memory, Adaptive Error Correction, Attention
L3: ",L2
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"FACTS Benchmark Suite systematically evaluates factuality and grounding behavior across diverse factuality-related tasks, aiming to separate genuine knowledge/grounding from plausible-sounding hallucinations. It emphasizes accuracy under ambiguity, correct attribution/grounding when evidence is available, and restraint when evidence is insufficient.","L1: Language Comprehension, Language Production
L2: Semantic Understanding & Context Recognition, Working Memory, Logical Reasoning
L3: Inhibitory Control, Self-reflection",L3
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA evaluates practical physical commonsense reasoning across many languages and cultural contexts, focusing on whether models choose actions or explanations consistent with everyday physics and affordances. It is designed to test generalization of physical reasoning beyond English-centric distributions.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Spatial Representation & Mapping, Scene Understanding & Visual Reasoning
L3: ",L2
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 is a long-context evaluation where multiple similar “needle” requests/responses are embedded within a large “haystack,” and the model must retrieve and reproduce the response associated with a specified needle. The 8-needle variant increases interference, testing robust retrieval, disambiguation, and context tracking at long lengths.","L1: 
L2: Working Memory, Attention, Episodic Memory, Semantic Understanding & Context Recognition
L3: Inhibitory Control",L3
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval measures performance on well-specified professional knowledge-work tasks spanning many occupations, judged by expert human evaluators via head-to-head comparisons. Tasks often require producing realistic work artifacts (e.g., plans, analyses, spreadsheets/presentations) and making decisions that satisfy constraints and stakeholder needs.","L1: Language Production, Language Comprehension
L2: Planning, Decision-making, Working Memory, Semantic Understanding & Context Recognition
L3: Social Reasoning & Theory of Mind, Self-reflection",L3
SWE-Lancer,https://openai.com/index/swe-lancer/,https://arxiv.org/abs/2502.12115,"SWE-Lancer evaluates software engineering agents on scoped, real-world engineering tasks that more closely resemble contracted feature work or bugfix “tickets,” often requiring multi-file changes and integration awareness. It emphasizes end-to-end task completion quality, including reasoning about codebase structure and producing correct patches.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Working Memory, Logical Reasoning, Adaptive Error Correction
L3: ",L2
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath is an expert-level mathematics benchmark intended to measure advanced mathematical problem solving beyond routine competition questions, often requiring deep multi-step derivations. It targets rigorous reasoning, careful handling of definitions/constraints, and sustained problem solving over long solution chains.","L1: 
L2: Logical Reasoning, Planning, Working Memory, Adaptive Error Correction, Attention
L3: Cognitive Flexibility",L3
