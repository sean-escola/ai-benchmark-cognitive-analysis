Benchmark,Website,Paper,Description,Cognitive Functions
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates software-engineering agents on real GitHub issues where the model must produce a patch that makes a repository’s tests pass. The “Verified” subset uses tasks curated to be solvable and reliably testable, emphasizing end-to-end coding, debugging, and adherence to repo conventions rather than isolated coding puzzles.","Planning, Logical Reasoning, Adaptive Error Correction, Working Memory, Language Comprehension, Language Production"
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 measures an agent’s ability to complete practical tasks in a command-line environment (e.g., installing tools, manipulating files, running programs, and interpreting outputs). It stresses iterative troubleshooting under tool feedback and multi-step execution across heterogeneous task types.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Language Comprehension"
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench tests interactive customer-support style agent behavior across domains (e.g., retail/airline/telecom) with policies, tools/APIs, and multi-turn simulated users. Success requires tracking constraints, following (and interpreting) policy, and making correct tool calls over long dialogues.","Language Comprehension, Language Production, Social Reasoning & Theory of Mind, Decision-making, Planning, Working Memory, Inhibitory Control"
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,ARC-AGI is a few-shot abstract reasoning benchmark where models infer hidden transformation rules from small sets of grid input-output examples and apply them to new inputs. It targets fluid generalization to novel tasks with minimal data and strong compositional structure.,"Logical Reasoning, Cognitive Flexibility, Working Memory, Visual Perception, Spatial Representation & Mapping, Attention"
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon agent coherence and strategy by simulating management of a vending-machine business over extended time (e.g., inventory, pricing, supplier negotiation). The score reflects cumulative outcomes from many dependent decisions made under evolving constraints.","Planning, Decision-making, Reward Mechanisms, Working Memory, Episodic Memory, Semantic Understanding & Context Recognition"
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam (HLE) is a broad, frontier-level benchmark spanning difficult academic reasoning, specialized knowledge, and multimodal questions. It often rewards tool-like thinking (e.g., structured analysis) and robustness to tricky prompts rather than memorized fact recall.","Language Comprehension, Language Production, Logical Reasoning, Semantic Understanding & Context Recognition, Multisensory Integration, Visual Perception, Working Memory"
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 evaluates competition-style mathematics problem solving using the 2025 AIME questions, typically under short-answer grading. It stresses multi-step symbolic reasoning and careful constraint tracking rather than rote computation.","Logical Reasoning, Working Memory, Attention, Planning"
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,GPQA Diamond is a curated subset of extremely challenging graduate-level science multiple-choice questions designed to be resistant to shallow pattern matching. It emphasizes deep domain reasoning and precise discrimination among closely related hypotheses.,"Language Comprehension, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory"
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends MMLU-style evaluation to multiple languages, measuring broad academic knowledge and reasoning across many subjects under multilingual prompts. It probes how well a model maintains meaning and performs reasoning when linguistic surface forms change.","Language Comprehension, Language Production, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory"
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro is a harder multimodal benchmark built to evaluate expert-level understanding and reasoning over images plus text across many disciplines. Tasks often require reading diagrams, plots, and structured visuals and integrating them with problem statements to select correct answers.","Visual Perception, Scene Understanding & Visual Reasoning, Multisensory Integration, Visual Attention & Eye Movements, Logical Reasoning, Working Memory"
MathArena Apex,https://matharena.ai/?view=problem&comp=apex--apex_2025,https://arxiv.org/abs/2505.23281,MathArena Apex is a suite focused on difficult mathematics problems intended to differentiate frontier models on advanced multi-step reasoning. It is commonly used to compare solution quality and consistency on hard problem sets where small errors cascade.,"Logical Reasoning, Working Memory, Attention, Planning"
ScreenShot-Pro,https://gui-agent.github.io/grounding-leaderboard/,https://arxiv.org/abs/2504.07981,"ScreenShot-Pro evaluates GUI grounding and computer-use competence by asking models to answer questions or take actions based on screenshots of software interfaces. It stresses locating relevant UI elements, interpreting layout, and mapping intent to interface-relevant decisions.","Visual Perception, Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Spatial Representation & Mapping, Decision-making, Planning"
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning tests understanding of scientific-paper figures and visual artifacts (e.g., plots, diagrams) paired with questions requiring reasoning rather than pure OCR. It focuses on extracting meaning from visual encodings and connecting them to scientific claims.","Visual Perception, Scene Understanding & Visual Reasoning, Semantic Understanding & Context Recognition, Logical Reasoning, Multisensory Integration, Working Memory"
OmniDocBench 1.5,https://github.com/opendatalab/OmniDocBench,https://arxiv.org/abs/2412.07626,"OmniDocBench 1.5 evaluates document understanding and conversion quality across mixed-content pages (text, formulas, tables, and reading order). It emphasizes faithful parsing of structured documents and robustness to diverse layouts and rendering styles.","Visual Perception, Visual Attention & Eye Movements, Attention, Semantic Understanding & Context Recognition, Working Memory"
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU extends multimodal understanding to video, requiring models to answer questions that depend on temporally distributed visual evidence. It probes integration of events over time, tracking entities/actions, and combining audio/text cues when present.","Visual Perception, Multisensory Integration, Cognitive Timing & Predictive Modeling, Working Memory, Attention, Scene Understanding & Visual Reasoning"
LiveCodeBench Pro,https://livecodebenchpro.com/projects/livecodebench-pro/leaderboard,https://arxiv.org/abs/2506.11928,"LiveCodeBench Pro measures coding ability on tasks designed to reduce contamination and better reflect real programming problem solving. It evaluates producing correct solutions under test-based grading, often requiring iterative reasoning about edge cases and constraints.","Logical Reasoning, Planning, Adaptive Error Correction, Working Memory, Language Comprehension, Language Production"
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"The FACTS Benchmark Suite systematically evaluates factuality and grounding by testing whether models make accurate, supportable claims under varied conditions (e.g., long responses, ambiguous prompts, and retrieval/grounding setups). It is aimed at measuring error rates and the tendency to hallucinate or overclaim.","Semantic Understanding & Context Recognition, Logical Reasoning, Inhibitory Control, Self-reflection, Working Memory"
SimpleQA Verified,https://www.kaggle.com/benchmarks/deepmind/simpleqa-verified,https://arxiv.org/abs/2509.07968,"SimpleQA Verified evaluates short-form question answering with verified ground truth, emphasizing precision and avoidance of fabricated details. Because questions are straightforward, performance depends heavily on accurate recall/verification behavior and calibrated uncertainty.","Language Comprehension, Semantic Understanding & Context Recognition, Inhibitory Control, Working Memory"
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA targets physical commonsense and pragmatic reasoning about everyday interactions, extended across many languages (non-parallel). It probes whether models can choose plausible actions or explanations grounded in physical constraints and typical human behavior.","Semantic Understanding & Context Recognition, Logical Reasoning, Spatial Representation & Mapping, Sensorimotor Coordination, Planning, Language Comprehension"
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) is a long-context multi-round coreference retrieval task where multiple similar “needle” requests are embedded in a long “haystack,” and the model must reproduce the specific response associated with a target needle. It stresses accurate retrieval under interference, long-range dependency tracking, and maintaining consistency across extended contexts.","Working Memory, Attention, Episodic Memory, Language Comprehension, Cognitive Timing & Predictive Modeling"
