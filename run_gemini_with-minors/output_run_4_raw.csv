Benchmark,Website,Paper,Description,Cognitive Functions
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro evaluates software engineering agents on realistic, large-scale repository tasks where the model must produce a correct patch that passes project tests. Compared with SWE-bench Verified, it is designed to be harder and more contamination-resistant, spanning multiple languages and more complex engineering workflows.","Planning, Decision-making, Logical Reasoning, Working Memory, Adaptive Error Correction, Language Comprehension, Language Production"
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld measures an agent’s ability to operate a computer (e.g., navigate GUIs, open apps, edit files, and complete multi-step tasks) from screen observations and interaction primitives. It targets end-to-end computer-use competence under partial observability, combining perception with long-horizon action sequencing.","Visual Perception, Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Planning, Decision-making, Sensorimotor Coordination, Working Memory, Cognitive Flexibility, Attention"
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,ARC-AGI is a grid-based abstraction and reasoning benchmark where models infer hidden transformation rules from only a few input–output examples. It is intended to probe fluid reasoning and generalization to novel tasks rather than reliance on memorized patterns.,"Logical Reasoning, Working Memory, Cognitive Flexibility, Spatial Representation & Mapping, Scene Understanding & Visual Reasoning, Attention"
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon agent coherence and strategy by simulating the operation of a vending machine business over an extended period. Agents must manage inventory, pricing, supplier negotiation, and cash flow while adapting to changing conditions to maximize final balance.","Planning, Decision-making, Working Memory, Episodic Memory, Reward Mechanisms, Cognitive Flexibility"
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas benchmarks real-world tool use via the Model Context Protocol, requiring models to discover, call, and coordinate tools across multi-step workflows. Success depends on correct API selection, argument construction, error handling, and synthesizing tool outputs into accurate responses.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Cognitive Flexibility, Language Comprehension, Language Production"
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity agent skills on real software vulnerabilities at scale, including identifying known vulnerabilities from descriptions and discovering previously unknown issues. It emphasizes practical reasoning over codebases, reproducing issues, and iterating toward correct findings under realistic constraints.","Logical Reasoning, Planning, Decision-making, Adaptive Error Correction, Working Memory, Language Comprehension"
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a challenging multimodal benchmark spanning frontier academic and professional domains, aiming to test broad expert-level knowledge and reasoning. Questions often require multi-step inference and careful synthesis, and can be evaluated with or without external tools depending on the setup.","Language Comprehension, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Visual Perception, Scene Understanding & Visual Reasoning"
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a high-quality subset of GPQA consisting of difficult, graduate-level multiple-choice science questions designed to be “Google-proof.” It probes deep domain understanding and reasoning rather than shallow pattern matching, with strong separation between experts and non-experts.","Language Comprehension, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory"
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro extends multimodal academic evaluation with harder, more expert-oriented questions that combine images (e.g., charts, diagrams, figures) and text. Models must interpret visual evidence and perform domain reasoning to select or produce correct answers.","Visual Perception, Scene Understanding & Visual Reasoning, Multisensory Integration, Language Comprehension, Logical Reasoning, Working Memory, Attention"
OmniDocBench 1.5,https://github.com/opendatalab/OmniDocBench,https://arxiv.org/abs/2412.07626,"OmniDocBench 1.5 evaluates document understanding/OCR pipelines across diverse layouts, including text, tables, formulas, and reading order. The benchmark emphasizes faithful extraction and structured reconstruction, stressing robustness to complex formatting and dense visual structure.","Visual Perception, Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Language Comprehension, Language Production, Working Memory, Attention"
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU evaluates multimodal reasoning over videos, requiring models to integrate information across frames and answer questions grounded in dynamic visual content. It targets temporal understanding (events, changes, causality) in addition to static visual recognition.","Visual Perception, Scene Understanding & Visual Reasoning, Cognitive Timing & Predictive Modeling, Working Memory, Attention, Language Comprehension"
LiveCodeBench Pro,https://livecodebenchpro.com/projects/livecodebench-pro/leaderboard,https://arxiv.org/abs/2506.11928,"LiveCodeBench Pro measures competitive programming and coding problem-solving ability, typically via pass@k and/or ELO-style ratings on a curated set of tasks. It stresses translating problem statements into correct, executable solutions with iterative debugging and edge-case handling.","Logical Reasoning, Planning, Working Memory, Adaptive Error Correction, Language Comprehension, Language Production, Decision-making"
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"The FACTS Benchmark Suite systematically evaluates factuality, including whether models produce correct claims, avoid hallucinations, and appropriately handle uncertainty across diverse tasks. It emphasizes reliable knowledge use and restraint when evidence is insufficient.","Semantic Understanding & Context Recognition, Language Comprehension, Language Production, Logical Reasoning, Inhibitory Control, Self-reflection"
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA tests practical commonsense reasoning about everyday physical interactions across many languages, focusing on choosing plausible actions or outcomes. It probes whether models retain commonsense competence under multilingual variation rather than relying on English-only cues.","Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning, Decision-making, Spatial Representation & Mapping"
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) is a long-context retrieval and multi-round coreference benchmark where multiple similar “needle” interactions are embedded in a long “haystack.” Models must track references and reproduce the correct response corresponding to a specified needle, stressing accuracy under extreme context length.","Working Memory, Attention, Semantic Understanding & Context Recognition, Language Comprehension"
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval evaluates economically meaningful, well-specified knowledge-work tasks across many occupations using expert human judging and head-to-head comparisons. Outputs are often real work artifacts (e.g., spreadsheets, plans, analyses), emphasizing end-to-end task completion quality rather than isolated QA.","Planning, Decision-making, Language Production, Language Comprehension, Logical Reasoning, Working Memory, Semantic Understanding & Context Recognition"
SWE-Lancer,https://openai.com/index/swe-lancer/,https://arxiv.org/abs/2502.12115,"SWE-Lancer evaluates software engineering performance in settings meant to reflect real freelance/contract work, where completing tasks correctly has measurable practical value. It emphasizes end-to-end engineering competence: understanding requirements, making code changes in a repository, and producing verifiable results.","Planning, Decision-making, Logical Reasoning, Working Memory, Adaptive Error Correction, Language Comprehension, Language Production"
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath is a difficult mathematics benchmark targeting advanced problem-solving and proof-like reasoning across tiers of difficulty. It is designed to test genuine mathematical generalization and rigor, often benefiting from multi-step derivations and careful error checking.","Logical Reasoning, Working Memory, Planning, Cognitive Flexibility, Adaptive Error Correction"
