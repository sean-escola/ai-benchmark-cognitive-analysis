Benchmark,Website,Paper,Description,Cognitive Functions
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro is a large-scale software engineering benchmark where a model must produce patches that resolve real issues in real repositories, validated by running tests. Compared with SWE-bench Verified, Pro is designed to be harder and more contamination-resistant, and includes broader project and language diversity.","Planning, Logical Reasoning, Adaptive Error Correction, Working Memory, Language Comprehension, Language Production"
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld evaluates multimodal computer-use agents on completing realistic tasks in an operating-system-like environment (e.g., navigating UIs, managing files, using applications) under step limits. It emphasizes end-to-end interaction with graphical interfaces rather than answering questions about screenshots.","Visual Perception, Scene Understanding & Visual Reasoning, Attention, Decision-making, Planning, Spatial Representation & Mapping, Sensorimotor Coordination"
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,ARC-AGI is a “few-shot” abstract reasoning benchmark where models infer hidden transformation rules from a small set of input–output grid examples and apply them to new grids. It targets generalization to novel patterns rather than recall of domain knowledge.,"Logical Reasoning, Working Memory, Cognitive Flexibility, Planning, Spatial Representation & Mapping, Visual Perception"
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 measures long-horizon agent performance in a simulated vending-machine business over many sequential decisions. Agents must manage inventory, pricing, supplier interactions, and budgeting to maximize long-term outcomes under uncertainty.","Planning, Decision-making, Reward Mechanisms, Working Memory, Cognitive Timing & Predictive Modeling, Social Reasoning & Theory of Mind, Semantic Understanding & Context Recognition"
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas evaluates real-world tool use via the Model Context Protocol (MCP), testing whether models can discover appropriate tools, call them with correct schemas, recover from errors, and compose multi-step workflows. Tasks are designed to resemble production integrations where reliability and correct API usage matter.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Language Comprehension"
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity agents on tasks such as locating known vulnerabilities in real open-source projects and, in some settings, discovering new ones. It tests an agent’s ability to interpret vulnerability descriptions, inspect codebases, and produce correct fixes or findings under realistic constraints.","Logical Reasoning, Planning, Adaptive Error Correction, Semantic Understanding & Context Recognition, Working Memory, Inhibitory Control"
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a broad, frontier-difficulty benchmark spanning expert-level questions across disciplines (often including multimodal items). It is intended to probe advanced reasoning and knowledge integration, and can be run with or without tools such as search or code execution.","Language Comprehension, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Scene Understanding & Visual Reasoning, Multisensory Integration"
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a high-quality subset of GPQA consisting of challenging graduate-level science multiple-choice questions, selected to be hard for non-experts. It aims to measure deep scientific reasoning and knowledge rather than shallow pattern matching.","Language Comprehension, Logical Reasoning, Working Memory, Semantic Understanding & Context Recognition"
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro is an advanced multimodal benchmark with expert-level questions over many subjects that require interpreting images (e.g., diagrams, plots, figures) alongside text. It emphasizes multi-step reasoning grounded in visual evidence and domain knowledge.","Visual Perception, Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Language Comprehension, Spatial Representation & Mapping, Working Memory"
OmniDocBench 1.5,https://github.com/opendatalab/OmniDocBench,https://arxiv.org/abs/2412.07626,"OmniDocBench 1.5 evaluates document understanding and OCR-style capabilities across complex layouts, including text, tables, formulas, and reading order. It measures whether a model can accurately extract and structure information from visually rich documents.","Visual Perception, Visual Attention & Eye Movements, Language Comprehension, Semantic Understanding & Context Recognition, Working Memory, Scene Understanding & Visual Reasoning"
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU extends multimodal understanding to video, requiring models to answer questions by integrating information across frames and time. It targets temporal comprehension, event reasoning, and grounded interpretation of visual dynamics.","Visual Perception, Attention, Working Memory, Cognitive Timing & Predictive Modeling, Scene Understanding & Visual Reasoning, Multisensory Integration"
LiveCodeBench Pro,https://livecodebenchpro.com/projects/livecodebench-pro/leaderboard,https://arxiv.org/abs/2506.11928,"LiveCodeBench Pro evaluates coding ability on competitive-programming and software-development style tasks, scored with rigorous execution-based grading and often summarized via ELO-style ratings. It focuses on writing correct programs under realistic constraints, not just explaining code.","Planning, Logical Reasoning, Adaptive Error Correction, Working Memory, Language Comprehension, Language Production"
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"The FACTS Benchmark Suite systematically evaluates factuality and faithfulness, measuring whether model outputs stay supported by provided sources or known ground truth and avoid hallucinated claims. It is designed to stress reliability under different prompting and retrieval conditions.","Language Comprehension, Semantic Understanding & Context Recognition, Working Memory, Inhibitory Control, Self-reflection"
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA is a multilingual, culturally diverse variant of physical commonsense reasoning evaluation, asking which of two solutions better accomplishes a practical goal. It probes whether models can apply everyday physical and procedural knowledge across languages and contexts.","Language Comprehension, Logical Reasoning, Semantic Understanding & Context Recognition, Cognitive Flexibility"
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) is a long-context evaluation where multiple similar “needle” requests are embedded in a long “haystack” of content, and the model must retrieve and reproduce the correct referenced response. The 8-needle setting stresses robustness when many distractors and near-duplicates appear across long inputs.","Working Memory, Attention, Language Comprehension, Semantic Understanding & Context Recognition"
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval evaluates economically valuable, well-specified professional knowledge work by having models produce real artifacts (e.g., spreadsheets, presentations, plans) across many occupations and grading outputs via expert human judgments and pairwise comparisons. It emphasizes end-to-end quality, instruction-following, and practical decision-making rather than short-form Q&A.","Planning, Decision-making, Language Comprehension, Language Production, Working Memory, Social Reasoning & Theory of Mind, Adaptive Error Correction, Self-reflection"
SWE-Lancer,https://openai.com/index/swe-lancer/,https://arxiv.org/abs/2502.12115,"SWE-Lancer is a software engineering benchmark oriented toward real-world development work that resembles contracted “freelance” tasks, often requiring multi-step changes, debugging, and integration across a codebase. It emphasizes completing practical engineering objectives end-to-end rather than solving isolated coding puzzles.","Planning, Decision-making, Logical Reasoning, Adaptive Error Correction, Working Memory, Language Comprehension, Language Production"
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath is an expert-level mathematics benchmark designed to be difficult for frontier models and resistant to memorization, emphasizing novel problem-solving. It targets deep multi-step mathematical reasoning, often with optional tool use (e.g., computation) depending on the evaluation setup.","Logical Reasoning, Working Memory, Planning, Cognitive Flexibility, Semantic Understanding & Context Recognition"
