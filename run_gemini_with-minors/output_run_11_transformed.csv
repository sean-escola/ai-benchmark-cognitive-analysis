Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro evaluates software engineering agents on a large set of real GitHub issues spanning multiple programming languages and realistic repository contexts. Systems must understand the issue, locate relevant code, implement a correct patch, and pass tests under a standardized evaluation harness.","L1: Language Comprehension, Language Production
L2: Logical Reasoning, Planning, Working Memory, Adaptive Error Correction
L3: ",L2
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld measures an agent’s ability to use a graphical operating system to complete end-user tasks (e.g., navigating apps, configuring settings, manipulating files) from visual observations. It emphasizes long-horizon, multi-step interaction where the agent must perceive screen state, choose actions, and recover from mistakes.","L1: Visual Perception
L2: Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Planning, Decision-making, Sensorimotor Coordination, Working Memory
L3: ",L2
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI is a few-shot abstract reasoning benchmark where models infer hidden transformation rules from small sets of input–output grid examples. Success requires generalizing to novel tasks with minimal prior task-specific instruction, emphasizing flexible pattern induction over memorization.","L1: Visual Perception
L2: Logical Reasoning, Working Memory, Spatial Representation & Mapping
L3: Cognitive Flexibility",L3
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon agent coherence by simulating operation of a vending-machine business over many decisions and time steps. Agents must manage inventory, pricing, supplier interactions, and budgeting to maximize final balance while adapting to changing conditions.","L1: 
L2: Planning, Decision-making, Reward Mechanisms, Working Memory, Episodic Memory, Semantic Understanding & Context Recognition
L3: ",L2
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas evaluates real-world tool use via the Model Context Protocol by requiring models to discover appropriate tools, call them correctly, and compose multi-step workflows across APIs. The benchmark stresses reliable orchestration: correct tool selection, parameterization, error handling, and synthesis of results into an answer.","L1: Language Comprehension
L2: Planning, Decision-making, Working Memory, Adaptive Error Correction, Semantic Understanding & Context Recognition
L3: ",L2
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym tests cybersecurity capability through tasks such as identifying, reproducing, and sometimes discovering vulnerabilities in real open-source codebases from natural-language vulnerability descriptions. Agents must reason about program behavior, navigate projects, and propose concrete fixes or exploit-relevant analyses under time/attempt constraints.","L1: Language Comprehension
L2: Logical Reasoning, Planning, Working Memory, Adaptive Error Correction
L3: ",L2
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a frontier, multi-domain benchmark designed to probe advanced academic and professional reasoning, often requiring synthesis across specialized knowledge. It includes challenging questions (often multimodal) intended to be difficult to answer by retrieval alone, emphasizing careful reasoning and verification.","L1: Language Comprehension, Visual Perception
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory
L3: ",L2
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a high-quality subset of GPQA consisting of very difficult multiple-choice science questions intended to be “Google-proof.” It targets expert-level reasoning and deep scientific understanding rather than superficial recall, with strong emphasis on selecting the correct option among plausible distractors.","L1: Language Comprehension
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory
L3: ",L2
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro evaluates multimodal understanding and reasoning over problems that combine text with images such as diagrams, charts, and scientific figures. It emphasizes robust visual grounding and multi-step reasoning in expert-style questions across many disciplines.","L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Logical Reasoning, Working Memory
L3: ",L2
OmniDocBench 1.5,https://github.com/opendatalab/OmniDocBench,https://arxiv.org/abs/2412.07626,"OmniDocBench 1.5 measures document understanding and OCR-centric capabilities on complex, real-world documents containing text, tables, formulas, and reading-order structure. Systems must accurately extract and structure content, which stresses fine-grained layout understanding and faithful transcription.","L1: Visual Perception, Language Comprehension
L2: Visual Attention & Eye Movements, Semantic Understanding & Context Recognition, Working Memory
L3: ",L2
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU extends multimodal reasoning to video, requiring models to answer questions that depend on temporally distributed visual evidence. It tests whether systems can track events across time, integrate cues across frames, and maintain coherent interpretations of dynamic scenes.","L1: Visual Perception
L2: Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Working Memory, Multisensory Integration
L3: Cognitive Timing & Predictive Modeling",L3
LiveCodeBench Pro,https://livecodebenchpro.com/projects/livecodebench-pro/leaderboard,https://arxiv.org/abs/2506.11928,"LiveCodeBench Pro evaluates coding performance on competitively-styled programming tasks with strong controls intended to reduce contamination and reflect current difficulty. Models must generate correct, executable solutions under constraints and are often evaluated with robust correctness checks and leaderboard-style ratings.","L1: Language Comprehension, Language Production
L2: Logical Reasoning, Planning, Working Memory, Adaptive Error Correction
L3: ",L2
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"The FACTS Benchmark Suite systematically evaluates factuality by testing whether model outputs remain grounded, avoid fabrication, and correctly handle uncertain or unsupported claims. It typically probes both retrieval-style accuracy and the model’s tendency to overclaim when evidence is insufficient.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning
L3: Inhibitory Control, Self-reflection",L3
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,Global PIQA evaluates commonsense physical reasoning across many languages and cultural contexts by presenting everyday situations and asking for the more plausible/appropriate outcome or explanation. It stresses robustness of pragmatic reasoning beyond English-centric distributions.,"L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning
L3: ",L2
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) is a long-context retrieval and reasoning evaluation where multiple similar “needle” requests are embedded within a long “haystack,” and the model must reproduce the correct response associated with a specified needle. It probes whether models can maintain and use precise references across very long inputs without confusion.","L1: 
L2: Working Memory, Attention, Semantic Understanding & Context Recognition
L3: ",L2
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval evaluates well-specified professional knowledge-work tasks across many occupations, judging the quality of real work products (e.g., spreadsheets, slides, plans) relative to human professionals. It emphasizes end-to-end task execution quality rather than narrow multiple-choice accuracy.","L1: Language Production, Language Comprehension
L2: Planning, Decision-making, Semantic Understanding & Context Recognition, Working Memory
L3: ",L2
SWE-Lancer,https://openai.com/index/swe-lancer/,https://arxiv.org/abs/2502.12115,"SWE-Lancer evaluates software engineering ability in settings closer to real-world engineering workflows, emphasizing producing patches that satisfy task requirements under realistic constraints. It targets agentic coding behaviors such as iterative debugging, repo navigation, and aligning changes to specification-like requests.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Working Memory, Adaptive Error Correction, Logical Reasoning
L3: ",L2
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath measures advanced mathematical problem-solving on expert-level questions designed to be challenging for current frontier models. Success requires multi-step derivations, careful symbolic manipulation, and high precision rather than approximate or heuristic answers.","L1: 
L2: Logical Reasoning, Working Memory, Planning, Semantic Understanding & Context Recognition
L3: Cognitive Flexibility",L3
