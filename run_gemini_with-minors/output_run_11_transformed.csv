Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,SWE-bench Verified evaluates software engineering agents on real GitHub issues by requiring a model to produce a patch that makes a failing test suite pass. The “Verified” subset emphasizes tasks confirmed solvable and uses a standardized evaluation harness to check correctness via tests and repository state changes.,"L1: Language Comprehension
L2: Planning, Working Memory, Adaptive Error Correction, Logical Reasoning, Decision-making
L3: ",L2
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 evaluates agentic performance in command-line environments where models must navigate files, run programs, install dependencies, and debug issues using terminal tools. Success depends on choosing appropriate commands, interpreting outputs/errors, and iterating until an objective state is reached.","L1: Language Comprehension
L2: Planning, Adaptive Error Correction, Working Memory, Decision-making, Logical Reasoning
L3: ",L2
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench measures an agent’s ability to handle multi-turn customer-support style tasks while using tools/APIs and complying with domain policies (e.g., retail, airline, telecom). It tests whether the agent can maintain dialogue state, follow constraints, and execute correct tool calls to resolve the user’s request end-to-end.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Working Memory
L3: Inhibitory Control, Social Reasoning & Theory of Mind",L3
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,ARC-AGI is a few-shot abstract reasoning benchmark where systems infer hidden rules from a small number of input–output grid examples and generate the correct output for a new grid. The tasks aim to probe flexible induction and compositional generalization rather than memorized knowledge.,"L1: Visual Perception
L2: Logical Reasoning, Working Memory, Spatial Representation & Mapping, Scene Understanding & Visual Reasoning, Attention
L3: Cognitive Flexibility",L3
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon autonomous business management in a simulated vending-machine setting, typically spanning a full simulated year. Agents must plan inventory, pricing, supplier interactions, and budgeting across many sequential decisions to maximize final balance and avoid failure modes.","L1: 
L2: Planning, Decision-making, Reward Mechanisms, Working Memory, Adaptive Error Correction
L3: Cognitive Timing & Predictive Modeling",L3
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam (HLE) is a frontier, often multimodal benchmark intended to test difficult knowledge and reasoning problems across many domains. Tasks are designed to stress deep multi-step inference, careful reading of prompts and artifacts, and robust answering under uncertainty.","L1: Language Comprehension, Visual Perception
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Scene Understanding & Visual Reasoning
L3: ",L2
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 evaluates competition-style mathematics problem solving, typically requiring multi-step derivations and precise final numeric answers. It is used to measure symbolic manipulation, structured reasoning, and the ability to avoid subtle arithmetic or logic errors.","L1: 
L2: Logical Reasoning, Working Memory, Planning, Adaptive Error Correction
L3: ",L2
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a high-difficulty subset of graduate-level science multiple-choice questions designed to be hard to answer via superficial pattern matching. It emphasizes deep conceptual understanding and multi-step reasoning across physics, chemistry, and biology.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory
L3: Inhibitory Control",L3
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends broad academic knowledge evaluation to multiple languages, testing whether models can answer MMLU-style questions across subjects in non-English settings. It probes both multilingual comprehension and whether reasoning and knowledge transfer across languages and domains.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory
L3: ",L2
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro is a multimodal benchmark emphasizing expert-level understanding and reasoning over images and text across many disciplines. Compared with earlier MMMU settings, it is designed to be more challenging and to better test robust visual grounding and cross-modal reasoning.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Multisensory Integration, Attention, Logical Reasoning, Working Memory
L3: ",L2
MathArena Apex,https://matharena.ai/?view=problem&comp=apex--apex_2025,https://arxiv.org/abs/2505.23281,"MathArena Apex aggregates difficult math problems intended to stress frontier mathematical reasoning under standardized evaluation conditions. It emphasizes solving novel problems with long, structured chains of inference rather than relying on short heuristics.","L1: 
L2: Logical Reasoning, Working Memory, Planning, Adaptive Error Correction
L3: ",L2
ScreenShot-Pro,https://gui-agent.github.io/grounding-leaderboard/,https://arxiv.org/abs/2504.07981,"ScreenShot-Pro evaluates whether models can understand and act on high-resolution GUI screenshots, often requiring identification of interface elements and correct grounding of actions to screen regions. It targets practical “computer use” competence where success depends on accurate visual localization and stepwise interaction decisions.","L1: Visual Perception
L2: Visual Attention & Eye Movements, Spatial Representation & Mapping, Sensorimotor Coordination, Planning, Decision-making, Working Memory
L3: ",L2
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning focuses on reasoning over scientific charts/figures from papers, requiring models to interpret plotted data, axes, legends, and annotations. It tests whether a model can extract quantitative/relational information from visuals and integrate it with the question’s textual constraints.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Logical Reasoning, Semantic Understanding & Context Recognition, Attention, Working Memory
L3: ",L2
OmniDocBench 1.5,https://github.com/opendatalab/OmniDocBench,https://arxiv.org/abs/2412.07626,"OmniDocBench 1.5 evaluates document understanding and OCR-like extraction across complex layouts including text blocks, tables, formulas, and reading order. It probes how well models preserve structure and accurately transcribe/parse visually presented documents into correct textual and structured outputs.","L1: Visual Perception, Language Production
L2: Scene Understanding & Visual Reasoning, Attention, Working Memory
L3: ",L2
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU tests multimodal understanding and reasoning over short videos paired with questions, stressing temporal comprehension beyond single images. Tasks often require integrating events across time, tracking state changes, and answering with grounded multi-step reasoning.","L1: Visual Perception
L2: Working Memory, Scene Understanding & Visual Reasoning, Attention, Multisensory Integration
L3: Cognitive Timing & Predictive Modeling",L3
LiveCodeBench Pro,https://livecodebenchpro.com/projects/livecodebench-pro/leaderboard,https://arxiv.org/abs/2506.11928,"LiveCodeBench Pro evaluates coding ability on a curated set of programming tasks with emphasis on practical problem solving and robustness under standardized conditions. It typically measures whether the model can synthesize correct algorithms, implement them precisely, and handle edge cases and failures through iteration.","L1: Language Comprehension
L2: Logical Reasoning, Planning, Working Memory, Adaptive Error Correction, Decision-making
L3: ",L2
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"FACTS Benchmark Suite systematically evaluates factuality by measuring whether model outputs remain consistent with provided sources, known truths, or task constraints across a variety of factuality-focused tests. It emphasizes calibration-like behavior: avoiding unsupported claims, resolving conflicts, and maintaining faithful generation.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning
L3: Inhibitory Control, Self-reflection",L3
SimpleQA Verified,https://www.kaggle.com/benchmarks/deepmind/simpleqa-verified,https://arxiv.org/abs/2509.07968,"SimpleQA Verified evaluates factual accuracy on short, unambiguous questions with verified answers and standardized grading. It primarily probes whether models can retrieve the correct fact and avoid confident hallucinations when uncertain.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition
L3: Inhibitory Control",L3
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA evaluates physical commonsense and practical reasoning across languages and cultural contexts, aiming to test whether models choose plausible actions/explanations in everyday situations. It stresses generalization of intuitive physics and affordance reasoning beyond English-centric data distributions.","L1: Language Comprehension
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Spatial Representation & Mapping
L3: Cognitive Flexibility",L3
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) is a long-context, multi-round coreference/retrieval evaluation where multiple similar “needle” interactions are embedded within a long “haystack,” and the model must reproduce the correct response tied to a specified needle. It targets sustained context tracking and precise reference resolution under heavy distractors.","L1: Language Comprehension
L2: Working Memory, Attention, Episodic Memory, Semantic Understanding & Context Recognition
L3: ",L2
