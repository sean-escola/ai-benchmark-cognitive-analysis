Benchmark,Website,Paper,Description,Cognitive Functions
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates software-engineering ability by asking a model to generate patches that fix real issues in open-source Python repositories, judged by whether unit tests pass. The “Verified” subset uses human-validated tasks intended to be solvable and to reduce ambiguity in problem statements and grading.","Planning, Logical Reasoning, Working Memory, Adaptive Error Correction, Semantic Understanding & Context Recognition"
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 measures agentic performance on real command-line tasks, where the model must interact with a terminal environment to accomplish objectives (e.g., debugging, installing tools, manipulating files, running programs). Success depends on executing correct sequences of actions and recovering from errors under realistic constraints.","Planning, Decision-making, Working Memory, Adaptive Error Correction, Language Comprehension"
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench evaluates interactive tool-using agents in customer-support style scenarios (e.g., retail, airline, telecom) that require multi-turn dialogue plus API/tool calls while following domain policies. It probes whether an agent can consistently adhere to constraints, maintain state across turns, and resolve user goals through correct procedures.","Social Reasoning & Theory of Mind, Empathy, Inhibitory Control, Planning, Decision-making, Working Memory, Language Comprehension"
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI tests “fluid intelligence” via few-shot abstract pattern induction on grid transformation puzzles, where the system must infer rules from a small set of input-output examples. It is designed to emphasize generalization to novel tasks over memorization, with performance reflecting the ability to discover latent structure and apply it reliably.","Logical Reasoning, Cognitive Flexibility, Working Memory, Spatial Representation & Mapping, Scene Understanding & Visual Reasoning, Visual Perception"
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 measures long-horizon agent coherence by having a model run a simulated vending-machine business over many sequential decisions, optimizing for outcomes like final balance. The agent must plan, manage inventory and pricing, negotiate with simulated counterparts, and adapt strategy as conditions change over time.","Planning, Decision-making, Reward Mechanisms, Working Memory, Cognitive Timing & Predictive Modeling"
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a large, challenging benchmark spanning expert-level questions across many domains, with an emphasis on difficult reasoning and broad knowledge under realistic, sometimes multimodal prompts. It is often used to compare frontier models’ ability to solve complex problems with and without tool assistance (e.g., search or code).","Language Comprehension, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Visual Perception, Multisensory Integration"
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 evaluates competition-style mathematics problem solving, typically requiring multi-step derivations and careful symbolic manipulation. It stresses correctness under time-like constraints and sensitivity to small logical errors that can derail a solution.","Logical Reasoning, Working Memory, Planning"
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a difficult multiple-choice science benchmark intended to be “Google-proof,” focusing on questions where superficial retrieval is insufficient and reasoning matters. The Diamond subset emphasizes higher-quality items that better discriminate expert-level understanding.","Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Language Comprehension"
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends MMLU to multiple languages and subjects, testing whether models can answer academic and professional questions across diverse linguistic contexts. It probes robustness of knowledge and reasoning when the same underlying skills must be expressed and understood in different languages.","Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory"
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro is a multimodal benchmark covering many disciplines where questions require integrating text with images such as diagrams, charts, and figures. It targets expert-level multimodal reasoning, stressing accurate perception plus domain knowledge and structured inference.","Visual Perception, Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Working Memory, Language Comprehension"
MathArena Apex,https://matharena.ai/?view=problem&comp=apex--apex_2025,https://arxiv.org/abs/2505.23281,"MathArena Apex aggregates harder mathematics problems intended to stress frontier-level mathematical reasoning and solution robustness beyond standard contest items. It is commonly used to measure how well models sustain long, correct reasoning chains and avoid subtle algebraic or logical mistakes.","Logical Reasoning, Working Memory, Planning, Adaptive Error Correction"
ScreenShot-Pro,https://gui-agent.github.io/grounding-leaderboard/,https://arxiv.org/abs/2504.07981,"ScreenShot-Pro evaluates GUI understanding from screenshots, requiring models or agents to interpret interface layouts and identify relevant UI elements for tasks. It emphasizes precise visual grounding—mapping instructions to the correct on-screen targets under varied, high-resolution interfaces.","Visual Perception, Visual Attention & Eye Movements, Spatial Representation & Mapping, Scene Understanding & Visual Reasoning, Sensorimotor Coordination, Planning"
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning focuses on questions grounded in scientific figures from research papers, testing whether models can extract and reason over visual evidence with technical context. Performance depends on accurately reading plots/diagrams and integrating them with scientific language and quantitative reasoning.","Visual Perception, Scene Understanding & Visual Reasoning, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Language Comprehension"
OmniDocBench 1.5,https://github.com/opendatalab/OmniDocBench,https://arxiv.org/abs/2412.07626,"OmniDocBench 1.5 evaluates document understanding and OCR across heterogeneous elements such as text, formulas, tables, and reading order. It measures whether systems can faithfully reconstruct structured content from complex document images and preserve layout-dependent meaning.","Visual Perception, Scene Understanding & Visual Reasoning, Spatial Representation & Mapping, Working Memory, Semantic Understanding & Context Recognition"
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU tests multimodal reasoning over videos, where answers require integrating information across time rather than from a single frame. It stresses temporal event understanding, long-range dependencies, and coherent summarization of visual evidence.","Visual Perception, Scene Understanding & Visual Reasoning, Working Memory, Cognitive Timing & Predictive Modeling, Attention"
LiveCodeBench Pro,https://livecodebenchpro.com/projects/livecodebench-pro/leaderboard,https://arxiv.org/abs/2506.11928,"LiveCodeBench Pro evaluates coding ability on programming tasks designed to better reflect contemporary, non-stale coding requirements, often scored with functional tests and competitive-style metrics. It probes algorithmic reasoning, implementation accuracy, and iterative debugging under realistic constraints.","Logical Reasoning, Planning, Working Memory, Adaptive Error Correction, Language Comprehension"
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,The FACTS Benchmark Suite systematically evaluates factuality by measuring whether models produce accurate claims and appropriately avoid unsupported assertions across diverse settings. It targets both knowledge correctness and reliability behaviors such as resisting hallucination when evidence is insufficient.,"Semantic Understanding & Context Recognition, Inhibitory Control, Self-reflection, Working Memory, Language Production"
SimpleQA Verified,https://www.kaggle.com/benchmarks/deepmind/simpleqa-verified,https://arxiv.org/abs/2509.07968,"SimpleQA Verified is a short-form factual question answering benchmark with verification-oriented grading to reduce ambiguity and reward correctness. It is designed to test whether models can provide concise, accurate answers and refrain from confident errors on simple-looking queries.","Semantic Understanding & Context Recognition, Language Comprehension, Inhibitory Control, Language Production"
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA evaluates physical commonsense reasoning across many languages, typically using short scenarios that require choosing the more plausible action or outcome. It targets grounded everyday reasoning about objects, forces, affordances, and simple causal consequences under multilingual variation.","Spatial Representation & Mapping, Sensorimotor Coordination, Logical Reasoning, Language Comprehension, Semantic Understanding & Context Recognition"
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) evaluates long-context, multi-round coreference and retrieval by inserting multiple similar “needle” requests into a long “haystack” dialogue and asking the model to reproduce the correct response for a specific needle. It stresses maintaining attention over very long contexts and resolving confusable references accurately.","Working Memory, Attention, Language Comprehension, Episodic Memory, Semantic Understanding & Context Recognition"
