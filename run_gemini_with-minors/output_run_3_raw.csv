Benchmark,Website,Paper,Description,Cognitive Functions
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro is a large-scale software engineering benchmark where a model must understand an issue in a real repository, modify code, and produce a patch that passes the project’s tests. Compared with SWE-bench Verified, it is designed to be harder and more representative of professional, multi-language engineering work and realistic repo constraints.","Language Comprehension, Language Production, Logical Reasoning, Planning, Working Memory, Adaptive Error Correction, Decision-making"
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld evaluates computer-use agents on completing tasks inside a desktop operating system using screenshots and UI interactions, requiring multi-step navigation through applications and settings. It emphasizes robust action selection under partial observability and interface variability rather than single-shot question answering.","Visual Perception, Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Planning, Decision-making, Working Memory, Attention, Sensorimotor Coordination"
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI measures “fluid” abstraction and generalization by asking models to infer hidden rules from a few input–output grid examples and produce the correct output for a new grid. The tasks are intentionally novel and low-data, stressing compositional reasoning over memorization.","Logical Reasoning, Cognitive Flexibility, Working Memory, Visual Perception, Scene Understanding & Visual Reasoning, Spatial Representation & Mapping, Planning"
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon agent coherence in a simulated vending-machine business over many sequential decisions, with outcomes reflected in final financial performance. Strong performance requires sustained strategy, adapting to changing conditions, and managing resources across extended interaction histories.","Planning, Decision-making, Working Memory, Reward Mechanisms, Cognitive Flexibility, Self-reflection, Language Comprehension, Language Production"
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas evaluates real-world tool use via the Model Context Protocol (MCP), where models must discover appropriate tools, invoke them correctly, and chain multiple calls to complete tasks. The benchmark stresses robustness to tool errors, schema/argument correctness, and workflow execution across heterogeneous services.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Language Comprehension, Language Production, Attention"
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym tests cybersecurity agent capabilities on tasks involving finding known vulnerabilities from high-level descriptions and discovering new vulnerabilities in real codebases. It measures code comprehension, exploit-relevant reasoning, and the ability to iteratively test hypotheses against program behavior.","Language Comprehension, Logical Reasoning, Planning, Working Memory, Adaptive Error Correction, Decision-making"
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam (HLE) is a frontier, expert-level benchmark spanning many subjects (often including multimodal items) intended to probe advanced reasoning and knowledge at the edge of human expertise. It is commonly reported both with and without tools (e.g., search/code) to separate core reasoning from tool-augmented performance.","Language Comprehension, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Visual Perception, Scene Understanding & Visual Reasoning"
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,GPQA Diamond is a high-quality subset of GPQA composed of very difficult graduate-level science multiple-choice questions selected to be resistant to simple web search and superficial pattern matching. It tests whether models can apply deep domain knowledge and reasoning to choose the correct option under strong distractors.,"Language Comprehension, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Inhibitory Control"
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro is a challenging multimodal benchmark covering many academic disciplines, where models answer questions requiring joint reasoning over images (charts, diagrams, figures) and text. It emphasizes expert-level visual–text integration and multi-step reasoning beyond basic recognition.","Multisensory Integration, Visual Perception, Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Language Comprehension, Logical Reasoning, Working Memory, Attention"
OmniDocBench 1.5,https://github.com/opendatalab/OmniDocBench,https://arxiv.org/abs/2412.07626,"OmniDocBench 1.5 evaluates document understanding and OCR quality across diverse layouts including text blocks, tables, formulas, and reading order. It targets end-to-end extraction fidelity and structural parsing from real document images rather than only plain-text transcription.","Visual Perception, Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Spatial Representation & Mapping, Language Comprehension, Working Memory, Attention"
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU evaluates multimodal reasoning over video by requiring models to answer questions that depend on temporal dynamics, events, and context that unfold across frames. It stresses integrating information across time rather than treating the input as a single static image.","Visual Perception, Scene Understanding & Visual Reasoning, Cognitive Timing & Predictive Modeling, Working Memory, Attention, Logical Reasoning"
LiveCodeBench Pro,https://livecodebenchpro.com/projects/livecodebench-pro/leaderboard,https://arxiv.org/abs/2506.11928,"LiveCodeBench Pro benchmarks code-generation and problem-solving on competitive-programming-style tasks curated to reflect modern coding difficulty and reduce leakage. It evaluates whether a model can design algorithms, implement correct solutions, and handle edge cases under realistic constraints.","Language Comprehension, Language Production, Logical Reasoning, Planning, Working Memory, Adaptive Error Correction, Decision-making"
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"The FACTS Benchmark Suite systematically evaluates factuality and grounding by testing whether model outputs remain consistent with provided sources and avoid unsupported claims. It focuses on error types such as hallucinations, overconfident fabrication, and failures to respect evidence constraints across varied tasks.","Language Comprehension, Language Production, Semantic Understanding & Context Recognition, Logical Reasoning, Inhibitory Control, Self-reflection, Attention"
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA evaluates commonsense and practical reasoning across many languages and cultural contexts, often using non-parallel multilingual items to reduce direct translation artifacts. It probes whether models can generalize everyday physical and social intuition beyond English-centric data distributions.","Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning, Social Reasoning & Theory of Mind, Cognitive Flexibility"
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) is a long-context evaluation where multiple similar “needle” requests are embedded in a long “haystack,” and the model must retrieve and reproduce the correct response for a specified needle. It stresses sustained context tracking, interference resistance among similar items, and accurate cross-reference resolution over long inputs.","Working Memory, Attention, Language Comprehension, Semantic Understanding & Context Recognition, Inhibitory Control"
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval evaluates economically meaningful knowledge-work tasks across many occupations by comparing model outputs to professional standards using expert human judgments. Tasks often require producing real artifacts (e.g., spreadsheets, presentations, plans) and balancing multiple constraints to meet user objectives.","Planning, Decision-making, Language Production, Language Comprehension, Semantic Understanding & Context Recognition, Working Memory, Self-reflection, Social Reasoning & Theory of Mind"
SWE-Lancer,https://openai.com/index/swe-lancer/,https://arxiv.org/abs/2502.12115,"SWE-Lancer evaluates end-to-end software engineering on realistic repository tasks that resemble professional development workflows (e.g., implementing features, fixing bugs, integrating changes). It emphasizes producing correct patches and navigating project constraints, often requiring iterative debugging and tool-mediated verification.","Language Comprehension, Language Production, Logical Reasoning, Planning, Working Memory, Adaptive Error Correction, Decision-making"
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath is an expert-level mathematics benchmark intended to measure frontier mathematical reasoning on problems designed to be hard for current models and less susceptible to memorization. It emphasizes multi-step derivations, careful handling of definitions and constraints, and high-precision symbolic reasoning.","Logical Reasoning, Working Memory, Planning, Cognitive Flexibility, Inhibitory Control"
