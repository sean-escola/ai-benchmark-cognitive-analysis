Benchmark,Website,Paper,Description,Cognitive Functions
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates agentic software engineering by giving a model a real GitHub issue and repository state, requiring it to produce a patch that fixes the problem and passes tests. The Verified subset filters to tasks confirmed solvable and reliably testable, emphasizing end-to-end repo understanding, editing, and debugging rather than isolated coding snippets.","Planning, Adaptive Error Correction, Working Memory, Logical Reasoning, Semantic Understanding & Context Recognition"
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 measures how well an agent can complete real command-line tasks in a sandboxed terminal environment (e.g., manipulating files, running build tools, diagnosing failures). Success requires choosing and sequencing shell actions, interpreting tool outputs, and iterating after errors under resource and time constraints.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition"
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench evaluates interactive tool-using agents in simulated customer-support domains (e.g., retail, airline, telecom) where the model must follow policies while using APIs over multi-turn dialogues. It stresses reliable tool invocation, maintaining constraints across turns, and balancing helpfulness with policy compliance.","Language Comprehension, Language Production, Planning, Decision-making, Inhibitory Control, Social Reasoning & Theory of Mind, Working Memory"
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI tests fluid, example-driven reasoning on novel grid-based puzzles: models infer an implicit rule from a few input–output demonstrations and generate the correct transformation for a new input. It aims to reduce reliance on memorized knowledge and instead emphasize abstraction, compositionality, and generalization from sparse data.","Logical Reasoning, Cognitive Flexibility, Working Memory, Spatial Representation & Mapping, Scene Understanding & Visual Reasoning"
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon autonomy by having an agent run a simulated vending-machine business over many steps, optimizing inventory, pricing, suppliers, and cash flow. High scores require sustained strategy, adaptation to changing conditions, and coherent bookkeeping over extended interactions.","Planning, Decision-making, Reward Mechanisms, Working Memory, Cognitive Timing & Predictive Modeling, Adaptive Error Correction"
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a difficult, broad benchmark spanning advanced academic and professional questions (often multimodal), intended to probe frontier reasoning and knowledge at the edge of what typical experts can solve quickly. It emphasizes multi-step problem solving, careful reading, and (when enabled) effective use of external tools like search or code to validate answers.","Language Comprehension, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Scene Understanding & Visual Reasoning, Visual Perception"
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 uses problems from the American Invitational Mathematics Examination to test competition-style mathematical reasoning. Tasks generally require multi-step derivations, algebraic manipulation, and precise final numeric answers, making it sensitive to small reasoning or arithmetic errors.","Logical Reasoning, Working Memory, Adaptive Error Correction"
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a high-quality subset of graduate-level multiple-choice science questions designed to be hard to answer by superficial pattern matching or quick web lookup. It probes deep conceptual understanding and careful elimination among close distractors across physics, chemistry, and biology.","Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Language Comprehension"
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends MMLU to multiple languages, testing broad academic knowledge and reasoning across subjects with standardized multiple-choice questions. It evaluates whether capabilities transfer across languages and scripts while preserving subject-matter competence.","Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory"
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro is a harder, more evaluation-focused variant of MMMU for multimodal expert reasoning, combining images (e.g., diagrams, charts) with textual questions across disciplines. It targets robust visual-text integration and reasoning under more demanding question design and scoring.","Multisensory Integration, Visual Perception, Scene Understanding & Visual Reasoning, Language Comprehension, Logical Reasoning, Working Memory"
MathArena Apex,https://matharena.ai/?view=problem&comp=apex--apex_2025,https://arxiv.org/abs/2505.23281,"MathArena Apex aggregates challenging math problems and evaluates models under consistent settings to compare advanced mathematical reasoning performance. The focus is on solving difficult, multi-step problems (often beyond standard competitions), making planning and error checking critical.","Logical Reasoning, Planning, Working Memory, Adaptive Error Correction"
ScreenShot-Pro,https://gui-agent.github.io/grounding-leaderboard/,https://arxiv.org/abs/2504.07981,"ScreenShot-Pro evaluates GUI understanding and action grounding from screenshots, requiring a model to interpret interface elements and identify or act on the correct targets. It emphasizes spatial/layout understanding of complex UIs and reliable mapping from language instructions to visual regions.","Visual Perception, Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Spatial Representation & Mapping, Sensorimotor Coordination, Language Comprehension, Planning"
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning tests multimodal reasoning over scientific paper figures and layouts, often requiring interpreting plots, annotations, and visual structure to answer questions. It stresses visual analysis plus domain-aware inference rather than simple OCR or captioning.","Visual Perception, Scene Understanding & Visual Reasoning, Attention, Semantic Understanding & Context Recognition, Logical Reasoning, Language Comprehension"
OmniDocBench 1.5,https://github.com/opendatalab/OmniDocBench,https://arxiv.org/abs/2412.07626,"OmniDocBench 1.5 evaluates document understanding across diverse layouts, including text, tables, formulas, and reading order, typically using edit-distance style metrics against ground truth. It targets robust extraction and structure recovery from visually complex documents rather than plain-text QA.","Visual Perception, Attention, Semantic Understanding & Context Recognition, Scene Understanding & Visual Reasoning, Language Comprehension, Working Memory"
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU extends multimodal understanding to video, requiring models to integrate information across frames and time to answer questions. It probes temporal scene understanding, event inference, and cross-modal grounding between visual content and language prompts.","Visual Perception, Multisensory Integration, Cognitive Timing & Predictive Modeling, Working Memory, Scene Understanding & Visual Reasoning, Language Comprehension"
LiveCodeBench Pro,https://livecodebenchpro.com/projects/livecodebench-pro/leaderboard,https://arxiv.org/abs/2506.11928,"LiveCodeBench Pro measures coding performance under more realistic, execution-grounded conditions, often emphasizing iterative development, debugging, and correctness on unseen tasks. It is designed to better reflect practical software creation by testing whether produced code actually works end-to-end.","Planning, Adaptive Error Correction, Working Memory, Logical Reasoning, Semantic Understanding & Context Recognition"
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"FACTS Benchmark Suite systematically evaluates factuality by checking whether model outputs remain grounded and accurate across a variety of factuality stress tests. It targets hallucination tendencies, calibration under uncertainty, and robustness of factual claims under different prompting or task formats.","Semantic Understanding & Context Recognition, Inhibitory Control, Logical Reasoning, Self-reflection, Working Memory, Language Production"
SimpleQA Verified,https://www.kaggle.com/benchmarks/deepmind/simpleqa-verified,https://arxiv.org/abs/2509.07968,"SimpleQA Verified is a factual question answering benchmark with verified answers and evaluation procedures intended to reduce ambiguity and scoring noise. It focuses on precision for short, objective questions where hallucinated or overconfident responses are penalized.","Semantic Understanding & Context Recognition, Language Comprehension, Language Production, Inhibitory Control"
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA evaluates physical commonsense reasoning across many languages, typically asking models to choose the more plausible solution to a practical, real-world goal. It tests whether everyday physical intuition and procedural plausibility transfer across linguistic and cultural contexts.","Semantic Understanding & Context Recognition, Logical Reasoning, Spatial Representation & Mapping, Sensorimotor Coordination, Language Comprehension"
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) is a long-context retrieval-and-reproduction evaluation where multiple similar “needle” interactions are embedded in long “haystack” contexts and the model must reproduce the correct response for a specified needle. It probes robust attention allocation, interference resistance among similar passages, and accurate recall at long context lengths.","Working Memory, Attention, Language Comprehension, Episodic Memory, Inhibitory Control"
