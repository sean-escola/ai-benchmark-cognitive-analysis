Benchmark,Website,Paper,Description,Cognitive Functions
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro evaluates software engineering agents on real repository issues that require producing correct patches across multiple languages and realistic project structures. It emphasizes end-to-end problem solving: understanding the bug report, navigating code, implementing a fix, and ensuring tests/builds pass under a standardized harness.","Language Comprehension, Language Production, Logical Reasoning, Planning, Working Memory, Adaptive Error Correction"
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld measures “computer use” capability in a full operating-system-like environment, where agents must complete multi-step tasks by interacting with GUIs and applications. Success requires perceiving screen state, deciding actions (click/type/scroll), and adapting to dynamic UI feedback and errors over long trajectories.","Visual Perception, Scene Understanding & Visual Reasoning, Attention, Planning, Decision-making, Sensorimotor Coordination, Working Memory, Adaptive Error Correction"
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI is a few-shot pattern induction benchmark where models infer latent transformation rules from a small set of input–output grid examples and apply them to a new grid. It targets generalization to novel tasks rather than memorization, demanding flexible abstraction and structured reasoning over spatial relations.","Logical Reasoning, Cognitive Flexibility, Working Memory, Spatial Representation & Mapping, Scene Understanding & Visual Reasoning, Attention"
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon agent coherence by simulating the operation of a vending machine business over many decisions (e.g., sourcing, pricing, inventory management, negotiation). The metric is typically economic performance over time, rewarding sustained strategy, adaptation to changing conditions, and error recovery.","Planning, Decision-making, Reward Mechanisms, Working Memory, Adaptive Error Correction, Language Comprehension, Language Production"
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas evaluates real-world tool use via the Model Context Protocol, focusing on whether models can discover, call, and sequence tools correctly across multi-step workflows. Tasks stress robust API interaction, argument formatting, handling failures/retries, and synthesizing tool outputs into correct final responses.","Planning, Decision-making, Working Memory, Adaptive Error Correction, Language Comprehension, Semantic Understanding & Context Recognition"
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym benchmarks cybersecurity agent capability on tasks such as identifying known vulnerabilities from descriptions and discovering issues in real open-source code. It emphasizes systematic investigation, patch/PoC-style reasoning, and precise interaction with codebases and tooling under realistic constraints.","Logical Reasoning, Planning, Adaptive Error Correction, Working Memory, Attention, Language Comprehension, Language Production"
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam (HLE) is a multimodal, frontier-difficulty benchmark spanning many expert domains, where questions often require integrating specialized knowledge with multi-step reasoning. It is designed to probe broad general intelligence under hard, diverse prompts rather than narrow task-specific skills.","Language Comprehension, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Visual Perception, Scene Understanding & Visual Reasoning"
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a high-quality subset of graduate-level, “Google-proof” multiple-choice science questions intended to be difficult for non-experts. It measures deep scientific reasoning and careful discrimination among plausible distractors under no-tool settings.","Language Comprehension, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Inhibitory Control"
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro is a professional-grade multimodal benchmark requiring models to answer expert questions grounded in images and text across many disciplines. It targets higher-level visual understanding (charts, diagrams, interfaces) together with domain reasoning and robust instruction following.","Visual Perception, Scene Understanding & Visual Reasoning, Multisensory Integration, Language Comprehension, Logical Reasoning, Visual Attention & Eye Movements, Working Memory"
OmniDocBench 1.5,https://github.com/opendatalab/OmniDocBench,https://arxiv.org/abs/2412.07626,"OmniDocBench 1.5 evaluates document understanding and OCR robustness across heterogeneous layouts, including text, tables, formulas, and reading order. It measures whether systems can faithfully transcribe and structurally interpret complex documents rather than only recognizing plain text.","Visual Perception, Visual Attention & Eye Movements, Spatial Representation & Mapping, Scene Understanding & Visual Reasoning, Language Comprehension, Working Memory"
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU evaluates multimodal reasoning over videos, requiring models to integrate information across time and across visual/audio-language cues to answer questions. It stresses temporal grounding, event understanding, and recalling relevant moments from longer sequences.","Visual Perception, Auditory Processing, Multisensory Integration, Cognitive Timing & Predictive Modeling, Working Memory, Attention, Scene Understanding & Visual Reasoning, Language Comprehension"
LiveCodeBench Pro,https://livecodebenchpro.com/projects/livecodebench-pro/leaderboard,https://arxiv.org/abs/2506.11928,"LiveCodeBench Pro evaluates coding ability on competitively sourced, time-aligned programming problems to reduce contamination and better reflect current difficulty. It measures algorithmic problem solving, code synthesis, and iterative debugging under standardized scoring such as ELO or pass rates.","Logical Reasoning, Planning, Working Memory, Adaptive Error Correction, Language Comprehension, Language Production"
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"The FACTS Benchmark Suite systematically evaluates factuality by testing whether models maintain truthfulness and avoid unsupported claims across varied settings and question styles. It is intended to measure reliability beyond simple QA, including robustness to misleading contexts and subtle confounders.","Semantic Understanding & Context Recognition, Language Comprehension, Working Memory, Inhibitory Control, Self-reflection, Logical Reasoning"
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,Global PIQA extends physical commonsense and practical reasoning evaluation across many languages and cultural contexts using non-parallel multilingual data. It probes whether models can select plausible actions/solutions in everyday physical situations without relying on English-only shortcuts.,"Language Comprehension, Logical Reasoning, Semantic Understanding & Context Recognition, Cognitive Flexibility"
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) is a long-context evaluation where multiple similar “needle” requests are embedded within a large “haystack” of text, and the model must retrieve the correct referenced response. It targets robust coreference and retrieval under heavy distractors across long contexts.","Working Memory, Attention, Episodic Memory, Language Comprehension, Semantic Understanding & Context Recognition, Inhibitory Control"
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval evaluates well-specified professional knowledge-work tasks spanning many occupations, with human expert judges comparing model outputs to industry professional work products. It emphasizes end-to-end artifact quality (e.g., spreadsheets, presentations, plans) and practical correctness under real constraints.","Planning, Decision-making, Language Production, Language Comprehension, Working Memory, Adaptive Error Correction, Self-reflection"
SWE-Lancer,https://openai.com/index/swe-lancer/,https://arxiv.org/abs/2502.12115,"SWE-Lancer evaluates software engineering agents on realistic tasks that go beyond isolated functions, often requiring coordinated changes and verification in larger codebases. The benchmark is intended to reflect practical engineering workflows, including triage, implementation choices, and iteration toward a working solution.","Planning, Decision-making, Logical Reasoning, Working Memory, Adaptive Error Correction, Language Comprehension, Language Production"
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath evaluates advanced mathematics capability on expert-level problems, often requiring multi-stage derivations, theorem application, and careful symbolic manipulation. It is designed to be challenging for frontier models and to distinguish genuine reasoning from superficial pattern matching.","Logical Reasoning, Working Memory, Planning, Adaptive Error Correction, Semantic Understanding & Context Recognition"
