Benchmark,Website,Paper,Description,Cognitive Functions
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates real-world software engineering by asking a model/agent to generate code patches that fix issues in open-source repositories, validated by tests. The “Verified” subset contains tasks that were confirmed by human engineers to be solvable, emphasizing end-to-end debugging and patch generation rather than isolated coding trivia.","Planning, Logical Reasoning, Adaptive Error Correction, Working Memory, Language Comprehension, Language Production"
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 measures an agent’s ability to complete practical tasks in a command-line environment (e.g., navigating files, using CLI tools, configuring environments, and running programs). It stresses iterative diagnose-and-fix behavior under tool constraints, where progress depends on correctly interpreting tool outputs and adapting actions.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Language Comprehension"
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench evaluates interactive agent performance in customer-support style scenarios (Retail/Airline/Telecom) that require multi-turn dialogue plus API/tool use while adhering to domain policies. Success depends on maintaining consistent state over turns, resolving user goals, and following constraints even when they conflict with user requests.","Language Comprehension, Language Production, Social Reasoning & Theory of Mind, Decision-making, Planning, Working Memory, Inhibitory Control"
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI tests fluid reasoning by presenting few-shot grid transformation puzzles where the underlying rule must be inferred from a small set of input–output examples. It is designed to reduce reliance on memorized knowledge and instead emphasize abstraction, generalization, and robust rule induction on novel tasks.","Logical Reasoning, Cognitive Flexibility, Working Memory, Spatial Representation & Mapping, Visual Perception, Scene Understanding & Visual Reasoning"
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon autonomous decision-making in a simulated vending-machine business over many steps, scoring agents by final financial outcomes. The agent must plan, negotiate, manage inventory, set prices, and adapt to changing conditions while maintaining coherence across extended interaction histories.","Planning, Decision-making, Reward Mechanisms, Working Memory, Self-reflection"
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a frontier, multimodal benchmark spanning advanced academic and professional questions intended to probe the limits of model knowledge and reasoning. It includes tasks that may benefit from tool use (e.g., code or retrieval) but also tests direct problem-solving and synthesis across domains.","Language Comprehension, Logical Reasoning, Semantic Understanding & Context Recognition, Visual Perception, Working Memory"
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 is a competitive mathematics benchmark based on the American Invitational Mathematics Examination problems, typically requiring multi-step derivations rather than rote recall. It stresses precision, algebraic manipulation, and chaining intermediate results under strict answer formats.","Logical Reasoning, Working Memory, Planning"
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a difficult multiple-choice benchmark of graduate-level science questions designed to be “Google-proof,” emphasizing reasoning over superficial pattern matching. The Diamond subset targets high-quality items that reliably separate expert from non-expert performance.","Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Language Comprehension"
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends broad academic knowledge testing into multiple languages, evaluating whether models can answer subject-matter questions beyond English. It probes multilingual comprehension and the ability to apply learned concepts consistently across linguistic variations.","Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory"
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro evaluates expert-level multimodal understanding across disciplines, requiring models to integrate text with figures, diagrams, charts, and other visuals to answer questions. It emphasizes grounded visual reasoning and cross-modal alignment rather than text-only knowledge recall.","Visual Perception, Scene Understanding & Visual Reasoning, Multisensory Integration, Language Comprehension, Logical Reasoning, Working Memory"
MathArena Apex,https://matharena.ai/?view=problem&comp=apex--apex_2025,https://arxiv.org/abs/2505.23281,"MathArena Apex is a high-difficulty math reasoning benchmark suite intended to measure performance on challenging, multi-step problems beyond standard contest-level items. It stresses robust solution planning, error sensitivity, and maintaining correctness across long derivations.","Logical Reasoning, Working Memory, Planning, Adaptive Error Correction"
ScreenShot-Pro,https://gui-agent.github.io/grounding-leaderboard/,https://arxiv.org/abs/2504.07981,"ScreenShot-Pro (GUI grounding) evaluates how well models understand software screenshots and answer questions or take actions that depend on precise interface comprehension. Tasks require locating relevant UI elements, interpreting layout and visual cues, and mapping intent to correct interface operations.","Visual Perception, Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Spatial Representation & Mapping, Sensorimotor Coordination, Planning"
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning tests chart and figure understanding drawn from scientific documents (e.g., arXiv papers), requiring models to interpret visual encodings and relate them to textual questions. It probes whether a model can extract quantitative/relational information from plots and integrate it with context.","Visual Perception, Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory"
OmniDocBench 1.5,https://github.com/opendatalab/OmniDocBench,https://arxiv.org/abs/2412.07626,"OmniDocBench 1.5 evaluates document understanding and OCR across diverse document elements such as text, tables, formulas, and reading order. The benchmark emphasizes faithful extraction and structured interpretation under realistic document layouts and noise.","Visual Perception, Visual Attention & Eye Movements, Language Comprehension, Semantic Understanding & Context Recognition, Working Memory"
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU extends multimodal reasoning to video, requiring models to answer questions that depend on temporal events, actions, and visual context across frames. It stresses integrating information over time rather than relying on a single static image.","Visual Perception, Scene Understanding & Visual Reasoning, Working Memory, Cognitive Timing & Predictive Modeling, Multisensory Integration"
LiveCodeBench Pro,https://livecodebenchpro.com/projects/livecodebench-pro/leaderboard,https://arxiv.org/abs/2506.11928,"LiveCodeBench Pro measures coding ability on contemporary programming problems with an emphasis on realistic development constraints and evaluation via execution/verification. It probes iterative program synthesis where correctness depends on writing, testing, and refining code against hidden cases.","Planning, Logical Reasoning, Adaptive Error Correction, Working Memory, Language Comprehension, Language Production"
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"The FACTS Benchmark Suite systematically evaluates factuality and grounding behaviors across multiple factuality-related tasks, focusing on whether models make claims that are supported and appropriately qualified. It emphasizes calibrating assertions, resisting unsupported inference, and maintaining consistency across outputs.","Semantic Understanding & Context Recognition, Inhibitory Control, Self-reflection, Language Comprehension, Language Production"
SimpleQA Verified,https://www.kaggle.com/benchmarks/deepmind/simpleqa-verified,https://arxiv.org/abs/2509.07968,"SimpleQA Verified is a short-form factual question-answering benchmark with verified ground truth, designed to measure accuracy on concise information queries. It targets reliability and overconfidence control in direct answers rather than extended reasoning chains.","Semantic Understanding & Context Recognition, Language Comprehension, Language Production, Inhibitory Control"
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA evaluates physical commonsense and practical reasoning across languages, focusing on everyday interactions with objects and environments. It probes whether models can infer plausible actions/outcomes in the physical world while remaining robust across multilingual and cultural contexts.","Semantic Understanding & Context Recognition, Logical Reasoning, Spatial Representation & Mapping, Sensorimotor Coordination, Language Comprehension"
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) tests long-context, multi-round co-reference resolution by inserting multiple similar “needle” requests into long conversational/document “haystacks” and asking the model to reproduce the correct referenced content. It stresses sustained context tracking, retrieval under interference, and precise matching over very long inputs.","Working Memory, Attention, Language Comprehension, Episodic Memory, Inhibitory Control"
