Benchmark,Website,Paper,Description,Cognitive Functions
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro evaluates models on realistic software engineering issues drawn from real repositories, where the model must produce patches that satisfy hidden tests. Compared with easier coding evals, it emphasizes longer problem statements, multi-file changes, and robustness against superficial pattern matching, making it a stronger proxy for professional coding work.","Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning, Planning, Decision-making, Working Memory, Adaptive Error Correction, Language Production"
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld evaluates multimodal “computer use” agents that must complete tasks by interacting with a desktop operating system (e.g., navigating GUIs, opening apps, editing files) based on visual observations. It tests whether models can translate natural-language goals into sequences of grounded UI actions while coping with interface variability and errors.","Visual Perception, Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Planning, Decision-making, Sensorimotor Coordination, Spatial Representation & Mapping, Working Memory, Adaptive Error Correction, Language Comprehension"
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI measures fluid, few-shot abstract reasoning on grid-based puzzles: given a small set of input–output examples, models must infer the underlying rule and generate the correct output for a new input. It is designed to reward compositional rule induction and generalization rather than memorization of domain knowledge.","Logical Reasoning, Working Memory, Cognitive Flexibility, Spatial Representation & Mapping, Scene Understanding & Visual Reasoning, Planning, Attention"
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon agent performance in a simulated vending-machine business over an extended period, requiring many sequential decisions (pricing, inventory, supplier negotiation) to maximize final balance. The benchmark stresses sustained coherence, strategic adaptation to changing conditions, and avoiding compounding errors over time.","Planning, Decision-making, Working Memory, Reward Mechanisms, Adaptive Error Correction, Semantic Understanding & Context Recognition, Language Comprehension, Language Production"
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas evaluates real-world tool use through the Model Context Protocol (MCP), where models must discover, invoke, and chain API-like tools to complete tasks in production-like environments. It emphasizes correct tool selection, parameterization, multi-step workflow execution, and recovery from tool or environment errors.","Planning, Decision-making, Language Comprehension, Semantic Understanding & Context Recognition, Working Memory, Adaptive Error Correction, Inhibitory Control, Language Production"
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates agentic cybersecurity capability across large-scale tasks including identifying known vulnerabilities from descriptions and, in some settings, discovering new ones in real open-source codebases. It stresses code comprehension, hypothesis-driven debugging, and iterative testing with real tooling constraints.","Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning, Planning, Decision-making, Working Memory, Adaptive Error Correction"
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a multimodal benchmark at the frontier of academic and professional knowledge, spanning difficult questions that often require multi-step reasoning and careful interpretation. Variants may allow tools (e.g., search or code) to test end-to-end problem solving and verification behaviors under realistic conditions.","Language Comprehension, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Visual Perception, Scene Understanding & Visual Reasoning, Planning, Decision-making"
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a high-quality subset of extremely challenging, graduate-level multiple-choice science questions intended to be resistant to shallow lookup strategies. It primarily tests deep conceptual understanding and reasoning in physics, chemistry, and biology under tight answer constraints.","Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Decision-making"
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro is a difficult multimodal benchmark covering many disciplines where models must answer questions that combine text with images such as diagrams, plots, and complex visual layouts. It targets higher-level multimodal reasoning rather than simple recognition, including interpreting figures and integrating multiple evidence sources.","Visual Perception, Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Multisensory Integration, Language Comprehension, Logical Reasoning, Working Memory"
OmniDocBench 1.5,https://github.com/opendatalab/OmniDocBench,https://arxiv.org/abs/2412.07626,"OmniDocBench 1.5 evaluates document understanding and OCR-centric capabilities across heterogeneous content, including text, tables, formulas, and reading order. It tests whether a model can accurately extract and structure information from realistic documents with challenging layouts.","Visual Perception, Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Language Comprehension, Semantic Understanding & Context Recognition, Working Memory"
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU evaluates multimodal understanding and reasoning over videos, requiring models to integrate information across time and answer questions about events, actions, and context. The benchmark stresses temporal integration and maintaining coherence over long multimodal sequences.","Visual Perception, Visual Attention & Eye Movements, Cognitive Timing & Predictive Modeling, Working Memory, Scene Understanding & Visual Reasoning, Multisensory Integration, Language Comprehension, Logical Reasoning"
LiveCodeBench Pro,https://livecodebenchpro.com/projects/livecodebench-pro/leaderboard,https://arxiv.org/abs/2506.11928,"LiveCodeBench Pro evaluates coding ability using competitive-programming–style tasks with up-to-date, contamination-resistant curation and automated execution-based grading. It emphasizes correct algorithm selection, implementation quality, and iterative debugging against failing tests.","Language Comprehension, Logical Reasoning, Planning, Decision-making, Working Memory, Adaptive Error Correction, Language Production"
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"The FACTS Benchmark Suite systematically evaluates LLM factuality across multiple settings, focusing on whether generated claims are supported, consistent, and appropriately qualified. It is designed to probe hallucination tendencies, calibration, and the ability to avoid confidently stating unsupported information.","Language Comprehension, Language Production, Semantic Understanding & Context Recognition, Inhibitory Control, Self-reflection, Working Memory"
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA measures physical commonsense and practical reasoning across many languages and cultures, focusing on selecting plausible actions or solutions in everyday scenarios. It stresses transferring intuitive physical and procedural knowledge across linguistic contexts rather than relying on English-only cues.","Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning, Cognitive Flexibility, Spatial Representation & Mapping"
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,MRCR v2 (8-needle) evaluates long-context multi-round co-reference resolution by embedding multiple similar “needle” interactions inside long “haystack” transcripts and asking the model to retrieve the correct referenced response. It tests robustness of retrieval under interference and the ability to attend to the right instance among many near-duplicates.,"Working Memory, Episodic Memory, Attention, Language Comprehension, Semantic Understanding & Context Recognition"
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval evaluates well-specified professional knowledge work across many occupations by comparing model outputs against industry professionals, often using expert human judging and artifact-quality criteria. Tasks include producing real work products (e.g., spreadsheets, slides, plans), testing end-to-end execution quality rather than isolated question answering.","Planning, Decision-making, Language Production, Language Comprehension, Semantic Understanding & Context Recognition, Working Memory, Self-reflection, Social Reasoning & Theory of Mind"
SWE-Lancer,https://openai.com/index/swe-lancer/,https://arxiv.org/abs/2502.12115,"SWE-Lancer evaluates software engineering agents on realistic development tasks that require producing correct patches and navigating larger, messier codebases than typical coding puzzles. It emphasizes reliability, change management across files, and aligning implementation details with written requirements and repository conventions.","Language Comprehension, Semantic Understanding & Context Recognition, Planning, Decision-making, Working Memory, Adaptive Error Correction, Logical Reasoning, Language Production"
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath evaluates expert-level mathematics, including problems that require deep multi-step reasoning and, in some settings, tool-assisted computation. It is designed to be difficult for both memorization-based approaches and shallow pattern matching, highlighting true mathematical problem-solving ability.","Logical Reasoning, Working Memory, Planning, Cognitive Flexibility, Semantic Understanding & Context Recognition, Attention"
