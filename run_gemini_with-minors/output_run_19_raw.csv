Benchmark,Website,Paper,Description,Cognitive Functions
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,SWE-bench Verified evaluates autonomous software engineering by asking a model to generate patches that fix real issues in open-source Python repositories. The “Verified” subset uses human-validated tasks and stricter checking to ensure the reported solutions truly resolve the issue and tests pass.,"Planning, Logical Reasoning, Working Memory, Adaptive Error Correction, Language Comprehension, Language Production"
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 measures an agent’s ability to complete real-world tasks in a command-line environment, such as installing dependencies, running programs, debugging failures, and manipulating files. It emphasizes iterative troubleshooting under tool constraints and noisy system feedback.","Planning, Adaptive Error Correction, Decision-making, Working Memory, Language Comprehension, Language Production"
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench evaluates tool-using customer-support agents in simulated domains (e.g., retail, airline, telecom) where the model must converse with a user and call APIs while following domain policies. It stresses multi-turn consistency, correct procedural execution, and adherence to constraints despite user pressure and edge cases.","Social Reasoning & Theory of Mind, Language Comprehension, Language Production, Planning, Decision-making, Inhibitory Control, Working Memory, Empathy"
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,ARC-AGI tests fluid reasoning on novel grid-transformation puzzles: the model infers an underlying rule from a few input–output examples and applies it to a new input. The benchmark is designed to reduce reliance on memorized knowledge and reward systematic abstraction and generalization.,"Logical Reasoning, Cognitive Flexibility, Working Memory, Spatial Representation & Mapping, Visual Perception, Scene Understanding & Visual Reasoning"
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon agent coherence and strategy by simulating the operation of a vending machine business over extended time (e.g., a year). The agent must manage inventory, pricing, supplier interactions, and changing conditions to maximize final profit/balance.","Planning, Decision-making, Working Memory, Reward Mechanisms, Adaptive Error Correction, Language Comprehension, Language Production"
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a frontier, often multimodal benchmark spanning expert-level questions across many domains where answers require nontrivial reasoning and synthesis rather than rote recall. It is frequently used to compare tool-free performance versus tool-augmented settings (e.g., search/code), highlighting end-to-end problem solving.","Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Planning, Multisensory Integration, Visual Perception"
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 uses problems from the American Invitational Mathematics Examination, requiring multi-step mathematical reasoning with precise final numeric answers. It is commonly evaluated both without tools and with calculation tools to separate reasoning from arithmetic execution.","Logical Reasoning, Working Memory, Planning, Adaptive Error Correction, Language Comprehension"
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a high-quality subset of graduate-level, Google-resistant multiple-choice science questions (physics, chemistry, biology). It targets deep conceptual understanding and careful discrimination among close distractors under strict answer formats.","Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Language Comprehension, Decision-making"
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends broad academic testing to many languages, measuring knowledge and reasoning across subjects under multilingual prompts. It probes whether models can preserve correctness and nuanced understanding across translation and culturally varied formulations.","Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Cognitive Flexibility"
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro is a multimodal benchmark emphasizing expert-level perception-plus-reasoning over images and text across many disciplines. Compared with earlier MMMU settings, it is designed to be more challenging and better reflect real analysis of diagrams, figures, and visual evidence.","Visual Perception, Scene Understanding & Visual Reasoning, Multisensory Integration, Language Comprehension, Logical Reasoning, Working Memory, Visual Attention & Eye Movements"
MathArena Apex,https://matharena.ai/?view=problem&comp=apex--apex_2025,https://arxiv.org/abs/2505.23281,"MathArena Apex is a curated, difficult mathematics evaluation intended to stress rigorous multi-step reasoning beyond standard competition sets. It typically includes problems where solution quality depends on correct decomposition, symbolic manipulation, and error-checking rather than surface patterns.","Logical Reasoning, Working Memory, Planning, Adaptive Error Correction, Language Comprehension"
ScreenShot-Pro,https://gui-agent.github.io/grounding-leaderboard/,https://arxiv.org/abs/2504.07981,"ScreenShot-Pro evaluates GUI understanding from high-resolution screenshots, requiring models to interpret interface layout, locate relevant elements, and answer questions or act based on visual evidence. It targets robust screen grounding, including small text/icons and spatial relationships typical of professional software.","Visual Perception, Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Spatial Representation & Mapping, Sensorimotor Coordination, Planning, Working Memory"
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning evaluates understanding and reasoning over scientific charts/figures drawn from papers, often requiring reading visual encodings and combining them with textual context. It stresses quantitative and relational inference from plots, legends, axes, and annotations.","Visual Perception, Scene Understanding & Visual Reasoning, Logical Reasoning, Language Comprehension, Working Memory, Visual Attention & Eye Movements"
OmniDocBench 1.5,https://github.com/opendatalab/OmniDocBench,https://arxiv.org/abs/2412.07626,"OmniDocBench 1.5 benchmarks document understanding/OCR across diverse document components such as text blocks, tables, formulas, and reading order. It measures whether models can faithfully reconstruct structured content rather than only recognizing isolated text snippets.","Visual Perception, Attention, Working Memory, Language Comprehension, Semantic Understanding & Context Recognition, Scene Understanding & Visual Reasoning"
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,Video-MMMU evaluates multimodal reasoning over videos paired with questions spanning multiple disciplines and everyday scenarios. It tests whether models can integrate temporally distributed visual information with language prompts to answer correctly.,"Visual Perception, Multisensory Integration, Working Memory, Cognitive Timing & Predictive Modeling, Language Comprehension, Scene Understanding & Visual Reasoning"
LiveCodeBench Pro,https://livecodebenchpro.com/projects/livecodebench-pro/leaderboard,https://arxiv.org/abs/2506.11928,"LiveCodeBench Pro measures competitive coding performance on continuously updated programming tasks, typically graded via hidden tests and summarized with an Elo-style rating. It is designed to reduce training contamination and to reflect real coding ability under time/feedback constraints.","Logical Reasoning, Planning, Adaptive Error Correction, Working Memory, Language Comprehension, Language Production"
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"The FACTS Benchmark Suite systematically evaluates factuality by testing whether model outputs remain consistent with evidence, avoid unsupported claims, and handle uncertainty appropriately. It emphasizes claim-level correctness and robustness to prompts that could induce confident hallucinations.","Semantic Understanding & Context Recognition, Language Comprehension, Language Production, Inhibitory Control, Self-reflection, Working Memory"
SimpleQA Verified,https://www.kaggle.com/benchmarks/deepmind/simpleqa-verified,https://arxiv.org/abs/2509.07968,"SimpleQA Verified is a factual question answering benchmark with verified ground truth, designed to measure accuracy and reduce ambiguity in evaluation. It is used to quantify hallucination tendencies and the ability to answer concisely and correctly.","Semantic Understanding & Context Recognition, Language Comprehension, Language Production, Inhibitory Control"
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA evaluates physical commonsense reasoning across many languages, focusing on plausible actions and interactions in everyday environments. It tests whether models can generalize commonsense judgments beyond English and across diverse linguistic formulations.","Semantic Understanding & Context Recognition, Logical Reasoning, Language Comprehension, Cognitive Flexibility, Decision-making"
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) is a long-context evaluation where multiple similar “needle” requests are embedded across lengthy multi-round conversations, and the model must retrieve the correct associated response. The 8-needle setting increases confusability and stresses precise context tracking over long spans.","Working Memory, Attention, Semantic Understanding & Context Recognition, Episodic Memory, Inhibitory Control"
