Benchmark,Website,Paper,Description,Cognitive Functions
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates LLM-based coding agents on real GitHub issues by requiring them to generate patches that make a repository’s tests pass. The “Verified” variant filters to tasks confirmed by human annotators to be solvable and to have reliable evaluation signals, reducing noise from flaky tasks and underspecified bugs.","Planning, Logical Reasoning, Working Memory, Adaptive Error Correction, Language Comprehension, Language Production"
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 measures an agent’s ability to complete practical tasks in a command-line environment (e.g., installing dependencies, manipulating files, running programs, debugging failures). It emphasizes iterative interaction, tool use via shell commands, and recovering from errors under realistic resource and environment constraints.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Language Comprehension"
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench evaluates interactive agent performance in multi-turn, tool-using customer support scenarios (e.g., retail, airline, telecom) where models must follow domain policies while using APIs and conversation to resolve user needs. Success requires balancing helpfulness with constraint adherence over long dialogues with simulated users and systems.","Language Comprehension, Language Production, Social Reasoning & Theory of Mind, Inhibitory Control, Decision-making, Planning, Working Memory"
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,ARC-AGI tests fluid reasoning by presenting a few input–output grid examples that follow an unknown transformation rule and asking the model to infer the correct output for a new input. It is designed to minimize reliance on memorized knowledge and instead probe novel rule induction from sparse demonstrations.,"Logical Reasoning, Cognitive Flexibility, Working Memory, Spatial Representation & Mapping, Scene Understanding & Visual Reasoning, Visual Perception"
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 assesses long-horizon autonomous agency by simulating operation of a vending machine business over an extended period, including inventory, pricing, supplier interactions, and adapting to market dynamics. Performance is typically scored by business outcomes (e.g., final balance) that depend on sustained coherent strategy and execution.","Planning, Decision-making, Reward Mechanisms, Working Memory, Social Reasoning & Theory of Mind, Semantic Understanding & Context Recognition"
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam (HLE) is a multimodal benchmark intended to probe frontier-level academic reasoning and knowledge across many subjects, often requiring multi-step inference rather than recall. It includes difficult questions that may combine text with figures or other modalities and is commonly evaluated with strict grading protocols to reduce spurious correctness.","Language Comprehension, Logical Reasoning, Working Memory, Semantic Understanding & Context Recognition, Scene Understanding & Visual Reasoning, Visual Perception"
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 comprises competition-style mathematics problems that require constructing and executing multi-step solution paths under tight correctness criteria. It is frequently used to evaluate symbolic manipulation, quantitative reasoning, and robustness of stepwise derivations without tool assistance.","Logical Reasoning, Working Memory, Planning, Adaptive Error Correction"
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a high-quality subset of graduate-level, “Google-proof” multiple-choice science questions designed to be difficult for non-experts while solvable by domain experts. It targets deep conceptual understanding and careful reasoning under distractors rather than surface-level pattern matching.","Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory"
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends broad academic knowledge testing to multiple languages, evaluating whether models can understand questions and answer choices across diverse subjects beyond English. It probes cross-lingual robustness and whether reasoning and knowledge transfer remain stable under multilingual phrasing and cultural context.","Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory"
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro is a professional-grade multimodal benchmark that tests expert-level understanding and reasoning over images paired with text questions across many disciplines (e.g., charts, diagrams, scientific visuals). It emphasizes fine-grained perception plus multi-step reasoning to integrate visual evidence with domain knowledge.","Visual Perception, Scene Understanding & Visual Reasoning, Multisensory Integration, Language Comprehension, Logical Reasoning, Working Memory, Visual Attention & Eye Movements"
MathArena Apex,https://matharena.ai/?view=problem&comp=apex--apex_2025,https://arxiv.org/abs/2505.23281,"MathArena Apex aggregates very challenging mathematics problems intended to stress test advanced mathematical reasoning beyond standard contest sets. It is used to compare models’ ability to sustain long derivations, avoid subtle logical slips, and maintain correctness under higher difficulty distributions.","Logical Reasoning, Planning, Working Memory, Adaptive Error Correction"
ScreenShot-Pro,https://gui-agent.github.io/grounding-leaderboard/,https://arxiv.org/abs/2504.07981,"ScreenShot-Pro evaluates GUI understanding from screenshots by asking models to identify interface elements, interpret layouts, and (in agentic settings) decide where to click/type to accomplish goals. It targets precise visual grounding—mapping textual instructions to pixel-level UI affordances and spatial relationships.","Visual Perception, Visual Attention & Eye Movements, Spatial Representation & Mapping, Scene Understanding & Visual Reasoning, Planning, Sensorimotor Coordination"
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning focuses on scientific figure and chart reasoning, often requiring extracting quantitative and qualitative signals from plots, tables, and diagrams and then answering reasoning-heavy questions. It tests whether a model can correctly interpret visual evidence and connect it to scientific context and constraints.","Visual Perception, Scene Understanding & Visual Reasoning, Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory"
OmniDocBench 1.5,https://github.com/opendatalab/OmniDocBench,https://arxiv.org/abs/2412.07626,"OmniDocBench 1.5 evaluates document understanding and OCR robustness across heterogeneous document components such as text blocks, formulas, tables, and reading order. It stresses accurate recognition plus structural reconstruction, where small perception errors can cascade into large semantic mistakes.","Visual Perception, Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Language Comprehension, Working Memory"
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU measures multimodal understanding and reasoning over videos, typically requiring tracking entities and events across time and integrating them with text questions. It emphasizes temporal coherence and the ability to retain and use information that is only briefly visible in earlier frames.","Visual Perception, Cognitive Timing & Predictive Modeling, Working Memory, Scene Understanding & Visual Reasoning, Multisensory Integration, Attention"
LiveCodeBench Pro,https://livecodebenchpro.com/projects/livecodebench-pro/leaderboard,https://arxiv.org/abs/2506.11928,"LiveCodeBench Pro evaluates coding ability on programming tasks with execution-based checking, emphasizing whether produced code actually runs and solves the specified problem. It is designed to reflect practical coding performance, including debugging and iterative refinement when initial attempts fail.","Logical Reasoning, Planning, Working Memory, Adaptive Error Correction, Language Comprehension, Language Production"
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"The FACTS Benchmark Suite systematically measures factuality and grounding behavior across a collection of tests that probe when models make incorrect claims, fabricate details, or fail to attribute sources appropriately. It is intended to be more robust than single-dataset factuality checks by covering multiple failure modes and domains.","Language Comprehension, Semantic Understanding & Context Recognition, Inhibitory Control, Self-reflection, Working Memory"
SimpleQA Verified,https://www.kaggle.com/benchmarks/deepmind/simpleqa-verified,https://arxiv.org/abs/2509.07968,SimpleQA Verified is a short-form question answering benchmark with verified ground truth that targets factual correctness under concise prompts. It is often used to quantify hallucination propensity and precision on atomic facts where partial credit is not appropriate.,"Language Comprehension, Semantic Understanding & Context Recognition, Inhibitory Control"
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA evaluates physical commonsense reasoning across many languages and culturally diverse contexts, focusing on selecting or generating plausible actions/outcomes in everyday physical situations. It probes whether models can generalize intuitive physics and affordance reasoning beyond English-centric priors.","Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning, Spatial Representation & Mapping"
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) is a long-context evaluation where multiple similar “needle” queries are embedded throughout a large context (“haystack”), and the model must retrieve the correct referenced answer for a specified needle. It tests sustained attention and accurate retrieval under high interference from near-duplicate distractors over very long documents.","Working Memory, Attention, Episodic Memory, Language Comprehension, Inhibitory Control"
