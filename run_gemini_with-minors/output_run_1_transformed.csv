Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro evaluates software engineering agents on realistic repository-level tasks where a model must understand a large codebase, implement or fix functionality, and submit a patch that passes tests. Compared with SWE-bench Verified, Pro is larger and harder, covering multiple languages and aiming to be more contamination-resistant and industry-relevant.","L1: Language Comprehension, Language Production
L2: Logical Reasoning, Planning, Decision-making, Working Memory, Adaptive Error Correction, Semantic Understanding & Context Recognition
L3: ",L2
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld tests “computer use” by having an agent operate a real desktop-like operating system to complete end-to-end tasks (e.g., using apps, navigating settings, filling forms). It emphasizes multimodal perception of GUIs, long-horizon action sequences, and robust interaction under noisy or changing interface states.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Attention, Planning, Decision-making, Working Memory, Sensorimotor Coordination, Spatial Representation & Mapping, Adaptive Error Correction
L3: ",L2
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,ARC-AGI measures fluid reasoning by asking models to infer abstract transformations from a few input–output grid examples and apply them to a new grid. The tasks are designed to resist shortcut memorization and require discovering novel rules from minimal supervision.,"L1: 
L2: Logical Reasoning, Working Memory, Attention, Scene Understanding & Visual Reasoning, Spatial Representation & Mapping
L3: Cognitive Flexibility",L3
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon autonomous agency in a simulated vending machine business over an extended period (e.g., a full simulated year). Agents must manage inventory, pricing, supplier negotiation, and changing market conditions to maximize final profit while staying coherent across thousands of steps.","L1: 
L2: Planning, Decision-making, Reward Mechanisms, Working Memory, Episodic Memory, Semantic Understanding & Context Recognition, Adaptive Error Correction
L3: ",L2
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas benchmarks real-world tool use through the Model Context Protocol by testing whether a model can discover tools, call them correctly, handle errors, and compose multi-step workflows across multiple MCP servers. Tasks resemble production integrations where correct API sequencing and robust recovery matter as much as raw reasoning.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Working Memory, Adaptive Error Correction, Semantic Understanding & Context Recognition
L3: ",L2
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity agency by having models find known vulnerabilities in real open-source projects (given high-level weakness descriptions) and also attempt to discover new vulnerabilities. Success depends on reading code, forming hypotheses about exploit paths, and producing precise, testable vulnerability reports or patches.","L1: Language Comprehension
L2: Logical Reasoning, Planning, Decision-making, Working Memory, Adaptive Error Correction, Semantic Understanding & Context Recognition, Attention
L3: ",L2
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam (HLE) is a difficult, often multimodal benchmark intended to probe frontier knowledge and reasoning at (and beyond) typical expert-human levels across many domains. Questions often require integrating specialized concepts, careful reasoning under uncertainty, and sometimes interpreting figures or diagrams.","L1: Language Comprehension, Visual Perception
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Scene Understanding & Visual Reasoning
L3: ",L2
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a high-quality subset of graduate-level, “Google-proof” multiple-choice science questions where non-experts tend to fail while experts succeed. It stresses deep conceptual understanding and multi-step reasoning rather than retrieval of easily searchable facts.","L1: Language Comprehension
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Attention
L3: ",L2
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro evaluates expert-level multimodal understanding and reasoning across many disciplines, requiring models to answer questions grounded in images, charts, and technical visuals along with text. It tests whether models can connect visual evidence to domain knowledge and perform multi-step inference beyond simple recognition.","L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Logical Reasoning, Working Memory, Multisensory Integration, Attention
L3: ",L2
OmniDocBench 1.5,https://github.com/opendatalab/OmniDocBench,https://arxiv.org/abs/2412.07626,"OmniDocBench 1.5 evaluates document understanding/OCR systems on complex, real-world documents including text, tables, formulas, and reading-order structure. Models must extract or reconstruct content accurately while preserving layout and logical structure under diverse formatting and noise.","L1: Visual Perception, Language Comprehension
L2: Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Working Memory, Semantic Understanding & Context Recognition, Attention
L3: ",L2
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU extends multimodal reasoning to video, requiring models to answer questions that depend on events unfolding over time rather than a single frame. It tests temporal integration (tracking entities and actions), understanding visual scenes, and aligning video evidence with textual questions and answers.","L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Working Memory, Attention, Multisensory Integration
L3: Cognitive Timing & Predictive Modeling",L3
LiveCodeBench Pro,https://livecodebenchpro.com/projects/livecodebench-pro/leaderboard,https://arxiv.org/abs/2506.11928,"LiveCodeBench Pro evaluates coding capability on competitive-programming-style problems curated to reduce leakage and reflect contemporary difficulty. Models must derive correct algorithms, implement them reliably, and handle edge cases that often break superficially plausible solutions.","L1: Language Production, Language Comprehension
L2: Logical Reasoning, Planning, Working Memory, Adaptive Error Correction, Decision-making
L3: ",L2
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"The FACTS Benchmark Suite systematically measures factuality and grounding behavior across a collection of factuality-related tests, targeting when models produce incorrect statements, unsupported claims, or inconsistencies. It is designed to characterize reliability under realistic prompting conditions rather than only measuring knowledge recall.","L1: Language Comprehension, Language Production
L2: Semantic Understanding & Context Recognition, Working Memory, Attention
L3: Inhibitory Control, Self-reflection",L3
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA evaluates commonsense reasoning about physical interactions and everyday actions across many languages, emphasizing robustness beyond English-only settings. It tests whether models can apply intuitive physics and practical knowledge consistently under multilingual phrasing and cultural variation.","L1: Language Comprehension
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory
L3: Cognitive Flexibility",L3
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) is a long-context, multi-round coreference evaluation where multiple similar “needle” requests are embedded in a long “haystack” of dialogue and the model must reproduce the correct referenced response. It stresses precise retrieval and disambiguation under heavy context interference and long-range dependencies.","L1: Language Comprehension
L2: Working Memory, Attention, Episodic Memory, Semantic Understanding & Context Recognition
L3: Inhibitory Control",L3
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval evaluates well-specified professional knowledge-work tasks spanning many occupations by comparing model-produced work products (e.g., slides, spreadsheets, plans) against expert human outputs using human judging. It emphasizes end-to-end execution quality, instruction following, and producing usable artifacts rather than just answering questions.","L1: Language Production, Language Comprehension
L2: Planning, Decision-making, Working Memory, Semantic Understanding & Context Recognition, Adaptive Error Correction
L3: Social Reasoning & Theory of Mind",L3
SWE-Lancer,https://openai.com/index/swe-lancer/,https://arxiv.org/abs/2502.12115,"SWE-Lancer evaluates real-world software engineering work in a “freelance-like” setting, where models must deliver correct, integrated changes across a codebase and satisfy task requirements that resemble paid engineering tickets. It stresses reliability in larger, messier engineering workflows (understanding requirements, implementing, validating, and iterating).","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Working Memory, Adaptive Error Correction, Logical Reasoning, Semantic Understanding & Context Recognition
L3: ",L2
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath is a difficult mathematics benchmark targeting problems at or beyond typical graduate/research-level difficulty, designed to probe genuine mathematical reasoning under strong anti-contamination practices. It often requires long chains of deduction, careful abstraction, and error-prone intermediate steps where verification matters.","L1: 
L2: Logical Reasoning, Working Memory, Planning, Adaptive Error Correction, Attention, Semantic Understanding & Context Recognition
L3: ",L2
