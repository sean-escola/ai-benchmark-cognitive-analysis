Benchmark,Website,Paper,Description,Cognitive Functions
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,SWE-bench Verified evaluates software engineering agents on real GitHub issues by requiring a model to produce a patch that makes a failing test suite pass. The “Verified” subset emphasizes tasks confirmed solvable and uses a standardized evaluation harness to check correctness via tests and repository state changes.,"Planning, Working Memory, Adaptive Error Correction, Logical Reasoning, Decision-making, Language Comprehension"
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 evaluates agentic performance in command-line environments where models must navigate files, run programs, install dependencies, and debug issues using terminal tools. Success depends on choosing appropriate commands, interpreting outputs/errors, and iterating until an objective state is reached.","Planning, Adaptive Error Correction, Working Memory, Decision-making, Logical Reasoning, Language Comprehension"
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench measures an agent’s ability to handle multi-turn customer-support style tasks while using tools/APIs and complying with domain policies (e.g., retail, airline, telecom). It tests whether the agent can maintain dialogue state, follow constraints, and execute correct tool calls to resolve the user’s request end-to-end.","Language Comprehension, Language Production, Planning, Decision-making, Inhibitory Control, Social Reasoning & Theory of Mind, Working Memory"
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,ARC-AGI is a few-shot abstract reasoning benchmark where systems infer hidden rules from a small number of input–output grid examples and generate the correct output for a new grid. The tasks aim to probe flexible induction and compositional generalization rather than memorized knowledge.,"Logical Reasoning, Cognitive Flexibility, Working Memory, Spatial Representation & Mapping, Scene Understanding & Visual Reasoning, Visual Perception, Attention"
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon autonomous business management in a simulated vending-machine setting, typically spanning a full simulated year. Agents must plan inventory, pricing, supplier interactions, and budgeting across many sequential decisions to maximize final balance and avoid failure modes.","Planning, Decision-making, Reward Mechanisms, Working Memory, Adaptive Error Correction, Cognitive Timing & Predictive Modeling"
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam (HLE) is a frontier, often multimodal benchmark intended to test difficult knowledge and reasoning problems across many domains. Tasks are designed to stress deep multi-step inference, careful reading of prompts and artifacts, and robust answering under uncertainty.","Language Comprehension, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Visual Perception, Scene Understanding & Visual Reasoning"
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 evaluates competition-style mathematics problem solving, typically requiring multi-step derivations and precise final numeric answers. It is used to measure symbolic manipulation, structured reasoning, and the ability to avoid subtle arithmetic or logic errors.","Logical Reasoning, Working Memory, Planning, Adaptive Error Correction"
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a high-difficulty subset of graduate-level science multiple-choice questions designed to be hard to answer via superficial pattern matching. It emphasizes deep conceptual understanding and multi-step reasoning across physics, chemistry, and biology.","Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Language Comprehension, Inhibitory Control"
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends broad academic knowledge evaluation to multiple languages, testing whether models can answer MMLU-style questions across subjects in non-English settings. It probes both multilingual comprehension and whether reasoning and knowledge transfer across languages and domains.","Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory"
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro is a multimodal benchmark emphasizing expert-level understanding and reasoning over images and text across many disciplines. Compared with earlier MMMU settings, it is designed to be more challenging and to better test robust visual grounding and cross-modal reasoning.","Visual Perception, Scene Understanding & Visual Reasoning, Multisensory Integration, Attention, Logical Reasoning, Working Memory"
MathArena Apex,https://matharena.ai/?view=problem&comp=apex--apex_2025,https://arxiv.org/abs/2505.23281,"MathArena Apex aggregates difficult math problems intended to stress frontier mathematical reasoning under standardized evaluation conditions. It emphasizes solving novel problems with long, structured chains of inference rather than relying on short heuristics.","Logical Reasoning, Working Memory, Planning, Adaptive Error Correction"
ScreenShot-Pro,https://gui-agent.github.io/grounding-leaderboard/,https://arxiv.org/abs/2504.07981,"ScreenShot-Pro evaluates whether models can understand and act on high-resolution GUI screenshots, often requiring identification of interface elements and correct grounding of actions to screen regions. It targets practical “computer use” competence where success depends on accurate visual localization and stepwise interaction decisions.","Visual Perception, Visual Attention & Eye Movements, Spatial Representation & Mapping, Sensorimotor Coordination, Planning, Decision-making, Working Memory"
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning focuses on reasoning over scientific charts/figures from papers, requiring models to interpret plotted data, axes, legends, and annotations. It tests whether a model can extract quantitative/relational information from visuals and integrate it with the question’s textual constraints.","Visual Perception, Scene Understanding & Visual Reasoning, Logical Reasoning, Semantic Understanding & Context Recognition, Attention, Working Memory"
OmniDocBench 1.5,https://github.com/opendatalab/OmniDocBench,https://arxiv.org/abs/2412.07626,"OmniDocBench 1.5 evaluates document understanding and OCR-like extraction across complex layouts including text blocks, tables, formulas, and reading order. It probes how well models preserve structure and accurately transcribe/parse visually presented documents into correct textual and structured outputs.","Visual Perception, Scene Understanding & Visual Reasoning, Attention, Working Memory, Language Production"
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU tests multimodal understanding and reasoning over short videos paired with questions, stressing temporal comprehension beyond single images. Tasks often require integrating events across time, tracking state changes, and answering with grounded multi-step reasoning.","Visual Perception, Cognitive Timing & Predictive Modeling, Working Memory, Scene Understanding & Visual Reasoning, Attention, Multisensory Integration"
LiveCodeBench Pro,https://livecodebenchpro.com/projects/livecodebench-pro/leaderboard,https://arxiv.org/abs/2506.11928,"LiveCodeBench Pro evaluates coding ability on a curated set of programming tasks with emphasis on practical problem solving and robustness under standardized conditions. It typically measures whether the model can synthesize correct algorithms, implement them precisely, and handle edge cases and failures through iteration.","Logical Reasoning, Planning, Working Memory, Adaptive Error Correction, Decision-making, Language Comprehension"
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"FACTS Benchmark Suite systematically evaluates factuality by measuring whether model outputs remain consistent with provided sources, known truths, or task constraints across a variety of factuality-focused tests. It emphasizes calibration-like behavior: avoiding unsupported claims, resolving conflicts, and maintaining faithful generation.","Language Comprehension, Inhibitory Control, Semantic Understanding & Context Recognition, Logical Reasoning, Self-reflection"
SimpleQA Verified,https://www.kaggle.com/benchmarks/deepmind/simpleqa-verified,https://arxiv.org/abs/2509.07968,"SimpleQA Verified evaluates factual accuracy on short, unambiguous questions with verified answers and standardized grading. It primarily probes whether models can retrieve the correct fact and avoid confident hallucinations when uncertain.","Language Comprehension, Semantic Understanding & Context Recognition, Inhibitory Control"
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA evaluates physical commonsense and practical reasoning across languages and cultural contexts, aiming to test whether models choose plausible actions/explanations in everyday situations. It stresses generalization of intuitive physics and affordance reasoning beyond English-centric data distributions.","Logical Reasoning, Semantic Understanding & Context Recognition, Cognitive Flexibility, Spatial Representation & Mapping, Language Comprehension"
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) is a long-context, multi-round coreference/retrieval evaluation where multiple similar “needle” interactions are embedded within a long “haystack,” and the model must reproduce the correct response tied to a specified needle. It targets sustained context tracking and precise reference resolution under heavy distractors.","Working Memory, Attention, Episodic Memory, Language Comprehension, Semantic Understanding & Context Recognition"
