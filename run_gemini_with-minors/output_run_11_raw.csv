Benchmark,Website,Paper,Description,Cognitive Functions
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro evaluates software engineering agents on a large set of real GitHub issues spanning multiple programming languages and realistic repository contexts. Systems must understand the issue, locate relevant code, implement a correct patch, and pass tests under a standardized evaluation harness.","Language Comprehension, Language Production, Logical Reasoning, Planning, Working Memory, Adaptive Error Correction"
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld measures an agent’s ability to use a graphical operating system to complete end-user tasks (e.g., navigating apps, configuring settings, manipulating files) from visual observations. It emphasizes long-horizon, multi-step interaction where the agent must perceive screen state, choose actions, and recover from mistakes.","Visual Perception, Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Planning, Decision-making, Sensorimotor Coordination, Working Memory"
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI is a few-shot abstract reasoning benchmark where models infer hidden transformation rules from small sets of input–output grid examples. Success requires generalizing to novel tasks with minimal prior task-specific instruction, emphasizing flexible pattern induction over memorization.","Logical Reasoning, Cognitive Flexibility, Working Memory, Visual Perception, Spatial Representation & Mapping"
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon agent coherence by simulating operation of a vending-machine business over many decisions and time steps. Agents must manage inventory, pricing, supplier interactions, and budgeting to maximize final balance while adapting to changing conditions.","Planning, Decision-making, Reward Mechanisms, Working Memory, Episodic Memory, Semantic Understanding & Context Recognition"
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas evaluates real-world tool use via the Model Context Protocol by requiring models to discover appropriate tools, call them correctly, and compose multi-step workflows across APIs. The benchmark stresses reliable orchestration: correct tool selection, parameterization, error handling, and synthesis of results into an answer.","Language Comprehension, Planning, Decision-making, Working Memory, Adaptive Error Correction, Semantic Understanding & Context Recognition"
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym tests cybersecurity capability through tasks such as identifying, reproducing, and sometimes discovering vulnerabilities in real open-source codebases from natural-language vulnerability descriptions. Agents must reason about program behavior, navigate projects, and propose concrete fixes or exploit-relevant analyses under time/attempt constraints.","Language Comprehension, Logical Reasoning, Planning, Working Memory, Adaptive Error Correction"
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a frontier, multi-domain benchmark designed to probe advanced academic and professional reasoning, often requiring synthesis across specialized knowledge. It includes challenging questions (often multimodal) intended to be difficult to answer by retrieval alone, emphasizing careful reasoning and verification.","Language Comprehension, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Visual Perception"
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a high-quality subset of GPQA consisting of very difficult multiple-choice science questions intended to be “Google-proof.” It targets expert-level reasoning and deep scientific understanding rather than superficial recall, with strong emphasis on selecting the correct option among plausible distractors.","Language Comprehension, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory"
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro evaluates multimodal understanding and reasoning over problems that combine text with images such as diagrams, charts, and scientific figures. It emphasizes robust visual grounding and multi-step reasoning in expert-style questions across many disciplines.","Visual Perception, Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Language Comprehension, Logical Reasoning, Working Memory"
OmniDocBench 1.5,https://github.com/opendatalab/OmniDocBench,https://arxiv.org/abs/2412.07626,"OmniDocBench 1.5 measures document understanding and OCR-centric capabilities on complex, real-world documents containing text, tables, formulas, and reading-order structure. Systems must accurately extract and structure content, which stresses fine-grained layout understanding and faithful transcription.","Visual Perception, Visual Attention & Eye Movements, Language Comprehension, Semantic Understanding & Context Recognition, Working Memory"
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU extends multimodal reasoning to video, requiring models to answer questions that depend on temporally distributed visual evidence. It tests whether systems can track events across time, integrate cues across frames, and maintain coherent interpretations of dynamic scenes.","Visual Perception, Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Working Memory, Cognitive Timing & Predictive Modeling, Multisensory Integration"
LiveCodeBench Pro,https://livecodebenchpro.com/projects/livecodebench-pro/leaderboard,https://arxiv.org/abs/2506.11928,"LiveCodeBench Pro evaluates coding performance on competitively-styled programming tasks with strong controls intended to reduce contamination and reflect current difficulty. Models must generate correct, executable solutions under constraints and are often evaluated with robust correctness checks and leaderboard-style ratings.","Language Comprehension, Language Production, Logical Reasoning, Planning, Working Memory, Adaptive Error Correction"
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"The FACTS Benchmark Suite systematically evaluates factuality by testing whether model outputs remain grounded, avoid fabrication, and correctly handle uncertain or unsupported claims. It typically probes both retrieval-style accuracy and the model’s tendency to overclaim when evidence is insufficient.","Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning, Inhibitory Control, Self-reflection"
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,Global PIQA evaluates commonsense physical reasoning across many languages and cultural contexts by presenting everyday situations and asking for the more plausible/appropriate outcome or explanation. It stresses robustness of pragmatic reasoning beyond English-centric distributions.,"Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning"
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) is a long-context retrieval and reasoning evaluation where multiple similar “needle” requests are embedded within a long “haystack,” and the model must reproduce the correct response associated with a specified needle. It probes whether models can maintain and use precise references across very long inputs without confusion.","Working Memory, Attention, Semantic Understanding & Context Recognition"
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval evaluates well-specified professional knowledge-work tasks across many occupations, judging the quality of real work products (e.g., spreadsheets, slides, plans) relative to human professionals. It emphasizes end-to-end task execution quality rather than narrow multiple-choice accuracy.","Planning, Decision-making, Language Production, Language Comprehension, Semantic Understanding & Context Recognition, Working Memory"
SWE-Lancer,https://openai.com/index/swe-lancer/,https://arxiv.org/abs/2502.12115,"SWE-Lancer evaluates software engineering ability in settings closer to real-world engineering workflows, emphasizing producing patches that satisfy task requirements under realistic constraints. It targets agentic coding behaviors such as iterative debugging, repo navigation, and aligning changes to specification-like requests.","Language Comprehension, Language Production, Planning, Decision-making, Working Memory, Adaptive Error Correction, Logical Reasoning"
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath measures advanced mathematical problem-solving on expert-level questions designed to be challenging for current frontier models. Success requires multi-step derivations, careful symbolic manipulation, and high precision rather than approximate or heuristic answers.","Logical Reasoning, Working Memory, Planning, Cognitive Flexibility, Semantic Understanding & Context Recognition"
