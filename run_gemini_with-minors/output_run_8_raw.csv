Benchmark,Website,Paper,Description,Cognitive Functions
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates software engineering agents on real GitHub issues by requiring them to generate a patch that fixes the bug or implements the requested change and passes the repository’s tests. The ""Verified"" split uses tasks that have been manually validated as solvable with the provided repo state and tests, emphasizing end-to-end debugging and codebase navigation under realistic constraints.","Planning, Decision-making, Logical Reasoning, Working Memory, Adaptive Error Correction, Language Comprehension, Language Production, Semantic Understanding & Context Recognition"
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 measures how well an agent can complete real-world tasks in a command-line environment, such as installing tools, manipulating files, running programs, and interpreting logs and outputs. It stresses reliable tool use over multiple steps, recovery from errors, and maintaining task state across long action sequences.","Planning, Decision-making, Working Memory, Adaptive Error Correction, Logical Reasoning, Semantic Understanding & Context Recognition, Inhibitory Control"
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench evaluates interactive, policy-constrained customer support agents that must communicate with a simulated user and call domain APIs to resolve multi-turn tasks (e.g., retail orders, airline changes, telecom troubleshooting). It probes whether the agent can follow rules consistently while still being helpful under conversational pressure and ambiguous user intent.","Language Comprehension, Language Production, Social Reasoning & Theory of Mind, Decision-making, Planning, Inhibitory Control, Empathy, Working Memory"
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,ARC-AGI is a few-shot abstract reasoning benchmark in which models infer hidden transformations from a small number of input-output grid examples and produce the correct output grid for a new input. The tasks are designed to emphasize novelty and compositional pattern learning rather than memorization of domain facts.,"Logical Reasoning, Cognitive Flexibility, Working Memory, Spatial Representation & Mapping, Visual Perception, Scene Understanding & Visual Reasoning, Attention"
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon agent coherence by simulating a year of operating a vending machine business, starting from a small budget. The agent must make repeated decisions about inventory, pricing, supplier negotiation, and adapting to market dynamics, with success measured by final financial outcomes.","Planning, Decision-making, Reward Mechanisms, Working Memory, Semantic Understanding & Context Recognition, Adaptive Error Correction, Self-reflection"
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam (HLE) is a frontier, multi-domain benchmark intended to stress advanced academic reasoning, specialized knowledge, and (in some settings) multimodal understanding across a large set of difficult questions. Evaluations often compare tool-free reasoning versus tool-enabled configurations (e.g., web search or code) to assess end-to-end problem solving.","Language Comprehension, Language Production, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Multisensory Integration, Decision-making"
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 benchmarks competition-style mathematics problem solving, typically requiring multi-step symbolic reasoning, careful case analysis, and exact numeric answers. It is often used to measure reliability of chain-of-thought reasoning (tool-free) and the marginal benefit of external computation tools.","Logical Reasoning, Working Memory, Planning, Adaptive Error Correction, Cognitive Flexibility"
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a hard subset of GPQA consisting of high-quality, graduate-level multiple-choice questions in sciences where non-experts tend to fail but experts succeed. It targets deep conceptual understanding and multi-step reasoning under a fixed set of answer options, discouraging shallow pattern matching.","Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Decision-making, Inhibitory Control, Language Comprehension"
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends MMLU-style evaluation to multiple languages, testing broad academic knowledge and reasoning across many subjects via multiple-choice questions. It is used to assess multilingual generalization, robustness of learned concepts across languages, and consistency of knowledge retrieval.","Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Cognitive Flexibility"
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro is a multimodal benchmark that tests expert-level understanding and reasoning over images paired with text (e.g., diagrams, charts, scientific figures) across many disciplines. Compared with earlier multimodal benchmarks, it emphasizes harder questions, reduced shortcutting, and stronger evaluation protocols for vision-language reasoning.","Visual Perception, Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Visual Attention & Eye Movements, Working Memory, Language Comprehension"
MathArena Apex,https://matharena.ai/?view=problem&comp=apex--apex_2025,https://arxiv.org/abs/2505.23281,"MathArena Apex aggregates difficult math problems and evaluates models under standardized settings, often focusing on robust multi-step reasoning rather than single-trick questions. It is commonly used to compare frontier math capability across models and prompting/tool configurations.","Logical Reasoning, Working Memory, Planning, Adaptive Error Correction, Cognitive Flexibility"
ScreenShot-Pro,https://gui-agent.github.io/grounding-leaderboard/,https://arxiv.org/abs/2504.07981,"ScreenShot-Pro evaluates GUI understanding and interaction from screenshots, typically requiring the model to identify UI elements, interpret layouts, and produce grounded actions or answers (sometimes with tool support like screenshot capture). It targets practical screen-based reasoning relevant to computer-use agents operating across professional software interfaces.","Visual Perception, Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Sensorimotor Coordination, Planning, Decision-making, Working Memory"
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning tests whether models can answer questions that require interpreting scientific paper figures (often charts/plots) and performing quantitative or logical reasoning over them. It is designed to probe grounded, visual-to-symbolic reasoning rather than surface caption matching, and is frequently evaluated with optional Python tool use.","Scene Understanding & Visual Reasoning, Visual Perception, Logical Reasoning, Working Memory, Multisensory Integration, Attention, Semantic Understanding & Context Recognition"
OmniDocBench 1.5,https://github.com/opendatalab/OmniDocBench,https://arxiv.org/abs/2412.07626,"OmniDocBench 1.5 evaluates document understanding/OCR across complex layouts, including text, formulas, tables, and reading order. It measures whether a system can correctly perceive and structure information from heterogeneous document elements, which is critical for downstream reasoning and extraction.","Visual Perception, Attention, Semantic Understanding & Context Recognition, Working Memory, Scene Understanding & Visual Reasoning"
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU benchmarks multimodal reasoning over videos, requiring models to integrate information across frames and align it with language queries. Tasks often require temporal understanding (events, causality, and changes over time) in addition to visual recognition.","Visual Perception, Multisensory Integration, Cognitive Timing & Predictive Modeling, Attention, Working Memory, Scene Understanding & Visual Reasoning, Language Comprehension"
LiveCodeBench Pro,https://livecodebenchpro.com/projects/livecodebench-pro/leaderboard,https://arxiv.org/abs/2506.11928,"LiveCodeBench Pro evaluates coding ability on curated programming tasks with strong controls for temporal leakage and contamination, typically using pass@k or Elo-style summaries. It targets practical code synthesis and debugging skills that require iterative reasoning about program behavior and edge cases.","Planning, Logical Reasoning, Working Memory, Adaptive Error Correction, Decision-making, Language Comprehension, Language Production"
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"FACTS Benchmark Suite systematically evaluates factuality and grounding behaviors across multiple tasks designed to detect hallucinations, unsupported claims, and citation/attribution failures. It emphasizes faithfulness to provided evidence and careful handling of uncertainty rather than purely fluent generation.","Semantic Understanding & Context Recognition, Language Comprehension, Language Production, Inhibitory Control, Self-reflection, Logical Reasoning, Working Memory"
SimpleQA Verified,https://www.kaggle.com/benchmarks/deepmind/simpleqa-verified,https://arxiv.org/abs/2509.07968,"SimpleQA Verified measures factual question answering with verified ground truth and evaluation procedures intended to reduce noise and ambiguous labeling. It is commonly used to track whether models answer accurately, abstain appropriately, and avoid fabricating details when uncertain.","Language Comprehension, Semantic Understanding & Context Recognition, Inhibitory Control, Self-reflection, Working Memory"
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,Global PIQA evaluates commonsense and practical reasoning about physical interactions and everyday situations across many languages and culturally diverse contexts. It stresses whether models can select or generate plausible actions/outcomes grounded in intuitive physics and real-world constraints rather than language priors alone.,"Semantic Understanding & Context Recognition, Logical Reasoning, Spatial Representation & Mapping, Cognitive Flexibility, Language Comprehension, Working Memory"
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) is a long-context, multi-round coreference/recall evaluation where multiple similar “needle” requests are embedded throughout long conversational haystacks and the model must reproduce the correct response to a specified needle. The 8-needle variant increases interference and demands robust retrieval, disambiguation, and sustained context integration over long inputs.","Working Memory, Attention, Episodic Memory, Semantic Understanding & Context Recognition, Inhibitory Control, Logical Reasoning"
