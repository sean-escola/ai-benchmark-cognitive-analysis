Benchmark,Website,Paper,Description,Cognitive Functions
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro evaluates agentic software engineering by giving models real repositories and issues and requiring them to produce correct patches that pass tests. Compared to SWE-bench Verified, it is designed to be more difficult and more contamination-resistant, and includes multiple programming languages and more realistic engineering constraints.","Language Comprehension, Language Production, Planning, Decision-making, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition"
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld measures an agent’s ability to operate a computer by completing tasks in a realistic desktop operating-system environment using multimodal observations (e.g., screenshots) and action interfaces. Success requires selecting relevant UI elements, navigating multi-step workflows, and recovering from errors and unexpected states.","Visual Perception, Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Sensorimotor Coordination, Planning, Decision-making, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition"
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,ARC-AGI is a “fluid intelligence” benchmark where models infer the transformation rule from a few input–output grid examples and generate the correct output for a new grid. It emphasizes novel pattern induction and compositional reasoning under very limited demonstration data.,"Logical Reasoning, Cognitive Flexibility, Spatial Representation & Mapping, Working Memory, Planning, Attention"
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon autonomy by placing an agent in a simulated year-long vending-machine business and scoring the final financial outcome. The agent must manage inventory, pricing, supplier negotiations, and stochastic market dynamics over thousands of sequential decisions.","Planning, Decision-making, Reward Mechanisms, Working Memory, Cognitive Timing & Predictive Modeling, Adaptive Error Correction, Semantic Understanding & Context Recognition, Language Comprehension, Language Production"
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas evaluates tool-use over real (or production-like) APIs exposed via the Model Context Protocol, requiring models to discover appropriate tools, call them correctly, and compose results across multiple steps. Tasks stress robustness to tool errors, schema mismatches, and multi-server workflows.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Language Comprehension, Semantic Understanding & Context Recognition"
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity agents on tasks such as locating known vulnerabilities in real open-source projects and, in some settings, discovering new ones. It stresses reasoning over codebases, selecting investigative strategies, and generating correct, minimal fixes or exploitation-relevant analyses under realistic constraints.","Logical Reasoning, Planning, Decision-making, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition, Language Comprehension, Language Production"
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam (HLE) is a multimodal benchmark intended to probe frontier-level academic knowledge and reasoning across many specialized domains. Questions often require integrating technical text with figures or other modalities and producing precise, grounded answers under strict grading.","Language Comprehension, Language Production, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Visual Perception, Scene Understanding & Visual Reasoning, Multisensory Integration"
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a curated subset of extremely difficult, graduate-level multiple-choice science questions designed to be “Google-proof.” It emphasizes deep conceptual understanding and multi-step scientific reasoning rather than surface recall.","Language Comprehension, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory"
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro evaluates expert-level multimodal understanding and reasoning across many disciplines, combining text with images such as diagrams, charts, and scientific figures. The tasks stress extracting relevant visual evidence and integrating it with domain knowledge to answer questions reliably.","Visual Perception, Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Multisensory Integration, Language Comprehension, Logical Reasoning, Working Memory, Semantic Understanding & Context Recognition"
OmniDocBench 1.5,https://github.com/opendatalab/OmniDocBench,https://arxiv.org/abs/2412.07626,"OmniDocBench 1.5 evaluates document intelligence systems on end-to-end understanding of complex documents, including OCR text, formulas, tables, and reading order. It focuses on faithful extraction and structure preservation rather than only recognizing isolated tokens.","Visual Perception, Visual Attention & Eye Movements, Language Comprehension, Semantic Understanding & Context Recognition, Working Memory, Scene Understanding & Visual Reasoning"
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU evaluates multimodal reasoning over videos, requiring models to track events and attributes across time and answer questions that depend on temporal context. It stresses integrating frame-level visual cues into coherent narratives and using them for inference.","Visual Perception, Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Cognitive Timing & Predictive Modeling, Working Memory, Multisensory Integration, Language Comprehension, Logical Reasoning"
LiveCodeBench Pro,https://livecodebenchpro.com/projects/livecodebench-pro/leaderboard,https://arxiv.org/abs/2506.11928,"LiveCodeBench Pro benchmarks coding ability on fresh, competition-style programming problems and emphasizes contamination resistance by using time-based releases and controlled evaluation. Models must design algorithms, write correct implementations, and often debug iteratively against hidden tests.","Language Comprehension, Language Production, Logical Reasoning, Planning, Adaptive Error Correction, Working Memory, Decision-making"
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"The FACTS Benchmark Suite systematically evaluates factuality, including the tendency to hallucinate, to overstate confidence, or to introduce subtle factual errors across diverse formats. It targets reliability under real-world answer conditions rather than only task accuracy on closed-form exams.","Language Comprehension, Language Production, Semantic Understanding & Context Recognition, Inhibitory Control, Self-reflection, Working Memory"
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA evaluates commonsense and pragmatic reasoning about everyday physical and procedural situations across languages and locales, using non-parallel multilingual data to reduce translation artifacts. It emphasizes selecting plausible actions or explanations grounded in real-world constraints.","Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory"
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,MRCR v2 (8-needle) evaluates long-context multi-round coreference resolution by inserting multiple similar “needle” interactions into long “haystacks” and asking the model to retrieve the correct referenced response. It stresses sustained attention and accurate retrieval under high interference and long-range dependencies.,"Working Memory, Attention, Episodic Memory, Language Comprehension, Semantic Understanding & Context Recognition"
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval measures performance on well-specified professional knowledge-work tasks spanning many occupations, with outputs judged by expert humans (often in pairwise comparisons). Tasks include producing real work artifacts (e.g., spreadsheets, presentations, plans) and emphasize end-to-end usefulness and correctness.","Planning, Decision-making, Language Production, Language Comprehension, Semantic Understanding & Context Recognition, Working Memory, Adaptive Error Correction, Social Reasoning & Theory of Mind"
SWE-Lancer,https://openai.com/index/swe-lancer/,https://arxiv.org/abs/2502.12115,"SWE-Lancer evaluates software engineering agents on realistic engineering work that goes beyond isolated coding puzzles, emphasizing end-to-end task completion in repositories and workflows. It rewards robust debugging, correct changes under constraints, and effective prioritization across multiple steps.","Language Comprehension, Language Production, Planning, Decision-making, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition"
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath is an expert-level mathematics benchmark designed to test advanced mathematical reasoning on problems that are difficult for current frontier models and are curated for rigor. It emphasizes multi-step derivations, careful symbolic reasoning, and error-sensitive correctness criteria.","Logical Reasoning, Working Memory, Planning, Cognitive Flexibility, Adaptive Error Correction, Attention"
