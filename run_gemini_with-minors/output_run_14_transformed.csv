Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,SWE-bench Verified evaluates software engineering by asking a model to fix real issues in open-source Python repositories and submit patches that pass the project’s tests. The “Verified” variant focuses on problems confirmed solvable and uses standardized evaluation to measure end-to-end bug fixing and code change quality.,"L1: Language Comprehension, Language Production
L2: Planning, Logical Reasoning, Working Memory, Adaptive Error Correction
L3: ",L2
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 measures an agent’s ability to complete real command-line tasks (e.g., installing tools, manipulating files, running programs, debugging) in a sandboxed terminal environment. It emphasizes multi-step tool use with feedback from the environment (stdout/stderr, exit codes) and requires iterative troubleshooting.","L1: Language Comprehension
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory
L3: ",L2
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench evaluates interactive agent behavior in simulated service domains (e.g., retail, airline, telecom) where the model must follow policies while using tools/APIs across multiple turns. Success requires maintaining conversation state, executing correct tool calls, and balancing helpfulness with constraints.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Working Memory
L3: Inhibitory Control, Social Reasoning & Theory of Mind",L3
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,ARC-AGI tests fluid reasoning through grid-based puzzles where the system must infer hidden rules from a few input–output examples and produce the correct output for a new input. It is designed to reward abstraction and generalization rather than memorization of specific tasks.,"L1: Visual Perception
L2: Logical Reasoning, Working Memory, Spatial Representation & Mapping, Planning
L3: Cognitive Flexibility",L3
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon agent coherence by simulating management of a vending machine business over an extended time period, scoring outcomes like final balance. The agent must make many sequential decisions (inventory, pricing, suppliers) under changing conditions, where early mistakes compound over time.","L1: 
L2: Planning, Decision-making, Reward Mechanisms, Working Memory, Semantic Understanding & Context Recognition, Adaptive Error Correction
L3: ",L2
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a difficult multimodal benchmark spanning advanced knowledge and reasoning across many domains, with questions intended to be at or beyond typical expert difficulty. It stresses integrating evidence (sometimes from images) with rigorous multi-step reasoning to arrive at final answers.","L1: Language Comprehension, Language Production, Visual Perception
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Scene Understanding & Visual Reasoning
L3: ",L2
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 benchmarks competition-style mathematics using problems from the American Invitational Mathematics Examination. It emphasizes multi-step derivations, careful constraint handling, and exact final numeric answers rather than explanation quality.","L1: 
L2: Logical Reasoning, Working Memory, Planning, Adaptive Error Correction
L3: ",L2
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a high-difficulty multiple-choice science QA benchmark designed to be resistant to shallow pattern matching and web-search shortcuts. The Diamond subset focuses on the most carefully curated questions, emphasizing deep scientific reasoning under uncertainty.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Decision-making
L3: ",L2
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends broad academic knowledge testing across many subjects into multiple languages, assessing whether a model can understand prompts and answer correctly beyond English. It measures multilingual generalization of knowledge and reasoning across diverse domains.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory
L3: ",L2
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro evaluates multimodal understanding and reasoning across expert-level topics by requiring models to answer questions grounded in images (diagrams, charts, figures) plus text. It stresses robust visual grounding, cross-modal integration, and disciplined reasoning under multiple-choice constraints.","L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Working Memory, Visual Attention & Eye Movements
L3: ",L2
MathArena Apex,https://matharena.ai/?view=problem&comp=apex--apex_2025,https://arxiv.org/abs/2505.23281,"MathArena Apex aggregates difficult math problems to compare frontier models under a standardized protocol, emphasizing hard multi-step reasoning and quantitative accuracy. It is typically used to distinguish top-end mathematical problem-solving performance where small reliability differences matter.","L1: 
L2: Logical Reasoning, Working Memory, Planning, Adaptive Error Correction
L3: ",L2
ScreenShot-Pro,https://gui-agent.github.io/grounding-leaderboard/,https://arxiv.org/abs/2504.07981,ScreenShot-Pro evaluates GUI understanding by asking models to answer questions or identify actionable targets given high-resolution screenshots of real software interfaces. It tests whether a model can ground language to visual UI elements and support precise interaction-oriented reasoning.,"L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Spatial Representation & Mapping, Planning, Decision-making, Sensorimotor Coordination
L3: ",L2
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning tests scientific figure understanding by asking questions that require interpreting charts and visual evidence from research-style figures, often benefiting from quantitative reasoning. It emphasizes grounding answers in visual data, cross-referencing captions/labels, and avoiding unsupported claims.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Attention
L3: ",L2
OmniDocBench 1.5,https://github.com/opendatalab/OmniDocBench,https://arxiv.org/abs/2412.07626,"OmniDocBench 1.5 evaluates document understanding and OCR-style extraction across complex layouts, including text blocks, formulas, tables, and reading order. It measures whether a model can accurately parse structured documents and preserve layout-dependent meaning.","L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Working Memory, Attention
L3: ",L2
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU extends multimodal reasoning to video, requiring models to answer questions based on temporal visual content (and accompanying text). It probes whether models can integrate information across frames and maintain coherent event understanding over time.","L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Multisensory Integration, Working Memory, Attention
L3: Cognitive Timing & Predictive Modeling",L3
LiveCodeBench Pro,https://livecodebenchpro.com/projects/livecodebench-pro/leaderboard,https://arxiv.org/abs/2506.11928,"LiveCodeBench Pro evaluates coding ability on problems intended to reduce contamination and better reflect real programming skill, often under time/versioned releases. It focuses on producing correct solutions and handling tricky edge cases, serving as a proxy for practical code reasoning reliability.","L1: Language Comprehension, Language Production
L2: Logical Reasoning, Planning, Working Memory, Adaptive Error Correction
L3: ",L2
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"FACTS Benchmark Suite systematically evaluates factuality by testing whether model outputs remain grounded in provided sources and avoid hallucinations across diverse tasks. It emphasizes citation/attribution behavior, faithfulness to context, and robustness to misleading prompts or gaps in evidence.","L1: Language Comprehension, Language Production
L2: Semantic Understanding & Context Recognition, Working Memory, Attention
L3: Inhibitory Control, Self-reflection",L3
SimpleQA Verified,https://www.kaggle.com/benchmarks/deepmind/simpleqa-verified,https://arxiv.org/abs/2509.07968,"SimpleQA Verified is a short-form factual question answering benchmark with verified ground truth and an emphasis on reliable, non-hallucinated responses. It aims to measure precision on straightforward knowledge queries where incorrect confident answers are especially harmful.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Working Memory
L3: Inhibitory Control",L3
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA evaluates practical, physically grounded commonsense reasoning across languages, focusing on selecting or generating plausible solutions to everyday interaction scenarios. It targets whether models generalize intuitive physical and procedural knowledge beyond English-centric distributions.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Spatial Representation & Mapping, Sensorimotor Coordination
L3: Cognitive Flexibility",L3
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,MRCR v2 (8-needle) tests long-context retrieval and multi-round coreference resolution by inserting multiple similar “needle” items into long “haystacks” and asking the model to recover the correct referenced response. The 8-needle setting stresses robust attention control and interference resistance under heavy distractors.,"L1: Language Comprehension
L2: Working Memory, Attention, Episodic Memory
L3: Inhibitory Control",L3
