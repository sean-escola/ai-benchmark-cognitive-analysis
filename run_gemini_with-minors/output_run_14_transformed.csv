Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro evaluates software engineering agents on a large set of real GitHub issues that require producing correct patches in repository context and passing tests. It is designed to be more challenging and contamination-resistant than SWE-bench Verified, with broader language coverage and more realistic engineering workflows.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Working Memory, Adaptive Error Correction, Logical Reasoning, Semantic Understanding & Context Recognition
L3: ",L2
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld measures an agent’s ability to use a graphical operating system to complete user-specified tasks (e.g., navigating apps, editing files, changing settings) through multimodal perception and tool-like actions. It emphasizes end-to-end computer-use competence under step limits and interface variability.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Attention, Planning, Decision-making, Working Memory, Spatial Representation & Mapping, Sensorimotor Coordination
L3: ",L2
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,ARC-AGI tests abstract pattern induction using small grid-based input–output examples where the rule must be inferred from only a few demonstrations. Success requires composing novel transformations rather than relying on memorized domain knowledge.,"L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Logical Reasoning, Working Memory, Attention
L3: Cognitive Flexibility",L3
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon agent coherence and strategy by simulating operation of a vending-machine business over an extended period with many sequential decisions. Agents must manage inventory, pricing, supplier interactions, and cash flow to maximize final balance under changing conditions.","L1: 
L2: Planning, Decision-making, Reward Mechanisms, Working Memory, Episodic Memory, Semantic Understanding & Context Recognition, Adaptive Error Correction
L3: ",L2
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas evaluates real-world tool use through the Model Context Protocol (MCP), where models must discover tools, call them correctly, handle failures, and synthesize results across multi-step workflows. Tasks resemble production integrations with authentic APIs and structured tool schemas.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Working Memory, Adaptive Error Correction, Semantic Understanding & Context Recognition
L3: ",L2
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity agent capability on large-scale tasks involving identifying known vulnerabilities from descriptions and, in some settings, discovering new issues in real open-source code. It typically rewards correct exploitation/patching behavior under realistic constraints and single-attempt scoring regimes.","L1: Language Comprehension
L2: Logical Reasoning, Planning, Decision-making, Working Memory, Adaptive Error Correction, Semantic Understanding & Context Recognition
L3: ",L2
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a multimodal benchmark intended to probe frontier academic knowledge and reasoning across a wide range of difficult questions, sometimes involving diagrams, figures, or specialized domains. It is often evaluated both with and without external tools (e.g., search or code execution) to distinguish internal reasoning from tool-augmented performance.","L1: Language Comprehension, Visual Perception
L2: Scene Understanding & Visual Reasoning, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory
L3: ",L2
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a hard subset of graduate-level, “Google-proof” multiple-choice science questions curated to be high quality and resistant to superficial pattern matching. It targets deep scientific reasoning and domain knowledge under constrained answer formats.","L1: Language Comprehension
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory
L3: ",L2
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro is an advanced multimodal understanding and reasoning benchmark spanning many disciplines, with questions grounded in images (e.g., charts, diagrams, scenes) and accompanying text. It stresses expert-level visual reasoning, cross-domain knowledge, and precise option selection.","L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Multisensory Integration, Visual Attention & Eye Movements, Logical Reasoning, Working Memory
L3: ",L2
OmniDocBench 1.5,https://github.com/opendatalab/OmniDocBench,https://arxiv.org/abs/2412.07626,"OmniDocBench 1.5 evaluates document understanding and OCR in complex, real-world layouts, including text blocks, tables, formulas, and reading order. It emphasizes structured extraction fidelity and layout-aware reconstruction rather than only plain-text recognition.","L1: Visual Perception, Language Comprehension
L2: Visual Attention & Eye Movements, Spatial Representation & Mapping, Scene Understanding & Visual Reasoning, Working Memory
L3: ",L2
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU evaluates multimodal reasoning over videos, requiring models to integrate information across frames and time to answer questions about events, actions, and visual details. Performance depends on tracking temporal context and maintaining coherence over long video inputs.","L1: Visual Perception
L2: Attention, Working Memory, Scene Understanding & Visual Reasoning, Multisensory Integration
L3: Cognitive Timing & Predictive Modeling",L3
LiveCodeBench Pro,https://livecodebenchpro.com/projects/livecodebench-pro/leaderboard,https://arxiv.org/abs/2506.11928,"LiveCodeBench Pro measures code-generation and debugging ability on competitively styled programming tasks designed for robust, up-to-date evaluation. It typically emphasizes producing correct, executable solutions under time- and reasoning-like constraints, and is often summarized with rating-style metrics (e.g., ELO).","L1: Language Comprehension, Language Production
L2: Logical Reasoning, Planning, Working Memory, Adaptive Error Correction
L3: ",L2
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"The FACTS Benchmark Suite systematically evaluates factuality-related behaviors, including grounding, resisting hallucinations, and maintaining consistency under varied prompts and contexts. It aims to capture reliability across multiple factuality subtests rather than a single QA score.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Working Memory, Logical Reasoning
L3: Inhibitory Control",L3
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA evaluates commonsense and physical reasoning across many languages and culturally diverse contexts, emphasizing robustness beyond English-only benchmarks. Questions target practical understanding of everyday interactions, constraints, and outcomes rather than specialized academic knowledge.","L1: Language Comprehension
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory
L3: Cognitive Flexibility",L3
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,MRCR v2 (8-needle) tests long-context multi-round co-reference resolution by embedding multiple similar “needle” requests in long “haystack” conversations and asking the model to reproduce a specific needle’s answer. It stresses reliable retrieval and disambiguation as context length grows.,"L1: Language Comprehension
L2: Working Memory, Attention, Episodic Memory, Semantic Understanding & Context Recognition
L3: ",L2
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval evaluates economically meaningful, well-specified professional knowledge work across many occupations by comparing model outputs to expert human work products. Tasks often require producing artifacts (e.g., slides, spreadsheets, plans) judged by humans for quality and correctness.","L1: Language Production, Language Comprehension
L2: Planning, Decision-making, Working Memory, Semantic Understanding & Context Recognition
L3: Self-reflection",L3
SWE-Lancer,https://openai.com/index/swe-lancer/,https://arxiv.org/abs/2502.12115,"SWE-Lancer evaluates agentic software engineering on realistic tasks that go beyond isolated coding puzzles, often requiring iterative debugging, repository navigation, and integration-quality fixes. It is designed to reflect end-to-end engineering competence under practical constraints and evaluation harnesses.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Working Memory, Adaptive Error Correction, Logical Reasoning, Semantic Understanding & Context Recognition
L3: ",L2
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath measures high-end mathematical problem solving, including questions intended to be difficult even for strong models and to resist memorization. It emphasizes rigorous multi-step reasoning and error-free symbolic/quantitative manipulation, sometimes with tool-enabled variants for computation.","L1: 
L2: Logical Reasoning, Working Memory, Planning, Adaptive Error Correction, Semantic Understanding & Context Recognition
L3: ",L2
