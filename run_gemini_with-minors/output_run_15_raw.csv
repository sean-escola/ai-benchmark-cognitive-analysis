Benchmark,Website,Paper,Description,Cognitive Functions
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates software-engineering agents on real GitHub issues where the model must produce code patches that pass the project’s test suite. The “Verified” subset is curated to remove ambiguous or unsolvable tasks, focusing the score on reliably planning, implementing, and validating fixes under realistic repo constraints.","Planning, Logical Reasoning, Adaptive Error Correction, Working Memory, Language Comprehension, Decision-making"
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 measures an agent’s ability to complete real command-line tasks by interacting with a Unix-like environment (e.g., running programs, inspecting files, and iterating on errors). It emphasizes long-horizon execution with tool feedback loops, where success depends on choosing correct shell actions and recovering from failures.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Language Comprehension"
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench evaluates tool-using customer-support agents in simulated domains (e.g., retail, airline, telecom) that require multi-turn dialogue plus API calls under policy constraints. It tests whether an agent can follow rules, gather needed information from a user, and execute correct tool actions to resolve a case end-to-end.","Language Comprehension, Language Production, Social Reasoning & Theory of Mind, Decision-making, Planning, Inhibitory Control"
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI tests “fluid” abstract reasoning by presenting a small set of input-output grid examples and requiring the model to infer the latent rule and produce the correct output for a new grid. The tasks are designed to be novel and compositional, rewarding flexible hypothesis formation rather than rote pattern matching.","Logical Reasoning, Cognitive Flexibility, Working Memory, Spatial Representation & Mapping, Scene Understanding & Visual Reasoning"
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon agent coherence and strategy by simulating operation of a vending-machine business over an extended period. The agent must manage inventory, pricing, supplier interactions, and cash flow, making many sequential decisions that trade off short-term costs against long-term profit.","Planning, Decision-making, Reward Mechanisms, Working Memory, Cognitive Timing & Predictive Modeling, Semantic Understanding & Context Recognition"
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a broad, frontier-difficulty benchmark spanning many expert-level questions, often including multimodal items and requiring nontrivial reasoning. It aims to approximate challenging real-world problem solving where models must integrate knowledge and reasoning (and sometimes tools) to reach correct answers.","Language Comprehension, Language Production, Logical Reasoning, Scene Understanding & Visual Reasoning, Semantic Understanding & Context Recognition, Working Memory"
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 uses problems from a high-level mathematics competition that require multi-step derivations, careful case analysis, and precise numerical answers. It stresses reliability of symbolic reasoning and the ability to carry intermediate results correctly across a solution.","Logical Reasoning, Working Memory, Planning, Adaptive Error Correction"
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond consists of graduate-level, “Google-proof” multiple-choice science questions selected to be difficult for non-experts and to reduce simple retrieval advantages. It probes deep scientific understanding and multi-step reasoning under time/attention constraints typical of exam-style questions.","Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Language Comprehension"
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends broad academic knowledge testing across many subjects and multiple non-English languages, measuring whether a model retains competence beyond English-only settings. It captures how well models generalize semantic knowledge and reasoning across linguistic variation and domain contexts.","Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory"
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro is a multimodal, expert-level benchmark where questions require combining visual inputs (diagrams, plots, screenshots, scientific figures) with text to answer. It targets robust multimodal grounding, cross-domain reasoning, and careful interpretation of visual evidence.","Visual Perception, Scene Understanding & Visual Reasoning, Multisensory Integration, Language Comprehension, Logical Reasoning, Visual Attention & Eye Movements"
MathArena Apex,https://matharena.ai/?view=problem&comp=apex--apex_2025,https://arxiv.org/abs/2505.23281,"MathArena Apex is a harder mathematics evaluation suite designed to stress advanced reasoning beyond standard contest sets, often requiring longer chains of deductions and higher precision. It is used to compare frontier “thinking” models on difficult math problem solving under standardized conditions.","Logical Reasoning, Planning, Working Memory, Adaptive Error Correction"
ScreenShot-Pro,https://gui-agent.github.io/grounding-leaderboard/,https://arxiv.org/abs/2504.07981,"ScreenShot-Pro evaluates GUI understanding from high-resolution screenshots, where the model must interpret interface state and answer questions or choose correct interactions. It emphasizes visually grounded decision-making about UI elements, layout, and task-relevant affordances.","Visual Perception, Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Decision-making, Planning, Sensorimotor Coordination"
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning tests scientific figure understanding and reasoning over charts/plots/diagrams drawn from research contexts, often requiring quantitative reading and multi-step inference. Many setups pair the benchmark with a Python tool to evaluate how models combine visual extraction with computation.","Scene Understanding & Visual Reasoning, Visual Perception, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Attention"
OmniDocBench 1.5,https://github.com/opendatalab/OmniDocBench,https://arxiv.org/abs/2412.07626,"OmniDocBench 1.5 evaluates document understanding and OCR-style extraction across diverse layouts, including text, tables, formulas, and reading order. It measures whether a system can reliably perceive structured documents and reconstruct faithful textual/structural outputs.","Visual Perception, Scene Understanding & Visual Reasoning, Attention, Language Comprehension, Working Memory"
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,Video-MMMU evaluates multimodal reasoning over short videos paired with questions that require tracking events and visual details over time. It tests temporal integration of observations and the ability to maintain coherent interpretations as new frames provide additional evidence.,"Visual Perception, Scene Understanding & Visual Reasoning, Cognitive Timing & Predictive Modeling, Working Memory, Attention, Multisensory Integration"
LiveCodeBench Pro,https://livecodebenchpro.com/projects/livecodebench-pro/leaderboard,https://arxiv.org/abs/2506.11928,"LiveCodeBench Pro measures coding ability on tasks designed to resemble real programming work, emphasizing correctness under time pressure and resistance to memorization via ongoing updates. It captures iterative development behaviors such as debugging, refining solutions, and aligning outputs to specifications.","Logical Reasoning, Planning, Adaptive Error Correction, Working Memory, Language Comprehension"
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"FACTS Benchmark Suite systematically evaluates factuality by testing whether model outputs remain grounded, consistent, and verifiable across a range of factuality stressors. It focuses on correctness and calibration behaviors that reduce hallucinations in knowledge-intensive settings.","Semantic Understanding & Context Recognition, Language Comprehension, Inhibitory Control, Self-reflection, Working Memory"
SimpleQA Verified,https://www.kaggle.com/benchmarks/deepmind/simpleqa-verified,https://arxiv.org/abs/2509.07968,"SimpleQA Verified evaluates short-form factual question answering with verified ground truth, emphasizing accuracy over verbosity. It is commonly used to track hallucination rates and whether models can reliably produce the correct fact when the question is straightforward but brittle to errors.","Language Comprehension, Semantic Understanding & Context Recognition, Inhibitory Control, Working Memory"
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA measures pragmatic/physical commonsense reasoning across languages and cultural contexts, typically via choosing the more plausible solution or outcome in everyday situations. It tests whether models can generalize intuitive action-and-effect understanding beyond English-centric priors.","Semantic Understanding & Context Recognition, Logical Reasoning, Spatial Representation & Mapping, Sensorimotor Coordination, Language Comprehension"
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,MRCR v2 (8-needle) tests long-context retrieval and multi-round coreference resolution by embedding multiple similar “needle” requests within a large “haystack” of dialogue and asking the model to reproduce the correct response for a specific needle. The 8-needle setting stresses sustained attention and accurate tracking of repeated entities and instructions over very long contexts.,"Working Memory, Attention, Language Comprehension, Episodic Memory, Semantic Understanding & Context Recognition"
