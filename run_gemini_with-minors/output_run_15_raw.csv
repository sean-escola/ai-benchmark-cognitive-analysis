Benchmark,Website,Paper,Description,Cognitive Functions
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro evaluates software engineering agents on realistic repository-level tasks where the model must produce code changes that make tests pass. Compared to earlier SWE-bench variants, it targets harder, more industrially representative problems and broader language coverage, stressing end-to-end debugging and patch generation.","Language Comprehension, Language Production, Logical Reasoning, Planning, Working Memory, Adaptive Error Correction"
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld tests multimodal “computer use” agents that must complete tasks inside a desktop operating system by interacting with GUI elements (e.g., apps, windows, menus) over multiple steps. Success depends on perceiving screen state, choosing actions, and recovering from mistakes under step and time limits.","Visual Perception, Scene Understanding & Visual Reasoning, Attention, Decision-making, Planning, Sensorimotor Coordination, Working Memory, Adaptive Error Correction"
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,ARC-AGI is a few-shot abstract reasoning benchmark where models infer hidden rules from a small set of input–output grid examples and apply them to a new grid. It is designed to emphasize novel pattern induction and compositional generalization rather than memorized knowledge.,"Logical Reasoning, Cognitive Flexibility, Working Memory, Spatial Representation & Mapping, Visual Perception, Scene Understanding & Visual Reasoning"
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon agent coherence by having an agent run a simulated vending machine business over an extended period, making thousands of interconnected decisions. High performance requires sustained strategy, reacting to changing market conditions, and coordinating tool-mediated interactions (e.g., supplier communication) within the simulation.","Planning, Decision-making, Working Memory, Reward Mechanisms, Adaptive Error Correction, Semantic Understanding & Context Recognition, Social Reasoning & Theory of Mind"
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas measures real-world tool use through the Model Context Protocol by requiring models to discover tools, invoke them correctly, and compose multi-step workflows across external services. Tasks stress correct API selection, argument construction, error handling, and synthesizing tool outputs into final answers.","Planning, Decision-making, Working Memory, Adaptive Error Correction, Language Comprehension, Language Production, Semantic Understanding & Context Recognition"
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity capability on real software by asking agents to identify known vulnerabilities from high-level descriptions and, in some settings, to find new issues. It emphasizes understanding codebases, reasoning about exploit conditions, and iteratively testing hypotheses to converge on a valid report or patch pathway.","Logical Reasoning, Planning, Working Memory, Adaptive Error Correction, Semantic Understanding & Context Recognition, Language Comprehension"
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a large, frontier-difficulty benchmark spanning expert-level questions across many domains, often requiring multi-step reasoning and, in some settings, tool use. It is intended to probe breadth of knowledge and the ability to synthesize and justify answers under hard, sometimes multimodal prompts.","Language Comprehension, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Visual Perception"
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a high-quality subset of graduate-level, Google-proof multiple-choice science questions designed to be difficult for non-experts. It probes precise scientific reasoning and domain knowledge under adversarially selected questions where shallow pattern matching tends to fail.","Language Comprehension, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory"
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro is a rigorous multimodal benchmark covering expert-level problems across many disciplines, combining images/diagrams with text questions and multiple-choice answers. It targets visual grounding, diagram and chart interpretation, and multi-step reasoning that integrates visual evidence with domain concepts.","Visual Perception, Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Language Comprehension, Working Memory, Attention"
OmniDocBench 1.5,https://github.com/opendatalab/OmniDocBench,https://arxiv.org/abs/2412.07626,"OmniDocBench 1.5 evaluates document understanding and OCR quality across challenging layouts, including formulas, tables, and reading order. It stresses extracting structured content from noisy, heterogeneous documents while preserving layout-dependent meaning and sequence.","Visual Perception, Scene Understanding & Visual Reasoning, Spatial Representation & Mapping, Attention, Language Comprehension, Working Memory"
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU assesses multimodal understanding and reasoning over videos paired with questions that require integrating information across time. It targets comprehension of events, temporal dependencies, and cross-frame evidence aggregation rather than single-image recognition.","Visual Perception, Scene Understanding & Visual Reasoning, Cognitive Timing & Predictive Modeling, Working Memory, Attention, Multisensory Integration"
LiveCodeBench Pro,https://livecodebenchpro.com/projects/livecodebench-pro/leaderboard,https://arxiv.org/abs/2506.11928,"LiveCodeBench Pro evaluates coding skill on competitive-programming-style and practical coding tasks, typically emphasizing correctness under single-attempt constraints. It tests algorithmic problem solving, translating problem statements into executable code, and debugging subtle failures.","Logical Reasoning, Planning, Working Memory, Adaptive Error Correction, Language Comprehension, Language Production"
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,FACTS Benchmark Suite evaluates factuality by testing whether model outputs remain faithful to grounded sources and whether they avoid hallucinated or unsupported claims across diverse factuality settings. It emphasizes reliable claim generation and discrimination between supported vs. unsupported statements.,"Language Comprehension, Language Production, Semantic Understanding & Context Recognition, Inhibitory Control, Working Memory, Decision-making"
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,Global PIQA tests commonsense physical reasoning and practical “what would work” judgments across many languages and cultural contexts using non-parallel prompts. It emphasizes robust everyday reasoning that transfers beyond a single language or region-specific phrasing.,"Language Comprehension, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory"
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) is a long-context, multi-round coreference/recall evaluation where multiple similar “needle” interactions are embedded in a long “haystack,” and the model must reproduce the correct response corresponding to a specified needle. It probes whether models can track, bind, and retrieve the right information under heavy interference from near-duplicates.","Working Memory, Attention, Episodic Memory, Language Comprehension, Semantic Understanding & Context Recognition"
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval evaluates economically relevant professional knowledge work by asking models to produce real workplace artifacts (e.g., slides, spreadsheets, plans) spanning many occupations, judged against industry professionals. It stresses end-to-end task execution, adherence to specifications, and producing actionable deliverables rather than short-form answers.","Planning, Decision-making, Language Production, Language Comprehension, Semantic Understanding & Context Recognition, Working Memory, Self-reflection"
SWE-Lancer,https://openai.com/index/swe-lancer/,https://arxiv.org/abs/2502.12115,"SWE-Lancer evaluates agentic software engineering in repository contexts that require implementing changes, running checks, and iterating toward a correct solution under realistic constraints. It focuses on reliable execution of longer workflows (triage → implementation → verification) and robustness to intermediate failures.","Planning, Decision-making, Working Memory, Adaptive Error Correction, Language Comprehension, Language Production, Logical Reasoning"
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath is an expert-level mathematics benchmark intended to measure progress on hard, research-adjacent math problems that are challenging for current models. It emphasizes multi-step derivations, precise symbolic/quantitative manipulation, and maintaining consistency across long solution chains.","Logical Reasoning, Working Memory, Planning, Cognitive Flexibility, Adaptive Error Correction"
