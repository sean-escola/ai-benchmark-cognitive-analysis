Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates software engineering agents on real GitHub issues by requiring them to generate patches that make a project’s tests pass. The “Verified” subset adds stronger task validity checks (e.g., solvable by humans, reliable evaluation) to reduce spurious passes and better reflect end-to-end debugging and code-change skill.","L1: Language Comprehension, Language Production
L2: Planning, Logical Reasoning, Working Memory, Adaptive Error Correction
L3: ",L2
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 measures an agent’s ability to solve realistic tasks in a command-line environment, typically involving multi-step operations such as file manipulation, package/tool usage, debugging, and system inspection. It stresses reliable action sequencing under partial feedback, where intermediate mistakes must be detected and corrected to reach a working final state.","L1: Language Comprehension
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory
L3: ",L2
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench evaluates interactive, tool-using customer-support agents in simulated domains (e.g., retail, airline, telecom) where the agent must follow policies while helping a user through multi-turn problems. It emphasizes robust dialogue state tracking, correct API usage, and handling adversarial or ambiguous user behavior without violating constraints.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Working Memory
L3: Inhibitory Control, Social Reasoning & Theory of Mind",L3
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI tests “fluid” abstraction and generalization by presenting a few input–output grid examples and asking models to infer the underlying rule to transform a new input grid. The tasks are designed to be novel and compositional, discouraging memorization and rewarding flexible hypothesis formation and rule induction.","L1: Visual Perception
L2: Logical Reasoning, Working Memory, Spatial Representation & Mapping, Scene Understanding & Visual Reasoning
L3: Cognitive Flexibility",L3
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon autonomous performance in a simulated business setting where an agent manages a vending-machine operation over an extended period. Success requires maintaining coherent goals, optimizing inventory/pricing/supplier interactions, and adapting strategies as conditions change across many decisions.","L1: 
L2: Planning, Decision-making, Reward Mechanisms, Working Memory, Episodic Memory, Semantic Understanding & Context Recognition
L3: ",L2
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a challenging, broad benchmark intended to probe frontier reasoning and knowledge across disciplines, often including multimodal questions and tasks requiring deep synthesis. It aims to reduce shortcut solutions by using difficult, diverse items that stress careful reasoning rather than shallow pattern matching.","L1: Language Comprehension, Visual Perception
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Scene Understanding & Visual Reasoning, Multisensory Integration
L3: ",L2
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 consists of competition-style mathematics problems that require multi-step symbolic reasoning, algebraic manipulation, and careful constraint tracking. Performance is sensitive to logical consistency and error checking across several intermediate steps rather than single-fact recall.","L1: 
L2: Logical Reasoning, Working Memory, Adaptive Error Correction, Planning
L3: ",L2
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a difficult multiple-choice science benchmark curated to be “Google-proof,” emphasizing reasoning over rote retrieval. The Diamond subset targets high-quality questions where experts reliably agree on the correct answer and non-experts tend to fail, increasing the need for deep conceptual understanding.","L1: Language Comprehension
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory
L3: ",L2
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends broad academic knowledge testing to multiple languages, covering many subjects and requiring consistent performance across diverse linguistic contexts. It probes whether models preserve reasoning and domain knowledge when comprehension and expression shift across languages.","L1: Language Comprehension, Language Production
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory
L3: ",L2
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro evaluates multimodal expert-level understanding by combining images (e.g., diagrams, charts, figures) with questions spanning multiple disciplines. It stresses integrating visual evidence with text instructions and domain knowledge to perform high-level reasoning.","L1: Visual Perception, Language Comprehension
L2: Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Working Memory
L3: ",L2
MathArena Apex,https://matharena.ai/?view=problem&comp=apex--apex_2025,https://arxiv.org/abs/2505.23281,"MathArena Apex is a hard mathematics evaluation suite designed to measure advanced problem-solving under competition-like conditions, often emphasizing complex, multi-stage derivations. It is used to compare models’ mathematical reasoning robustness across a curated set of difficult items.","L1: 
L2: Logical Reasoning, Working Memory, Planning, Adaptive Error Correction
L3: ",L2
ScreenShot-Pro,https://gui-agent.github.io/grounding-leaderboard/,https://arxiv.org/abs/2504.07981,"ScreenShot-Pro evaluates how well models understand and act on information embedded in UI screenshots, such as locating relevant widgets, interpreting interface state, and answering grounded questions. It targets vision-language grounding and spatial layout understanding common in computer-use agents.","L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Spatial Representation & Mapping, Working Memory
L3: ",L2
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning focuses on scientific figure and document-based visual reasoning, requiring models to interpret plots, equations, and structured layouts typical of research papers. Tasks emphasize extracting correct evidence from visuals and performing reasoning steps that link figure content to the question.","L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Logical Reasoning, Working Memory, Semantic Understanding & Context Recognition
L3: ",L2
OmniDocBench 1.5,https://github.com/opendatalab/OmniDocBench,https://arxiv.org/abs/2412.07626,"OmniDocBench 1.5 evaluates document understanding and OCR-style extraction across heterogeneous elements such as text blocks, formulas, tables, and reading order. It measures whether a model can accurately parse structured documents and preserve semantics when converting from visual layouts to machine-readable text.","L1: Visual Perception, Language Production, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Attention, Working Memory, Semantic Understanding & Context Recognition
L3: ",L2
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU extends multimodal understanding to the temporal domain by asking questions about videos that require integrating cues across frames. It stresses tracking events, actions, and state changes over time rather than relying on a single keyframe.","L1: Visual Perception
L2: Working Memory, Scene Understanding & Visual Reasoning, Multisensory Integration, Attention
L3: Cognitive Timing & Predictive Modeling",L3
LiveCodeBench Pro,https://livecodebenchpro.com/projects/livecodebench-pro/leaderboard,https://arxiv.org/abs/2506.11928,"LiveCodeBench Pro evaluates coding ability on time-sensitive, execution-checked programming tasks intended to reduce contamination and reflect realistic development constraints. It rewards producing correct, runnable solutions and iteratively fixing errors under a single-attempt or limited-attempt evaluation regime.","L1: Language Comprehension, Language Production
L2: Logical Reasoning, Planning, Adaptive Error Correction, Working Memory
L3: ",L2
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"FACTS Benchmark Suite systematically evaluates LLM factuality, including whether models produce statements supported by evidence and avoid hallucinations across different settings and prompt styles. It targets reliability in knowledge-intensive responses, emphasizing calibration and resisting unsupported generation.","L1: Language Comprehension, Language Production
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory
L3: Inhibitory Control",L3
SimpleQA Verified,https://www.kaggle.com/benchmarks/deepmind/simpleqa-verified,https://arxiv.org/abs/2509.07968,SimpleQA Verified measures short-form factual question answering with verification-oriented ground truth and scoring designed to penalize unsupported claims. It focuses on precision in direct answers and the ability to refrain from guessing when uncertain.,"L1: Language Comprehension, Language Production
L2: Semantic Understanding & Context Recognition, Working Memory
L3: Inhibitory Control",L3
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,Global PIQA evaluates commonsense physical reasoning across languages and cultural contexts by testing which actions or explanations are plausible in everyday physical situations. It probes whether models maintain practical intuitive physics and commonsense priors when the linguistic surface form varies.,"L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning
L3: Cognitive Flexibility",L3
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) is a long-context retrieval and multi-round coreference task where multiple similar “needle” requests are embedded in a large “haystack,” and the model must recover the correct referenced response. It stresses sustained attention, interference resistance, and accurate retrieval across long spans of context.","L1: Language Comprehension
L2: Working Memory, Attention, Episodic Memory
L3: Inhibitory Control",L3
