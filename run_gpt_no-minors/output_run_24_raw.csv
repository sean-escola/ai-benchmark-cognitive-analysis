Benchmark,Website,Paper,Description,Cognitive Functions
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 evaluates autonomous agents on real command-line tasks inside containerized environments, requiring them to execute shell commands, inspect files, install dependencies, and iteratively debug. It probes end-to-end agent reliability under multi-step execution where mistakes must be detected and corrected through interaction with the environment.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Language Comprehension (minor), Language Production (minor)"
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp measures deep research and browsing ability by having models answer questions that require discovering and synthesizing information from the web (or a controlled corpus, depending on the setup). Success depends on selecting search strategies, tracking evidence across pages, and producing grounded, well-supported final answers.","Planning, Decision-making, Attention, Episodic Memory (minor), Working Memory, Semantic Understanding & Context Recognition, Language Comprehension, Language Production"
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld evaluates computer-using agents that operate a full desktop environment to complete real-world tasks across applications (e.g., browser, files, settings). It stresses perception-to-action loops: interpreting GUI state from screenshots/DOM-like cues and executing sequences of UI actions to achieve goals.","Visual Perception, Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Sensorimotor Coordination, Planning, Decision-making, Working Memory (minor)"
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI tests “fluid” abstract reasoning by asking models to infer latent transformation rules from a few input–output grid examples and apply them to new inputs. The tasks are designed to emphasize novel pattern induction rather than memorized knowledge, often requiring compositional and counterfactual reasoning over spatial structures.","Logical Reasoning, Cognitive Flexibility, Working Memory, Spatial Representation & Mapping, Attention (minor)"
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon agent coherence and strategy in a simulated year-long vending business, where models must manage inventory, pricing, supplier interactions, and finances over many steps. It emphasizes sustained goal pursuit, adapting to changing conditions, and accumulating returns through sequential decisions.","Planning, Decision-making, Reward Mechanisms, Working Memory (minor), Episodic Memory (minor), Adaptive Error Correction (minor), Motivational Drives (minor)"
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates agentic cybersecurity skills over large suites of real-world vulnerability tasks, including identifying known vulnerabilities from descriptions and discovering new ones in open-source codebases. It requires iterative hypothesis testing, code navigation, exploitation reasoning, and careful handling of error feedback from tools and builds.","Logical Reasoning, Planning, Decision-making, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition (minor), Inhibitory Control (minor)"
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"SpreadsheetBench tests whether models can correctly manipulate and reason over complex spreadsheets, often involving formulas, table structure, data cleaning, and multi-step transformations. It measures the ability to maintain consistency across many dependent cells and to verify outcomes after edits.","Working Memory, Attention, Logical Reasoning, Adaptive Error Correction, Planning (minor), Spatial Representation & Mapping (minor), Language Comprehension (minor)"
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a frontier academic benchmark spanning expert-level questions (often multi-modal) intended to probe advanced reasoning and broad knowledge under realistic difficulty. It emphasizes multi-step inference, careful interpretation of problem statements and artifacts, and producing final answers under strong uncertainty.","Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Visual Perception (minor), Scene Understanding & Visual Reasoning (minor), Self-reflection (minor)"
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a high-difficulty, multiple-choice science benchmark curated to be resistant to simple web lookup and to require genuine domain reasoning. The Diamond subset focuses on questions where experts reliably answer correctly while non-experts frequently fail, stressing deep conceptual understanding.","Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory (minor), Decision-making (minor)"
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro evaluates multimodal, multi-discipline reasoning with images and text, often involving diagrams, charts, and scientific/technical visuals. Compared to standard MMMU, the “Pro” setting is intended to increase difficulty and reduce shortcutting, requiring more faithful visual grounding and cross-domain inference.","Visual Perception, Scene Understanding & Visual Reasoning, Multisensory Integration, Visual Attention & Eye Movements (minor), Logical Reasoning, Spatial Representation & Mapping (minor), Language Comprehension"
MathArena Apex,https://matharena.ai/?view=problem&comp=apex--apex_2025,https://arxiv.org/abs/2505.23281,"MathArena Apex is a competitive mathematics benchmark/evaluation suite aimed at measuring frontier mathematical problem-solving under standardized conditions. It emphasizes constructing correct solution strategies, carrying out multi-step symbolic manipulation, and avoiding subtle arithmetic or logic errors under pressure.","Logical Reasoning, Working Memory, Planning (minor), Adaptive Error Correction (minor), Language Comprehension (minor)"
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning measures reasoning over figures and content drawn from scientific papers (e.g., plots, diagrams, tables) and typically expects answers grounded in the visual evidence. It stresses connecting natural-language questions to visual elements and performing quantitative or relational inference based on the figure.","Visual Perception, Scene Understanding & Visual Reasoning, Multisensory Integration, Attention (minor), Logical Reasoning, Semantic Understanding & Context Recognition (minor), Language Comprehension, Language Production (minor)"
OmniDocBench 1.5,https://github.com/opendatalab/OmniDocBench,https://arxiv.org/abs/2412.07626,"OmniDocBench 1.5 evaluates document understanding and OCR across diverse document elements such as text blocks, formulas, tables, and reading order. It tests whether models can faithfully extract and structure information from complex page layouts, not just recognize characters.","Visual Perception, Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning (minor), Language Comprehension, Working Memory (minor), Spatial Representation & Mapping (minor)"
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU evaluates multimodal understanding over videos, requiring models to integrate information across frames and answer questions involving events, temporal relations, and context. It emphasizes maintaining coherence over time and performing reasoning that depends on sequences rather than single images.","Visual Perception, Cognitive Timing & Predictive Modeling, Episodic Memory, Attention, Scene Understanding & Visual Reasoning, Multisensory Integration (minor), Working Memory (minor)"
LiveCodeBench Pro,https://livecodebenchpro.com/projects/livecodebench-pro/leaderboard,https://arxiv.org/abs/2506.11928,"LiveCodeBench Pro measures coding ability on competitive-style and practical programming tasks, typically evaluated by executing solutions against test cases and ranking performance (e.g., ELO). It stresses producing runnable code under constraints and iteratively correcting mistakes based on failing tests.","Logical Reasoning, Planning, Adaptive Error Correction, Working Memory, Language Production (minor), Language Comprehension (minor)"
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"FACTS Benchmark Suite systematically evaluates factuality, measuring whether model outputs are supported by evidence and whether they avoid hallucinated or incorrect claims across varied settings. It targets reliability behaviors such as attributing uncertainty, maintaining consistency, and resisting unsupported elaboration.","Semantic Understanding & Context Recognition, Language Production, Inhibitory Control, Self-reflection (minor), Working Memory (minor)"
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA evaluates commonsense physical reasoning across many languages, testing whether models can choose plausible actions or outcomes in everyday situations without relying on English-only priors. It emphasizes robust semantic understanding under multilingual variation and pragmatic knowledge about the physical world.","Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning, Social Reasoning & Theory of Mind (minor), Cognitive Flexibility (minor)"
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) is a long-context co-reference and retrieval-style evaluation where multiple similar “needle” requests are embedded across lengthy “haystack” dialogs or documents. Models must identify and reproduce the correct target response associated with a specific needle, stressing robust attention and context tracking at scale.","Working Memory, Attention, Episodic Memory, Language Comprehension, Inhibitory Control (minor)"
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval evaluates well-specified professional knowledge-work tasks spanning many occupations, judged by experts via pairwise comparisons against human outputs. It focuses on producing high-quality work artifacts (e.g., plans, analyses, presentations/spreadsheets) with strong adherence to constraints and real-world usefulness.","Planning, Decision-making, Language Production, Semantic Understanding & Context Recognition, Working Memory (minor), Adaptive Error Correction (minor), Social Reasoning & Theory of Mind (minor)"
SWE-Lancer,https://openai.com/index/swe-lancer/,https://arxiv.org/abs/2502.12115,"SWE-Lancer evaluates agentic software engineering on realistic tasks that resemble industry “tickets,” often requiring repository understanding, patch creation, and end-to-end debugging across a project. It stresses reliable tool-mediated iteration, interpreting failures, and converging on correct changes under time/complexity constraints.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Logical Reasoning, Language Comprehension (minor), Language Production (minor)"
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks tests structured reasoning over graph-structured data described in text, such as following edges, finding parents, or performing BFS-like traversals within a token budget. It probes whether models can maintain state across multi-step symbolic navigation without losing track of previously visited nodes or constraints.","Spatial Representation & Mapping, Working Memory, Attention, Logical Reasoning, Planning (minor)"
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon evaluates general tool-use competence across diverse APIs and environments, typically requiring multi-step tool selection, execution, and synthesis into a final response. It measures whether agents can reliably follow tool protocols, recover from tool errors, and coordinate long workflows without drift.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Attention (minor), Language Comprehension (minor), Language Production (minor)"
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath evaluates expert-level mathematics problem solving designed to be challenging for frontier models, emphasizing proofs, deep derivations, and non-routine reasoning rather than textbook pattern matching. It stresses sustained multi-step inference, careful bookkeeping, and error-averse symbolic reasoning.","Logical Reasoning, Working Memory, Planning (minor), Adaptive Error Correction (minor), Semantic Understanding & Context Recognition (minor)"
