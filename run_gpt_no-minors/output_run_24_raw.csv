Benchmark,Website,Paper,Description,Cognitive Functions
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates software engineering agents by asking them to generate code patches that fix real GitHub issues in Python repositories, with correctness determined by running the project’s tests. The “Verified” subset filters to tasks that have been manually validated as solvable and reliably testable, reducing noise from flaky or underspecified issues.","Logical Reasoning, Planning, Working Memory, Adaptive Error Correction, Decision-making, Language Comprehension (minor), Language Production (minor)"
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro is a larger, more difficult software engineering benchmark designed to be more realistic and contamination-resistant than earlier SWE-bench variants, spanning multiple programming languages. Models must understand a repository and issue description, implement a correct fix, and pass evaluation tests under stricter task curation.","Logical Reasoning, Planning, Working Memory, Adaptive Error Correction, Decision-making, Cognitive Flexibility (minor), Language Comprehension (minor)"
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp evaluates “deep research” style agents that must search, read, and synthesize information from the web (or a controlled corpus) to answer difficult questions. Performance depends on iterative querying, evidence gathering, and consolidating findings into a final grounded answer.","Planning, Decision-making, Attention, Working Memory, Language Comprehension, Semantic Understanding & Context Recognition, Adaptive Error Correction (minor), Episodic Memory (minor)"
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench measures interactive agent performance in customer-service style simulations (e.g., retail, airline, telecom) where the model must converse with a user simulator and call APIs while adhering to domain policies. Success requires multi-turn state tracking, policy compliance, and robust tool-mediated problem resolution.","Social Reasoning & Theory of Mind, Inhibitory Control, Decision-making, Planning, Working Memory, Language Comprehension, Language Production, Empathy (minor)"
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI is a grid-based “fluid intelligence” benchmark in which models infer abstract transformation rules from a few input–output examples and apply them to a novel input. It emphasizes generalization to new tasks with minimal examples, rather than recalling domain facts.","Logical Reasoning, Cognitive Flexibility, Working Memory, Spatial Representation & Mapping, Visual Perception, Scene Understanding & Visual Reasoning, Attention (minor)"
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas evaluates real-world tool use through the Model Context Protocol (MCP), requiring models to discover appropriate tools, invoke them correctly, and complete multi-step workflows across APIs and servers. Tasks stress reliable orchestration (including handling errors/retries) and synthesizing tool outputs into correct final responses.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Language Comprehension (minor), Language Production (minor), Semantic Understanding & Context Recognition"
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam (HLE) is a large, difficult multimodal benchmark intended to probe frontier knowledge and reasoning across many subjects, often requiring synthesis beyond straightforward recall. It is commonly reported both with and without tools (e.g., search or code), highlighting differences between pure reasoning and tool-augmented problem solving.","Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory (minor), Planning (minor), Visual Perception (minor), Scene Understanding & Visual Reasoning (minor), Multisensory Integration (minor)"
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 evaluates competition-style mathematics problem solving, typically requiring multi-step derivations, algebraic manipulation, and careful case handling. Scores are usually reported without tools (pure reasoning) and sometimes with tools (e.g., Python) to separate reasoning skill from calculation support.","Logical Reasoning, Working Memory, Planning, Adaptive Error Correction (minor), Semantic Understanding & Context Recognition (minor)"
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a high-quality subset of GPQA consisting of very challenging graduate-level multiple-choice questions in physics, chemistry, and biology, designed to be hard to answer by superficial web search. It targets deep scientific understanding and careful reasoning under distractor options.","Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory (minor), Decision-making (minor), Language Comprehension"
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends MMLU to a multilingual setting, testing knowledge and reasoning across many academic subjects and multiple languages. It probes whether models maintain subject competence and reasoning consistency when operating outside English.","Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory (minor), Language Production (minor)"
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro is a more challenging, expert-oriented version of MMMU for multimodal understanding, where models answer questions that require jointly reasoning over images (charts, diagrams, figures) and text across diverse disciplines. It emphasizes robust visual reasoning and domain-grounded interpretation rather than simple recognition.","Visual Perception, Scene Understanding & Visual Reasoning, Multisensory Integration, Visual Attention & Eye Movements (minor), Language Comprehension, Logical Reasoning, Spatial Representation & Mapping (minor)"
ScreenShot-Pro,https://gui-agent.github.io/grounding-leaderboard/,https://arxiv.org/abs/2504.07981,"ScreenShot-Pro evaluates GUI understanding from screenshots, requiring models to interpret screen layouts, read UI text, and answer questions that depend on visual grounding in real interfaces. It is commonly used to measure how well agents can understand software screens as a prerequisite for reliable computer-use automation.","Visual Perception, Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Spatial Representation & Mapping, Language Comprehension, Working Memory (minor), Sensorimotor Coordination (minor)"
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning tests whether models can answer questions about scientific figures and content from arXiv-style papers, often requiring extracting quantitative/structural information from charts and linking it to textual context. It emphasizes higher-order interpretation of technical visuals rather than generic image captioning.","Scene Understanding & Visual Reasoning, Visual Perception, Multisensory Integration, Language Comprehension, Logical Reasoning, Visual Attention & Eye Movements (minor), Semantic Understanding & Context Recognition (minor)"
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU extends multimodal understanding to videos, requiring models to integrate information across frames and time to answer questions about events, interactions, and visual evidence. It targets temporal integration and cross-modal reasoning under long-context visual inputs.","Visual Perception, Scene Understanding & Visual Reasoning, Multisensory Integration, Working Memory, Attention, Cognitive Timing & Predictive Modeling (minor), Language Comprehension (minor)"
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,MRCR v2 (8-needle) evaluates long-context multi-round coreference resolution by inserting multiple similar “needle” interactions into long “haystacks” and asking the model to retrieve the correct referenced response. It stresses precise attention control and retrieval over long sequences with high distractor density.,"Working Memory, Attention, Language Comprehension, Episodic Memory, Semantic Understanding & Context Recognition, Inhibitory Control (minor)"
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval measures performance on well-specified professional knowledge-work tasks across many occupations, where models produce real work artifacts such as spreadsheets, presentations, schedules, or analyses. It emphasizes end-to-end task execution quality judged against human professional outputs, often including tool use and iterative refinement.","Planning, Decision-making, Language Production, Semantic Understanding & Context Recognition, Adaptive Error Correction (minor), Self-reflection (minor), Working Memory (minor)"
SWE-Lancer,https://openai.com/index/swe-lancer/,https://arxiv.org/abs/2502.12115,"SWE-Lancer evaluates agentic software engineering on longer, more realistic engineering work that resembles “freelance” tasks, often spanning multiple files, constraints, and higher-level requirements. It is designed to better capture end-to-end engineering competence beyond narrow bug fixes.","Planning, Logical Reasoning, Decision-making, Working Memory, Adaptive Error Correction, Language Comprehension (minor), Language Production (minor)"
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks tests structured reasoning over graph-like data embedded in text, requiring models to perform multi-step traversals (e.g., BFS-style reasoning) and answer queries that depend on correct intermediate states. It targets systematic compositional reasoning and resistance to distraction over long, algorithmic contexts.","Logical Reasoning, Working Memory, Attention, Spatial Representation & Mapping, Planning (minor), Adaptive Error Correction (minor)"
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon evaluates an agent’s ability to solve tasks by selecting, sequencing, and correctly calling tools across multi-step workflows, often under realistic constraints (API schemas, partial failures, or multi-turn requirements). It emphasizes reliable tool orchestration and integrating tool outputs into coherent final results.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Attention (minor), Language Comprehension (minor), Language Production (minor)"
HMMT,https://matharena.ai/?view=problem&comp=hmmt--hmmt_feb_2025,https://arxiv.org/abs/2505.23281,"HMMT (Harvard-MIT Mathematics Tournament) problems in MathArena-style evaluations target advanced contest mathematics, typically requiring inventive multi-step reasoning, proofs or proof-like derivations, and careful handling of edge cases. Compared to easier contest sets, it is used to probe depth and robustness of mathematical reasoning.","Logical Reasoning, Working Memory, Planning, Adaptive Error Correction (minor), Cognitive Flexibility (minor)"
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath is an expert-level mathematics benchmark designed to be substantially harder than standard contest math, targeting multi-step reasoning that often benefits from sustained exploration and verification. It is frequently reported with tool assistance (e.g., Python) to separate reasoning quality from arithmetic burden while still stressing correctness.","Logical Reasoning, Planning, Working Memory, Adaptive Error Correction, Cognitive Flexibility (minor)"
