Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates software engineering agents on real GitHub issues where the goal is to produce a code patch that makes a hidden test suite pass. The Verified subset consists of problems curated/validated to be solvable and to reduce noisy or ambiguous instances, emphasizing reliable end-to-end bug fixing and feature implementation.","L1: Language Comprehension (minor), Language Production (minor)
L2: Planning, Logical Reasoning, Adaptive Error Correction, Working Memory, Decision-making (minor)
L3: ",L2
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro is a larger, more difficult software engineering benchmark designed to be more contamination-resistant and more representative of professional, multi-language development. Tasks require generating patches that satisfy evaluation harnesses/tests, often involving deeper repo understanding, refactoring, and multi-file changes.","L1: Language Comprehension (minor)
L2: Planning, Logical Reasoning, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition, Decision-making (minor)
L3: ",L2
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp evaluates “deep research” style agents that must answer questions by searching and synthesizing evidence from the web (or a controlled document index in some variants). It stresses query formulation, iterative retrieval, cross-checking sources, and producing a grounded final answer from multi-document evidence.","L1: Language Production (minor)
L2: Planning, Attention, Working Memory, Semantic Understanding & Context Recognition, Episodic Memory (minor), Decision-making (minor)
L3: ",L2
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench measures an agent’s ability to complete multi-turn customer-support tasks (e.g., retail, airline, telecom) by interacting with simulated users and programmatic APIs while following domain-specific policies. Success depends on maintaining dialogue state, choosing correct tool/API actions, and adhering to constraints over long interactions.","L1: Language Comprehension (minor), Language Production (minor)
L2: Decision-making, Planning, Working Memory
L3: Social Reasoning & Theory of Mind, Inhibitory Control, Empathy (minor)",L3
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI evaluates fluid reasoning on novel abstract grid-transformation puzzles where only a few input–output examples are provided. Models must infer the underlying rule and generalize it to a new input, emphasizing systematic generalization rather than memorization of known tasks.","L1: Visual Perception
L2: Logical Reasoning, Spatial Representation & Mapping, Scene Understanding & Visual Reasoning, Working Memory (minor), Attention (minor)
L3: Cognitive Flexibility",L3
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas evaluates real-world tool use via the Model Context Protocol (MCP), where models must discover tools, invoke them correctly, manage errors, and compose multi-step workflows across services. It targets robust agentic behavior under realistic API constraints and multi-call dependencies.","L1: 
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition (minor)
L3: Inhibitory Control (minor)",L2
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a large, frontier-difficulty benchmark spanning many domains and modalities, intended to probe broad expert-level knowledge and reasoning. Questions often require multi-step inference, careful reading of problem statements, and (in tool-enabled settings) combining external resources with internal reasoning.","L1: Language Comprehension, Language Production (minor), Visual Perception (minor)
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Multisensory Integration (minor)
L3: ",L2
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 is a competition-math benchmark based on the American Invitational Mathematics Examination problems. It emphasizes precise symbolic reasoning, multi-step derivations, and error-free arithmetic/algebra under time-test-style question formats.","L1: 
L2: Logical Reasoning, Working Memory, Planning (minor), Adaptive Error Correction (minor)
L3: ",L2
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a difficult multiple-choice science QA benchmark (physics, chemistry, biology) curated so that non-experts frequently fail while domain experts succeed. It tests deep scientific knowledge plus reasoning that is intended to be resistant to shallow web search strategies.","L1: 
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory (minor), Decision-making (minor)
L3: ",L2
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends broad academic knowledge testing across many subjects into multiple languages, evaluating whether models can understand and answer domain questions beyond English. The benchmark stresses multilingual comprehension and cross-lingual robustness rather than tool use or long-horizon interaction.","L1: Language Comprehension, Language Production (minor)
L2: Semantic Understanding & Context Recognition, Working Memory (minor)
L3: ",L2
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro evaluates expert-level multimodal understanding and reasoning over images and text, typically using questions drawn from many disciplines (e.g., science, engineering, charts/diagrams). It targets accurate visual interpretation combined with grounded, multi-step reasoning under challenging distractors.","L1: Visual Perception, Language Comprehension (minor)
L2: Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Attention (minor)
L3: ",L2
ScreenShot-Pro,https://gui-agent.github.io/grounding-leaderboard/,https://arxiv.org/abs/2504.07981,"ScreenShot-Pro evaluates GUI understanding from high-resolution screenshots, requiring models to identify interface elements and answer questions grounded in spatial layout and visual cues. It stresses precise visual grounding (often akin to “where to click/what is shown”) rather than general image captioning.","L1: Visual Perception
L2: Visual Attention & Eye Movements, Spatial Representation & Mapping, Scene Understanding & Visual Reasoning, Decision-making (minor), Sensorimotor Coordination (minor)
L3: ",L2
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning focuses on interpreting scientific figures (often from arXiv-style papers) and answering reasoning questions that require extracting quantitative/structural information from plots, diagrams, and figure captions. It emphasizes chart/figure comprehension and chaining visual evidence with domain reasoning.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Logical Reasoning, Multisensory Integration, Working Memory (minor), Attention (minor)
L3: ",L2
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU extends multimodal understanding to the video setting, requiring models to reason over temporal visual information plus text. Tasks commonly require integrating events across frames, tracking entities, and using context to answer questions about actions, causality, or procedural steps.","L1: Visual Perception
L2: Working Memory, Scene Understanding & Visual Reasoning, Multisensory Integration (minor), Attention (minor)
L3: Cognitive Timing & Predictive Modeling",L3
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) measures long-context retrieval and multi-round coreference resolution by placing multiple similar “needle” interactions inside a long “haystack” of text. The model must recover the correct referenced content among many distractors, stressing robust context tracking at scale.","L1: Language Comprehension
L2: Working Memory, Attention, Episodic Memory
L3: Inhibitory Control (minor)",L2
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval evaluates economically relevant, well-specified professional knowledge-work tasks across many occupations, judged by expert humans via pairwise comparisons. It emphasizes producing practical artifacts (e.g., plans, spreadsheets, writeups) that satisfy constraints, communicate clearly, and reflect professional standards.","L1: Language Production
L2: Planning, Decision-making, Semantic Understanding & Context Recognition
L3: Self-reflection (minor), Social Reasoning & Theory of Mind (minor)",L2
SWE-Lancer,https://openai.com/index/swe-lancer/,https://arxiv.org/abs/2502.12115,"SWE-Lancer evaluates agentic software engineering on tasks intended to resemble freelance/contractor work, emphasizing end-to-end delivery quality rather than narrow unit-test fixes. It typically stresses interpreting ambiguous requirements, making design choices, implementing robust solutions, and iterating toward completion.","L1: Language Comprehension (minor)
L2: Planning, Decision-making, Logical Reasoning, Adaptive Error Correction, Working Memory (minor)
L3: ",L2
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks evaluates structured long-context reasoning by embedding graph traversal tasks (e.g., BFS-style walks, parent-pointer recovery) into textual sequences. Models must follow graph-defined constraints over many steps, testing whether they can reliably compute and track discrete structure in context.","L1: 
L2: Spatial Representation & Mapping, Working Memory, Logical Reasoning, Attention (minor)
L3: Cognitive Timing & Predictive Modeling (minor)",L2
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon evaluates how well models can solve tasks by selecting and using tools across multi-step workflows, often under time/latency or correctness constraints. It targets robust tool calling, argument construction, error handling, and integrating tool outputs into a coherent final response.","L1: Language Production (minor)
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory
L3: Inhibitory Control (minor)",L2
HMMT,https://matharena.ai/?view=problem&comp=hmmt--hmmt_feb_2025,https://arxiv.org/abs/2505.23281,"HMMT is a competition-math benchmark based on Harvard-MIT Mathematics Tournament problems, which are typically harder and more proof-like than standard school math. It emphasizes deep multi-step reasoning, strategy selection across solution paths, and maintaining correctness over long derivations.","L1: 
L2: Logical Reasoning, Working Memory, Planning (minor), Adaptive Error Correction (minor)
L3: ",L2
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath targets expert-level mathematics with problems designed to be challenging for frontier models and to better measure genuine mathematical reasoning progress. Many items require extended multi-step derivations and careful control of intermediate results, often with optional tool use for computation checking.","L1: 
L2: Logical Reasoning, Working Memory, Planning, Adaptive Error Correction (minor)
L3: ",L2
