Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 evaluates agentic coding and systems skills by having models operate in a real command-line environment to complete multi-step tasks (e.g., debugging, building, configuring, and running programs) via tool use. Success depends on choosing correct commands, interpreting outputs, and iteratively fixing mistakes under realistic constraints.","L1: Language Comprehension (minor)
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory
L3: ",L2
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp evaluates deep research and browsing competence by requiring models/agents to find, verify, and synthesize information from web-like corpora under tool-use constraints. It stresses end-to-end information seeking: query formulation, evidence gathering, and producing grounded answers.","L1: Language Comprehension
L2: Planning, Decision-making, Semantic Understanding & Context Recognition, Episodic Memory (minor), Working Memory, Attention (minor)
L3: ",L2
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld benchmarks multimodal computer-use agents that must complete tasks within operating-system-like desktop environments using screenshots and interaction actions. It measures robust perception-to-action loops, including navigation, form filling, file/app manipulation, and recovery from interface errors.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements (minor), Planning, Decision-making, Sensorimotor Coordination, Working Memory
L3: ",L2
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,ARC-AGI tests fluid reasoning by asking models to infer the transformation rule from a few input-output grid examples and apply it to a new grid. It is designed to reduce reliance on memorized knowledge and instead emphasize abstract pattern discovery and generalization.,"L1: Visual Perception (minor)
L2: Logical Reasoning, Working Memory, Spatial Representation & Mapping, Attention (minor)
L3: Cognitive Flexibility",L3
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon agent coherence by running a simulated vending-machine business over extended time, requiring thousands of decisions (inventory, pricing, suppliers, communication). Performance is typically measured by final profit/balance, reflecting sustained planning, adaptation, and error recovery.","L1: 
L2: Planning, Decision-making, Reward Mechanisms, Adaptive Error Correction, Working Memory
L3: Self-reflection (minor), Motivational Drives (minor)",L2
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity capability on real-world vulnerability tasks, including locating known vulnerabilities from descriptions and discovering previously unknown issues in open-source codebases. It stresses correct diagnosis, exploit/bug reasoning, and writing or testing patches under realistic tooling workflows.","L1: Language Comprehension
L2: Logical Reasoning, Planning, Adaptive Error Correction, Working Memory
L3: Inhibitory Control (minor)",L2
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"SpreadsheetBench measures an agent’s ability to understand and manipulate spreadsheets to solve practical problems (e.g., cleaning data, writing formulas, restructuring tables, generating outputs). It typically requires multi-step operations, careful bookkeeping across cells/sheets, and verification of computed results.","L1: Visual Perception (minor)
L2: Planning, Decision-making, Working Memory, Logical Reasoning, Adaptive Error Correction, Attention (minor)
L3: ",L2
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam (HLE) is a broad, frontier-difficulty benchmark spanning many domains (often including multimodal questions) intended to stress advanced reasoning and expert knowledge. Depending on configuration, models may be evaluated with or without tools such as search and code execution, emphasizing both problem solving and grounded synthesis.","L1: Language Comprehension, Language Production (minor)
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Multisensory Integration (minor), Scene Understanding & Visual Reasoning (minor)
L3: ",L2
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,GPQA Diamond is a high-difficulty subset of GPQA consisting of graduate-level science multiple-choice questions designed to be “Google-proof” and to separate experts from non-experts. It emphasizes precise scientific reasoning and careful discrimination among plausible distractors.,"L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory
L3: Inhibitory Control (minor)",L2
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro evaluates expert-level multimodal understanding and reasoning across many disciplines using images combined with text questions. It stresses interpreting diagrams, plots, and visual scenes while integrating textual context to answer structured questions accurately.","L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Multisensory Integration, Working Memory, Attention (minor)
L3: ",L2
MathArena Apex,https://matharena.ai/?view=problem&comp=apex--apex_2025,https://arxiv.org/abs/2505.23281,"MathArena Apex is a competitive math reasoning evaluation composed of challenging problems intended to differentiate frontier models under standardized conditions. It emphasizes multi-step symbolic reasoning, careful constraint tracking, and producing correct final numeric/algebraic answers.","L1: 
L2: Logical Reasoning, Working Memory, Adaptive Error Correction (minor), Planning (minor)
L3: ",L2
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning evaluates scientific figure/chart understanding and reasoning over visual content paired with questions, often benefiting from tool-assisted analysis (e.g., Python) in some settings. It stresses extracting quantitative and structural information from figures and integrating it with textual scientific context to answer correctly.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements (minor), Multisensory Integration, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory (minor)
L3: ",L2
OmniDocBench 1.5,https://github.com/opendatalab/OmniDocBench,https://arxiv.org/abs/2412.07626,"OmniDocBench 1.5 evaluates document understanding and OCR-style extraction on heterogeneous documents containing text, formulas, tables, and layout structure. Metrics typically reflect how accurately models reproduce content and ordering, stressing robust perception of layout and symbols.","L1: Visual Perception, Language Comprehension
L2: Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning (minor), Working Memory (minor)
L3: ",L2
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU evaluates multimodal reasoning over videos, requiring models to integrate information across frames and time to answer questions. It stresses temporal understanding (events, causality, changes) and combining visual evidence with language instructions.","L1: Visual Perception, Language Comprehension (minor)
L2: Scene Understanding & Visual Reasoning, Working Memory, Attention (minor), Multisensory Integration (minor)
L3: Cognitive Timing & Predictive Modeling",L3
LiveCodeBench Pro,https://livecodebenchpro.com/projects/livecodebench-pro/leaderboard,https://arxiv.org/abs/2506.11928,"LiveCodeBench Pro evaluates coding ability on realistic programming tasks under standardized scoring (often reported via an ELO-like rating across problems). It emphasizes writing correct programs, interpreting specifications, and iterating when initial solutions fail.","L1: Language Comprehension
L2: Logical Reasoning, Planning, Adaptive Error Correction, Working Memory
L3: ",L2
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"The FACTS Benchmark Suite systematically evaluates factuality and grounding, testing whether model outputs remain consistent with available evidence and avoid unsupported claims. It targets hallucination-like errors across diverse settings, measuring reliability rather than raw generative fluency.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Working Memory (minor)
L3: Inhibitory Control, Self-reflection (minor)",L3
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA extends physical commonsense question answering across many languages and cultural contexts, probing whether models can infer plausible actions or outcomes in everyday physical situations. It emphasizes robust reasoning about the physical world beyond English-only phrasing.","L1: Language Comprehension
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Spatial Representation & Mapping (minor)
L3: Cognitive Flexibility (minor)",L2
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (multi-round coreference resolution) tests long-context robustness by inserting multiple similar “needle” interactions within large “haystack” conversations and asking the model to reproduce the response associated with a particular needle. The 8-needle setting increases interference, stressing retrieval and disambiguation under substantial context length.","L1: Language Comprehension
L2: Working Memory, Episodic Memory, Attention, Semantic Understanding & Context Recognition
L3: Inhibitory Control (minor)",L2
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval evaluates economically meaningful, well-specified professional tasks across many occupations, with expert human judging of produced work products (e.g., slides, spreadsheets, plans). It is designed to reflect real knowledge-work performance rather than narrow academic QA, often emphasizing artifact quality and end-to-end execution.","L1: Language Production, Language Comprehension
L2: Planning, Decision-making, Working Memory
L3: Self-reflection (minor), Social Reasoning & Theory of Mind (minor)",L2
SWE-Lancer,https://openai.com/index/swe-lancer/,https://arxiv.org/abs/2502.12115,"SWE-Lancer evaluates software engineering performance on tasks resembling real development work (e.g., implementing features, fixing bugs, making repository changes) under agent-like constraints. It emphasizes long-horizon codebase understanding, patch construction, and validation against tests or specifications.","L1: Language Comprehension (minor)
L2: Planning, Adaptive Error Correction, Working Memory, Logical Reasoning, Decision-making (minor)
L3: ",L2
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks tests reasoning over graph-structured data presented in-text, often requiring multi-step traversals (e.g., BFS-style exploration or parent/neighbor queries) across long contexts. It measures whether models can maintain and manipulate explicit relational structure rather than relying on surface heuristics.","L1: 
L2: Spatial Representation & Mapping, Working Memory, Logical Reasoning, Attention (minor)
L3: Cognitive Timing & Predictive Modeling (minor)",L2
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon evaluates general tool-use competence across diverse APIs and environments, requiring models to select tools, format calls correctly, handle failures, and compose multi-step workflows to complete tasks. It targets practical agent reliability: correct action sequencing, state tracking, and robust recovery.","L1: Language Comprehension
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory
L3: Inhibitory Control (minor)",L2
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath is a challenging advanced-mathematics benchmark intended to measure progress on expert-level problem solving beyond standard contest math. It emphasizes deep multi-step derivations, careful symbol manipulation, and verification of results, sometimes with tool-enabled settings in practice.","L1: 
L2: Logical Reasoning, Working Memory, Planning, Adaptive Error Correction (minor)
L3: Cognitive Flexibility (minor)",L2
