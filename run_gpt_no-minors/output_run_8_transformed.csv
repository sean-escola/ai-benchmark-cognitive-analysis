Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 evaluates agentic software engineering in real command-line environments, where models must execute shell commands, inspect files, install dependencies, and iteratively fix issues to complete tasks. Success depends on choosing correct tool actions under resource constraints and recovering from execution/runtime errors.","L1: Language Comprehension (minor)
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory
L3: ",L2
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp measures an agent’s ability to perform deep web research to answer questions that require gathering evidence across multiple sources. The benchmark emphasizes search strategy, source triage, synthesis, and producing a final grounded answer rather than purely recalling facts.","L1: Language Comprehension, Language Production (minor)
L2: Planning, Attention, Working Memory, Episodic Memory (minor), Semantic Understanding & Context Recognition, Logical Reasoning
L3: ",L2
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld evaluates multimodal “computer use” by having models complete tasks in a desktop operating system (apps, settings, files, web) via screenshots and UI actions. It stresses robust perception of interfaces and long-horizon action sequences with frequent branching and error recovery.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Sensorimotor Coordination, Planning, Decision-making, Adaptive Error Correction, Working Memory (minor)
L3: ",L2
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,ARC-AGI tests “fluid intelligence” by requiring models to infer abstract transformation rules from a few input–output grid examples and apply them to new inputs. It is designed to reduce reliance on memorized knowledge and instead reward pattern discovery and rule induction.,"L1: 
L2: Logical Reasoning, Working Memory, Spatial Representation & Mapping, Attention (minor)
L3: Cognitive Flexibility",L3
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon agent coherence in a business-management simulation (e.g., inventory, suppliers, pricing, and demand shocks) over many steps. Models must maintain consistent strategy, adapt to changing conditions, and avoid compounding planning errors over time.","L1: 
L2: Planning, Decision-making, Reward Mechanisms, Adaptive Error Correction (minor), Working Memory (minor)
L3: Motivational Drives (minor)",L2
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity agent capabilities on large collections of tasks involving identifying, reproducing, or discovering vulnerabilities in real software projects. It emphasizes interpreting technical descriptions, navigating codebases and build systems, and producing correct exploit/patch outcomes under realistic constraints.","L1: 
L2: Logical Reasoning, Planning, Adaptive Error Correction, Semantic Understanding & Context Recognition, Working Memory (minor)
L3: ",L2
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"SpreadsheetBench tests whether models can understand, edit, and compute with complex spreadsheets using realistic workflows (formulas, tables, formatting, and multi-step transformations). It requires keeping track of dependencies and verifying outputs across many cells and sheets.","L1: 
L2: Logical Reasoning, Working Memory, Planning, Attention, Adaptive Error Correction (minor), Semantic Understanding & Context Recognition (minor)
L3: ",L2
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a broad, frontier-level benchmark spanning difficult questions across academic domains, often requiring multi-step reasoning and (in some settings) tool use such as search or code execution. It is intended to probe the upper tail of model competence, including synthesis and error-prone edge cases.","L1: Language Comprehension, Visual Perception (minor)
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory (minor), Scene Understanding & Visual Reasoning (minor), Planning (minor)
L3: ",L2
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a hard subset of graduate-level, “Google-proof” multiple-choice science questions where superficial retrieval is unlikely to suffice. It emphasizes careful reading, domain reasoning, and discriminating among closely related answer choices.","L1: Language Comprehension
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory (minor)
L3: Inhibitory Control (minor)",L2
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro evaluates expert-level multimodal understanding and reasoning across disciplines, requiring models to answer questions grounded in images (e.g., diagrams, plots, tables) plus text. It stresses integrating visual evidence with domain knowledge and multi-step inference.","L1: Visual Perception, Language Comprehension (minor)
L2: Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Visual Attention & Eye Movements (minor)
L3: ",L2
MathArena Apex,https://matharena.ai/?view=problem&comp=apex--apex_2025,https://arxiv.org/abs/2505.23281,"MathArena Apex aggregates challenging competition-style mathematics problems and evaluates whether models can reliably solve them (often with strict correctness). It focuses on structured derivations, symbolic manipulation, and maintaining long chains of intermediate constraints without drifting.","L1: 
L2: Logical Reasoning, Working Memory, Planning (minor), Adaptive Error Correction (minor)
L3: Cognitive Flexibility (minor)",L2
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning tests scientific figure and chart understanding from arXiv-style documents, requiring models to interpret visual encodings and answer reasoning questions grounded in figures. It often rewards translating visual structure into quantitative or relational conclusions, sometimes aided by tool use (e.g., Python).","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Multisensory Integration (minor), Logical Reasoning, Semantic Understanding & Context Recognition (minor)
L3: ",L2
OmniDocBench 1.5,https://github.com/opendatalab/OmniDocBench,https://arxiv.org/abs/2412.07626,"OmniDocBench 1.5 evaluates document understanding/OCR quality across diverse document elements (text blocks, formulas, tables, and reading order). It measures how well systems preserve structure and content fidelity when extracting information from complex page layouts.","L1: Visual Perception, Language Comprehension (minor)
L2: Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning (minor), Working Memory (minor), Semantic Understanding & Context Recognition (minor)
L3: ",L2
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU evaluates multimodal reasoning over video, where answers may depend on temporal events, object interactions, and narrative context across frames. It stresses integrating information over time rather than relying on a single image snapshot.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Working Memory, Multisensory Integration (minor), Attention (minor)
L3: Cognitive Timing & Predictive Modeling",L3
LiveCodeBench Pro,https://livecodebenchpro.com/projects/livecodebench-pro/leaderboard,https://arxiv.org/abs/2506.11928,"LiveCodeBench Pro evaluates coding performance on fresh, competitive-programming-style and software tasks designed to reduce contamination, often scored with strong correctness criteria. It emphasizes translating problem statements into correct algorithms and producing executable implementations under time/complexity constraints.","L1: Language Comprehension (minor), Language Production (minor)
L2: Planning, Logical Reasoning, Adaptive Error Correction, Working Memory
L3: ",L2
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"The FACTS Benchmark Suite systematically evaluates factuality, measuring whether model outputs are supported by available evidence and whether they avoid hallucinated or fabricated claims. It typically probes grounded generation, attribution, and robustness to prompts that tempt confident but incorrect statements.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Episodic Memory (minor)
L3: Inhibitory Control, Self-reflection (minor)",L3
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA evaluates physical commonsense reasoning about everyday interactions with objects and environments, collected across languages (non-parallel). It targets whether models can infer plausible actions and outcomes in the physical world beyond culture- or language-specific surface cues.","L1: Language Comprehension
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Spatial Representation & Mapping (minor)
L3: Cognitive Flexibility (minor)",L2
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) measures long-context retrieval and multi-round coreference: multiple similar “needle” turns are embedded inside long “haystacks,” and the model must reproduce the correct response corresponding to a specific needle. It stresses maintaining and selecting the right discourse state amid heavy interference.","L1: Language Comprehension
L2: Working Memory, Attention, Episodic Memory
L3: Inhibitory Control (minor)",L2
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval evaluates well-specified professional knowledge-work tasks across many occupations, judged via human side-by-side comparisons of the produced work artifacts (e.g., spreadsheets, presentations, plans). It emphasizes end-to-end task execution quality, adherence to constraints, and producing usable deliverables.","L1: Language Production
L2: Planning, Decision-making, Semantic Understanding & Context Recognition, Working Memory (minor), Adaptive Error Correction (minor)
L3: Social Reasoning & Theory of Mind (minor)",L2
SWE-Lancer,https://openai.com/index/swe-lancer/,https://arxiv.org/abs/2502.12115,"SWE-Lancer evaluates agentic software engineering on realistic repository-based tasks that often require multi-step debugging, modification, and validation beyond a single patch. It emphasizes choosing effective engineering actions and coordinating tool use to ship correct changes under constraints.","L1: Language Comprehension (minor)
L2: Planning, Adaptive Error Correction, Decision-making, Working Memory
L3: Inhibitory Control (minor)",L2
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks evaluates reasoning over graph-structured data, typically requiring models to follow edges, track parent/neighbor relationships, and answer traversal queries over long contexts. It stresses precise state tracking and resisting distraction from many similar nodes and paths.","L1: 
L2: Spatial Representation & Mapping, Logical Reasoning, Working Memory, Attention
L3: ",L2
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon evaluates tool-using agents on multi-step tasks that require selecting appropriate tools, calling them with correct arguments, and composing results into a final answer. It stresses reliable action sequencing and graceful recovery from tool errors or ambiguous intermediate outputs.","L1: Language Comprehension (minor)
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory
L3: Inhibitory Control (minor)",L2
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath is a high-difficulty mathematics benchmark targeting expert-level problem solving, with tiered problems that increasingly require novel derivations and careful verification. It is designed to be challenging even for strong models and to better reflect frontier mathematical reasoning performance.","L1: 
L2: Logical Reasoning, Working Memory, Planning (minor), Adaptive Error Correction (minor)
L3: Cognitive Flexibility (minor)",L2
