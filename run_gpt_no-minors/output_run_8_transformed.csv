Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates whether a model can generate correct patches for real GitHub issues by running repository tests after applying the proposed fix. The “Verified” subset contains tasks that have been manually validated to be solvable and to have reliable evaluation signals, emphasizing end-to-end debugging and code change correctness.","L1: Language Comprehension (minor)
L2: Planning, Adaptive Error Correction, Logical Reasoning, Working Memory, Decision-making (minor)
L3: ",L2
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro is a more difficult and broader software engineering benchmark than SWE-bench Verified, designed to better reflect professional engineering work (including harder repos, more complex bug contexts, and broader languages/settings depending on the version). It evaluates patch correctness via automated testing and aims to be more contamination-resistant and industrially relevant.","L1: Language Comprehension (minor)
L2: Planning, Adaptive Error Correction, Logical Reasoning, Working Memory, Decision-making (minor)
L3: ",L2
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp evaluates “deep research” style web-browsing agents: models must locate and synthesize information from many documents to answer challenging questions. It stresses search strategy, source cross-checking, and long-horizon evidence integration rather than memorized facts.","L1: Language Comprehension, Language Production (minor)
L2: Planning, Decision-making, Attention, Working Memory, Episodic Memory (minor), Semantic Understanding & Context Recognition
L3: ",L2
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench measures an agent’s ability to complete multi-turn customer-support tasks by interacting with tools/APIs and a simulated user while adhering to domain policies (e.g., retail, airline, telecom). Success depends on maintaining state across turns, choosing correct tool actions, and following constraints under realistic conversational pressure.","L1: Language Comprehension, Language Production
L2: Decision-making, Planning, Working Memory
L3: Social Reasoning & Theory of Mind, Inhibitory Control, Empathy (minor)",L3
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,ARC-AGI evaluates fluid reasoning over novel grid-transformation puzzles given only a handful of demonstrations (few-shot pattern induction). It emphasizes compositional generalization and rule inference on problems designed to resist training-set memorization.,"L1: Visual Perception
L2: Logical Reasoning, Working Memory, Spatial Representation & Mapping, Attention (minor), Planning (minor)
L3: Cognitive Flexibility",L3
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas evaluates tool use in real or production-like Model Context Protocol (MCP) environments, where models must discover tools, invoke them with correct arguments, and compose multi-step workflows. It stresses robustness to tool errors, correct sequencing of calls, and faithful synthesis of tool outputs into final answers.","L1: Language Comprehension (minor), Language Production (minor)
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition
L3: ",L2
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,Humanity’s Last Exam (HLE) is a difficult multimodal benchmark intended to probe frontier knowledge and reasoning across a broad range of expert domains. Questions often require integrating text with diagrams/images and (in tool-enabled settings) searching or computing to produce grounded answers.,"L1: Language Comprehension, Visual Perception (minor)
L2: Logical Reasoning, Working Memory, Semantic Understanding & Context Recognition, Scene Understanding & Visual Reasoning (minor), Multisensory Integration (minor)
L3: ",L2
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 consists of competition-style mathematics problems that require multi-step derivations and precise final answers, typically without external references. It is commonly used to assess mathematical reasoning, symbolic manipulation, and error-free execution of long solution chains.","L1: Language Comprehension (minor)
L2: Logical Reasoning, Working Memory, Adaptive Error Correction (minor), Planning (minor)
L3: ",L2
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a high-quality subset of GPQA comprising very challenging graduate-level multiple-choice questions in biology, chemistry, and physics that are designed to be “Google-proof.” It probes deep scientific reasoning and understanding rather than shallow recall or easy pattern matching.","L1: Language Comprehension
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory (minor), Decision-making (minor)
L3: ",L2
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends MMLU to multiple languages, testing knowledge and reasoning across many academic subjects with standardized multiple-choice questions. It is used to assess multilingual understanding, cross-lingual generalization, and broad factual/academic competence.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning (minor), Working Memory (minor)
L3: ",L2
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro is an advanced multimodal benchmark that evaluates expert-level understanding and reasoning over images plus text (e.g., charts, diagrams, documents) across many disciplines. Compared to earlier multimodal benchmarks, it emphasizes harder problems, stronger evaluation protocols, and professional-grade visual reasoning.","L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning (minor), Visual Attention & Eye Movements (minor), Working Memory (minor)
L3: ",L2
ScreenShot-Pro,https://gui-agent.github.io/grounding-leaderboard/,https://arxiv.org/abs/2504.07981,"ScreenShot-Pro evaluates models on understanding high-resolution screenshots of real software interfaces and answering questions that require precise visual grounding (e.g., locating UI elements, reading values, interpreting layout). It is designed to measure GUI understanding and screenshot-based reasoning that supports computer-use agents.","L1: Visual Perception, Language Comprehension (minor)
L2: Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Spatial Representation & Mapping, Decision-making (minor)
L3: ",L2
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning tests reasoning over scientific figures (and associated context) from arXiv-style papers, requiring models to interpret plots, diagrams, and visual encodings to answer questions. It targets scientific visual reasoning, quantitative interpretation, and faithful extraction of evidence from figures.","L1: Visual Perception, Language Comprehension (minor)
L2: Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Working Memory (minor)
L3: ",L2
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU evaluates multimodal understanding and reasoning over videos paired with text questions, covering temporal events, actions, and causal relations. It probes whether models can integrate information across frames and maintain coherent interpretations over time.","L1: Visual Perception, Language Comprehension (minor)
L2: Scene Understanding & Visual Reasoning, Multisensory Integration, Working Memory, Attention
L3: Cognitive Timing & Predictive Modeling (minor)",L2
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 is a long-context evaluation for multi-round co-reference/recall: multiple similar “needle” requests are embedded within long “haystacks,” and the model must retrieve the correct response corresponding to a specified needle. The 8-needle variant increases interference and tests whether the model can preserve and recover precise details across very long inputs.","L1: Language Comprehension
L2: Working Memory, Attention, Episodic Memory, Semantic Understanding & Context Recognition (minor)
L3: ",L2
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval evaluates well-specified professional knowledge-work tasks spanning many occupations (e.g., spreadsheets, presentations, operational plans), judged by expert humans via pairwise comparisons. It emphasizes producing usable work artifacts under realistic constraints, including following instructions, structuring deliverables, and making trade-offs aligned with task goals.","L1: Language Production, Language Comprehension
L2: Planning, Decision-making, Semantic Understanding & Context Recognition, Adaptive Error Correction (minor)
L3: Self-reflection (minor)",L2
SWE-Lancer,https://openai.com/index/swe-lancer/,https://arxiv.org/abs/2502.12115,"SWE-Lancer evaluates agentic software engineering beyond single isolated patches, emphasizing more realistic workflows such as navigating larger codebases, handling ambiguous requirements, and producing changes that meet evaluation criteria end-to-end. It is intended to better reflect the reliability and autonomy demands of real engineering work compared to narrower coding benchmarks.","L1: Language Comprehension (minor)
L2: Planning, Adaptive Error Correction, Working Memory, Logical Reasoning, Decision-making (minor)
L3: ",L2
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks tests structured reasoning over graphs described in text, requiring models to traverse nodes/edges (e.g., BFS-like procedures, parent-pointer recovery) and output correct derived information. It targets algorithmic reasoning and the ability to maintain and manipulate structured relational state over long contexts.","L1: 
L2: Logical Reasoning, Working Memory, Spatial Representation & Mapping, Attention (minor), Planning (minor)
L3: ",L2
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon evaluates general tool-use competence across diverse APIs and tasks, emphasizing correct selection of tools, argument construction, iterative debugging, and synthesis of tool outputs into a coherent final answer. It is designed to measure robustness of agentic workflows under tool errors, partial results, and multi-step dependencies.","L1: Language Comprehension (minor), Language Production (minor)
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition (minor)
L3: ",L2
HMMT,https://matharena.ai/?view=problem&comp=hmmt--hmmt_feb_2025,https://arxiv.org/abs/2505.23281,"HMMT consists of high-difficulty contest mathematics problems (e.g., Harvard-MIT Math Tournament sets) that require multi-step proofs/derivations and careful case analysis. It is used to assess advanced mathematical reasoning under time-competition style constraints and sensitivity to small logical errors.","L1: Language Comprehension (minor)
L2: Logical Reasoning, Working Memory, Planning (minor), Adaptive Error Correction (minor)
L3: ",L2
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath is an expert-level mathematics benchmark designed to be difficult for frontier models and to reduce the usefulness of memorization, often requiring deep multi-step reasoning and (in some settings) tool-assisted computation. It aims to measure progress on the hardest tiers of formal and informal mathematical problem solving.","L1: 
L2: Logical Reasoning, Working Memory, Planning, Adaptive Error Correction (minor), Semantic Understanding & Context Recognition (minor)
L3: ",L2
