Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates software-engineering agents on real GitHub issues where the model must generate a code patch that makes tests pass. The Verified split uses human validation to ensure tasks are solvable and correctly specified, emphasizing end-to-end debugging and implementation rather than isolated code generation.","L1: 
L2: Planning, Adaptive Error Correction, Logical Reasoning, Working Memory, Semantic Understanding & Context Recognition, Decision-making (minor)
L3: ",L2
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro is a more difficult software-engineering benchmark designed to be more diverse and contamination-resistant, with tasks spanning multiple programming languages and more complex repos. It stresses robust agentic workflows—understanding requirements, locating relevant files, implementing fixes, and validating via tests.","L1: 
L2: Planning, Adaptive Error Correction, Logical Reasoning, Working Memory, Semantic Understanding & Context Recognition
L3: Cognitive Flexibility",L3
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp evaluates “deep research” browsing agents that must search, read, and synthesize information from a web corpus to answer complex questions. It emphasizes retrieval strategy, cross-document evidence integration, and maintaining correctness under noisy or incomplete sources.","L1: Language Comprehension
L2: Planning, Attention, Episodic Memory, Semantic Understanding & Context Recognition, Logical Reasoning
L3: ",L2
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench tests interactive customer-support agents in simulated domains (e.g., retail, airline, telecom) that require multi-turn dialogue, tool/API usage, and policy compliance. Success requires coordinating procedural actions with natural-language explanations while avoiding policy violations and handling user constraints.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Working Memory
L3: Social Reasoning & Theory of Mind, Inhibitory Control, Empathy (minor)",L3
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI measures fluid, few-shot abstraction by asking models to infer the underlying rule mapping input grids to output grids from a handful of examples. The tasks are designed to require novel pattern induction and compositional reasoning rather than memorization of domain knowledge.","L1: Visual Perception (minor)
L2: Logical Reasoning, Working Memory, Spatial Representation & Mapping, Scene Understanding & Visual Reasoning
L3: Cognitive Flexibility",L3
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas evaluates real-world tool use through the Model Context Protocol (MCP), requiring models to discover tools, call them correctly, recover from errors, and synthesize results across multi-step workflows. It targets practical API orchestration skills under realistic constraints (latency, retries, partial failures).","L1: Language Comprehension (minor), Language Production (minor)
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition
L3: ",L2
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam (HLE) is a large, frontier-knowledge benchmark with challenging questions across many domains and modalities, intended to probe expert-level reasoning and generalization. Tasks often require integrating specialized background knowledge with careful multi-step inference and, in tool-enabled settings, disciplined evidence use.","L1: Language Comprehension
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Scene Understanding & Visual Reasoning (minor), Multisensory Integration (minor), Planning (minor)
L3: ",L2
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 consists of competition-style math problems that require structured derivations, algebraic manipulation, and clever transformations under time-like constraints. The benchmark is commonly used to assess mathematical reasoning and reliability without relying on external tools.","L1: Language Comprehension (minor)
L2: Logical Reasoning, Planning, Working Memory, Attention (minor)
L3: ",L2
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a hard multiple-choice science benchmark designed to be “Google-proof,” emphasizing questions that non-experts typically miss while experts can answer. It stresses precise scientific reasoning, discrimination between close options, and avoidance of plausible-sounding distractors.","L1: Language Comprehension (minor)
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Decision-making
L3: Inhibitory Control (minor)",L2
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends MMLU to multiple languages, testing broad academic knowledge and reasoning across many subjects in non-English settings. It probes whether a model’s conceptual understanding and test-taking robustness transfer across linguistic contexts.","L1: Language Comprehension, Language Production (minor)
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory (minor)
L3: ",L2
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro is a multimodal benchmark of expert-level questions spanning many disciplines that require joint reasoning over images (e.g., diagrams, plots, tables) and text. It emphasizes compositional visual reasoning, grounding language in visual evidence, and handling complex answer choices.","L1: Visual Perception, Language Comprehension
L2: Multisensory Integration, Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements (minor), Logical Reasoning, Working Memory (minor)
L3: ",L2
ScreenShot-Pro,https://gui-agent.github.io/grounding-leaderboard/,https://arxiv.org/abs/2504.07981,"ScreenShot-Pro evaluates screenshot understanding for GUI-centric tasks, such as identifying relevant UI elements and inferring the correct action or answer from the interface state. It targets fine-grained visual grounding, layout/affordance understanding, and robustness to real-world UI variability.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Spatial Representation & Mapping, Visual Attention & Eye Movements, Decision-making (minor), Sensorimotor Coordination (minor)
L3: ",L2
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning tests whether models can answer reasoning-heavy questions about figures and visual elements drawn from scientific documents (e.g., charts and plots). It emphasizes extracting quantitative/structural relationships from visuals and mapping them into correct textual conclusions.","L1: Visual Perception, Language Comprehension (minor)
L2: Scene Understanding & Visual Reasoning, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory (minor)
L3: ",L2
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU evaluates multimodal understanding and reasoning over video, requiring models to integrate information across frames and time to answer questions. It probes temporal coherence, event understanding, and selective attention to relevant moments in long clips.","L1: Visual Perception, Language Comprehension (minor)
L2: Attention, Episodic Memory, Scene Understanding & Visual Reasoning, Multisensory Integration (minor)
L3: Cognitive Timing & Predictive Modeling",L3
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) is a long-context, multi-round co-reference resolution evaluation where multiple similar “needle” interactions are embedded in a long “haystack,” and the model must retrieve the correct referenced response. It stresses accurate indexing of repeated patterns and robustness to interference across long contexts.","L1: Language Comprehension
L2: Working Memory, Episodic Memory, Attention, Semantic Understanding & Context Recognition
L3: ",L2
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval evaluates well-specified professional knowledge-work tasks across many occupations, often requiring creation of concrete work artifacts (e.g., slides, spreadsheets, plans) judged by expert humans. It measures end-to-end task execution quality, including adherence to constraints, organization, and practical decision quality.","L1: Language Production
L2: Planning, Decision-making, Semantic Understanding & Context Recognition, Adaptive Error Correction (minor), Working Memory (minor)
L3: Self-reflection (minor)",L2
SWE-Lancer,https://openai.com/index/swe-lancer/,https://arxiv.org/abs/2502.12115,"SWE-Lancer evaluates software-engineering agents on realistic tasks that more closely resemble contracted engineering work, emphasizing reliability, scope management, and producing correct patches within constraints. It targets longer-horizon engineering competence, including navigating ambiguous requirements and integrating changes safely.","L1: 
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition
L3: Cognitive Flexibility",L3
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks tests systematic multi-step traversal in graph-structured problems described in text, requiring models to follow edges and maintain state across long sequences. It is designed to stress compositional, stepwise reasoning under distractors and long-range dependencies.","L1: Language Comprehension (minor)
L2: Spatial Representation & Mapping, Working Memory, Attention, Logical Reasoning
L3: ",L2
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon evaluates general tool-use competence across diverse tasks that require selecting among tools, executing multi-step tool calls, and producing validated final outputs. It emphasizes robust orchestration behaviors such as error handling, iterative refinement, and maintaining task state over extended workflows.","L1: 
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition (minor)
L3: Inhibitory Control (minor)",L2
HMMT,https://matharena.ai/?view=problem&comp=hmmt--hmmt_feb_2025,https://arxiv.org/abs/2505.23281,"HMMT (Harvard-MIT Mathematics Tournament) problems are advanced contest-style questions that demand creative proof/derivation strategies and careful symbolic reasoning. As an evaluation set, it probes high-end mathematical problem solving beyond routine textbook exercises.","L1: Language Comprehension (minor)
L2: Logical Reasoning, Planning, Working Memory, Attention (minor)
L3: ",L2
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath is an expert-level mathematics benchmark intended to measure progress at the frontier of mathematical reasoning, including problems that resemble research-grade or olympiad-plus difficulty. It stresses deep multi-step derivations, strategy selection, and correctness under high ambiguity and long solution chains.","L1: 
L2: Logical Reasoning, Planning, Working Memory, Attention (minor), Adaptive Error Correction (minor)
L3: Cognitive Flexibility",L3
