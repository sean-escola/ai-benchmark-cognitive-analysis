Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 evaluates agentic coding and sysadmin-style problem solving in real terminal environments (e.g., running commands, editing files, debugging, and recovering from failures). Success depends on correctly sequencing actions under environment feedback and constraints rather than producing code snippets in isolation.","L1: Language Comprehension (minor)
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory
L3: ",L2
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp measures deep-research performance: answering difficult questions by searching a corpus/web index, selecting evidence, and synthesizing a final response. It stresses reliable information gathering and cross-checking across multiple sources with limited attempts.","L1: Language Production (minor)
L2: Planning, Decision-making, Attention, Working Memory, Semantic Understanding & Context Recognition
L3: ",L2
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld evaluates multimodal “computer use” agents that must complete tasks inside operating-system GUIs (apps, settings, file managers, browsers) by perceiving screens and taking actions. It emphasizes robust interaction in long, error-prone trajectories where the environment changes in response to actions.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Sensorimotor Coordination, Planning, Attention, Adaptive Error Correction (minor)
L3: ",L2
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI tests “fluid intelligence” via few-shot abstract pattern discovery on grid transformation puzzles: given a few input-output examples, infer the rule and produce the correct output for a new input. The tasks are designed to require compositional abstraction and generalization beyond memorized patterns.","L1: 
L2: Logical Reasoning, Working Memory, Spatial Representation & Mapping, Attention (minor)
L3: Cognitive Flexibility",L3
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon agent coherence and business decision-making in a year-long simulated vending-machine operation. Agents must manage inventory, pricing, supplier negotiation, and cash flow across many steps where early choices affect later outcomes.","L1: 
L2: Planning, Decision-making, Reward Mechanisms, Working Memory
L3: Self-reflection (minor)",L2
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym measures cybersecurity agent competence on real-world vulnerability tasks, including reproducing known vulnerabilities from descriptions and discovering issues in open-source codebases. It requires iterative hypothesis testing, code navigation, and careful validation against ground truth outcomes.","L1: 
L2: Logical Reasoning, Planning, Adaptive Error Correction, Semantic Understanding & Context Recognition, Working Memory
L3: Inhibitory Control (minor)",L2
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"SpreadsheetBench evaluates the ability to understand, navigate, and edit spreadsheets to solve realistic analytic tasks (e.g., formulas, formatting, aggregation, and structured data manipulation). It stresses precise multi-step operations where small mistakes propagate into incorrect final artifacts.","L1: 
L2: Working Memory, Attention, Planning, Logical Reasoning, Semantic Understanding & Context Recognition
L3: ",L2
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a large multimodal benchmark intended to probe frontier-level academic knowledge and reasoning across diverse domains. Questions often require integrating specialized information, handling long contexts, and (in tool-enabled settings) using external resources responsibly.","L1: Language Comprehension, Visual Perception (minor)
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Scene Understanding & Visual Reasoning (minor)
L3: ",L2
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a high-difficulty subset of GPQA consisting of graduate-level, “Google-proof” multiple-choice science questions curated to be challenging for non-experts. It targets deep conceptual understanding and careful reasoning rather than surface retrieval.","L1: Language Comprehension
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory (minor)
L3: ",L2
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro is an expert-level multimodal benchmark spanning many disciplines where models answer questions grounded in images (diagrams, charts, scientific figures) plus text. It emphasizes cross-modal integration, precise visual understanding, and domain reasoning under multiple-choice formats.","L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Multisensory Integration, Attention (minor), Logical Reasoning (minor)
L3: ",L2
MathArena Apex,https://matharena.ai/?view=problem&comp=apex--apex_2025,https://arxiv.org/abs/2505.23281,"MathArena Apex evaluates advanced mathematical problem solving via a competition-style arena of difficult problems, often emphasizing multi-step derivations and correctness under strict grading. It is designed to differentiate strong mathematical reasoning from pattern-based answering.","L1: 
L2: Logical Reasoning, Working Memory, Planning (minor), Adaptive Error Correction (minor)
L3: ",L2
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning evaluates reasoning over content derived from scientific papers, emphasizing interpreting technical visuals and associated text (e.g., figures, plots, captions) to answer questions. It stresses faithful grounding in evidence and multi-step scientific inference rather than generic image captioning.","L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Logical Reasoning, Attention (minor)
L3: ",L2
OmniDocBench 1.5,https://github.com/opendatalab/OmniDocBench,https://arxiv.org/abs/2412.07626,"OmniDocBench 1.5 evaluates document understanding and OCR across heterogeneous layouts, including text, tables, formulas, and reading order. Metrics emphasize accurate extraction and structure preservation, reflecting real-world document digitization requirements.","L1: Visual Perception
L2: Visual Attention & Eye Movements, Semantic Understanding & Context Recognition, Working Memory (minor), Scene Understanding & Visual Reasoning (minor)
L3: ",L2
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU tests video understanding and reasoning across many domains, requiring models to integrate temporal visual information with language questions. Tasks commonly require tracking events over time, inferring causality, and combining frame-level cues into coherent answers.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Working Memory, Attention (minor)
L3: Cognitive Timing & Predictive Modeling",L3
LiveCodeBench Pro,https://livecodebenchpro.com/projects/livecodebench-pro/leaderboard,https://arxiv.org/abs/2506.11928,"LiveCodeBench Pro evaluates coding ability on fresh, competitive-programming-style tasks with strong anti-contamination intent and standardized grading. It stresses writing correct programs under time/complexity constraints and iteratively fixing bugs based on feedback.","L1: 
L2: Logical Reasoning, Planning, Working Memory, Adaptive Error Correction
L3: Inhibitory Control (minor)",L2
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,The FACTS Benchmark Suite systematically evaluates factuality: whether model outputs remain supported by available evidence and avoid confabulation across diverse factuality-related tasks. It is designed to measure both accuracy and robustness of truthfulness under realistic prompts.,"L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Working Memory (minor)
L3: Inhibitory Control, Self-reflection (minor)",L3
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,Global PIQA evaluates physical commonsense reasoning across many languages using non-parallel (non-translated) datasets to reduce translation artifacts and measure true multilingual generalization. Items focus on understanding everyday physical interactions and plausible actions in the world.,"L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning
L3: Cognitive Flexibility (minor)",L2
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 is a long-context evaluation where multiple similar “needle” items are embedded in a long “haystack,” and the model must retrieve the correct referenced answer for a specified needle. The 8-needle setting increases distractors and tests stable retrieval under high interference.","L1: Language Comprehension (minor)
L2: Working Memory, Attention, Episodic Memory, Semantic Understanding & Context Recognition (minor)
L3: ",L2
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval evaluates economically valuable, well-specified knowledge work across many occupations, often requiring producing realistic professional artifacts (documents, plans, analyses) judged against human professional baselines. It emphasizes end-to-end task execution quality, not just question answering.","L1: Language Production
L2: Planning, Decision-making, Semantic Understanding & Context Recognition, Working Memory (minor)
L3: Social Reasoning & Theory of Mind (minor)",L2
SWE-Lancer,https://openai.com/index/swe-lancer/,https://arxiv.org/abs/2502.12115,"SWE-Lancer evaluates agentic software engineering on realistic tasks (often scoped like freelance/contract work), where models must understand a repository and produce patches that satisfy requirements. It stresses robustness, debugging, and correct integration with existing code and tests.","L1: 
L2: Planning, Logical Reasoning, Adaptive Error Correction, Working Memory
L3: Inhibitory Control (minor)",L2
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks evaluates reasoning over graph-structured data represented in long text contexts, requiring multi-hop traversal and answering queries about paths, parents, or reachability. It is designed to probe systematic reasoning under long-context load and distractor interference.","L1: 
L2: Spatial Representation & Mapping, Logical Reasoning, Working Memory, Attention
L3: Cognitive Flexibility (minor)",L2
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon evaluates tool-using agents on diverse, multi-step tasks requiring selecting appropriate tools, calling them correctly, and composing results into a final answer. It stresses orchestration reliability (correct sequencing, argument formatting, and error recovery) rather than tool-free reasoning alone.","L1: Language Comprehension (minor)
L2: Planning, Decision-making, Working Memory, Adaptive Error Correction
L3: ",L2
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath is a benchmark of expert-level mathematics intended to be difficult even for strong models, with problems curated to reduce memorization and require genuine insight. It targets deep multi-step mathematical reasoning and careful verification of derived results.","L1: 
L2: Logical Reasoning, Working Memory, Planning (minor), Adaptive Error Correction (minor)
L3: ",L2
