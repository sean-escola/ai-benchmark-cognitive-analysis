Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,SWE-bench Verified evaluates software engineering agents on real GitHub issues where the task is to produce a patch that makes the repository’s tests pass. The “Verified” subset uses problems curated/validated to be solvable and to reduce evaluation noise via stronger checking and human verification of tasks and outcomes.,"L1: Language Comprehension (minor)
L2: Planning, Logical Reasoning, Working Memory, Adaptive Error Correction, Decision-making (minor)
L3: ",L2
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro is a harder, more contamination-resistant software engineering benchmark spanning multiple programming languages and more complex, realistic repository states. Models must understand an issue, localize relevant code, implement a fix, and satisfy automated tests under stricter evaluation protocols.","L1: Language Comprehension (minor)
L2: Planning, Logical Reasoning, Working Memory, Adaptive Error Correction, Decision-making
L3: ",L2
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp evaluates “browse-the-web” research agents on information-seeking tasks where answers must be found by navigating and extracting evidence from online sources (often across multiple pages). It emphasizes tool-mediated search, source triangulation, and maintaining coherence over multi-step browsing sessions.","L1: Language Comprehension (minor)
L2: Planning, Decision-making, Attention, Working Memory, Semantic Understanding & Context Recognition
L3: ",L2
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench measures agent performance in simulated customer-support environments (e.g., retail/airline/telecom) requiring multi-turn dialogue plus correct API/tool usage under domain policies. Success depends on tracking user state, following constraints, and executing reliable action sequences to resolve cases end-to-end.","L1: Language Production (minor)
L2: Planning, Decision-making, Working Memory
L3: Social Reasoning & Theory of Mind, Inhibitory Control (minor)",L3
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI is a fluid-intelligence benchmark of abstract pattern induction from a few input–output grid examples, requiring models to infer the latent rule and generalize to a new grid. It is designed to minimize reliance on memorized knowledge and instead probe flexible reasoning over novel tasks.","L1: Visual Perception (minor)
L2: Logical Reasoning, Spatial Representation & Mapping, Working Memory, Planning (minor)
L3: Cognitive Flexibility",L3
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas evaluates real-world tool use via the Model Context Protocol, where models must discover appropriate tools, call them with correct schemas/parameters, and integrate returned data into a final response. Tasks often require multi-step workflows, error handling, and coordination across multiple tools or servers.","L1: Language Comprehension (minor)
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition (minor)
L3: ",L2
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a broad, frontier-focused benchmark spanning difficult questions across many domains, often requiring multi-step reasoning and, in some settings, multimodal understanding. It targets robust problem solving and synthesis rather than narrow task specialization.","L1: Language Comprehension, Visual Perception (minor)
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Scene Understanding & Visual Reasoning (minor)
L3: ",L2
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 benchmarks competition-level mathematical problem solving drawn from the American Invitational Mathematics Examination. Problems require symbolic manipulation, multi-step derivations, and careful constraint handling under time-tested contest formats.","L1: 
L2: Logical Reasoning, Working Memory, Planning, Adaptive Error Correction (minor), Attention (minor)
L3: ",L2
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a high-difficulty, “Google-proof” multiple-choice science benchmark composed of expert-authored questions where non-experts tend to fail. It emphasizes deep conceptual understanding and reasoning in physics, chemistry, and biology, rather than surface retrieval.","L1: Language Comprehension (minor)
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory (minor)
L3: ",L2
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends MMLU to multiple languages, testing broad academic knowledge and reasoning across many subjects and non-English settings. It stresses multilingual comprehension and consistent reasoning under varied linguistic and cultural phrasing.","L1: Language Comprehension, Language Production (minor)
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory (minor)
L3: ",L2
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro is a more challenging variant of MMMU for expert-level multimodal understanding, combining images (e.g., charts, diagrams, figures) with text questions across diverse disciplines. It probes the ability to ground reasoning in visual evidence and integrate it with domain knowledge.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Visual Attention & Eye Movements (minor)
L3: ",L2
ScreenShot-Pro,https://gui-agent.github.io/grounding-leaderboard/,https://arxiv.org/abs/2504.07981,"ScreenShot-Pro evaluates GUI understanding and grounded interaction from high-resolution screenshots, often requiring identifying UI elements, reading small text, and executing interface-relevant reasoning. It is commonly used to assess computer-use agents’ screenshot grounding and interface decision reliability.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Spatial Representation & Mapping (minor), Decision-making (minor)
L3: ",L2
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning tests understanding of scientific figures (and associated context) from arXiv papers, requiring models to answer questions that depend on accurate visual interpretation and scientific reasoning. It targets faithful extraction of quantitative/structural cues from figures rather than generic caption paraphrase.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Logical Reasoning, Multisensory Integration (minor), Working Memory (minor)
L3: ",L2
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU extends multimodal understanding to video, evaluating whether models can integrate information across time (multiple frames) to answer questions about events, procedures, and visual evidence. It stresses temporal integration and robustness to visual detail changes across the clip.","L1: Visual Perception
L2: Working Memory, Scene Understanding & Visual Reasoning, Attention (minor)
L3: Cognitive Timing & Predictive Modeling",L3
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) is a long-context evaluation where multiple similar “needle” mentions are embedded within lengthy documents, and the model must retrieve or reproduce the information corresponding to a specified needle. The 8-needle setting stresses interference resistance and precise cross-document/long-span reference tracking.","L1: Language Comprehension (minor)
L2: Working Memory, Attention, Episodic Memory
L3: Inhibitory Control (minor)",L2
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval measures performance on well-specified, economically relevant knowledge-work tasks across many occupations, with outputs judged against industry professionals (e.g., spreadsheets, presentations, plans, analyses). It emphasizes producing usable artifacts under constraints, prioritizing quality, correctness, and task completion over open-ended chat.","L1: Language Production
L2: Planning, Decision-making, Semantic Understanding & Context Recognition, Adaptive Error Correction (minor)
L3: Self-reflection (minor)",L2
SWE-Lancer,https://openai.com/index/swe-lancer/,https://arxiv.org/abs/2502.12115,"SWE-Lancer evaluates real-world, contractor-like software engineering tasks that often involve larger scope than single bug fixes, such as implementing features, refactors, or multi-file changes under practical constraints. It is intended to better reflect professional engineering workflows and end-to-end delivery quality.","L1: 
L2: Planning, Decision-making, Logical Reasoning, Working Memory, Adaptive Error Correction
L3: ",L2
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks tests long-context, algorithmic traversal over graph-structured data described in text, requiring models to follow specified walk rules and report derived properties (e.g., reachability or parent relationships). It targets systematic state tracking and resistance to distractors across lengthy sequences.","L1: 
L2: Logical Reasoning, Working Memory, Attention, Spatial Representation & Mapping, Planning (minor)
L3: ",L2
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon is a tool-use benchmark where models must solve tasks by selecting, sequencing, and correctly parameterizing calls to multiple tools, then synthesizing outputs into a coherent solution. It emphasizes reliability of tool orchestration, handling tool errors, and maintaining task state across multi-step workflows.","L1: 
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory, Attention (minor)
L3: ",L2
HMMT,https://matharena.ai/?view=problem&comp=hmmt--hmmt_feb_2025,https://arxiv.org/abs/2505.23281,"HMMT (Harvard–MIT Mathematics Tournament) problems are advanced contest-math questions that typically require creative insight, multi-step reasoning, and careful proof-like argumentation or construction. As an evaluation set, it probes harder and more diverse competition mathematics than many standard test-style math benchmarks.","L1: 
L2: Logical Reasoning, Planning, Working Memory, Adaptive Error Correction (minor)
L3: Cognitive Flexibility (minor)",L2
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath is a benchmark of expert-level mathematics designed to be difficult for current models, often requiring deep, multi-stage reasoning and (in some settings) tool-assisted computation. It aims to separate shallow pattern matching from robust mathematical problem solving at and beyond typical competition levels.","L1: 
L2: Logical Reasoning, Planning, Working Memory, Adaptive Error Correction (minor)
L3: Cognitive Flexibility (minor)",L2
