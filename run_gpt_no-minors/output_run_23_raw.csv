Benchmark,Website,Paper,Description,Cognitive Functions
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates software engineering agents by asking them to generate patches that fix real issues in open-source Python repositories, with tasks filtered/validated to be solvable and reliably testable. Success is measured by running the project’s tests (or task-specific checks) to verify the proposed code change resolves the issue without breaking the codebase.","Logical Reasoning, Planning, Working Memory, Adaptive Error Correction, Semantic Understanding & Context Recognition, Language Comprehension (minor)"
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro is a harder software engineering benchmark designed to better reflect industrially relevant development work, extending beyond the original setting with broader task diversity and stronger resistance to trivial memorization. Models must produce correct code changes under realistic repository constraints, typically requiring deeper debugging, refactoring, and multi-file modifications.","Logical Reasoning, Planning, Working Memory, Adaptive Error Correction, Cognitive Flexibility, Semantic Understanding & Context Recognition"
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp evaluates “deep research” style agents that must browse and synthesize information to answer difficult questions, typically requiring multi-step retrieval and cross-checking across sources. It targets end-to-end search behaviors—query formulation, evidence gathering, and final justification—rather than isolated QA from a fixed context.","Planning, Decision-making, Attention, Working Memory, Episodic Memory (minor), Semantic Understanding & Context Recognition, Language Comprehension, Language Production (minor)"
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench measures how well an agent can complete multi-turn customer-support tasks by interacting with a simulated user and calling domain APIs while obeying policies. It emphasizes robust tool use across long dialogues (e.g., retail orders/returns, airline changes, telecom troubleshooting) and consistent adherence to constraints despite ambiguity and user pressure.","Social Reasoning & Theory of Mind, Language Comprehension, Language Production, Decision-making, Planning, Inhibitory Control, Working Memory (minor), Reward Mechanisms (minor)"
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI tests fluid intelligence via novel grid-based pattern transformation tasks: given a few input–output examples, the model must infer the hidden rule and generate the correct output for a new input. The tasks are designed to be out-of-distribution for rote memorization and reward abstraction, compositionality, and systematic generalization.","Cognitive Flexibility, Logical Reasoning, Working Memory, Spatial Representation & Mapping, Visual Perception, Scene Understanding & Visual Reasoning, Attention (minor)"
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas evaluates real-world tool use through the Model Context Protocol (MCP), where models must discover the right tools and execute multi-step workflows across production-like tool servers. Tasks require correct API invocation, error handling, and synthesis of tool outputs into a coherent final response.","Planning, Decision-making, Working Memory, Adaptive Error Correction, Semantic Understanding & Context Recognition, Language Comprehension (minor)"
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a broad, frontier-difficulty benchmark spanning many domains and (often) multiple modalities, intended to stress-test advanced reasoning and expert knowledge. Questions are designed to be challenging for current models, rewarding careful multi-step inference and precise, evidence-based answering (with or without tools depending on the evaluation setup).","Logical Reasoning, Semantic Understanding & Context Recognition, Language Comprehension, Working Memory (minor), Planning (minor), Multisensory Integration (minor), Scene Understanding & Visual Reasoning (minor)"
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 evaluates mathematical problem solving using questions from the American Invitational Mathematics Examination, typically requiring multi-step symbolic reasoning and careful handling of constraints. Performance is usually reported as accuracy over the contest problems, sometimes with optional tool use (e.g., Python) depending on the evaluation protocol.","Logical Reasoning, Working Memory, Cognitive Flexibility (minor), Attention (minor), Semantic Understanding & Context Recognition (minor)"
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a high-quality subset of GPQA, consisting of graduate-level multiple-choice questions in science that are deliberately difficult to answer by simple web lookup or shallow pattern matching. It targets deep domain knowledge plus reasoning under distractors, often emphasizing careful reading and elimination.","Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory (minor), Attention (minor), Language Comprehension (minor)"
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends MMLU into multiple languages, testing broad academic knowledge and reasoning across many subjects in non-English settings. It probes whether models can transfer competencies across languages while maintaining factual accuracy and consistent reasoning behavior.","Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning (minor), Language Production (minor), Working Memory (minor)"
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro is a challenging multimodal benchmark that requires jointly reasoning over images and text across many disciplines (e.g., diagrams, charts, scientific figures, and technical visuals). It emphasizes expert-level multimodal understanding and structured reasoning beyond simple recognition.","Multisensory Integration, Visual Perception, Scene Understanding & Visual Reasoning, Logical Reasoning, Attention (minor), Language Comprehension (minor)"
ScreenShot-Pro,https://gui-agent.github.io/grounding-leaderboard/,https://arxiv.org/abs/2504.07981,"ScreenShot-Pro evaluates GUI understanding from screenshots, where the model must interpret complex user interfaces and answer questions grounded in on-screen elements. It stresses fine-grained visual grounding (buttons, menus, tables, settings) and spatial/layout reasoning typical of professional software workflows.","Visual Perception, Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Spatial Representation & Mapping, Decision-making (minor), Language Comprehension (minor)"
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning tests scientific-figure and paper-centric visual reasoning using plots, charts, and visual elements from research-style documents. The goal is to assess whether a model can extract quantitative/relational information from figures and integrate it with the accompanying textual context to answer reasoning-heavy questions.","Scene Understanding & Visual Reasoning, Visual Perception, Logical Reasoning, Multisensory Integration, Working Memory (minor), Attention (minor)"
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU extends multimodal understanding to the temporal domain by requiring reasoning over videos alongside text questions. Tasks typically require integrating information across frames, tracking changes over time, and combining visual cues with language to answer questions about events, procedures, or interactions.","Visual Perception, Cognitive Timing & Predictive Modeling, Working Memory, Scene Understanding & Visual Reasoning, Multisensory Integration (minor), Attention (minor)"
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) evaluates long-context retrieval and multi-round coreference resolution by embedding multiple similar “needle” interactions within a long “haystack” of dialogue. The model must correctly identify and reproduce the response associated with a specific needle, stressing robustness to interference and distractors at long context lengths.","Working Memory, Attention, Episodic Memory, Language Comprehension, Inhibitory Control (minor), Semantic Understanding & Context Recognition (minor)"
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval evaluates well-specified professional “knowledge work” by asking models to produce real workplace artifacts (e.g., spreadsheets, presentations, plans, analyses) across many occupations and industries, judged by expert comparison to human professionals. It targets end-to-end task execution quality—following requirements, producing usable deliverables, and showing practical judgment under constraints.","Planning, Decision-making, Language Production, Semantic Understanding & Context Recognition, Self-reflection (minor), Adaptive Error Correction (minor), Working Memory (minor)"
SWE-Lancer,https://openai.com/index/swe-lancer/,https://arxiv.org/abs/2502.12115,"SWE-Lancer evaluates advanced software engineering performance with an emphasis on realistic developer workflows, such as implementing fixes or changes within larger codebases under constraints. Compared with simpler coding tests, it is intended to better capture practical engineering competence (e.g., navigating projects, applying correct changes, and maintaining overall correctness).","Logical Reasoning, Planning, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition"
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks evaluates a model’s ability to follow and reason over graph-structured data by performing multi-step traversals (e.g., reachability, parent/neighbor queries) within a provided representation. It stresses systematic multi-hop reasoning and maintaining state over long sequences of relational steps, often in long-context settings.","Spatial Representation & Mapping, Working Memory, Logical Reasoning, Attention (minor), Planning (minor)"
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon measures tool-using proficiency across diverse tasks where a model must select appropriate tools, call them correctly, and iteratively refine results to solve multi-step problems. It emphasizes orchestration reliability (including error recovery), not just final-answer quality, reflecting real agent deployments that depend on external APIs and utilities.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Inhibitory Control (minor), Semantic Understanding & Context Recognition (minor)"
HMMT,https://matharena.ai/?view=problem&comp=hmmt--hmmt_feb_2025,https://arxiv.org/abs/2505.23281,"HMMT evaluates competition-style mathematics using problems from the Harvard-MIT Mathematics Tournament, which often require creative insights, careful case analysis, and multi-step derivations. It is typically harder and more proof/strategy flavored than standard short-form math QA, stressing robustness on novel problem structures.","Logical Reasoning, Working Memory, Cognitive Flexibility (minor), Attention (minor), Semantic Understanding & Context Recognition (minor)"
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath is an expert-level mathematics benchmark designed to test advanced reasoning on problems closer to research and high-end olympiad difficulty, sometimes with tiered difficulty levels. It targets deep multi-step derivations, precise symbolic manipulation, and sustained correctness under challenging, unfamiliar problem setups.","Logical Reasoning, Working Memory, Planning (minor), Cognitive Flexibility (minor), Attention (minor)"
