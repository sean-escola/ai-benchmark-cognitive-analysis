Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 evaluates agentic coding and system-operation ability in real command-line environments, where models must execute multi-step tasks using shell commands and files. It stresses robustness to tool errors, environment state changes, and iterative debugging under realistic constraints.","L1: Language Comprehension (minor)
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory
L3: ",L2
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp measures deep research performance over a fixed web/document corpus, requiring models to search, retrieve evidence, and synthesize a final answer. It emphasizes long-horizon information seeking, source selection, and maintaining correctness while navigating many distractors.","L1: 
L2: Planning, Attention, Semantic Understanding & Context Recognition, Working Memory, Decision-making (minor)
L3: ",L2
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld benchmarks multimodal computer-use agents on realistic desktop operating system tasks (e.g., navigating UIs, editing, downloading, configuring settings). Success requires interpreting screens, deciding actions, and executing step-by-step interactions in a dynamic environment.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Sensorimotor Coordination, Planning, Attention (minor)
L3: ",L2
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,ARC-AGI is a “few-example” abstract reasoning benchmark where models infer latent rules from small sets of input-output grid pairs and apply them to new inputs. It targets fluid reasoning and the ability to generalize to novel compositional patterns rather than recall training distributions.,"L1: 
L2: Logical Reasoning, Working Memory, Spatial Representation & Mapping, Attention (minor)
L3: Cognitive Flexibility",L3
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon autonomous business management in a simulated vending-machine company, typically spanning many decisions across an in-simulated year. Models must plan, adapt to market dynamics, manage inventory and finances, and often negotiate via communication channels.","L1: 
L2: Planning, Decision-making, Reward Mechanisms, Working Memory
L3: Social Reasoning & Theory of Mind (minor)",L2
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity agent skills on real-world vulnerability tasks, including identifying known vulnerabilities from descriptions and discovering new ones in codebases. It stresses systematic exploration, hypothesis testing, and careful verification under adversarial-like conditions.","L1: 
L2: Logical Reasoning, Planning, Adaptive Error Correction, Working Memory (minor)
L3: Inhibitory Control (minor)",L2
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"SpreadsheetBench measures an agent’s ability to operate on complex spreadsheets derived from realistic workflows, including editing formulas, cleaning data, and producing correct outputs. It emphasizes multi-step manipulation, error checking, and maintaining consistency across many dependent cells.","L1: 
L2: Working Memory, Logical Reasoning, Planning, Adaptive Error Correction, Attention (minor)
L3: ",L2
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a broad, frontier-level benchmark intended to probe advanced academic reasoning and multimodal understanding across many domains. It includes questions that reward careful synthesis, long reasoning chains, and (in tool-enabled settings) correct use of external resources without contamination.","L1: Language Comprehension, Visual Perception (minor)
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory
L3: ",L2
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a difficult multiple-choice science QA benchmark designed to be resistant to shallow pattern matching, targeting expert-level questions in physics, chemistry, and biology. It rewards precise reasoning over domain knowledge and careful elimination of plausible distractors.","L1: Language Comprehension (minor)
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory (minor)
L3: ",L2
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro evaluates expert-level multimodal understanding and reasoning over images and text across many disciplines (e.g., charts, diagrams, documents, scientific figures). It stresses integrating visual evidence with textual instructions and performing multi-step inference rather than simple recognition.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Visual Attention & Eye Movements (minor)
L3: ",L2
MathArena Apex,https://matharena.ai/?view=problem&comp=apex--apex_2025,https://arxiv.org/abs/2505.23281,"MathArena Apex is a competition-style mathematics benchmark emphasizing hard, multi-step problem solving and proof-like reasoning, often evaluated under strict correctness criteria. It probes persistence through complex derivations and sensitivity to small algebraic or logical errors.","L1: 
L2: Logical Reasoning, Working Memory, Adaptive Error Correction, Planning (minor)
L3: ",L2
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning tests scientific figure and paper-figure reasoning, requiring models to interpret plots/diagrams and answer questions that depend on correctly extracting and relating visual details. It goes beyond OCR by emphasizing structured inference from visual scientific evidence.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Logical Reasoning, Visual Attention & Eye Movements (minor), Semantic Understanding & Context Recognition (minor)
L3: ",L2
OmniDocBench 1.5,https://github.com/opendatalab/OmniDocBench,https://arxiv.org/abs/2412.07626,"OmniDocBench 1.5 evaluates document understanding and reconstruction across mixed content types such as text, tables, formulas, and reading order. It measures how well models perceive document layout and generate faithful structured outputs under complex formatting.","L1: Visual Perception, Language Production
L2: Scene Understanding & Visual Reasoning, Attention, Working Memory (minor)
L3: ",L2
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU benchmarks multimodal understanding and reasoning over videos paired with questions, requiring temporal integration of events, objects, and actions. It stresses selecting relevant frames, tracking changes over time, and synthesizing video evidence with textual prompts.","L1: Visual Perception
L2: Visual Attention & Eye Movements, Working Memory, Scene Understanding & Visual Reasoning
L3: Cognitive Timing & Predictive Modeling (minor)",L2
LiveCodeBench Pro,https://livecodebenchpro.com/projects/livecodebench-pro/leaderboard,https://arxiv.org/abs/2506.11928,"LiveCodeBench Pro evaluates coding ability on time-aware, competition-style programming tasks with an emphasis on generalization and resistance to contamination via evolving problem sets. It probes algorithmic reasoning, iterative debugging, and producing correct code under single-attempt constraints.","L1: Language Production (minor)
L2: Logical Reasoning, Planning, Adaptive Error Correction, Working Memory (minor)
L3: ",L2
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"FACTS Benchmark Suite evaluates factuality and grounding, measuring whether model outputs remain faithful to provided evidence and avoid hallucinations across diverse factuality tasks. It targets reliability mechanisms: verifying claims, resisting confabulation, and producing calibrated responses.","L1: Language Production (minor)
L2: Semantic Understanding & Context Recognition, Working Memory (minor)
L3: Inhibitory Control, Self-reflection (minor)",L3
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA evaluates physical commonsense and practical reasoning across many languages, focusing on choosing the more plausible solution to everyday physical-interaction scenarios. It probes whether models can generalize intuitive physics and affordances beyond English-centric cues.","L1: Language Comprehension (minor)
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Spatial Representation & Mapping (minor), Sensorimotor Coordination (minor)
L3: ",L2
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) tests long-context multi-round coreference and retrieval: multiple similar “needle” requests are embedded in long “haystack” dialogues, and the model must reproduce the correct referenced response. It primarily stresses robust retrieval over long contexts and interference resistance.","L1: Language Comprehension (minor)
L2: Working Memory, Attention, Episodic Memory
L3: ",L2
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval evaluates economically valuable, well-specified knowledge work across many occupations by judging the quality of produced work artifacts (e.g., plans, analyses, spreadsheets, presentations). It measures end-to-end competence: understanding requirements, making tradeoffs, and producing professional deliverables.","L1: Language Production
L2: Decision-making, Planning, Semantic Understanding & Context Recognition
L3: Social Reasoning & Theory of Mind (minor)",L2
SWE-Lancer,https://openai.com/index/swe-lancer/,https://arxiv.org/abs/2502.12115,"SWE-Lancer evaluates real-world software engineering work where models must implement changes and fixes across codebases, often emphasizing higher-level engineering judgment beyond isolated functions. It stresses scoping tasks, navigating dependencies, and producing robust patches that pass checks.","L1: 
L2: Planning, Logical Reasoning, Adaptive Error Correction, Working Memory (minor), Decision-making (minor)
L3: ",L2
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks tests long-context reasoning over graph-structured data, where models must follow or infer paths/relations across many nodes and edges from textual encodings. It stresses systematic traversal, resisting distractors, and maintaining correctness over extended sequences.","L1: 
L2: Spatial Representation & Mapping, Working Memory, Attention, Logical Reasoning (minor)
L3: ",L2
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon evaluates general-purpose tool use across heterogeneous APIs/tools, requiring models to select tools, call them correctly, handle errors, and compose multi-step workflows. It measures agentic reliability in planning and execution under realistic tool interfaces and constraints.","L1: 
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory (minor)
L3: Inhibitory Control (minor)",L2
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath is a hard mathematics benchmark intended to measure progress at the research frontier, with tiers spanning from advanced contest level to very difficult expert problems. It stresses deep multi-step reasoning, precise symbolic manipulation, and careful validation of derived results.","L1: 
L2: Logical Reasoning, Working Memory, Adaptive Error Correction, Planning (minor)
L3: ",L2
