Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates agentic software engineering by asking models to produce code patches that fix real issues in Python repositories, with success determined by passing repository test suites. The “Verified” subset consists of tasks that have been human-checked to be solvable and reliably graded, reducing noise from ambiguous or broken tasks.","L1: Language Comprehension (minor)
L2: Planning, Adaptive Error Correction, Logical Reasoning, Working Memory, Decision-making (minor)
L3: ",L2
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro is a more difficult software engineering benchmark emphasizing realistic, industrially relevant tasks and stronger contamination resistance than earlier SWE-bench variants. It expands task diversity (including multiple languages) and stresses end-to-end debugging, patch synthesis, and adherence to repository-specific constraints under a single-attempt evaluation setting.","L1: Language Comprehension (minor)
L2: Planning, Adaptive Error Correction, Logical Reasoning, Working Memory, Decision-making (minor)
L3: ",L2
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp evaluates “deep research” agents by requiring models to answer difficult questions using browsing/search over a constrained document collection, emphasizing reproducibility and source-based justification. It tests whether an agent can iteratively search, read, synthesize across multiple documents, and avoid being derailed by irrelevant retrievals or distractors.","L1: 
L2: Planning, Semantic Understanding & Context Recognition, Working Memory, Attention, Decision-making, Episodic Memory (minor)
L3: ",L2
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench measures interactive tool-using customer-support agents in simulated domains (e.g., retail, airline, telecom) that must follow policies while resolving multi-turn user issues. It stresses maintaining dialog state, selecting correct API calls, handling constraints, and balancing helpfulness with policy compliance across long conversations.","L1: Language Comprehension, Language Production
L2: Decision-making, Planning
L3: Social Reasoning & Theory of Mind, Inhibitory Control, Empathy (minor)",L3
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,ARC-AGI is a few-shot “fluid intelligence” benchmark where models infer hidden rules from a handful of grid input-output examples and generate the correct output grid for a new input. It targets generalization to novel abstract patterns and compositional rule discovery rather than memorized domain knowledge.,"L1: Visual Perception (minor)
L2: Logical Reasoning, Spatial Representation & Mapping, Working Memory, Attention (minor)
L3: Cognitive Flexibility",L3
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas evaluates real-world tool use via the Model Context Protocol (MCP), requiring models to discover tools, call them correctly, recover from errors, and integrate outputs into final answers. Tasks typically involve multi-step workflows across heterogeneous services, emphasizing reliable action selection and tool-result grounding.","L1: Language Comprehension (minor)
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition (minor)
L3: ",L2
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a large, frontier-level benchmark of difficult questions spanning many domains, designed to probe advanced reasoning and expert knowledge, often in multimodal formats. It emphasizes robust problem solving and synthesis under uncertainty, and is frequently reported both with and without tool assistance (e.g., search, code).","L1: Language Comprehension
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Scene Understanding & Visual Reasoning (minor), Decision-making (minor)
L3: ",L2
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 consists of competition-style mathematics problems (typically requiring multi-step derivations and careful case handling) drawn from the 2025 AIME exam. It is used to measure symbolic and quantitative reasoning robustness, often reported with and without computational tools.","L1: 
L2: Logical Reasoning, Working Memory, Planning, Adaptive Error Correction (minor), Semantic Understanding & Context Recognition (minor)
L3: ",L2
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a high-quality subset of GPQA with challenging graduate-level multiple-choice science questions intended to be difficult to answer via shallow pattern matching or simple web lookup. It probes deep scientific reasoning, precise reading of technical language, and discriminating between closely competing options.","L1: Language Comprehension
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory (minor), Decision-making (minor)
L3: ",L2
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends MMLU to multilingual settings, testing broad academic knowledge and reasoning across many subjects and multiple non-English languages. It emphasizes cross-lingual robustness: understanding questions, mapping them to relevant knowledge, and selecting correct answers under standardized formats.","L1: Language Comprehension, Language Production (minor)
L2: Semantic Understanding & Context Recognition, Logical Reasoning (minor), Working Memory (minor)
L3: ",L2
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro is an advanced multimodal understanding benchmark that requires reasoning over images (e.g., diagrams, charts, scenes, documents) together with text prompts across many disciplines. It targets expert-level visual-linguistic reasoning, including extracting relevant visual evidence and integrating it with domain knowledge to answer questions accurately.","L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Working Memory (minor)
L3: ",L2
ScreenShot-Pro,https://gui-agent.github.io/grounding-leaderboard/,https://arxiv.org/abs/2504.07981,"ScreenShot-Pro evaluates GUI understanding and grounding from high-resolution screenshots, where models must interpret interface layouts and identify relevant elements to answer questions or take implied actions. It probes spatially precise visual reasoning about widgets, text, icons, and their relations within complex software UIs.","L1: Visual Perception, Language Comprehension (minor)
L2: Scene Understanding & Visual Reasoning, Spatial Representation & Mapping, Visual Attention & Eye Movements, Planning (minor)
L3: ",L2
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning tests reasoning over scientific figures and chart-like visuals drawn from research papers, typically requiring careful extraction of quantitative/structural information and linking it to the question context. Many settings allow computational assistance (e.g., Python), reflecting real analysis workflows that combine perception, interpretation, and calculation.","L1: Visual Perception, Language Comprehension (minor)
L2: Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Working Memory (minor)
L3: ",L2
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU evaluates multimodal understanding over video, requiring models to answer questions that depend on events unfolding across time as well as associated textual context. It stresses temporal integration, attention over frames, and reasoning about actions, causality, and sequences rather than single-image recognition.","L1: Visual Perception
L2: Working Memory, Attention, Scene Understanding & Visual Reasoning, Multisensory Integration (minor)
L3: Cognitive Timing & Predictive Modeling",L3
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) is a long-context evaluation where multiple similar “needle” interactions are embedded in long conversation “haystacks,” and the model must retrieve and reproduce the correct response corresponding to a specified needle. It targets robust multi-round co-reference and retrieval under heavy distractors and very long contexts.","L1: Language Comprehension
L2: Working Memory, Episodic Memory, Attention, Semantic Understanding & Context Recognition (minor)
L3: ",L2
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval measures performance on well-specified, economically valuable knowledge-work tasks across many occupations, graded via expert human comparisons (often as wins/ties/losses against professionals). Tasks frequently require producing structured deliverables (e.g., spreadsheets, presentations, plans) and following constraints over multi-step workflows.","L1: Language Production
L2: Planning, Decision-making, Semantic Understanding & Context Recognition, Working Memory (minor), Adaptive Error Correction (minor)
L3: ",L2
SWE-Lancer,https://openai.com/index/swe-lancer/,https://arxiv.org/abs/2502.12115,"SWE-Lancer evaluates software engineering on tasks resembling real paid “freelance/contract” issues, emphasizing end-to-end patch delivery aligned with user requirements and repository constraints. Compared with simpler coding benchmarks, it more strongly reflects practical engineering judgment, iteration, and solution selection under ambiguity.","L1: Language Comprehension (minor)
L2: Planning, Adaptive Error Correction, Logical Reasoning, Decision-making, Working Memory (minor)
L3: ",L2
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks evaluates graph-structured reasoning expressed in language, such as following edges, finding parents, or performing BFS-like traversals from a textual description of a graph. It probes whether models can maintain and manipulate discrete relational structure over many steps without losing track of nodes and paths.","L1: 
L2: Spatial Representation & Mapping, Working Memory, Logical Reasoning, Attention (minor), Planning (minor)
L3: ",L2
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon is a tool-use benchmark where models must solve tasks by selecting from (and correctly invoking) tools, then synthesizing tool outputs into final responses. It stresses robust orchestration across multiple calls, handling tool errors, and deciding when additional actions are needed versus when an answer is complete.","L1: 
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory
L3: Inhibitory Control (minor)",L2
HMMT,https://matharena.ai/?view=problem&comp=hmmt--hmmt_feb_2025,https://arxiv.org/abs/2505.23281,"HMMT comprises high-difficulty mathematics competition problems (e.g., the Harvard-MIT Mathematics Tournament), often requiring inventive multi-step solutions and careful symbolic manipulation. It is commonly used to probe advanced mathematical reasoning beyond standard contest sets, including non-routine problem decomposition.","L1: 
L2: Logical Reasoning, Planning, Working Memory, Adaptive Error Correction (minor)
L3: Cognitive Flexibility (minor)",L2
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath is a benchmark of expert-level mathematics problems designed to test the limits of model reasoning and tool-assisted problem solving, with difficulty stratified into tiers. It emphasizes long-horizon derivations, precise algebraic/analytic reasoning, and maintaining coherence across complex intermediate steps.","L1: 
L2: Logical Reasoning, Planning, Working Memory, Adaptive Error Correction
L3: Cognitive Flexibility (minor)",L2
