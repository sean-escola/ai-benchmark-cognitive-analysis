Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 evaluates agentic software and systems tasks executed through a real command-line environment (e.g., using shell commands, editing files, running programs, and inspecting outputs). It emphasizes end-to-end autonomy, where models must diagnose issues, choose actions, and recover from errors under realistic tooling constraints.","L1: Language Comprehension (minor), Language Production (minor)
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory, Logical Reasoning
L3: ",L2
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp measures deep web-research capability: the agent must search, read, and synthesize information from multiple sources to answer difficult, often long-tail questions. It stresses retrieval strategy, evidence aggregation, and avoiding premature conclusions while navigating noisy information.","L1: Language Comprehension, Language Production (minor)
L2: Planning, Decision-making, Attention, Working Memory, Semantic Understanding & Context Recognition
L3: ",L2
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld evaluates multimodal “computer use” in interactive desktop environments, requiring models to operate GUIs (apps, file systems, browsers) to accomplish user goals. Success depends on interpreting screenshots, executing multi-step procedures, and handling interface variability and failures.","L1: Visual Perception
L2: Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Planning, Decision-making, Sensorimotor Coordination, Adaptive Error Correction (minor), Working Memory (minor)
L3: ",L2
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI is a fluid-intelligence benchmark of small grid-based transformation puzzles where the rule must be inferred from a few input–output examples. It targets generalization to novel concepts, compositional reasoning, and robustness to distribution shift rather than memorized knowledge.","L1: 
L2: Logical Reasoning, Working Memory, Spatial Representation & Mapping, Scene Understanding & Visual Reasoning, Attention (minor)
L3: Cognitive Flexibility",L3
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon autonomy by simulating operation of a vending-machine business over many decisions (pricing, inventory, supplier negotiation, budgeting). It rewards coherent strategy over extended time, adaptation to changing conditions, and consistent goal pursuit under resource constraints.","L1: 
L2: Planning, Decision-making, Reward Mechanisms, Working Memory, Adaptive Error Correction (minor)
L3: Cognitive Timing & Predictive Modeling (minor)",L2
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym assesses cybersecurity agent capability on real-world vulnerability tasks, including identifying known issues from descriptions and discovering new vulnerabilities in open-source codebases. The benchmark stresses systematic investigation, hypothesis testing, and correctness under adversarial-like conditions.","L1: 
L2: Logical Reasoning, Planning, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition (minor)
L3: Inhibitory Control (minor)",L2
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"SpreadsheetBench measures the ability to understand, manipulate, and verify complex spreadsheets (formulas, tables, formatting, and multi-step transformations). It reflects real analyst workflows where errors propagate, so careful checking and iterative refinement matter.","L1: Visual Perception (minor), Language Comprehension (minor)
L2: Logical Reasoning, Working Memory, Planning, Decision-making, Adaptive Error Correction
L3: ",L2
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a multimodal, frontier-knowledge exam-style benchmark spanning many domains and question types, intended to probe advanced reasoning and broad expertise. It emphasizes synthesizing information, handling ambiguous or underspecified prompts, and producing reliable answers under difficulty at the edge of current models.","L1: Language Comprehension, Language Production (minor)
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Scene Understanding & Visual Reasoning (minor)
L3: ",L2
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,GPQA Diamond is a high-quality subset of GPQA consisting of very challenging graduate-level science multiple-choice questions designed to resist simple web lookup. It targets deep domain reasoning and the ability to discriminate among plausible distractors.,"L1: Language Comprehension
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory (minor)
L3: Inhibitory Control (minor)",L2
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro evaluates expert-level multimodal understanding and reasoning across disciplines using text-plus-image questions (e.g., diagrams, plots, scientific visuals). It tests whether models can integrate visual evidence with language instructions to reach correct, grounded conclusions.","L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Working Memory (minor)
L3: ",L2
MathArena Apex,https://matharena.ai/?view=problem&comp=apex--apex_2025,https://arxiv.org/abs/2505.23281,MathArena Apex is a competition-style mathematics benchmark emphasizing difficult problems that require multi-step derivations rather than pattern matching. It is designed to compare advanced mathematical reasoning and solution robustness across models under consistent evaluation.,"L1: 
L2: Logical Reasoning, Working Memory, Planning, Adaptive Error Correction (minor)
L3: Inhibitory Control (minor)",L2
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning evaluates reasoning over scientific figures and charts drawn from arXiv-style papers, often requiring extracting quantitative relationships and interpreting axes, legends, and annotations. It probes grounded visual analysis and the ability to connect visual evidence to formal or numerical conclusions.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Working Memory (minor), Attention (minor)
L3: ",L2
OmniDocBench 1.5,https://github.com/opendatalab/OmniDocBench,https://arxiv.org/abs/2412.07626,"OmniDocBench 1.5 measures document understanding and OCR quality across mixed-layout documents, including text, tables, formulas, and reading order. It focuses on faithful structured extraction, layout-aware parsing, and robustness to diverse document styles.","L1: Visual Perception, Language Comprehension (minor)
L2: Scene Understanding & Visual Reasoning, Attention, Working Memory (minor)
L3: ",L2
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU evaluates multimodal reasoning over videos, requiring integration of information across time (and often across multiple scenes) to answer questions. It stresses temporal understanding, event tracking, and maintaining coherence across many frames.","L1: Visual Perception, Language Comprehension (minor)
L2: Scene Understanding & Visual Reasoning, Working Memory, Attention (minor)
L3: Cognitive Timing & Predictive Modeling",L3
LiveCodeBench Pro,https://livecodebenchpro.com/projects/livecodebench-pro/leaderboard,https://arxiv.org/abs/2506.11928,"LiveCodeBench Pro evaluates coding ability on up-to-date, competition-like programming problems, aiming to reduce training-set contamination by focusing on newer tasks. It emphasizes algorithmic reasoning, correct implementation, and iterative debugging under time/attempt constraints.","L1: Language Comprehension (minor), Language Production (minor)
L2: Logical Reasoning, Planning, Working Memory, Adaptive Error Correction
L3: ",L2
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"The FACTS Benchmark Suite systematically evaluates factuality and faithfulness, measuring whether model outputs stay supported by available evidence and avoid hallucinated or distorted claims. It targets reliability under diverse question types, prompting conditions, and evaluation settings.","L1: Language Production
L2: Semantic Understanding & Context Recognition, Working Memory (minor)
L3: Inhibitory Control, Self-reflection (minor)",L3
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA extends physical commonsense and everyday practical reasoning evaluation across languages and cultures, focusing on selecting actions or explanations that make sense in real-world situations. It probes grounded intuition about physical interactions and pragmatic constraints rather than specialist knowledge.","L1: Language Comprehension (minor)
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Spatial Representation & Mapping (minor)
L3: Social Reasoning & Theory of Mind (minor)",L2
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) is a long-context evaluation where multiple similar “needle” requests are embedded within a large “haystack” of dialogue or documents, and the model must retrieve the correct referenced response. It stresses robustness to distraction, precise reference resolution, and sustained context tracking.","L1: Language Comprehension
L2: Working Memory, Attention, Episodic Memory
L3: Inhibitory Control (minor)",L2
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval evaluates well-specified professional knowledge-work tasks across many occupations using expert human judging, often requiring models to produce real work products (e.g., plans, spreadsheets, presentations). It emphasizes end-to-end task completion quality, adherence to constraints, and practical usefulness rather than short-form QA accuracy.","L1: Language Production
L2: Planning, Decision-making, Semantic Understanding & Context Recognition, Working Memory (minor), Adaptive Error Correction (minor)
L3: Social Reasoning & Theory of Mind (minor)",L2
SWE-Lancer,https://openai.com/index/swe-lancer/,https://arxiv.org/abs/2502.12115,"SWE-Lancer evaluates advanced software engineering on realistic tasks that require understanding repositories, making correct changes, and meeting specifications reliably. Compared with simpler coding benchmarks, it stresses broader engineering judgment, integration constraints, and higher-level correctness criteria.","L1: 
L2: Planning, Logical Reasoning, Adaptive Error Correction, Working Memory, Decision-making (minor)
L3: Inhibitory Control (minor)",L2
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks evaluates long-context graph reasoning by embedding graph descriptions or edge lists within text and asking models to perform multi-step traversals (e.g., reachability, parent pointers, BFS-like operations). It probes whether models can maintain structured state and execute precise symbolic-like operations over lengthy inputs.","L1: 
L2: Spatial Representation & Mapping, Working Memory, Logical Reasoning, Attention, Planning (minor)
L3: ",L2
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon measures an agent’s ability to solve tasks by correctly selecting, invoking, and composing tools (APIs) across multi-step workflows. It stresses reliable tool planning, correct parameterization, and recovery from tool errors and unexpected outputs.","L1: Language Comprehension (minor), Language Production (minor)
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory
L3: ",L2
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath is an expert-level mathematics benchmark designed to be difficult for current systems and to better reflect frontier research-level mathematical problem solving. It emphasizes long, precise derivations, correctness under compounding intermediate steps, and robust handling of novel problem structures.","L1: 
L2: Logical Reasoning, Working Memory, Planning, Adaptive Error Correction (minor)
L3: Inhibitory Control (minor)",L2
