Benchmark,Website,Paper,Description,Cognitive Functions
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates automated software engineering by asking models to produce patches that fix real issues in open-source Python repositories, with correctness checked by running the project’s tests in a controlled harness. The “Verified” subset uses tasks curated to be solvable and reliably evaluable, emphasizing end-to-end debugging rather than toy coding problems.","Logical Reasoning, Planning, Adaptive Error Correction, Working Memory, Language Comprehension (minor), Language Production (minor)"
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro is a harder, more diverse software engineering benchmark designed to be more contamination-resistant and industrially relevant, spanning multiple programming languages and larger solution spaces. It tests whether a model can understand a repository, localize bugs or missing features, implement fixes, and satisfy automated verification (tests/linters/build steps).","Logical Reasoning, Planning, Adaptive Error Correction, Working Memory, Cognitive Flexibility, Language Comprehension (minor)"
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp evaluates “deep research” style web-browsing agents on questions whose answers are hard to obtain without multi-step searching, source comparison, and synthesis. It stresses robust information seeking under noise and ambiguity, including deciding what to search for, which sources to trust, and how to assemble a final answer from evidence.","Planning, Decision-making, Attention, Episodic Memory, Semantic Understanding & Context Recognition, Language Comprehension, Language Production (minor)"
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench measures an agent’s ability to carry out multi-turn customer-support tasks by interacting with simulated users and APIs while adhering to domain policies (e.g., retail, airline, telecom). It emphasizes policy-following under conversational pressure, tool/API correctness, and consistent decisions across long dialogues.","Social Reasoning & Theory of Mind, Decision-making, Inhibitory Control, Planning, Language Comprehension, Language Production, Empathy (minor)"
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI tests fluid reasoning on novel grid-based pattern transformation tasks where only a few input–output examples are provided and the model must infer the underlying rule. The benchmark is designed to minimize reliance on memorized knowledge, rewarding abstraction, compositional generalization, and flexible hypothesis testing.","Cognitive Flexibility, Logical Reasoning, Working Memory, Visual Perception, Scene Understanding & Visual Reasoning, Spatial Representation & Mapping"
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas evaluates real-world tool use via the Model Context Protocol (MCP), where models must discover relevant tools, call them with correct arguments, handle errors, and synthesize outputs across multi-step workflows. It is aimed at measuring agentic reliability in production-like API environments rather than single-shot QA.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Language Comprehension (minor), Language Production (minor)"
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam (HLE) is a frontier, multimodal academic benchmark spanning difficult questions across many domains, intended to probe the limits of model knowledge and reasoning under realistic uncertainty. It often rewards careful synthesis, multi-step derivations, and (in tool-enabled variants) strategic use of search or code to verify claims.","Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Language Comprehension, Scene Understanding & Visual Reasoning (minor), Multisensory Integration (minor), Planning (minor)"
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 is a competition-math benchmark drawn from the 2025 American Invitational Mathematics Examination, featuring short-answer problems requiring creative problem solving and precise calculation. It measures mathematical reasoning quality under time-pressured-style prompts rather than extended proofs.","Logical Reasoning, Working Memory, Cognitive Flexibility (minor), Adaptive Error Correction (minor)"
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a high-difficulty subset of GPQA consisting of “google-proof” multiple-choice science questions curated so non-experts tend to fail while experts succeed. It stresses deep scientific reasoning and careful discrimination among plausible distractors, not just surface-level factual recall.","Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory (minor), Decision-making (minor)"
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends MMLU-style academic subject testing to many languages, measuring multilingual knowledge and reasoning across a broad set of disciplines. It probes whether models can maintain performance under linguistic variation, translation ambiguity, and culturally different formulations of the same concepts.","Language Comprehension, Semantic Understanding & Context Recognition, Working Memory (minor), Language Production (minor)"
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro is a challenging multimodal benchmark that requires expert-level understanding and reasoning over images paired with text across many subjects (e.g., STEM diagrams, charts, tables, and technical visuals). It emphasizes fine-grained visual grounding plus multi-step reasoning to select or produce correct answers, often under stricter evaluation than earlier MMMU variants.","Visual Perception, Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Language Comprehension, Spatial Representation & Mapping (minor)"
ScreenShot-Pro,https://gui-agent.github.io/grounding-leaderboard/,https://arxiv.org/abs/2504.07981,"ScreenShot-Pro evaluates screenshot understanding for real software interfaces, requiring models to answer questions grounded in UI layout, widgets, and on-screen text. It measures whether a model can reliably parse dense visual scenes and reason about interface structure, often under high-resolution inputs where small details matter.","Visual Perception, Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Spatial Representation & Mapping, Language Comprehension, Sensorimotor Coordination (minor)"
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning focuses on reasoning over scientific figures (especially charts) drawn from research papers, combining visual extraction with quantitative and contextual interpretation. It tests whether a model can correctly read plotted variables, legends, axes, and trends, and then perform the downstream reasoning the figure implies.","Visual Perception, Scene Understanding & Visual Reasoning, Logical Reasoning, Language Comprehension, Working Memory (minor), Adaptive Error Correction (minor)"
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU evaluates multimodal understanding and reasoning over videos, where correct answers depend on integrating information across time (and often across multiple visual cues). It emphasizes temporal grounding, tracking state changes, and synthesizing events into coherent conclusions rather than static image recognition.","Visual Perception, Attention, Working Memory, Cognitive Timing & Predictive Modeling, Scene Understanding & Visual Reasoning, Multisensory Integration (minor), Auditory Processing (minor)"
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) is a long-context retrieval-and-coreference benchmark where multiple similar “needle” requests are embedded in long “haystacks,” and the model must retrieve the correct associated response. It targets robustness to interference, distractors, and repeated patterns as context length grows.","Working Memory, Attention, Episodic Memory, Language Comprehension, Inhibitory Control (minor)"
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval is an evaluation of well-specified professional knowledge work across many occupations, judged by experts via pairwise comparisons of real work products (e.g., spreadsheets, presentations, plans). It emphasizes end-to-end task execution quality: interpreting requirements, producing structured deliverables, and making good tradeoffs under constraints.","Planning, Decision-making, Language Comprehension, Language Production, Semantic Understanding & Context Recognition, Adaptive Error Correction (minor), Self-reflection (minor)"
SWE-Lancer,https://openai.com/index/swe-lancer/,https://arxiv.org/abs/2502.12115,"SWE-Lancer evaluates agentic software engineering on more open-ended or higher-autonomy coding tasks, where success depends on selecting actions, iterating with tools, and producing robust changes in realistic codebases. Compared with standard patch benchmarks, it places more weight on long-horizon orchestration and reliability under tooling and environment feedback.","Planning, Adaptive Error Correction, Logical Reasoning, Working Memory, Decision-making, Cognitive Flexibility (minor)"
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks tests long-context and structured reasoning by embedding graph-structured data (e.g., edges/relationships) into text and asking models to perform algorithmic traversals such as BFS-style walks or parent/ancestor queries. It probes whether models can maintain and manipulate discrete structures over many steps without losing consistency.","Spatial Representation & Mapping, Working Memory, Logical Reasoning, Attention, Planning (minor)"
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon is a tool-use benchmark where models must solve tasks by selecting and composing external tools/APIs, often across multi-step workflows with intermediate verification. It measures practical agent competence: choosing the right tool, formatting calls correctly, recovering from tool errors, and integrating outputs into a final response.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Language Comprehension (minor), Language Production (minor)"
HMMT,https://matharena.ai/?view=problem&comp=hmmt--hmmt_feb_2025,https://arxiv.org/abs/2505.23281,"HMMT (as used via MathArena) draws from Harvard-MIT Mathematics Tournament problems, which are typically harder and more proof-structured than many standard contest sets. It stresses multi-step mathematical reasoning, careful constraint handling, and error-free symbolic manipulation to reach exact answers.","Logical Reasoning, Working Memory, Cognitive Flexibility (minor), Adaptive Error Correction (minor)"
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath is an expert-level mathematics benchmark designed to assess the frontier of model capability on challenging, research-adjacent problems, with emphasis on difficulty stratification (tiers) and rigorous verification. It targets deep multi-step reasoning and robustness, often requiring nontrivial problem decomposition and sustained attention to detail.","Logical Reasoning, Working Memory, Cognitive Flexibility, Planning (minor), Adaptive Error Correction (minor)"
