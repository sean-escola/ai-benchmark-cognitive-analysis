Benchmark,Website,Paper,Description,Cognitive Functions
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 evaluates agentic coding and system-operation skill in real command-line environments (e.g., debugging, scripting, dependency management, and repo manipulation). Models are scored by whether they can complete end-to-end tasks under an agent harness that executes shell commands and validates outcomes.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Language Comprehension (minor)"
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp measures deep research performance when a model must search a fixed document collection (or web-like index), gather evidence, and answer complex questions. It emphasizes decomposing information needs, navigating retrieval constraints, and synthesizing a justified final response.","Planning, Attention, Semantic Understanding & Context Recognition, Episodic Memory (minor), Working Memory, Language Comprehension, Language Production (minor)"
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld evaluates multimodal computer-use agents that must complete tasks across operating-system applications using screenshots and interface actions. Success depends on correctly perceiving UI state, choosing actions in the right order, and recovering from interaction errors.","Visual Perception, Scene Understanding & Visual Reasoning, Attention, Sensorimotor Coordination, Planning, Decision-making, Adaptive Error Correction (minor), Working Memory"
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,ARC-AGI is a fluid-reasoning benchmark where models infer hidden rules from a few input–output grid examples and produce the correct output grid for a new input. It is designed to minimize reliance on memorized knowledge and instead test abstraction and rule induction under sparse supervision.,"Logical Reasoning, Cognitive Flexibility, Working Memory, Attention (minor), Spatial Representation & Mapping (minor), Scene Understanding & Visual Reasoning (minor)"
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon autonomous decision-making in a simulated business: models manage inventory, pricing, supplier negotiation, and budgeting over many steps. The score is based on sustained strategic performance (e.g., profitability) rather than single-turn accuracy.","Planning, Decision-making, Reward Mechanisms, Working Memory, Episodic Memory (minor), Adaptive Error Correction (minor), Motivational Drives (minor)"
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym measures cybersecurity agent capability on real-world vulnerability tasks, including identifying known weaknesses and attempting discovery of new issues in codebases. It emphasizes iterative hypothesis testing, tool-aided investigation, and producing a correct exploit/patch-relevant outcome.","Logical Reasoning, Planning, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition, Decision-making (minor), Inhibitory Control (minor)"
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"SpreadsheetBench evaluates an agent’s ability to understand, edit, and compute with complex spreadsheets derived from real workflows. Tasks often require locating relevant cells/tables, applying correct formulas or transformations, and verifying outputs under tool execution.","Planning, Decision-making, Working Memory, Attention, Adaptive Error Correction (minor), Semantic Understanding & Context Recognition"
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a broad, frontier-level benchmark spanning advanced questions across many disciplines, often including multimodal items. It targets the ability to integrate domain knowledge with multi-step reasoning and produce precise, well-supported answers (sometimes with tool use in certain settings).","Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Visual Perception (minor), Planning (minor)"
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,GPQA Diamond is a subset of extremely challenging graduate-level science multiple-choice questions curated to be resistant to shallow pattern matching. It probes whether a model can perform disciplined scientific reasoning and select the correct answer among strong distractors.,"Logical Reasoning, Semantic Understanding & Context Recognition, Language Comprehension, Working Memory (minor)"
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro is a high-difficulty multimodal benchmark covering many academic subjects where questions require interpreting images (e.g., diagrams, plots, scenes) alongside text. It tests integrated visual-language understanding and multi-step reasoning under expert-style prompts.","Visual Perception, Scene Understanding & Visual Reasoning, Language Comprehension, Logical Reasoning, Working Memory, Attention (minor)"
MathArena Apex,https://matharena.ai/?view=problem&comp=apex--apex_2025,https://arxiv.org/abs/2505.23281,"MathArena Apex aggregates hard competition-style and research-adjacent math problems to compare advanced mathematical reasoning. The benchmark emphasizes correctness on multi-step derivations, often with strong resistance to heuristic guessing.","Logical Reasoning, Working Memory, Adaptive Error Correction (minor), Planning (minor)"
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning evaluates scientific-figure and chart understanding using questions grounded in figures drawn from research papers. Models must extract quantitative/structural information from visuals and reason to an answer, sometimes with optional computational assistance.","Visual Perception, Scene Understanding & Visual Reasoning, Logical Reasoning, Attention, Semantic Understanding & Context Recognition (minor), Working Memory (minor)"
OmniDocBench 1.5,https://github.com/opendatalab/OmniDocBench,https://arxiv.org/abs/2412.07626,"OmniDocBench 1.5 measures document understanding/OCR quality across varied layouts including text, tables, formulas, and reading order. It emphasizes faithful extraction and structural reconstruction from complex pages rather than only plain-text transcription.","Visual Perception, Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements (minor), Language Comprehension (minor), Working Memory (minor)"
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU extends multimodal understanding to video by asking questions that require integrating information across frames and time. It targets temporal grounding, event understanding, and reasoning about dynamic visual content with accompanying text prompts.","Visual Perception, Scene Understanding & Visual Reasoning, Cognitive Timing & Predictive Modeling, Attention, Working Memory, Language Comprehension (minor)"
LiveCodeBench Pro,https://livecodebenchpro.com/projects/livecodebench-pro/leaderboard,https://arxiv.org/abs/2506.11928,"LiveCodeBench Pro evaluates coding ability on competitively sourced, time-relevant programming problems, aiming to reduce training contamination via recency and curation. It measures whether models can produce correct, executable solutions under realistic constraints.","Logical Reasoning, Planning, Working Memory, Adaptive Error Correction, Language Production (minor)"
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"The FACTS Benchmark Suite systematically evaluates factuality and hallucination-related behaviors across multiple tasks and settings. It emphasizes whether a model maintains truthfulness, resists fabrication under uncertainty, and stays consistent with available evidence or grounding signals.","Semantic Understanding & Context Recognition, Language Comprehension, Inhibitory Control, Self-reflection (minor), Working Memory (minor)"
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA evaluates commonsense and practical reasoning across many languages and cultural contexts, focusing on non-parallel, globally diverse data. It tests whether models can interpret intent and plausibility beyond English-centric priors.","Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning, Cognitive Flexibility (minor), Social Reasoning & Theory of Mind (minor)"
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) is a long-context evaluation where multiple similar “needle” interactions are embedded within long “haystack” dialogues and the model must retrieve the correct referenced content. It stresses robust context tracking, disambiguation under distractors, and accurate recall at long distances.","Working Memory, Attention, Episodic Memory, Semantic Understanding & Context Recognition, Language Comprehension (minor)"
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval measures performance on well-specified professional knowledge-work tasks across many occupations, judged by expert human evaluators via head-to-head comparisons. It emphasizes producing usable work products (e.g., plans, analyses, artifacts) with correct constraints, formatting, and decision quality.","Planning, Decision-making, Language Production, Semantic Understanding & Context Recognition, Working Memory (minor), Social Reasoning & Theory of Mind (minor), Self-reflection (minor)"
SWE-Lancer,https://openai.com/index/swe-lancer/,https://arxiv.org/abs/2502.12115,"SWE-Lancer evaluates real-world software engineering capability in repository-based tasks that require implementing changes, fixing bugs, and producing patches that pass validation. Compared with simpler coding tests, it emphasizes end-to-end engineering workflow competence and correctness under project context.","Planning, Logical Reasoning, Working Memory, Adaptive Error Correction, Decision-making (minor)"
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks evaluates reasoning over graph-structured data, requiring models to follow edges, perform traversals (e.g., BFS-like steps), and answer queries grounded in the graph. It stresses exact, stepwise state tracking under combinatorial structure rather than open-ended prose knowledge.","Spatial Representation & Mapping, Logical Reasoning, Working Memory, Attention (minor), Cognitive Timing & Predictive Modeling (minor)"
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon evaluates tool-using agents on multi-step tasks that require selecting, calling, and chaining external tools/APIs correctly. It emphasizes robustness to tool errors, proper argument construction, and coherent orchestration toward a validated end result.","Planning, Decision-making, Working Memory, Adaptive Error Correction, Attention (minor), Semantic Understanding & Context Recognition (minor)"
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath measures expert-level mathematics performance with problems designed to be hard for current models, organized by difficulty tiers. It targets sustained multi-step reasoning and precision, often in settings where computational tools may be allowed to support verification.","Logical Reasoning, Working Memory, Planning (minor), Adaptive Error Correction (minor)"
