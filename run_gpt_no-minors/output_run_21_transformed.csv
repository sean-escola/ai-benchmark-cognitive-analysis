Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates LLM-based software engineering by asking the model to produce code patches that fix real GitHub issues in Python repositories. The “Verified” split consists of problems that have been validated by human reviewers to be solvable and reliably testable, emphasizing correctness under repository tests and realistic debugging workflows.","L1: Language Comprehension (minor)
L2: Logical Reasoning, Planning, Adaptive Error Correction, Working Memory
L3: ",L2
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro is a harder successor-style benchmark that measures end-to-end software engineering over a broader and more challenging set of real-world tasks, spanning multiple programming languages. It aims to be more industrially relevant and contamination-resistant, stressing robust patch generation under complex project structure and test suites.","L1: Language Comprehension (minor)
L2: Logical Reasoning, Planning, Adaptive Error Correction, Working Memory, Decision-making (minor)
L3: ",L2
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp evaluates “deep research” behavior: answering difficult questions that require multi-step web search, reading, and synthesis rather than recalling a single fact. It stresses the agentic loop of query planning, evidence gathering, and consolidating sources into a supported final answer.","L1: Language Production (minor)
L2: Planning, Attention, Episodic Memory, Semantic Understanding & Context Recognition, Decision-making
L3: ",L2
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench measures interactive agent performance in simulated customer-service domains (e.g., retail, airline, telecom) where the model must converse with a user, call tools/APIs, and follow domain policies. The benchmark emphasizes multi-turn policy adherence, tool reliability, and resolving tasks under constraints.","L1: Language Comprehension, Language Production (minor)
L2: Decision-making, Planning
L3: Social Reasoning & Theory of Mind, Inhibitory Control, Empathy (minor)",L3
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI tests “fluid intelligence” on novel abstract pattern tasks: given a few input–output grid examples, the model must infer the underlying rule and produce the correct output grid for a new input. It is designed to reduce reliance on memorized knowledge and instead probe generalization to unseen task types.","L1: Visual Perception (minor)
L2: Logical Reasoning, Working Memory, Spatial Representation & Mapping, Scene Understanding & Visual Reasoning (minor)
L3: Cognitive Flexibility",L3
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas evaluates real-world tool use via the Model Context Protocol (MCP), requiring models to discover tools, invoke them with correct schemas, manage multi-step workflows, and integrate results into an answer. It stresses robustness to tool errors, sequencing across services, and correct synthesis from tool outputs.","L1: Language Comprehension (minor)
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition
L3: ",L2
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a large, difficult benchmark spanning frontier academic and professional questions, often requiring multi-step reasoning and (in some settings) tool use such as search or code. It is intended to probe broad expert-level competence across domains rather than narrow skill fitting.","L1: Language Comprehension
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Multisensory Integration (minor), Decision-making (minor)
L3: ",L2
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 uses problems from the American Invitational Mathematics Examination to assess competition-style mathematical reasoning. Questions typically require multi-step symbolic manipulation, careful case analysis, and error-free arithmetic/logical deductions under time-like constraints.","L1: 
L2: Logical Reasoning, Planning, Working Memory, Adaptive Error Correction (minor)
L3: ",L2
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a very challenging multiple-choice science benchmark designed to be difficult to answer via simple web search, targeting graduate-level reasoning in biology, chemistry, and physics. The “Diamond” subset is curated for high-quality questions that reliably separate experts from non-experts.","L1: Language Comprehension (minor)
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory (minor)
L3: ",L2
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends MMLU to multilingual settings, testing broad academic knowledge and reasoning across many subjects and multiple languages. It probes whether models can maintain consistent competence and comprehension beyond English, often under multiple-choice formats.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory (minor)
L3: ",L2
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro is a harder, more evaluation-focused variant of MMMU for expert-level multimodal understanding and reasoning, combining text with images such as diagrams, plots, and scientific figures. It emphasizes cross-domain visual reasoning and precise grounding of answers in visual evidence.","L1: Visual Perception, Language Comprehension (minor)
L2: Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Visual Attention & Eye Movements (minor)
L3: ",L2
ScreenShot-Pro,https://gui-agent.github.io/grounding-leaderboard/,https://arxiv.org/abs/2504.07981,"ScreenShot-Pro evaluates GUI understanding from high-resolution screenshots, often requiring models to localize relevant UI elements and answer questions grounded in interface layout. It targets visually grounded reasoning needed for computer-use agents (e.g., interpreting menus, settings, dashboards) rather than generic image captioning.","L1: Visual Perception, Language Comprehension (minor)
L2: Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Spatial Representation & Mapping (minor), Sensorimotor Coordination (minor)
L3: ",L2
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning focuses on answering reasoning questions about figures and content drawn from arXiv-style scientific papers, emphasizing chart/figure interpretation and grounded inference. It tests whether a model can extract the right visual evidence and connect it to the accompanying scientific context.","L1: Visual Perception, Language Comprehension (minor)
L2: Scene Understanding & Visual Reasoning, Logical Reasoning, Multisensory Integration, Working Memory (minor)
L3: ",L2
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU extends multimodal understanding to video, requiring temporal integration of visual information across frames to answer questions. Tasks stress tracking events, attributes, and relations over time rather than relying on a single static image.","L1: Visual Perception, Language Comprehension (minor)
L2: Working Memory, Attention, Multisensory Integration (minor), Scene Understanding & Visual Reasoning (minor)
L3: Cognitive Timing & Predictive Modeling (minor)",L2
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (multi-round coreference resolution) evaluates long-context retrieval and reference tracking by embedding multiple similar “needle” interactions inside long distractor “haystacks,” then asking the model to reproduce the correct response for a specified needle. The 8-needle variant stresses maintaining and retrieving the correct referent among many near-duplicates over long contexts.","L1: Language Comprehension
L2: Working Memory, Episodic Memory, Attention, Semantic Understanding & Context Recognition (minor)
L3: ",L2
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval measures performance on well-specified professional “knowledge work” tasks (e.g., producing presentations, spreadsheets, plans, analyses) across many occupations, often judged by experts in head-to-head comparisons. It aims to capture applied competence: producing usable artifacts, following constraints, and making practical decisions under realistic instructions.","L1: Language Production
L2: Planning, Decision-making, Semantic Understanding & Context Recognition, Working Memory (minor)
L3: Self-reflection (minor), Social Reasoning & Theory of Mind (minor)",L2
SWE-Lancer,https://openai.com/index/swe-lancer/,https://arxiv.org/abs/2502.12115,"SWE-Lancer evaluates advanced software engineering performance on realistic repository tasks, emphasizing reliability and end-to-end patch quality under constraints similar to professional workflows. Compared to simpler coding benchmarks, it more strongly stresses autonomous debugging, refactoring, and integrating changes that satisfy tests and requirements.","L1: Language Comprehension (minor)
L2: Planning, Adaptive Error Correction, Logical Reasoning, Working Memory, Decision-making (minor)
L3: ",L2
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks evaluates multi-step reasoning over graph-structured data presented in textual form, such as following parent pointers or performing breadth-first-like traversals. It tests whether models can reliably maintain and update an internal representation of a graph path/state across many steps without losing track.","L1: 
L2: Spatial Representation & Mapping, Working Memory, Logical Reasoning, Attention (minor)
L3: ",L2
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon evaluates complex tool-using agents on multi-step tasks that require selecting tools, composing calls, handling failures, and synthesizing results into a final deliverable. It emphasizes robustness and orchestration in realistic tool ecosystems rather than isolated single-call function usage.","L1: 
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition (minor)
L3: Inhibitory Control (minor)",L2
HMMT,https://matharena.ai/?view=problem&comp=hmmt--hmmt_feb_2025,https://arxiv.org/abs/2505.23281,"HMMT problems (e.g., from Harvard-MIT Mathematics Tournament sets) assess high-difficulty contest mathematics that typically requires creative multi-step reasoning and careful proof-like structure. Compared with standard exam items, these tasks more often demand strategic problem decomposition and non-routine insights.","L1: 
L2: Logical Reasoning, Planning, Working Memory
L3: Cognitive Flexibility (minor)",L2
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath targets expert-level mathematics problems intended to be challenging even for strong automated reasoning systems, often requiring deep multi-step derivations and careful control of intermediate results. It is designed to measure progress on difficult, research-adjacent mathematical reasoning rather than routine computation.","L1: 
L2: Logical Reasoning, Planning, Working Memory, Adaptive Error Correction (minor)
L3: Cognitive Flexibility (minor)",L2
