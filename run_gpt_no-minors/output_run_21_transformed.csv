Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 evaluates agentic coding and sysadmin-style problem solving in real terminal environments. Models must navigate a filesystem, run commands, inspect outputs, and iteratively repair their approach under execution constraints to complete end-to-end tasks.","L1: Language Comprehension (minor)
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory, Attention (minor)
L3: ",L2
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp evaluates “deep research” style question answering where the model must search and synthesize evidence from a controlled document collection or browsing setup. Success depends on formulating effective queries, integrating information across sources, and producing a supported final answer rather than a single-step response.","L1: Language Production (minor)
L2: Planning, Semantic Understanding & Context Recognition, Episodic Memory (minor), Working Memory, Attention
L3: ",L2
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld benchmarks multimodal “computer use” agents that operate a real desktop-like OS to accomplish tasks across applications. Agents must interpret screenshots/UI state, take multi-step actions (click/type/open menus), and recover from mistakes to reach goal states.","L1: Visual Perception
L2: Visual Attention & Eye Movements, Sensorimotor Coordination, Planning, Decision-making, Adaptive Error Correction
L3: ",L2
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI measures fluid intelligence via abstract grid transformation puzzles with only a few input–output examples per task. Models must infer latent rules and generalize them to a new input, emphasizing systematic generalization over memorization.","L1: 
L2: Logical Reasoning, Working Memory, Attention, Spatial Representation & Mapping
L3: Cognitive Flexibility",L3
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon autonomous agency in a simulated year-long vending-machine business. Agents must manage inventory, pricing, supplier interactions, and cashflow across thousands of steps, optimizing for final profit under uncertainty.","L1: 
L2: Planning, Decision-making, Reward Mechanisms, Working Memory, Adaptive Error Correction
L3: Self-reflection (minor)",L2
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity capabilities on large suites of tasks involving vulnerability discovery and exploitation reasoning in real-world software contexts. Agents must interpret bug descriptions or codebases, form hypotheses about weaknesses, and validate fixes or exploits via tool-driven workflows.","L1: 
L2: Logical Reasoning, Planning, Adaptive Error Correction, Semantic Understanding & Context Recognition, Working Memory (minor)
L3: ",L2
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"SpreadsheetBench measures the ability to understand and manipulate complex spreadsheets to answer questions or produce correct transformed files. It tests structured data reasoning, formula/aggregation understanding, and multi-step editing actions that must be consistent across a workbook.","L1: 
L2: Logical Reasoning, Planning, Working Memory, Attention, Adaptive Error Correction (minor), Semantic Understanding & Context Recognition (minor)
L3: ",L2
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a frontier, multi-domain (often multimodal) test intended to probe advanced reasoning and expert knowledge across many fields. Questions are designed to be difficult for models by requiring careful reading, multi-step inference, and integration of specialized concepts.","L1: Language Comprehension, Visual Perception (minor)
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Scene Understanding & Visual Reasoning (minor)
L3: ",L2
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a high-quality subset of GPQA consisting of very challenging multiple-choice questions in biology, chemistry, and physics. It targets deep scientific reasoning and knowledge that resists shallow pattern matching or simple web lookup.","L1: Language Comprehension
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory
L3: ",L2
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro evaluates expert-level multimodal understanding across disciplines, requiring models to answer questions grounded in images (diagrams, plots, tables) alongside text. It emphasizes visual reasoning, cross-modal grounding, and robust interpretation of technical visuals.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Attention (minor)
L3: ",L2
MathArena Apex,https://matharena.ai/?view=problem&comp=apex--apex_2025,https://arxiv.org/abs/2505.23281,"MathArena Apex is a competitive mathematics benchmark designed to stress advanced problem solving with difficult, often olympiad-style questions. It typically rewards coherent multi-step derivations and precise final answers under single-attempt grading.","L1: 
L2: Logical Reasoning, Working Memory, Attention (minor), Planning (minor)
L3: ",L2
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning evaluates reasoning over scientific figures and charts drawn from research papers, often requiring extraction of plotted relationships and quantitative/qualitative conclusions. It probes whether models can align textual questions to visual evidence and perform structured inference from that evidence.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Semantic Understanding & Context Recognition, Logical Reasoning, Attention (minor)
L3: ",L2
OmniDocBench 1.5,https://github.com/opendatalab/OmniDocBench,https://arxiv.org/abs/2412.07626,"OmniDocBench 1.5 evaluates document understanding and OCR across heterogeneous elements such as paragraphs, tables, formulas, and reading order. Systems must accurately recover text and structure from document images, reflecting robustness to layout complexity.","L1: Visual Perception, Language Comprehension
L2: Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Semantic Understanding & Context Recognition (minor)
L3: ",L2
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU extends multimodal understanding to videos, requiring models to answer questions that depend on temporal events, actions, and evolving visual context. It emphasizes integrating cues across frames rather than relying on a single snapshot.","L1: Visual Perception
L2: Attention, Working Memory, Scene Understanding & Visual Reasoning, Multisensory Integration (minor)
L3: Cognitive Timing & Predictive Modeling",L3
LiveCodeBench Pro,https://livecodebenchpro.com/projects/livecodebench-pro/leaderboard,https://arxiv.org/abs/2506.11928,"LiveCodeBench Pro evaluates coding ability on contemporary programming tasks with an emphasis on realistic developer workflows and up-to-date problems. Models must generate correct solutions under execution-based verification, often requiring debugging and iterative refinement within a single attempt.","L1: Language Production (minor)
L2: Logical Reasoning, Planning, Adaptive Error Correction, Working Memory
L3: ",L2
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"The FACTS Benchmark Suite systematically evaluates factuality and grounding, measuring whether model outputs remain faithful to provided evidence and avoid unsupported claims. It aggregates multiple factuality-oriented tests to characterize error patterns beyond a single QA metric.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition
L3: Inhibitory Control (minor), Self-reflection (minor)",L2
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA evaluates physical commonsense reasoning in a multilingual/non-parallel setting, probing whether models can generalize intuitive physics and everyday action plausibility across languages and cultures. It reduces reliance on memorized English-only artifacts by varying linguistic realizations.","L1: Language Comprehension
L2: Logical Reasoning, Semantic Understanding & Context Recognition
L3: Cognitive Flexibility (minor)",L2
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,MRCR v2 (8-needle) tests long-context multi-round coreference resolution by embedding multiple similar “needle” requests within long “haystack” dialogues and asking the model to reproduce the correct referenced answer. It stresses robust retrieval of the right mention under heavy interference.,"L1: Language Comprehension
L2: Working Memory, Attention, Episodic Memory
L3: ",L2
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval evaluates economically valuable, well-specified knowledge work across many occupations using human expert judging (often via pairwise comparisons). Tasks include creating professional artifacts (e.g., slides, spreadsheets, plans) where quality depends on meeting constraints and producing coherent deliverables.","L1: Language Production
L2: Planning, Decision-making, Semantic Understanding & Context Recognition
L3: Self-reflection (minor)",L2
SWE-Lancer,https://openai.com/index/swe-lancer/,https://arxiv.org/abs/2502.12115,"SWE-Lancer evaluates software engineering agents on real repository tasks that resemble practical bug-fixing and feature implementation work. Models must reason over codebases, produce patches, and satisfy tests/constraints that reflect production-like engineering requirements.","L1: Language Comprehension (minor)
L2: Logical Reasoning, Planning, Adaptive Error Correction, Working Memory
L3: ",L2
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks measures a model’s ability to perform algorithmic traversal and relational reasoning over graphs represented in long textual contexts. Typical tasks require following edges, tracking parents/paths, or executing BFS-like reasoning without losing state across many nodes.","L1: 
L2: Spatial Representation & Mapping, Working Memory, Attention, Logical Reasoning
L3: ",L2
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon evaluates tool-using agents on multi-step tasks that require selecting, sequencing, and correctly invoking external tools/APIs, then composing results into a final response. It targets reliability under tool errors, partial observability, and long action chains.","L1: Language Comprehension (minor)
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory, Attention (minor)
L3: ",L2
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath is an expert-level mathematics benchmark intended to measure progress on research-grade problem solving rather than routine competition items. Problems often require sustained, rigorous multi-step reasoning and careful handling of formal definitions and edge cases.","L1: 
L2: Logical Reasoning, Working Memory, Planning (minor)
L3: Cognitive Flexibility (minor)",L2
