Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates a model’s ability to solve real software engineering issues in real GitHub repositories by producing a code patch that makes the project’s tests pass. The “Verified” subset consists of tasks that have been validated (e.g., by human review) as solvable with the provided context and test harness, aiming to reduce ambiguous or underspecified issues.","L1: 
L2: Planning, Logical Reasoning, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition, Decision-making (minor)
L3: ",L2
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro is a harder, larger software engineering benchmark designed to better reflect professional-grade tasks and to be more resistant to shortcuts and contamination. It emphasizes end-to-end debugging and implementation in realistic repos (often spanning multiple files and non-trivial dependency interactions), stressing robustness under more demanding scenarios than SWE-bench Verified.","L1: 
L2: Planning, Logical Reasoning, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition
L3: Cognitive Flexibility",L3
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp evaluates deep-research style agent behavior: the model must search/browse to find information, reconcile evidence across sources, and produce a grounded final answer. The benchmark stresses multi-step information foraging, source selection, and maintaining coherence over longer investigative chains rather than single-shot recall.","L1: Language Comprehension
L2: Planning, Attention, Episodic Memory, Working Memory, Semantic Understanding & Context Recognition
L3: ",L2
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench measures how well an agent can complete multi-turn, tool-mediated customer-support tasks in simulated domains (e.g., retail, airline, telecom) while following domain policies. It probes whether the agent can interpret user intent, choose correct API actions, maintain state across turns, and adhere to constraints even when tempted by convenient loopholes.","L1: Language Comprehension
L2: Decision-making, Planning, Working Memory
L3: Social Reasoning & Theory of Mind, Inhibitory Control, Empathy (minor)",L3
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,ARC-AGI (Abstraction and Reasoning Corpus) tests few-shot fluid reasoning by asking models to infer hidden transformation rules from a small number of input–output grid examples and apply them to new inputs. It is intentionally designed to favor generalizable pattern induction and compositional reasoning over memorized knowledge.,"L1: Visual Perception (minor)
L2: Logical Reasoning, Spatial Representation & Mapping, Scene Understanding & Visual Reasoning, Working Memory
L3: Cognitive Flexibility",L3
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas evaluates real-world tool use through the Model Context Protocol (MCP), where models must discover relevant tools, call them correctly, handle errors, and synthesize results across multi-step workflows. It targets agent reliability in production-like tool ecosystems rather than isolated function-calling toy tasks.","L1: Language Comprehension
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition
L3: ",L2
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam (HLE) is a broad, frontier-level benchmark intended to stress advanced reasoning and expert knowledge across many domains, including multimodal questions. It emphasizes solving novel, difficult problems (often requiring multi-step reasoning) rather than short factual retrieval.","L1: Language Comprehension
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Planning (minor), Multisensory Integration, Scene Understanding & Visual Reasoning
L3: ",L2
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 is a competition-math benchmark based on the American Invitational Mathematics Examination, where problems require nontrivial algebraic, combinatorial, geometric, and number-theoretic reasoning. It is typically evaluated in a no-tools setting to measure internal symbolic reasoning rather than external computation.","L1: 
L2: Logical Reasoning, Working Memory, Planning (minor)
L3: Cognitive Flexibility (minor)",L2
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a high-difficulty, “Google-proof” multiple-choice science benchmark focused on graduate-level questions in physics, chemistry, and biology. The Diamond subset is curated for quality and difficulty, aiming to measure genuine scientific reasoning and deep conceptual understanding under strong distractors.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory (minor)
L3: ",L2
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends MMLU to multiple languages, testing broad academic knowledge and reasoning across many subjects in non-English settings. It probes whether models can transfer conceptual competence across languages rather than relying on English-only pattern familiarity.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning (minor), Working Memory (minor)
L3: ",L2
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro is an advanced multimodal benchmark that evaluates expert-level understanding and reasoning over images paired with text across many disciplines (e.g., science, engineering, medicine). Compared with earlier multimodal sets, it emphasizes harder questions, stronger distractors, and more demanding visual reasoning grounded in diagrams, tables, and figures.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Visual Attention & Eye Movements (minor), Working Memory (minor)
L3: ",L2
ScreenShot-Pro,https://gui-agent.github.io/grounding-leaderboard/,https://arxiv.org/abs/2504.07981,"ScreenShot-Pro evaluates how well models understand and act on information contained in high-resolution GUI screenshots, often requiring precise grounding to interface elements. Tasks stress spatial/layout understanding (what is where), interpreting UI semantics, and producing correct actions or answers under visually dense, real-world interfaces.","L1: Visual Perception
L2: Visual Attention & Eye Movements, Spatial Representation & Mapping, Scene Understanding & Visual Reasoning, Sensorimotor Coordination (minor), Planning (minor)
L3: ",L2
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning tests whether models can answer reasoning-heavy questions grounded in scientific figures (charts/plots/diagrams) from research papers, often paired with captions or surrounding text. It emphasizes extracting quantitative/relational information from visuals and combining it with scientific context to reach correct conclusions.","L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory (minor)
L3: ",L2
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU evaluates multimodal understanding and reasoning over video content, where key evidence may be distributed across time and scenes. It tests whether models can integrate temporally separated cues, track entities/events, and answer questions requiring more than single-frame recognition.","L1: Visual Perception, Auditory Processing (minor)
L2: Attention, Working Memory, Scene Understanding & Visual Reasoning, Multisensory Integration
L3: Cognitive Timing & Predictive Modeling",L3
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) is a long-context benchmark that inserts multiple similar “needle” interactions into a large “haystack” of content and asks the model to recover the correct response associated with a specific needle. It stresses robust retrieval under interference, multi-round reference tracking, and resisting distractors across very long sequences.","L1: Language Comprehension
L2: Working Memory, Attention, Episodic Memory
L3: Inhibitory Control (minor)",L2
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval measures performance on well-specified professional knowledge-work tasks across many occupations (e.g., creating spreadsheets, presentations, plans, and analyses), typically judged against outputs from human professionals. It emphasizes end-to-end task execution quality—following specifications, producing coherent artifacts, and making reasonable domain decisions under constraints.","L1: Language Production
L2: Planning, Decision-making, Semantic Understanding & Context Recognition, Working Memory (minor)
L3: Self-reflection (minor)",L2
SWE-Lancer,https://openai.com/index/swe-lancer/,https://arxiv.org/abs/2502.12115,"SWE-Lancer evaluates software engineering ability on tasks resembling real freelance/contractor work, where success depends on delivering correct, usable changes aligned with requirements. It stresses practical end-to-end coding competence, including understanding intent, implementing changes, and handling edge cases that arise in real projects.","L1: 
L2: Planning, Decision-making, Logical Reasoning, Adaptive Error Correction, Semantic Understanding & Context Recognition, Working Memory (minor)
L3: ",L2
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks is a long-context reasoning benchmark where the model must traverse or query properties of graphs described in text (e.g., follow edges, identify parents/paths, or perform structured walks). It targets precise state tracking and multi-step symbolic manipulation in contexts where small mistakes compound over many hops.","L1: 
L2: Spatial Representation & Mapping, Working Memory, Attention, Logical Reasoning, Planning
L3: Inhibitory Control (minor)",L2
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon evaluates general tool-use competence across a diverse set of tasks requiring selecting tools, composing multi-step tool plans, and integrating tool outputs into final answers. It emphasizes robustness to tool errors, correct parameterization, and the ability to keep a coherent workflow across multiple tool calls.","L1: Language Comprehension
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition (minor)
L3: ",L2
HMMT,https://matharena.ai/?view=problem&comp=hmmt--hmmt_feb_2025,https://arxiv.org/abs/2505.23281,"HMMT (as used in MathArena) draws from the Harvard–MIT Mathematics Tournament problems, which are typically harder and more proof/insight-oriented than many standard competition items. The benchmark probes deep mathematical reasoning, creative problem decomposition, and maintaining consistency across long multi-step derivations.","L1: 
L2: Logical Reasoning, Working Memory, Planning (minor)
L3: Cognitive Flexibility (minor)",L2
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath is an expert-level mathematics benchmark intended to measure progress near the frontier of formal and informal math problem solving, often stratified by difficulty tiers. It emphasizes sustained multi-step reasoning, error sensitivity, and the ability to navigate unfamiliar problem structures that require creative insight rather than routine calculation.","L1: 
L2: Logical Reasoning, Working Memory, Planning
L3: Cognitive Flexibility",L3
