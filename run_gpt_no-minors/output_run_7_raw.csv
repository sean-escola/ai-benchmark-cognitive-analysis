Benchmark,Website,Paper,Description,Cognitive Functions
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,SWE-bench Verified evaluates software engineering agents on real GitHub issues where the task is to produce a patch that makes the repository’s tests pass. The “Verified” subset uses problems curated/validated to be solvable and to reduce evaluation noise via stronger checking and human verification of tasks and outcomes.,"Planning, Logical Reasoning, Working Memory, Adaptive Error Correction, Decision-making (minor), Language Comprehension (minor)"
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro is a harder, more contamination-resistant software engineering benchmark spanning multiple programming languages and more complex, realistic repository states. Models must understand an issue, localize relevant code, implement a fix, and satisfy automated tests under stricter evaluation protocols.","Planning, Logical Reasoning, Working Memory, Adaptive Error Correction, Decision-making, Language Comprehension (minor)"
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp evaluates “browse-the-web” research agents on information-seeking tasks where answers must be found by navigating and extracting evidence from online sources (often across multiple pages). It emphasizes tool-mediated search, source triangulation, and maintaining coherence over multi-step browsing sessions.","Planning, Decision-making, Attention, Working Memory, Semantic Understanding & Context Recognition, Language Comprehension (minor)"
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench measures agent performance in simulated customer-support environments (e.g., retail/airline/telecom) requiring multi-turn dialogue plus correct API/tool usage under domain policies. Success depends on tracking user state, following constraints, and executing reliable action sequences to resolve cases end-to-end.","Social Reasoning & Theory of Mind, Planning, Decision-making, Working Memory, Inhibitory Control (minor), Language Production (minor)"
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI is a fluid-intelligence benchmark of abstract pattern induction from a few input–output grid examples, requiring models to infer the latent rule and generalize to a new grid. It is designed to minimize reliance on memorized knowledge and instead probe flexible reasoning over novel tasks.","Cognitive Flexibility, Logical Reasoning, Spatial Representation & Mapping, Working Memory, Planning (minor), Visual Perception (minor)"
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas evaluates real-world tool use via the Model Context Protocol, where models must discover appropriate tools, call them with correct schemas/parameters, and integrate returned data into a final response. Tasks often require multi-step workflows, error handling, and coordination across multiple tools or servers.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Language Comprehension (minor), Semantic Understanding & Context Recognition (minor)"
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a broad, frontier-focused benchmark spanning difficult questions across many domains, often requiring multi-step reasoning and, in some settings, multimodal understanding. It targets robust problem solving and synthesis rather than narrow task specialization.","Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Language Comprehension, Visual Perception (minor), Scene Understanding & Visual Reasoning (minor)"
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 benchmarks competition-level mathematical problem solving drawn from the American Invitational Mathematics Examination. Problems require symbolic manipulation, multi-step derivations, and careful constraint handling under time-tested contest formats.","Logical Reasoning, Working Memory, Planning, Adaptive Error Correction (minor), Attention (minor)"
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a high-difficulty, “Google-proof” multiple-choice science benchmark composed of expert-authored questions where non-experts tend to fail. It emphasizes deep conceptual understanding and reasoning in physics, chemistry, and biology, rather than surface retrieval.","Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory (minor), Language Comprehension (minor)"
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends MMLU to multiple languages, testing broad academic knowledge and reasoning across many subjects and non-English settings. It stresses multilingual comprehension and consistent reasoning under varied linguistic and cultural phrasing.","Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning, Language Production (minor), Working Memory (minor)"
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro is a more challenging variant of MMMU for expert-level multimodal understanding, combining images (e.g., charts, diagrams, figures) with text questions across diverse disciplines. It probes the ability to ground reasoning in visual evidence and integrate it with domain knowledge.","Visual Perception, Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Visual Attention & Eye Movements (minor)"
ScreenShot-Pro,https://gui-agent.github.io/grounding-leaderboard/,https://arxiv.org/abs/2504.07981,"ScreenShot-Pro evaluates GUI understanding and grounded interaction from high-resolution screenshots, often requiring identifying UI elements, reading small text, and executing interface-relevant reasoning. It is commonly used to assess computer-use agents’ screenshot grounding and interface decision reliability.","Visual Perception, Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Spatial Representation & Mapping (minor), Decision-making (minor)"
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning tests understanding of scientific figures (and associated context) from arXiv papers, requiring models to answer questions that depend on accurate visual interpretation and scientific reasoning. It targets faithful extraction of quantitative/structural cues from figures rather than generic caption paraphrase.","Scene Understanding & Visual Reasoning, Visual Perception, Logical Reasoning, Multisensory Integration (minor), Working Memory (minor)"
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU extends multimodal understanding to video, evaluating whether models can integrate information across time (multiple frames) to answer questions about events, procedures, and visual evidence. It stresses temporal integration and robustness to visual detail changes across the clip.","Visual Perception, Working Memory, Cognitive Timing & Predictive Modeling, Scene Understanding & Visual Reasoning, Attention (minor)"
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) is a long-context evaluation where multiple similar “needle” mentions are embedded within lengthy documents, and the model must retrieve or reproduce the information corresponding to a specified needle. The 8-needle setting stresses interference resistance and precise cross-document/long-span reference tracking.","Working Memory, Attention, Episodic Memory, Language Comprehension (minor), Inhibitory Control (minor)"
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval measures performance on well-specified, economically relevant knowledge-work tasks across many occupations, with outputs judged against industry professionals (e.g., spreadsheets, presentations, plans, analyses). It emphasizes producing usable artifacts under constraints, prioritizing quality, correctness, and task completion over open-ended chat.","Planning, Decision-making, Language Production, Semantic Understanding & Context Recognition, Adaptive Error Correction (minor), Self-reflection (minor)"
SWE-Lancer,https://openai.com/index/swe-lancer/,https://arxiv.org/abs/2502.12115,"SWE-Lancer evaluates real-world, contractor-like software engineering tasks that often involve larger scope than single bug fixes, such as implementing features, refactors, or multi-file changes under practical constraints. It is intended to better reflect professional engineering workflows and end-to-end delivery quality.","Planning, Decision-making, Logical Reasoning, Working Memory, Adaptive Error Correction"
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks tests long-context, algorithmic traversal over graph-structured data described in text, requiring models to follow specified walk rules and report derived properties (e.g., reachability or parent relationships). It targets systematic state tracking and resistance to distractors across lengthy sequences.","Logical Reasoning, Working Memory, Attention, Spatial Representation & Mapping, Planning (minor)"
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon is a tool-use benchmark where models must solve tasks by selecting, sequencing, and correctly parameterizing calls to multiple tools, then synthesizing outputs into a coherent solution. It emphasizes reliability of tool orchestration, handling tool errors, and maintaining task state across multi-step workflows.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Attention (minor)"
HMMT,https://matharena.ai/?view=problem&comp=hmmt--hmmt_feb_2025,https://arxiv.org/abs/2505.23281,"HMMT (Harvard–MIT Mathematics Tournament) problems are advanced contest-math questions that typically require creative insight, multi-step reasoning, and careful proof-like argumentation or construction. As an evaluation set, it probes harder and more diverse competition mathematics than many standard test-style math benchmarks.","Logical Reasoning, Planning, Working Memory, Cognitive Flexibility (minor), Adaptive Error Correction (minor)"
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath is a benchmark of expert-level mathematics designed to be difficult for current models, often requiring deep, multi-stage reasoning and (in some settings) tool-assisted computation. It aims to separate shallow pattern matching from robust mathematical problem solving at and beyond typical competition levels.","Logical Reasoning, Planning, Working Memory, Cognitive Flexibility (minor), Adaptive Error Correction (minor)"
