Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates software engineering agents on real GitHub issues by asking them to produce patches that make a repository’s tests pass, with human-verified solvable tasks and standardized evaluation. It emphasizes end-to-end debugging and code editing under realistic constraints (repo context, tests, and CI-like feedback loops).","L1: 
L2: Logical Reasoning, Planning, Working Memory, Adaptive Error Correction, Semantic Understanding & Context Recognition
L3: Inhibitory Control (minor)",L2
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro is a larger, harder software engineering benchmark designed to be more industrially representative and more contamination-resistant, spanning multiple programming languages and more complex repositories. Models must generate correct patches for realistic issues, typically requiring deeper codebase comprehension and iterative debugging.","L1: 
L2: Logical Reasoning, Planning, Working Memory, Adaptive Error Correction, Semantic Understanding & Context Recognition
L3: Cognitive Flexibility (minor)",L2
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp evaluates “deep research” capability by requiring models to answer questions that depend on finding and synthesizing information from a constrained web corpus/index, aiming for reproducibility across search systems. Success depends on forming search strategies, validating sources, and integrating evidence into a final answer.","L1: Language Comprehension
L2: Planning, Decision-making, Attention, Episodic Memory (minor), Semantic Understanding & Context Recognition
L3: ",L2
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench measures agentic performance in multi-turn, tool-mediated customer support simulations (e.g., retail, airline, telecom) where an agent must follow domain policies while using APIs to resolve user issues. It tests robust tool use, policy adherence, and coherent dialogue across long interactions.","L1: Language Comprehension, Language Production
L2: Decision-making, Planning
L3: Social Reasoning & Theory of Mind, Inhibitory Control, Empathy (minor)",L3
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,ARC-AGI is a “fluid intelligence” benchmark where models infer abstract rules from a few input–output grid examples and produce the correct output grid for a new input. It is designed to emphasize novel pattern induction and generalization rather than memorized knowledge.,"L1: 
L2: Logical Reasoning, Working Memory, Spatial Representation & Mapping, Scene Understanding & Visual Reasoning, Attention (minor)
L3: Cognitive Flexibility",L3
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas evaluates real-world tool use through the Model Context Protocol by requiring models to discover tools, call them correctly, handle errors, and compose multi-step workflows across multiple MCP servers. It emphasizes reliable action selection and correct parameterization in production-like API settings.","L1: 
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition
L3: Inhibitory Control (minor)",L2
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam (HLE) is a large, multi-modal benchmark intended to probe frontier knowledge and reasoning across many subjects, with questions that may require deep domain understanding and careful justification. Some evaluation setups allow tools (e.g., search or code) while others test closed-book reasoning, highlighting both knowledge retrieval and reasoning robustness.","L1: Language Comprehension, Language Production
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Scene Understanding & Visual Reasoning (minor), Working Memory (minor)
L3: ",L2
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 is a competition-style math benchmark drawn from the American Invitational Mathematics Examination, featuring short-answer problems that require multi-step reasoning and precise calculation. It is often used to measure symbolic/quantitative reasoning with minimal reliance on external knowledge.","L1: 
L2: Logical Reasoning, Working Memory, Planning (minor), Adaptive Error Correction (minor), Attention (minor)
L3: ",L2
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a difficult multiple-choice science QA benchmark curated to be “Google-proof,” with questions that non-experts usually miss while experts can solve. The Diamond subset emphasizes high-quality items in physics, chemistry, and biology that require careful reasoning over specialized concepts.","L1: Language Comprehension (minor)
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory (minor)
L3: ",L2
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends MMLU to multiple languages, testing academic knowledge and reasoning across many subjects via multilingual multiple-choice questions. It probes whether models can maintain meaning and reasoning fidelity across diverse languages and cultural/linguistic contexts.","L1: Language Comprehension, Language Production (minor)
L2: Semantic Understanding & Context Recognition, Logical Reasoning (minor)
L3: ",L2
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro is an enhanced, harder variant of MMMU for multimodal expert-level understanding, combining text with diagrams, charts, and images across many disciplines. It stresses grounded visual reasoning and cross-referencing visual evidence with textual context under more challenging prompts than earlier MMMU versions.","L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Multisensory Integration, Attention (minor), Working Memory (minor)
L3: ",L2
ScreenShot-Pro,https://gui-agent.github.io/grounding-leaderboard/,https://arxiv.org/abs/2504.07981,"ScreenShot-Pro evaluates models’ ability to understand high-resolution screenshots of software interfaces and answer questions that depend on precise GUI grounding (e.g., identifying elements, interpreting layouts, or reading small text). It targets practical visual comprehension for “computer use” and UI-centric workflows.","L1: Visual Perception, Language Comprehension (minor)
L2: Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Spatial Representation & Mapping (minor)
L3: ",L2
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning tests reasoning over scientific figures and associated paper context, emphasizing quantitative interpretation of plots, tables, and experimental results from arXiv-style documents. Many setups allow Python to compute derived quantities, making the benchmark sensitive to tool-augmented analysis and faithful figure reading.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Logical Reasoning, Multisensory Integration, Working Memory (minor), Attention (minor)
L3: ",L2
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU evaluates multimodal understanding and reasoning over videos paired with questions that may require tracking events across time, integrating visual cues with text, and using background knowledge. It extends multimodal QA beyond static images to temporal narratives and dynamic scenes.","L1: Visual Perception
L2: Attention, Scene Understanding & Visual Reasoning, Working Memory, Multisensory Integration (minor)
L3: Cognitive Timing & Predictive Modeling",L3
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,MRCR v2 (8-needle) is a long-context evaluation that inserts multiple similar “needle” requests into very long “haystack” conversations/documents and asks the model to retrieve the correct referenced response. It stresses robust coreference resolution and resistance to distraction over long contexts.,"L1: Language Comprehension
L2: Working Memory, Attention, Episodic Memory
L3: Inhibitory Control (minor)",L2
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval evaluates well-specified professional knowledge-work tasks spanning many occupations by comparing model-produced artifacts and decisions against expert human judgments in head-to-head matchups. Tasks often require planning deliverables (e.g., spreadsheets/slides), following constraints, and producing polished outputs suitable for real workflows.","L1: Language Production
L2: Planning, Decision-making, Semantic Understanding & Context Recognition
L3: Self-reflection (minor), Social Reasoning & Theory of Mind (minor)",L2
SWE-Lancer,https://openai.com/index/swe-lancer/,https://arxiv.org/abs/2502.12115,"SWE-Lancer evaluates software engineering work that resembles contracted development/bug-fixing, emphasizing end-to-end task completion and practical code changes that satisfy specified requirements. Compared with unit-issue patching, it aims to better reflect real “work product” constraints and engineering outcomes.","L1: 
L2: Planning, Logical Reasoning, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition, Decision-making (minor)
L3: ",L2
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks tests long-context structural reasoning by embedding graph descriptions in text and asking models to perform operations like traversal, reachability, or parent/ancestor queries. It is designed to probe whether models can reliably track discrete structure and follow algorithmic steps over many nodes/edges.","L1: 
L2: Spatial Representation & Mapping, Working Memory, Logical Reasoning, Attention, Planning (minor)
L3: ",L2
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon evaluates generalized tool-use competence across a wide set of tool APIs and multi-step tasks, focusing on selecting appropriate tools, forming correct calls, and composing results into final answers. It targets robustness to tool errors, schema variations, and the need for iterative refinement.","L1: 
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition
L3: Inhibitory Control (minor)",L2
HMMT,https://matharena.ai/?view=problem&comp=hmmt--hmmt_feb_2025,https://arxiv.org/abs/2505.23281,"HMMT problems (e.g., Feb 2025 set) provide competition-level mathematics questions that typically require creative multi-step derivations rather than routine calculation. They are used to test advanced mathematical reasoning, proof-like thinking, and error-prone symbolic manipulation under time-competition styles.","L1: 
L2: Logical Reasoning, Working Memory, Planning (minor), Adaptive Error Correction (minor), Attention (minor)
L3: ",L2
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath is an expert-level mathematics benchmark intended to be difficult even for strong models, with problems often requiring deep insight, careful chaining of lemmas, and rigorous quantitative reasoning. It is frequently evaluated with tool support (e.g., Python) to separate reasoning skill from arithmetic execution while still demanding correct mathematical strategy.","L1: 
L2: Logical Reasoning, Planning, Working Memory, Adaptive Error Correction (minor), Attention (minor)
L3: ",L2
