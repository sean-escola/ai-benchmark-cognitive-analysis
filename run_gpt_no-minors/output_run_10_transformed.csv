Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 evaluates agentic performance on real command-line tasks (e.g., inspecting repositories, running programs, manipulating files, and using CLI tools) under an agent harness. Success depends on producing correct sequences of shell actions and recovering from tool/runtime errors rather than only answering in natural language.","L1: Language Comprehension (minor), Language Production (minor)
L2: Planning, Adaptive Error Correction, Working Memory, Decision-making
L3: ",L2
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp measures “deep research” by requiring models to answer difficult questions using web-style information retrieval, often with evidence spread across sources. It emphasizes iterative search, reading, synthesis, and deciding when sufficient evidence has been gathered to finalize an answer.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Planning, Decision-making, Attention, Working Memory
L3: ",L2
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld evaluates multimodal computer-use agents performing tasks inside a desktop operating system environment, such as navigating GUIs, editing files, and using applications. It tests whether an agent can perceive screen states, choose appropriate UI actions, and complete multi-step goals under realistic interaction constraints.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Sensorimotor Coordination, Planning, Adaptive Error Correction, Attention (minor), Decision-making (minor)
L3: ",L2
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI is a fluid intelligence benchmark where systems infer latent rules from a few input-output grid examples and produce the correct output for a new grid. It targets generalization to novel puzzles with minimal examples, stressing abstraction rather than memorization of task families.","L1: Visual Perception (minor)
L2: Logical Reasoning, Working Memory, Spatial Representation & Mapping
L3: Cognitive Flexibility",L3
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon agent coherence and strategy in a simulated year-long vending-machine business. Agents must manage inventory, pricing, supplier interactions, and budget constraints over many decisions, so success reflects sustained planning and adaptation over time.","L1: 
L2: Planning, Decision-making, Reward Mechanisms, Working Memory, Adaptive Error Correction (minor)
L3: Self-reflection (minor)",L2
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity capabilities across large numbers of tasks, including identifying known vulnerabilities in real software and discovering new ones. It stresses reading technical context, forming hypotheses about bugs/exploits, and iteratively testing fixes or proof-of-concepts.","L1: 
L2: Logical Reasoning, Planning, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition
L3: ",L2
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"SpreadsheetBench measures an agent’s ability to understand, navigate, and manipulate complex spreadsheets derived from realistic scenarios. Tasks often require multi-step transformations, formula reasoning, and careful bookkeeping to avoid subtle consistency errors.","L1: 
L2: Working Memory, Logical Reasoning, Planning, Attention (minor), Adaptive Error Correction (minor)
L3: ",L2
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a large, frontier-difficulty benchmark spanning many academic domains with an emphasis on expert-level reasoning and (in some settings) multimodal understanding. It is designed to be hard to solve via shallow pattern matching, stressing synthesis and rigorous problem solving across topics.","L1: Language Comprehension, Visual Perception (minor)
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory
L3: ",L2
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a high-quality subset of very difficult, graduate-level multiple-choice science questions intended to resist simple web lookup. It probes deep scientific understanding and careful reasoning under distractors that can trap superficial strategies.","L1: Language Comprehension (minor)
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory
L3: ",L2
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro is a multimodal benchmark covering many disciplines where questions require reasoning over images (diagrams, plots, screenshots) combined with text. It emphasizes integrating visual evidence with domain knowledge to select or generate correct answers.","L1: Visual Perception
L2: Multisensory Integration, Scene Understanding & Visual Reasoning, Logical Reasoning, Working Memory (minor), Visual Attention & Eye Movements (minor)
L3: ",L2
MathArena Apex,https://matharena.ai/?view=problem&comp=apex--apex_2025,https://arxiv.org/abs/2505.23281,"MathArena Apex is a competition-style evaluation focused on difficult mathematical problem solving, often requiring multi-step derivations and careful symbolic manipulation. It is designed to separate models that can execute long reasoning chains from those that rely on short heuristics.","L1: 
L2: Logical Reasoning, Working Memory, Planning, Adaptive Error Correction (minor)
L3: ",L2
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning evaluates reasoning over scientific figures/charts from papers, requiring models to interpret visual encodings and connect them to accompanying textual context. Many items require multi-hop inference from plotted trends, axes, legends, and experimental conditions.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory (minor)
L3: ",L2
OmniDocBench 1.5,https://github.com/opendatalab/OmniDocBench,https://arxiv.org/abs/2412.07626,"OmniDocBench 1.5 evaluates document understanding and OCR-style extraction across heterogeneous layouts, including text blocks, tables, formulas, and reading order. It stresses robust parsing of structured documents rather than only recognizing isolated text.","L1: Visual Perception, Language Comprehension
L2: Attention, Semantic Understanding & Context Recognition, Working Memory (minor)
L3: ",L2
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU extends multimodal understanding to videos, requiring temporal comprehension of events, actions, and changes across frames alongside text questions. It emphasizes integrating information over time rather than relying on a single salient image.","L1: Visual Perception, Auditory Processing (minor)
L2: Scene Understanding & Visual Reasoning, Working Memory, Attention (minor)
L3: Cognitive Timing & Predictive Modeling",L3
LiveCodeBench Pro,https://livecodebenchpro.com/projects/livecodebench-pro/leaderboard,https://arxiv.org/abs/2506.11928,"LiveCodeBench Pro evaluates coding performance on fresh, competitive-programming-style and practical tasks with an emphasis on resisting training contamination. It measures whether a model can produce correct, executable code under time/iteration constraints typical of real coding workflows.","L1: Language Production (minor), Language Comprehension (minor)
L2: Logical Reasoning, Planning, Adaptive Error Correction, Working Memory
L3: ",L2
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"FACTS Benchmark Suite systematically evaluates factuality by testing whether model outputs remain grounded and accurate across a range of factuality-related tasks and perturbations. It targets errors like hallucinations, overconfident unsupported claims, and failures to track provenance-like constraints.","L1: Language Comprehension (minor)
L2: Semantic Understanding & Context Recognition, Working Memory (minor)
L3: Inhibitory Control, Self-reflection (minor)",L3
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA evaluates practical, everyday physical commonsense reasoning across many languages and cultural contexts (with non-parallel items rather than direct translations). It probes whether models can choose plausible actions or explanations grounded in real-world affordances rather than language-only priors.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Spatial Representation & Mapping (minor)
L3: Social Reasoning & Theory of Mind (minor)",L2
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,MRCR v2 (8-needle) tests long-context multi-round coreference and retrieval by embedding multiple similar “needle” interactions within long “haystack” transcripts and asking the model to reproduce the correct referenced content. It stresses maintaining and selecting the right entity/turn linkage under heavy distraction.,"L1: Language Comprehension
L2: Working Memory, Attention, Semantic Understanding & Context Recognition, Episodic Memory (minor)
L3: ",L2
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval evaluates well-specified professional knowledge-work tasks across many occupations, scored by expert human judges via comparisons to professional outputs. Tasks often require producing real artifacts (e.g., plans, analyses, presentations/spreadsheets) and coordinating multiple subtasks to a high standard.","L1: Language Production
L2: Planning, Decision-making, Semantic Understanding & Context Recognition, Working Memory (minor)
L3: Self-reflection (minor), Social Reasoning & Theory of Mind (minor)",L2
SWE-Lancer,https://openai.com/index/swe-lancer/,https://arxiv.org/abs/2502.12115,"SWE-Lancer evaluates software engineering ability on realistic development work, often framed as implementing or fixing issues within a repository under constraints similar to professional workflows. It emphasizes end-to-end problem solving: understanding requirements, making coherent code changes, and validating behavior.","L1: Language Comprehension (minor), Language Production (minor)
L2: Planning, Logical Reasoning, Adaptive Error Correction, Working Memory
L3: ",L2
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks tests reasoning over graph-structured data, commonly by requiring models to follow paths, parents/children relations, or BFS-like traversal described in text. It emphasizes consistent multi-step relational tracking, where a single slip in traversal logic can invalidate the final answer.","L1: 
L2: Spatial Representation & Mapping, Working Memory, Logical Reasoning, Attention (minor)
L3: ",L2
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon evaluates tool-using agents on multi-step tasks that require selecting, calling, and chaining tools correctly (often under noisy tool outputs or partial failures). It focuses on reliable orchestration, parameter selection, and recovery strategies to reach an end goal.","L1: 
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory, Attention (minor)
L3: ",L2
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath is a hard mathematics benchmark targeting research- and Olympiad-adjacent problem difficulty, often requiring long, precise derivations and careful case analysis. It is designed to measure genuine mathematical reasoning progress, including performance at the most difficult tiers.","L1: 
L2: Logical Reasoning, Working Memory, Planning, Adaptive Error Correction (minor)
L3: Cognitive Flexibility (minor)",L2
