Benchmark,Website,Paper,Description,Cognitive Functions
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates whether a model can generate correct patches for real GitHub issues by running repository tests after applying the proposed fix. The “Verified” subset contains tasks that have been manually validated to be solvable and to have reliable evaluation signals, emphasizing end-to-end debugging and code change correctness.","Planning, Adaptive Error Correction, Logical Reasoning, Working Memory, Language Comprehension (minor), Decision-making (minor)"
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro is a more difficult and broader software engineering benchmark than SWE-bench Verified, designed to better reflect professional engineering work (including harder repos, more complex bug contexts, and broader languages/settings depending on the version). It evaluates patch correctness via automated testing and aims to be more contamination-resistant and industrially relevant.","Planning, Adaptive Error Correction, Logical Reasoning, Working Memory, Decision-making (minor), Language Comprehension (minor)"
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp evaluates “deep research” style web-browsing agents: models must locate and synthesize information from many documents to answer challenging questions. It stresses search strategy, source cross-checking, and long-horizon evidence integration rather than memorized facts.","Planning, Decision-making, Attention, Working Memory, Episodic Memory (minor), Language Comprehension, Semantic Understanding & Context Recognition, Language Production (minor)"
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench measures an agent’s ability to complete multi-turn customer-support tasks by interacting with tools/APIs and a simulated user while adhering to domain policies (e.g., retail, airline, telecom). Success depends on maintaining state across turns, choosing correct tool actions, and following constraints under realistic conversational pressure.","Social Reasoning & Theory of Mind, Decision-making, Planning, Inhibitory Control, Working Memory, Language Comprehension, Language Production, Empathy (minor)"
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,ARC-AGI evaluates fluid reasoning over novel grid-transformation puzzles given only a handful of demonstrations (few-shot pattern induction). It emphasizes compositional generalization and rule inference on problems designed to resist training-set memorization.,"Logical Reasoning, Cognitive Flexibility, Working Memory, Spatial Representation & Mapping, Visual Perception, Attention (minor), Planning (minor)"
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas evaluates tool use in real or production-like Model Context Protocol (MCP) environments, where models must discover tools, invoke them with correct arguments, and compose multi-step workflows. It stresses robustness to tool errors, correct sequencing of calls, and faithful synthesis of tool outputs into final answers.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Language Comprehension (minor), Language Production (minor), Semantic Understanding & Context Recognition"
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,Humanity’s Last Exam (HLE) is a difficult multimodal benchmark intended to probe frontier knowledge and reasoning across a broad range of expert domains. Questions often require integrating text with diagrams/images and (in tool-enabled settings) searching or computing to produce grounded answers.,"Language Comprehension, Logical Reasoning, Working Memory, Semantic Understanding & Context Recognition, Visual Perception (minor), Scene Understanding & Visual Reasoning (minor), Multisensory Integration (minor)"
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 consists of competition-style mathematics problems that require multi-step derivations and precise final answers, typically without external references. It is commonly used to assess mathematical reasoning, symbolic manipulation, and error-free execution of long solution chains.","Logical Reasoning, Working Memory, Adaptive Error Correction (minor), Planning (minor), Language Comprehension (minor)"
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a high-quality subset of GPQA comprising very challenging graduate-level multiple-choice questions in biology, chemistry, and physics that are designed to be “Google-proof.” It probes deep scientific reasoning and understanding rather than shallow recall or easy pattern matching.","Logical Reasoning, Semantic Understanding & Context Recognition, Language Comprehension, Working Memory (minor), Decision-making (minor)"
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends MMLU to multiple languages, testing knowledge and reasoning across many academic subjects with standardized multiple-choice questions. It is used to assess multilingual understanding, cross-lingual generalization, and broad factual/academic competence.","Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning (minor), Working Memory (minor)"
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro is an advanced multimodal benchmark that evaluates expert-level understanding and reasoning over images plus text (e.g., charts, diagrams, documents) across many disciplines. Compared to earlier multimodal benchmarks, it emphasizes harder problems, stronger evaluation protocols, and professional-grade visual reasoning.","Visual Perception, Scene Understanding & Visual Reasoning, Multisensory Integration, Language Comprehension, Logical Reasoning (minor), Visual Attention & Eye Movements (minor), Working Memory (minor)"
ScreenShot-Pro,https://gui-agent.github.io/grounding-leaderboard/,https://arxiv.org/abs/2504.07981,"ScreenShot-Pro evaluates models on understanding high-resolution screenshots of real software interfaces and answering questions that require precise visual grounding (e.g., locating UI elements, reading values, interpreting layout). It is designed to measure GUI understanding and screenshot-based reasoning that supports computer-use agents.","Visual Perception, Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Spatial Representation & Mapping, Language Comprehension (minor), Decision-making (minor)"
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning tests reasoning over scientific figures (and associated context) from arXiv-style papers, requiring models to interpret plots, diagrams, and visual encodings to answer questions. It targets scientific visual reasoning, quantitative interpretation, and faithful extraction of evidence from figures.","Scene Understanding & Visual Reasoning, Visual Perception, Multisensory Integration, Logical Reasoning, Working Memory (minor), Language Comprehension (minor)"
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU evaluates multimodal understanding and reasoning over videos paired with text questions, covering temporal events, actions, and causal relations. It probes whether models can integrate information across frames and maintain coherent interpretations over time.","Visual Perception, Scene Understanding & Visual Reasoning, Multisensory Integration, Working Memory, Attention, Cognitive Timing & Predictive Modeling (minor), Language Comprehension (minor)"
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 is a long-context evaluation for multi-round co-reference/recall: multiple similar “needle” requests are embedded within long “haystacks,” and the model must retrieve the correct response corresponding to a specified needle. The 8-needle variant increases interference and tests whether the model can preserve and recover precise details across very long inputs.","Working Memory, Attention, Episodic Memory, Language Comprehension, Semantic Understanding & Context Recognition (minor)"
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval evaluates well-specified professional knowledge-work tasks spanning many occupations (e.g., spreadsheets, presentations, operational plans), judged by expert humans via pairwise comparisons. It emphasizes producing usable work artifacts under realistic constraints, including following instructions, structuring deliverables, and making trade-offs aligned with task goals.","Planning, Decision-making, Language Production, Language Comprehension, Semantic Understanding & Context Recognition, Self-reflection (minor), Adaptive Error Correction (minor)"
SWE-Lancer,https://openai.com/index/swe-lancer/,https://arxiv.org/abs/2502.12115,"SWE-Lancer evaluates agentic software engineering beyond single isolated patches, emphasizing more realistic workflows such as navigating larger codebases, handling ambiguous requirements, and producing changes that meet evaluation criteria end-to-end. It is intended to better reflect the reliability and autonomy demands of real engineering work compared to narrower coding benchmarks.","Planning, Adaptive Error Correction, Working Memory, Logical Reasoning, Decision-making (minor), Language Comprehension (minor)"
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks tests structured reasoning over graphs described in text, requiring models to traverse nodes/edges (e.g., BFS-like procedures, parent-pointer recovery) and output correct derived information. It targets algorithmic reasoning and the ability to maintain and manipulate structured relational state over long contexts.","Logical Reasoning, Working Memory, Spatial Representation & Mapping, Attention (minor), Planning (minor)"
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon evaluates general tool-use competence across diverse APIs and tasks, emphasizing correct selection of tools, argument construction, iterative debugging, and synthesis of tool outputs into a coherent final answer. It is designed to measure robustness of agentic workflows under tool errors, partial results, and multi-step dependencies.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition (minor), Language Comprehension (minor), Language Production (minor)"
HMMT,https://matharena.ai/?view=problem&comp=hmmt--hmmt_feb_2025,https://arxiv.org/abs/2505.23281,"HMMT consists of high-difficulty contest mathematics problems (e.g., Harvard-MIT Math Tournament sets) that require multi-step proofs/derivations and careful case analysis. It is used to assess advanced mathematical reasoning under time-competition style constraints and sensitivity to small logical errors.","Logical Reasoning, Working Memory, Planning (minor), Adaptive Error Correction (minor), Language Comprehension (minor)"
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath is an expert-level mathematics benchmark designed to be difficult for frontier models and to reduce the usefulness of memorization, often requiring deep multi-step reasoning and (in some settings) tool-assisted computation. It aims to measure progress on the hardest tiers of formal and informal mathematical problem solving.","Logical Reasoning, Working Memory, Planning, Adaptive Error Correction (minor), Semantic Understanding & Context Recognition (minor)"
