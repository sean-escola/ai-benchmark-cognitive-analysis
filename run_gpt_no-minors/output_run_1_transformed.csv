Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 evaluates agentic software engineering in real command-line environments, requiring models to navigate files, run programs, install dependencies, and debug using shell tools. Tasks emphasize end-to-end autonomy under realistic constraints (stateful environment, tool errors, and long action sequences) rather than isolated code generation.","L1: Language Comprehension (minor)
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory
L3: ",L2
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp measures “deep research” ability: agents must answer complex questions by searching a fixed web-like corpus and synthesizing evidence across multiple documents. It stresses retrieval strategy, source triage, and faithful synthesis under long, multi-step browsing trajectories.","L1: Language Production (minor)
L2: Planning, Attention, Semantic Understanding & Context Recognition, Working Memory
L3: ",L2
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld evaluates multimodal computer-use agents on realistic desktop tasks (e.g., navigating apps, webpages, and system dialogs) with step limits and high-resolution screenshots. Success depends on visually grounding UI elements, choosing correct actions, and adapting to dynamic interface states and errors.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Sensorimotor Coordination, Planning, Attention (minor)
L3: ",L2
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI tests fluid intelligence via few-shot induction of hidden rules from small grid input–output examples, requiring generalization to a new grid. It is designed to reduce reliance on memorized knowledge and instead probe abstract pattern discovery and compositional reasoning.","L1: 
L2: Logical Reasoning, Spatial Representation & Mapping, Working Memory
L3: Cognitive Flexibility",L3
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon autonomous business management in a simulated environment, where agents run a vending machine company over many in-game days. Agents must plan inventory, pricing, procurement, and communications while adapting to changing market conditions to maximize final balance.","L1: 
L2: Planning, Decision-making, Reward Mechanisms, Working Memory
L3: Self-reflection (minor)",L2
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity agent capability across large-scale tasks involving identifying known vulnerabilities and discovering new ones in real open-source codebases. It stresses multi-step investigation, hypothesis testing, and iterative debugging under realistic tooling and adversarial failure modes.","L1: 
L2: Logical Reasoning, Planning, Adaptive Error Correction, Working Memory
L3: Inhibitory Control (minor)",L2
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"SpreadsheetBench measures the ability to understand and manipulate complex spreadsheets using tools (e.g., editing cells, formulas, tables, and formatting) to produce correct outputs. Tasks often require structured reasoning over tabular data and iterative verification across multiple steps.","L1: 
L2: Logical Reasoning, Working Memory, Planning, Attention (minor)
L3: ",L2
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam (HLE) is a large multimodal benchmark intended to probe frontier knowledge and reasoning across diverse expert-level domains. Questions frequently require integrating text with figures or diagrams and producing well-justified answers, sometimes with optional tool use in evaluated setups.","L1: Language Comprehension, Visual Perception (minor)
L2: Logical Reasoning, Multisensory Integration, Semantic Understanding & Context Recognition
L3: ",L2
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a high-quality subset of graduate-level, “Google-proof” multiple-choice science questions where non-experts tend to fail. It emphasizes precise scientific reasoning and discrimination among closely related answer choices under high knowledge and rigor demands.","L1: Language Comprehension (minor)
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory (minor)
L3: ",L2
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro is a challenging multimodal understanding and reasoning benchmark spanning many disciplines, designed for expert-level vision-language reasoning. It tests interpreting images (charts, diagrams, scenes) alongside text prompts to answer questions that require grounded inference rather than surface recognition.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning
L3: ",L2
MathArena Apex,https://matharena.ai/?view=problem&comp=apex--apex_2025,https://arxiv.org/abs/2505.23281,"MathArena Apex aggregates difficult competition-style mathematics problems and evaluates models under standardized settings, often emphasizing rigorous solution finding rather than explanation quality. It is used to compare high-end mathematical reasoning and reliability across strong models.","L1: 
L2: Logical Reasoning, Working Memory, Planning (minor)
L3: ",L2
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning evaluates reasoning over figures and visual content from scientific papers, requiring models to answer questions that depend on correctly interpreting plots, schematics, and annotated graphics. It stresses visual grounding, cross-referencing captions/context, and multi-step inference.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Attention (minor)
L3: ",L2
OmniDocBench 1.5,https://github.com/opendatalab/OmniDocBench,https://arxiv.org/abs/2412.07626,"OmniDocBench 1.5 evaluates document understanding and OCR across heterogeneous layouts, including text, formulas, tables, and reading order. Metrics emphasize faithful extraction and structural correctness, reflecting real-world document pipelines where layout parsing and symbol accuracy matter.","L1: Visual Perception, Language Comprehension
L2: Visual Attention & Eye Movements, Working Memory (minor)
L3: ",L2
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU extends multimodal reasoning to video, requiring models to integrate information across frames and time to answer questions about events, actions, and causal structure. It targets temporal understanding beyond single-image perception, often requiring tracking entities and state changes.","L1: Visual Perception
L2: Working Memory, Scene Understanding & Visual Reasoning, Attention (minor)
L3: Cognitive Timing & Predictive Modeling",L3
LiveCodeBench Pro,https://livecodebenchpro.com/projects/livecodebench-pro/leaderboard,https://arxiv.org/abs/2506.11928,"LiveCodeBench Pro evaluates coding ability on competitive programming and practical coding tasks with strong controls against leakage and a leaderboard-style rating (e.g., Elo). It measures not only writing code but also selecting algorithms, handling edge cases, and iterating toward correct solutions.","L1: Language Production (minor)
L2: Logical Reasoning, Planning, Adaptive Error Correction, Working Memory
L3: ",L2
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"FACTS Benchmark Suite systematically evaluates factuality across multiple sub-benchmarks, focusing on whether model outputs are supported by evidence and avoid hallucinated claims. It targets faithfulness under pressure from plausible-sounding completions and tests robustness across domains and formats.","L1: Language Production (minor)
L2: Semantic Understanding & Context Recognition
L3: Inhibitory Control, Self-reflection (minor)",L3
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA is a multilingual/non-parallel extension of physical commonsense question answering, probing whether models can reason about everyday interactions with objects and environments across languages and cultures. Items emphasize plausible physical affordances and outcomes rather than specialist knowledge.","L1: Language Comprehension (minor)
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Spatial Representation & Mapping (minor)
L3: ",L2
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 is a long-context, multi-round coreference-style retrieval benchmark where many similar “needle” requests are embedded in a long “haystack” of dialogue, and the model must reproduce the correct referenced response. The 8-needle setting stresses sustained attention and interference resistance across long contexts.","L1: Language Comprehension (minor)
L2: Working Memory, Attention, Episodic Memory
L3: ",L2
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval evaluates well-specified professional knowledge-work tasks (e.g., producing spreadsheets, slides, schedules, and other artifacts) across many occupations, judged by expert humans in head-to-head comparisons. It probes end-to-end task execution quality, including planning, adherence to constraints, and producing usable deliverables.","L1: Language Production
L2: Planning, Decision-making, Semantic Understanding & Context Recognition
L3: Self-reflection (minor)",L2
SWE-Lancer,https://openai.com/index/swe-lancer/,https://arxiv.org/abs/2502.12115,"SWE-Lancer evaluates software engineering agents on realistic repository-level tasks, emphasizing autonomous patch generation and robustness across diverse problem types. It is designed to better reflect industrial workflows than short-form coding problems, rewarding correct integration with existing code and tests.","L1: Language Comprehension (minor)
L2: Planning, Adaptive Error Correction, Working Memory, Logical Reasoning
L3: ",L2
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks tests the ability to follow graph-structured relations embedded in text (e.g., BFS traversals or parent-pointer reasoning) under long-context settings. It stresses systematic state tracking and consistent multi-step traversal rather than open-ended generation.","L1: 
L2: Working Memory, Logical Reasoning, Attention, Spatial Representation & Mapping (minor)
L3: ",L2
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon evaluates tool-using agents on multi-step tasks requiring correct tool selection, parameterization, error handling, and synthesis of results across heterogeneous tools. It targets practical agent reliability, including recovering from tool failures and maintaining a coherent execution plan.","L1: 
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory
L3: Inhibitory Control (minor)",L2
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath is a hard mathematics benchmark designed to measure progress near the frontier of expert-level problem solving, with tiers reflecting increasing difficulty. It emphasizes rigorous multi-step reasoning and precision, often benefiting from careful decomposition and verification rather than pattern matching.","L1: 
L2: Logical Reasoning, Working Memory, Planning, Adaptive Error Correction (minor)
L3: ",L2
