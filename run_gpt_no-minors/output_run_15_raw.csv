Benchmark,Website,Paper,Description,Cognitive Functions
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 evaluates agentic software engineering and systems tasks carried out through a command-line interface (e.g., installing dependencies, running programs, debugging, and editing files) in realistic sandboxed environments. It emphasizes end-to-end problem solving where models must decide what commands to run, interpret outputs, and iteratively fix issues under constraints.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Logical Reasoning, Language Comprehension (minor)"
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp evaluates web-browsing research agents on information-seeking questions that require searching, reading, cross-checking sources, and synthesizing answers. It targets realistic browsing behavior such as query formulation, navigating pages, handling distractors, and citing evidence rather than relying on memorized facts.","Planning, Decision-making, Attention, Working Memory, Semantic Understanding & Context Recognition, Language Comprehension, Self-reflection (minor)"
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld evaluates multimodal computer-use agents on tasks in a full operating-system environment, requiring interaction with GUIs, applications, and files to accomplish goals. Success depends on interpreting screens, executing multi-step UI actions, and recovering from mistakes in long, tool-driven trajectories.","Visual Perception, Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Sensorimotor Coordination, Planning, Decision-making, Adaptive Error Correction, Working Memory (minor)"
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI is a fluid reasoning benchmark where models infer hidden transformations from a few input–output grid examples and must generate the correct output for a novel input. It is designed to stress generalization and abstraction over pattern memorization, often requiring compositional rules and counterexample handling.","Logical Reasoning, Cognitive Flexibility, Working Memory, Spatial Representation & Mapping, Scene Understanding & Visual Reasoning, Attention (minor)"
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon autonomous agent performance in a simulated year-long vending-machine business, where the agent must manage inventory, pricing, supplier negotiation, and cash flow. The score reflects sustained coherence, strategy, and adaptation across many sequential decisions with delayed consequences.","Planning, Decision-making, Working Memory, Adaptive Error Correction, Reward Mechanisms, Motivational Drives (minor), Social Reasoning & Theory of Mind (minor)"
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity agent capabilities on real-world vulnerability tasks at scale, including identifying known vulnerabilities from descriptions and discovering new ones in open-source projects. It emphasizes systematic investigation, tool-driven experimentation, and iterative debugging of hypotheses against program behavior.","Logical Reasoning, Planning, Decision-making, Adaptive Error Correction, Working Memory, Attention (minor), Inhibitory Control (minor)"
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"SpreadsheetBench evaluates an agent’s ability to understand, navigate, and manipulate complex spreadsheets to answer questions or produce correct transformed artifacts. Tasks typically require multi-step operations (formulas, filtering, formatting, pivot-like reasoning) and careful verification of outputs.","Planning, Decision-making, Working Memory, Logical Reasoning, Adaptive Error Correction, Visual Perception (minor), Scene Understanding & Visual Reasoning (minor)"
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam (HLE) is a difficult multimodal benchmark spanning frontier academic and professional questions intended to be hard for models without strong reasoning and tool use. It stresses synthesis across domains, careful reading of problem statements, and robustness to traps and underspecified details.","Language Comprehension, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Decision-making (minor), Scene Understanding & Visual Reasoning (minor), Visual Perception (minor)"
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a high-quality subset of GPQA consisting of graduate-level multiple-choice science questions designed to be “Google-proof.” It probes deep scientific reasoning and knowledge integration, typically requiring multi-step inference rather than surface recall.","Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory (minor), Language Comprehension (minor)"
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro is an advanced multimodal reasoning benchmark where models answer expert-level questions that require interpreting images (e.g., diagrams, charts, documents) alongside text across many disciplines. It emphasizes grounded visual reasoning, cross-referencing evidence in the image, and handling complex answer formats or settings.","Visual Perception, Scene Understanding & Visual Reasoning, Multisensory Integration, Visual Attention & Eye Movements (minor), Language Comprehension (minor), Logical Reasoning (minor)"
MathArena Apex,https://matharena.ai/?view=problem&comp=apex--apex_2025,https://arxiv.org/abs/2505.23281,"MathArena Apex evaluates advanced mathematical problem solving in a competition-style setting, often requiring long chains of symbolic reasoning and careful case analysis. It is designed to stress reliability on hard math, including where small mistakes can derail an otherwise correct approach.","Logical Reasoning, Working Memory, Adaptive Error Correction (minor), Planning (minor)"
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning evaluates whether models can correctly interpret and reason about figures, charts, and visual elements found in scientific papers (arXiv-style), often paired with technical textual context. It targets grounded scientific visual understanding, quantitative extraction, and reasoning over plotted or diagrammatic evidence.","Visual Perception, Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Semantic Understanding & Context Recognition (minor), Visual Attention & Eye Movements (minor)"
OmniDocBench 1.5,https://github.com/opendatalab/OmniDocBench,https://arxiv.org/abs/2412.07626,"OmniDocBench 1.5 evaluates document understanding and OCR robustness on heterogeneous documents that can include text, formulas, tables, and complex reading order. It measures how well models reconstruct structured content and preserve layout-sensitive semantics rather than only extracting plain text.","Visual Perception, Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Semantic Understanding & Context Recognition (minor), Language Comprehension (minor)"
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU evaluates multimodal understanding and reasoning over video, requiring models to integrate information across frames and time to answer questions. It stresses temporal grounding (events, actions, state changes) along with cross-modal alignment between visual content and text prompts.","Visual Perception, Multisensory Integration, Cognitive Timing & Predictive Modeling, Working Memory, Scene Understanding & Visual Reasoning (minor), Attention (minor)"
LiveCodeBench Pro,https://livecodebenchpro.com/projects/livecodebench-pro/leaderboard,https://arxiv.org/abs/2506.11928,"LiveCodeBench Pro evaluates coding capability on time-indexed, competitive-programming-like and practical coding tasks to reduce contamination, often scoring via pass@k or ELO-style ratings. It emphasizes writing correct executable solutions under problem constraints and handling edge cases.","Logical Reasoning, Planning, Working Memory, Adaptive Error Correction, Language Comprehension (minor)"
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"The FACTS Benchmark Suite systematically evaluates factuality, including whether model outputs are supported by evidence and whether they avoid unsupported or fabricated claims across diverse settings. It targets fine-grained faithfulness (claim-level correctness) and robustness to hallucination under realistic prompting.","Semantic Understanding & Context Recognition, Logical Reasoning (minor), Inhibitory Control, Self-reflection (minor), Working Memory (minor)"
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA evaluates physical commonsense reasoning across many languages, focusing on everyday situations where understanding of object affordances and plausible actions matters. It emphasizes whether models preserve commonsense judgments when the same underlying scenario is expressed in different languages.","Semantic Understanding & Context Recognition, Logical Reasoning, Language Comprehension, Multisensory Integration (minor), Spatial Representation & Mapping (minor)"
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) is a long-context multi-round coreference/retrieval evaluation where multiple similar “needle” interactions are embedded within a large “haystack,” and the model must reproduce the correct response for a specified needle. It stresses precise retrieval under interference and maintaining correct referents across very long contexts.","Working Memory, Attention, Episodic Memory, Language Comprehension (minor), Inhibitory Control (minor)"
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval evaluates economically valuable, well-specified knowledge-work tasks spanning many occupations (e.g., creating business artifacts like spreadsheets, schedules, presentations, and analyses) judged by human experts. It emphasizes real-world output quality, adherence to requirements, and end-to-end usefulness rather than short-form Q&A.","Planning, Decision-making, Language Production, Language Comprehension, Working Memory (minor), Semantic Understanding & Context Recognition (minor), Self-reflection (minor)"
SWE-Lancer,https://openai.com/index/swe-lancer/,https://arxiv.org/abs/2502.12115,"SWE-Lancer evaluates software engineering work on realistic tasks that resemble contracted “freelance” development or maintenance, typically requiring understanding a repository, implementing changes, and validating fixes. It emphasizes end-to-end patch quality, correct tool usage (tests/build), and robustness across diverse codebases.","Planning, Decision-making, Logical Reasoning, Adaptive Error Correction, Working Memory, Language Comprehension (minor)"
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks evaluates reasoning over graph-structured data presented in long contexts, including tasks like breadth-first search, parent tracking, and path reasoning under distractors. It stresses algorithmic consistency and accurate multi-hop traversal when the “graph” is encoded as text sequences.","Logical Reasoning, Working Memory, Attention, Spatial Representation & Mapping (minor), Planning (minor)"
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon evaluates tool-using agents on complex workflows that require selecting among many tools, calling them with correct arguments, and integrating results into a final answer. It emphasizes reliable orchestration under multi-step dependencies, error handling, and avoiding brittle prompt-following failures.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Inhibitory Control (minor), Semantic Understanding & Context Recognition (minor)"
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath evaluates expert-level mathematics problems intended to be beyond routine competition math, with tiers reflecting increasing difficulty and novelty. It emphasizes deep multi-step reasoning, proof-like discipline, and robustness to subtle pitfalls, often benefiting from external computation tools but still requiring correct conceptual setup.","Logical Reasoning, Working Memory, Planning (minor), Adaptive Error Correction (minor)"
