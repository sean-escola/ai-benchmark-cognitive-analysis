Benchmark,Website,Paper,Description,Cognitive Functions
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates software engineering agents on real GitHub issues by requiring them to generate a patch that passes unit tests and matches a human-verified solution criterion. It emphasizes correctness under realistic repo constraints, including reading code, editing multiple files, and running tests in an automated harness.","Logical Reasoning, Planning, Adaptive Error Correction, Working Memory, Language Comprehension, Language Production (minor), Decision-making (minor)"
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro is a larger, more difficult software engineering benchmark intended to be more contamination-resistant and industry-relevant, spanning multiple languages and harder issue types. Models must produce repository patches that satisfy tests and specifications across a broader set of real-world development tasks.","Planning, Logical Reasoning, Adaptive Error Correction, Working Memory, Decision-making (minor), Language Comprehension"
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp evaluates deep-research and web-browsing agents on questions that require locating and synthesizing information from multiple sources rather than recalling facts. The benchmark stresses search strategy, evidence integration, and maintaining goal-directed progress across long, tool-mediated interactions.","Planning, Attention, Working Memory, Semantic Understanding & Context Recognition, Language Comprehension, Decision-making, Episodic Memory (minor)"
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench measures interactive agent performance in simulated customer-support environments (e.g., retail, airline, telecom) where the agent must follow policies while calling APIs/tools over multiple turns. It probes whether an agent can reliably execute procedures, handle edge cases, and maintain consistent policy adherence under conversational pressure.","Social Reasoning & Theory of Mind, Decision-making, Planning, Inhibitory Control, Language Comprehension, Language Production, Working Memory, Adaptive Error Correction (minor)"
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI tests generalization to novel grid-based pattern-transformation tasks from only a few examples, aiming to isolate fluid reasoning rather than memorized knowledge. Success requires inferring latent rules, composing transformations, and handling distribution shifts across puzzles.","Cognitive Flexibility, Logical Reasoning, Working Memory, Spatial Representation & Mapping, Scene Understanding & Visual Reasoning, Attention (minor)"
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas evaluates real-world tool use via the Model Context Protocol by requiring models to discover tools, invoke them correctly, recover from errors, and synthesize outputs across multi-step workflows. Tasks resemble production API orchestration, emphasizing robustness and correct sequencing rather than single-shot Q&A.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Language Comprehension, Language Production (minor)"
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a difficult multimodal benchmark designed to sit near the frontier of expert knowledge and reasoning, including questions that often benefit from tools like search or code execution. It emphasizes high-level synthesis, cross-domain reasoning, and careful handling of uncertainty in complex prompts.","Logical Reasoning, Semantic Understanding & Context Recognition, Language Comprehension, Language Production, Multisensory Integration (minor), Scene Understanding & Visual Reasoning (minor), Planning (minor)"
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 evaluates competition-style mathematics problem solving, typically requiring multi-step derivations and careful symbolic manipulation. It measures correctness on short-answer problems where solution paths are nontrivial and prone to algebraic or logical slips.","Logical Reasoning, Working Memory, Adaptive Error Correction (minor), Planning (minor), Attention (minor)"
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,GPQA Diamond is a high-quality subset of GPQA consisting of challenging graduate-level multiple-choice questions in science designed to be resistant to superficial pattern matching. It probes deep domain understanding and reasoning under distractors where non-experts typically fail.,"Semantic Understanding & Context Recognition, Logical Reasoning, Language Comprehension, Working Memory (minor)"
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends broad academic evaluation across many subjects and multiple non-English languages, stressing multilingual comprehension and knowledge transfer. It is primarily a large-scale measure of text-based understanding and reasoning across diverse domains and languages.","Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning (minor), Language Production (minor)"
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro is a harder multimodal understanding benchmark spanning many disciplines, where models answer questions grounded in images (e.g., diagrams, charts, scenes) alongside text. It emphasizes interpreting visual evidence, integrating it with domain knowledge, and selecting correct answers under complex visual contexts.","Visual Perception, Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Visual Attention & Eye Movements (minor), Language Comprehension"
ScreenShot-Pro,https://gui-agent.github.io/grounding-leaderboard/,https://arxiv.org/abs/2504.07981,"ScreenShot-Pro evaluates UI screenshot understanding and grounding by asking models to interpret high-resolution software interfaces and answer questions that depend on layout, labels, and widget states. It targets practical visual-spatial reasoning over GUIs, including locating information and mapping instructions to screen regions.","Visual Perception, Scene Understanding & Visual Reasoning, Spatial Representation & Mapping, Visual Attention & Eye Movements, Semantic Understanding & Context Recognition (minor), Language Comprehension (minor)"
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning focuses on reasoning over scientific-paper figures and visual elements, often requiring extracting quantitative/structural information from charts or diagrams. It stresses visual-to-symbolic translation and multi-step inference grounded in figure content rather than pure text recall.","Visual Perception, Scene Understanding & Visual Reasoning, Logical Reasoning, Working Memory (minor), Language Comprehension (minor)"
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU extends multimodal understanding to video, requiring temporal integration of events, actions, and context across frames to answer questions. It evaluates whether models can maintain coherence over time and reason about dynamic scenes rather than static images.","Visual Perception, Cognitive Timing & Predictive Modeling, Working Memory, Scene Understanding & Visual Reasoning, Multisensory Integration (minor), Auditory Processing (minor), Attention (minor)"
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) is a long-context co-reference/retrieval-style evaluation where multiple similar “needle” interactions are embedded in a long “haystack,” and the model must reproduce the response corresponding to a specified needle. It measures robust information access, interference resistance, and accurate tracking across lengthy contexts.","Working Memory, Attention, Episodic Memory, Language Comprehension, Inhibitory Control (minor)"
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval evaluates economically meaningful knowledge-work tasks by comparing model-produced work artifacts (e.g., analyses, plans, presentations/spreadsheets) against industry professionals using expert judging. It targets end-to-end goal fulfillment under well-specified professional constraints, including producing usable deliverables.","Planning, Decision-making, Language Production, Semantic Understanding & Context Recognition, Self-reflection (minor), Adaptive Error Correction (minor)"
SWE-Lancer,https://openai.com/index/swe-lancer/,https://arxiv.org/abs/2502.12115,"SWE-Lancer evaluates software engineering in a setting that emphasizes higher-level engineering workflows and longer-horizon task completion beyond isolated bug fixes, including navigating repositories and producing patches that satisfy realistic constraints. It is intended to better capture agentic engineering competence and reliability on complex tasks.","Planning, Adaptive Error Correction, Logical Reasoning, Working Memory, Decision-making (minor), Language Comprehension"
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks tests whether models can follow structured graph traversal instructions (e.g., BFS-like walks) and answer queries that depend on consistent navigation through a graph-structured input. It emphasizes precise state tracking and systematic reasoning over relational structure rather than open-ended text generation.","Spatial Representation & Mapping, Working Memory, Logical Reasoning, Attention (minor), Planning (minor)"
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon evaluates tool-using agents on tasks that require selecting, sequencing, and validating calls across multiple tools and APIs, often with intermediate reasoning and error recovery. It stresses reliability under tool interface constraints and the ability to coordinate multi-step workflows to reach a final objective.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Language Comprehension (minor), Inhibitory Control (minor)"
HMMT,https://matharena.ai/?view=problem&comp=hmmt--hmmt_feb_2025,https://arxiv.org/abs/2505.23281,"HMMT (Harvard-MIT Mathematics Tournament) problems in this setting evaluate high-difficulty competition math with longer, more creative solution paths than typical standardized items. The benchmark probes deep multi-step reasoning, strategic decomposition, and precision under complex constraints.","Logical Reasoning, Planning, Working Memory, Adaptive Error Correction (minor), Attention (minor)"
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath targets expert-level mathematics, including problems intended to be difficult for both models and humans, and is often evaluated with tool assistance (e.g., Python) for verification and computation. It emphasizes deep reasoning, nontrivial problem decomposition, and robustness to subtle mathematical traps.","Logical Reasoning, Planning, Working Memory, Adaptive Error Correction (minor), Attention (minor)"
