Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates an agent’s ability to solve real GitHub software engineering issues by producing patches that pass a repository’s tests, using a curated set of human-verified solvable tasks. It emphasizes end-to-end debugging and code editing under realistic repository constraints rather than isolated coding questions.","L1: 
L2: Planning, Logical Reasoning, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition, Decision-making (minor)
L3: ",L2
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro is a more difficult, broader software engineering benchmark designed to be more contamination-resistant and industrially relevant, spanning multiple programming languages and harder real-world repo issues. It stresses robust patch generation, tool-driven iteration, and navigating larger codebases with more complex test suites.","L1: 
L2: Planning, Adaptive Error Correction, Logical Reasoning, Working Memory, Semantic Understanding & Context Recognition, Decision-making
L3: ",L2
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp evaluates deep research and browsing agents on questions that require multi-step web information gathering and synthesis rather than recalling memorized facts. Typical solutions demand searching, triangulating sources, and composing a final answer grounded in retrieved evidence.","L1: Language Production (minor)
L2: Planning, Decision-making, Attention, Episodic Memory, Semantic Understanding & Context Recognition
L3: ",L2
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench measures an agent’s ability to complete multi-turn customer-support-style tasks by interacting with simulated users and calling APIs while adhering to domain policies (e.g., retail, airline, telecom). It emphasizes policy-consistent tool use, long-horizon dialogue coherence, and correct action sequencing under constraints.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making
L3: Inhibitory Control, Social Reasoning & Theory of Mind (minor)",L3
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI tests “fluid” reasoning via novel grid-based pattern transformation puzzles where models infer the rule from a few examples and generalize to a new input. It is designed to reduce reliance on memorized knowledge, emphasizing abstraction, compositional rules, and robust generalization under few-shot conditions.","L1: Visual Perception (minor)
L2: Logical Reasoning, Working Memory, Spatial Representation & Mapping
L3: Cognitive Flexibility",L3
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas evaluates real-world tool use through the Model Context Protocol (MCP), where models must discover and invoke tools across multi-step workflows, handle errors, and integrate tool outputs into answers. It stresses operational correctness in API/tool calling and cross-tool coordination more than pure language generation.","L1: Language Comprehension (minor)
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition
L3: ",L2
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam (HLE) is a multimodal benchmark intended to probe frontier-level academic reasoning and knowledge across a wide range of difficult questions. It often benefits from tool use (e.g., search, code) and tests whether models can synthesize, verify, and explain answers rather than merely recall facts.","L1: Language Comprehension, Visual Perception (minor)
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Planning (minor)
L3: ",L2
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 consists of competition-style math problems that require multi-step symbolic reasoning and precise final numeric answers. It emphasizes structured problem solving, maintaining intermediate states, and avoiding small arithmetic or algebraic errors.","L1: 
L2: Logical Reasoning, Working Memory, Planning (minor), Adaptive Error Correction (minor)
L3: ",L2
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a high-difficulty, multiple-choice science benchmark designed to be “Google-proof,” focusing on questions that require deep understanding and careful reasoning rather than superficial recall. The Diamond subset emphasizes quality and difficulty, often demanding multi-step inference across scientific concepts.","L1: Language Comprehension (minor)
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory (minor)
L3: ",L2
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,MMMLU is a multilingual extension of MMLU that tests broad academic knowledge and reasoning across many subjects and multiple languages. It probes whether models can transfer competency across languages while preserving subject-matter reasoning quality.,"L1: Language Comprehension, Language Production (minor)
L2: Semantic Understanding & Context Recognition, Logical Reasoning (minor)
L3: ",L2
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro is a challenging multimodal benchmark covering many disciplines where models answer questions grounded in images (diagrams, plots, figures) alongside text. It emphasizes integrating visual evidence with domain knowledge and reasoning under complex question formats.","L1: Visual Perception, Language Comprehension (minor)
L2: Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning
L3: ",L2
ScreenShot-Pro,https://gui-agent.github.io/grounding-leaderboard/,https://arxiv.org/abs/2504.07981,"ScreenShot-Pro evaluates GUI understanding and grounding from high-resolution screenshots, requiring models to interpret interface layouts and identify relevant elements to answer questions or support interaction. It targets spatially grounded perception of UI components and reliable mapping from language instructions to on-screen affordances.","L1: Visual Perception
L2: Visual Attention & Eye Movements, Spatial Representation & Mapping, Scene Understanding & Visual Reasoning, Sensorimotor Coordination (minor), Planning (minor)
L3: ",L2
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning tests whether models can answer reasoning questions grounded in scientific documents, especially figures, charts, and technical content from papers. It emphasizes faithful interpretation of visual-scientific artifacts and multi-step inference that connects captions, text context, and plotted data.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory (minor)
L3: ",L2
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU evaluates multimodal understanding and reasoning over short videos, requiring models to integrate information across time (and sometimes audio/text cues) to answer questions. It stresses temporal integration, event understanding, and maintaining coherent representations across multiple frames.","L1: Visual Perception, Auditory Processing (minor)
L2: Attention, Episodic Memory, Multisensory Integration (minor)
L3: Cognitive Timing & Predictive Modeling",L3
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) is a long-context evaluation where multiple similar “needle” interactions are embedded within large “haystack” contexts, and the model must recover the correct response corresponding to a specific needle. It targets robust long-range retrieval, coreference-like tracking, and resistance to distractors in extended contexts.","L1: 
L2: Working Memory, Attention, Episodic Memory, Semantic Understanding & Context Recognition
L3: Inhibitory Control (minor)",L2
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval is a human-judged evaluation of economically valuable, well-specified knowledge work across many occupations, where models must produce professional artifacts (e.g., slides, spreadsheets, plans) rather than short answers. It stresses end-to-end project execution quality, adherence to constraints, and iterative refinement toward a usable deliverable.","L1: Language Production
L2: Planning, Decision-making, Semantic Understanding & Context Recognition, Adaptive Error Correction (minor)
L3: Self-reflection (minor)",L2
SWE-Lancer,https://openai.com/index/swe-lancer/,https://arxiv.org/abs/2502.12115,"SWE-Lancer evaluates software engineering performance in longer-horizon, more realistic “work-like” settings (e.g., implementing changes across repositories with practical constraints), often aiming to reflect economic value and reliability rather than toy problems. It emphasizes producing correct, maintainable patches and navigating ambiguous requirements under time and context constraints.","L1: 
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition, Logical Reasoning (minor)
L3: ",L2
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks measures a model’s ability to perform algorithmic reasoning over graphs described in text, such as traversals and parent/neighbor queries across many steps. It stresses consistent state tracking across long sequences and faithful execution of discrete traversal rules.","L1: 
L2: Working Memory, Spatial Representation & Mapping, Logical Reasoning, Attention (minor)
L3: ",L2
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon evaluates general tool-use competence across diverse tool APIs and multi-step tasks, emphasizing selecting appropriate tools, calling them correctly, and integrating outputs into the solution. It focuses on reliability under tool errors, composition of multiple tools, and robust orchestration rather than raw factual recall.","L1: 
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition (minor)
L3: ",L2
HMMT,https://matharena.ai/?view=problem&comp=hmmt--hmmt_feb_2025,https://arxiv.org/abs/2505.23281,"HMMT is a competition mathematics benchmark (e.g., Harvard-MIT Math Tournament sets) featuring problems that often require deeper multi-step reasoning than typical standardized tests. It emphasizes constructing solution strategies, managing intermediate lemmas, and maintaining precision under complex algebraic/combinatorial manipulations.","L1: 
L2: Logical Reasoning, Working Memory, Planning (minor), Adaptive Error Correction (minor)
L3: ",L2
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath is an expert-level mathematics evaluation designed to be highly challenging and more resistant to memorization, often requiring sustained reasoning and (in some settings) tool-assisted computation. It targets the ability to make progress on novel, research-adjacent math problems with careful logical control and verification.","L1: 
L2: Logical Reasoning, Working Memory, Planning, Adaptive Error Correction (minor)
L3: Cognitive Flexibility (minor)",L2
