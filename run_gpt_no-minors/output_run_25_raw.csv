Benchmark,Website,Paper,Description,Cognitive Functions
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates an agent’s ability to fix real issues in open-source GitHub repositories by generating code patches that pass the project’s tests. The “Verified” subset consists of tasks that have been validated as solvable and reliably graded, emphasizing end-to-end debugging rather than toy coding puzzles.","Planning, Logical Reasoning, Working Memory, Adaptive Error Correction, Semantic Understanding & Context Recognition (minor), Language Comprehension (minor), Language Production (minor)"
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro is a harder software engineering benchmark with more tasks and stronger emphasis on real-world engineering complexity (e.g., broader codebases, more varied requirements, and more demanding evaluation). It targets agentic development skills such as diagnosing failures, implementing correct fixes, and avoiding regressions across a rigorous suite.","Planning, Logical Reasoning, Working Memory, Adaptive Error Correction, Decision-making (minor), Language Comprehension (minor), Language Production (minor)"
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp evaluates “deep research” style browsing agents on questions that require multi-step web search, reading, cross-checking, and synthesis rather than recalling a single fact. Performance depends on navigating information sources, selecting relevant evidence, and maintaining a coherent research plan across tool calls.","Planning, Attention, Working Memory, Semantic Understanding & Context Recognition, Decision-making (minor), Language Comprehension (minor), Self-reflection (minor)"
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench measures an agent’s ability to complete multi-turn customer-support tasks in simulated domains (e.g., retail, airline, telecom) while using APIs and following policies. It emphasizes robust tool use under constraints, consistency across turns, and policy-compliant decision-making under realistic interaction dynamics.","Decision-making, Planning, Inhibitory Control, Social Reasoning & Theory of Mind, Language Comprehension, Language Production, Working Memory (minor)"
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI tests fluid intelligence by requiring models to infer latent transformation rules from a few input–output grid examples and apply them to new inputs. Tasks are designed to be novel and compositional, stressing generalization, abstraction, and systematic reasoning on spatial patterns.","Cognitive Flexibility, Logical Reasoning, Spatial Representation & Mapping, Visual Perception, Working Memory, Planning (minor), Attention (minor)"
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas evaluates real-world tool use through the Model Context Protocol by testing whether models can discover appropriate tools, call them correctly, handle errors, and compose multi-step workflows across servers. It emphasizes practical agent reliability in production-like API environments rather than single-shot Q&A.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Language Comprehension (minor), Language Production (minor)"
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam (HLE) is a broad, frontier difficulty benchmark spanning complex questions across many domains, often requiring deep reasoning, synthesis, and sometimes multimodal interpretation. It is intended to stress near-boundary capabilities on challenging, knowledge-intensive and reasoning-intensive items.","Language Comprehension, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory (minor), Scene Understanding & Visual Reasoning (minor), Visual Perception (minor)"
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 evaluates competition-style mathematics problem solving, typically requiring multi-step symbolic reasoning and careful constraint handling. It targets precise derivations and error-free arithmetic/algebra under tight problem statements.","Logical Reasoning, Working Memory, Planning, Adaptive Error Correction (minor), Attention (minor)"
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a high-difficulty multiple-choice science QA benchmark curated to be challenging for non-experts and resistant to simple web lookup. It stresses rigorous reasoning over complex scientific content across physics, chemistry, and biology.","Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory (minor), Language Comprehension (minor)"
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends MMLU-style knowledge and reasoning evaluation into multiple languages, testing performance across many subjects and diverse linguistic contexts. It probes multilingual understanding and the ability to apply learned knowledge consistently across languages.","Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning (minor), Language Production (minor), Working Memory (minor)"
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,MMMU-Pro is a multimodal benchmark that evaluates expert-level understanding and reasoning over images paired with text across many disciplines. It emphasizes integrating visual evidence with domain knowledge to answer multiple-choice or structured questions.,"Visual Perception, Scene Understanding & Visual Reasoning, Multisensory Integration, Language Comprehension, Working Memory (minor)"
ScreenShot-Pro,https://gui-agent.github.io/grounding-leaderboard/,https://arxiv.org/abs/2504.07981,"ScreenShot-Pro evaluates GUI and screenshot understanding, where models must interpret high-resolution interfaces and answer questions or take actions grounded in on-screen elements. It targets reliable visual grounding, layout reasoning, and selecting correct targets in complex UIs.","Visual Perception, Visual Attention & Eye Movements, Spatial Representation & Mapping, Scene Understanding & Visual Reasoning, Sensorimotor Coordination (minor), Planning (minor)"
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning tests scientific-figure and paper-content reasoning using arXiv-style figures and associated questions, often requiring quantitative and diagrammatic interpretation. It emphasizes extracting structured information from visual scientific artifacts and integrating it with textual context.","Scene Understanding & Visual Reasoning, Visual Perception, Semantic Understanding & Context Recognition, Logical Reasoning, Language Comprehension (minor)"
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU evaluates multimodal understanding over videos, requiring temporal integration to answer questions about events, actions, and context. It stresses tracking changing scenes and maintaining coherence across frames rather than single-image interpretation.","Visual Perception, Scene Understanding & Visual Reasoning, Working Memory, Cognitive Timing & Predictive Modeling, Attention (minor), Multisensory Integration (minor)"
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,MRCR v2 (8-needle) evaluates long-context multi-round coreference resolution by embedding multiple similar “needle” interactions inside long “haystacks” and asking the model to retrieve the correct referenced response. It primarily measures robust retrieval and disambiguation under heavy context interference.,"Working Memory, Episodic Memory, Attention, Language Comprehension, Semantic Understanding & Context Recognition (minor)"
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval measures performance on well-specified professional knowledge-work tasks across many occupations, judged by expert evaluators on output quality (often including artifacts like documents, spreadsheets, or plans). It targets end-to-end task execution quality under realistic workplace specifications.","Planning, Decision-making, Language Production, Semantic Understanding & Context Recognition, Self-reflection (minor), Working Memory (minor)"
SWE-Lancer,https://openai.com/index/swe-lancer/,https://arxiv.org/abs/2502.12115,"SWE-Lancer evaluates agentic software engineering in a more “job-like” setting, emphasizing longer-horizon execution and realistic constraints for completing engineering tasks end-to-end. It targets the ability to understand a codebase, implement correct changes, and iteratively resolve failures in a production-like workflow.","Planning, Adaptive Error Correction, Logical Reasoning, Working Memory, Decision-making (minor), Language Comprehension (minor), Language Production (minor)"
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks evaluates long-context reasoning over graph-structured data, such as following edges, recovering paths, or answering queries that require consistent traversal and bookkeeping. It stresses systematic multi-step reasoning where small errors compound across long dependency chains.","Spatial Representation & Mapping, Working Memory, Logical Reasoning, Attention (minor), Planning (minor)"
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon benchmarks an agent’s ability to use many tools/APIs across heterogeneous tasks, emphasizing correct tool selection, parameterization, and integration of results into final answers. It is designed to probe robustness of tool-use policies under multi-step workflows and partial failures.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Language Comprehension (minor), Language Production (minor)"
HMMT,https://matharena.ai/?view=problem&comp=hmmt--hmmt_feb_2025,https://arxiv.org/abs/2505.23281,"HMMT (as used in MathArena) evaluates high-difficulty contest mathematics problems that often require creative problem decomposition, proof-style reasoning, and careful symbolic manipulation. It is generally harder and more varied than standard competition sets, stressing reliability under complexity.","Logical Reasoning, Planning, Working Memory, Cognitive Flexibility (minor), Attention (minor)"
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath is an expert-level mathematics evaluation intended to measure progress on problems beyond routine contest math, including more research-adjacent and technically demanding items. It stresses deep multi-step reasoning, precision, and the ability to manage complex intermediate states over long solutions.","Logical Reasoning, Planning, Working Memory, Cognitive Flexibility (minor), Adaptive Error Correction (minor)"
