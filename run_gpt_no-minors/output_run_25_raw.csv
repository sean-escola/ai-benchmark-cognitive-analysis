Benchmark,Website,Paper,Description,Cognitive Functions
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 evaluates agentic coding and systems ability by placing models in real command-line environments where they must accomplish tasks via shell commands, file edits, and program execution. It emphasizes end-to-end problem solving under realistic tooling, including debugging, dependency management, and iterative refinement.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Language Comprehension (minor)"
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp measures deep research and browsing competence: models must search, read, and synthesize information from documents to answer questions that typically require multi-step retrieval and verification. The benchmark stresses source-grounded answering and the ability to manage context while navigating large information spaces.","Planning, Attention, Episodic Memory, Semantic Understanding & Context Recognition, Language Production (minor)"
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld evaluates computer-use agents in operating-system-like desktop environments, requiring navigation of GUIs and execution of multi-step tasks (e.g., using apps, settings, files). Success depends on interpreting screenshots/DOM-like signals and reliably translating intent into sequences of actions under step limits.","Visual Perception, Visual Attention & Eye Movements, Sensorimotor Coordination, Planning, Working Memory"
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,ARC-AGI (Abstraction and Reasoning Corpus) tests fluid reasoning by asking models to infer latent rules from a few input-output grid examples and apply them to new inputs. It aims to minimize reliance on memorized knowledge and instead measure compositional generalization and abstraction.,"Logical Reasoning, Cognitive Flexibility, Working Memory, Spatial Representation & Mapping, Visual Perception (minor)"
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon autonomous agent behavior in a simulated vending-machine business over extended time (e.g., a year of decisions). Models must manage inventory, pricing, supplier interactions, and budgeting, rewarding sustained coherence and strategic adaptation.","Planning, Decision-making, Reward Mechanisms, Working Memory, Cognitive Timing & Predictive Modeling (minor)"
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity agents on tasks such as locating known vulnerabilities in real projects and discovering new ones under realistic codebase constraints. It stresses reasoning about programs, exploiting or validating weaknesses, and producing correct fixes or proof-of-vulnerability outputs.","Logical Reasoning, Planning, Adaptive Error Correction, Working Memory, Attention (minor)"
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"SpreadsheetBench measures an agent’s ability to understand, edit, and compute within complex spreadsheets based on realistic tasks (e.g., cleaning data, building formulas, restructuring sheets). It tests both semantic understanding of tabular layouts and precise multi-step tool operations.","Semantic Understanding & Context Recognition, Working Memory, Planning, Attention, Adaptive Error Correction (minor)"
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a frontier, multimodal benchmark designed to probe difficult questions spanning advanced academic and real-world domains. Items often require multi-step reasoning and cross-referencing information across text and images, making it sensitive to both reasoning and perceptual robustness.","Language Comprehension, Logical Reasoning, Scene Understanding & Visual Reasoning, Visual Perception (minor), Working Memory (minor)"
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a high-difficulty subset of GPQA consisting of graduate-level, multiple-choice science questions designed to be resistant to superficial lookup. It primarily measures scientific reasoning and precise understanding of technical language under strong distractors.","Logical Reasoning, Language Comprehension, Working Memory, Semantic Understanding & Context Recognition (minor)"
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro is a challenging multimodal benchmark emphasizing expert-level reasoning over images and text across many disciplines. It stresses interpreting diagrams, charts, and visual scenes while applying domain knowledge and careful elimination among answer choices.","Scene Understanding & Visual Reasoning, Visual Perception, Language Comprehension, Logical Reasoning, Visual Attention & Eye Movements (minor)"
MathArena Apex,https://matharena.ai/?view=problem&comp=apex--apex_2025,https://arxiv.org/abs/2505.23281,"MathArena Apex is a competitive mathematics evaluation focused on difficult, multi-step problems where small mistakes cascade. It is designed to measure reliable mathematical reasoning, not just pattern completion, and is often used to compare “thinking” settings and tool-assisted variants.","Logical Reasoning, Working Memory, Planning, Adaptive Error Correction (minor)"
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning evaluates reasoning over scientific figures and content from arXiv-style papers, requiring models to extract and manipulate information from charts/plots and accompanying text. It emphasizes visually grounded scientific inference and quantitative interpretation.","Scene Understanding & Visual Reasoning, Visual Perception, Logical Reasoning, Semantic Understanding & Context Recognition, Attention (minor)"
OmniDocBench 1.5,https://github.com/opendatalab/OmniDocBench,https://arxiv.org/abs/2412.07626,"OmniDocBench 1.5 evaluates document understanding and OCR across diverse layouts such as text, tables, formulas, and reading order. It targets robust extraction and structural interpretation rather than plain-text transcription alone.","Visual Perception, Scene Understanding & Visual Reasoning, Attention, Language Comprehension (minor), Working Memory (minor)"
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU extends multimodal evaluation to video, requiring temporal understanding of events, actions, and context to answer questions. It probes whether models can integrate information across frames and maintain coherent situation models over time.","Visual Perception, Working Memory, Attention, Cognitive Timing & Predictive Modeling, Scene Understanding & Visual Reasoning (minor)"
LiveCodeBench Pro,https://livecodebenchpro.com/projects/livecodebench-pro/leaderboard,https://arxiv.org/abs/2506.11928,"LiveCodeBench Pro evaluates coding ability on programming tasks curated to better reflect contemporary, contamination-resistant assessment and practical problem-solving. It typically measures correctness under a single-attempt or controlled-evaluation setting and supports leaderboard-style comparison.","Logical Reasoning, Planning, Adaptive Error Correction, Working Memory, Language Production (minor)"
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"The FACTS Benchmark Suite systematically evaluates factuality by testing whether a model’s generated statements stay consistent with trusted references or provided evidence. It targets hallucination, citation faithfulness, and the ability to avoid asserting unsupported claims under pressure to be helpful.","Semantic Understanding & Context Recognition, Inhibitory Control, Self-reflection (minor), Language Production (minor)"
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA evaluates commonsense physical reasoning across many languages and locales, aiming to reduce English- and culture-specific artifacts. It tests whether models can choose plausible actions or explanations grounded in everyday physical interactions.","Logical Reasoning, Semantic Understanding & Context Recognition, Language Comprehension, Social Reasoning & Theory of Mind (minor)"
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,MRCR v2 (8-needle) evaluates long-context multi-round coreference and retrieval by embedding multiple similar “needle” requests inside long “haystack” dialogues and asking the model to reproduce the correct referenced response. It stresses robust attention control and resistance to confusion among near-duplicates across very long contexts.,"Working Memory, Attention, Semantic Understanding & Context Recognition, Episodic Memory (minor)"
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval evaluates economically valuable knowledge work by having models produce real professional artifacts (e.g., presentations, spreadsheets, schedules) across many occupations, often judged in head-to-head comparisons against human outputs. It emphasizes end-to-end task execution quality, formatting, and practical decision usefulness under realistic constraints.","Planning, Decision-making, Language Production, Semantic Understanding & Context Recognition, Self-reflection (minor)"
SWE-Lancer,https://openai.com/index/swe-lancer/,https://arxiv.org/abs/2502.12115,"SWE-Lancer evaluates software engineering in a patch-generation setting, focusing on realistic coding work such as implementing features, fixing bugs, and modifying repositories according to task specifications. It aims to reflect agentic development workflows where correctness is verified via tests and repo state changes.","Planning, Logical Reasoning, Adaptive Error Correction, Working Memory, Decision-making (minor)"
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks evaluates reasoning over graph-structured data embedded in long contexts, requiring models to follow paths, compute relationships (e.g., BFS-style reachability), or track parent/neighbor relations. It probes whether models can perform consistent, multi-hop symbolic traversal rather than relying on shallow pattern matching.","Spatial Representation & Mapping, Working Memory, Attention, Logical Reasoning"
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon evaluates tool-using agents on tasks that require selecting, invoking, and composing multiple tools/APIs to reach correct outcomes under realistic error modes. It emphasizes reliable orchestration—knowing when to call tools, how to interpret outputs, and how to recover when tool results are incomplete or noisy.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition (minor)"
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath evaluates advanced mathematics at or near the research frontier, with problems intended to be difficult for strong models and often requiring tool-assisted computation or careful proof-style reasoning. It is designed to be more resistant to memorization and to highlight genuine improvements in deep mathematical competence.","Logical Reasoning, Working Memory, Planning, Cognitive Flexibility (minor)"
