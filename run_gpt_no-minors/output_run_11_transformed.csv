Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates software engineering agents on real GitHub issues by requiring them to generate a patch that makes the repository’s tests pass, with tasks filtered/validated to be solvable and reliably graded. It emphasizes end-to-end debugging and code modification under realistic project constraints rather than isolated coding puzzles.","L1: Language Comprehension
L2: Planning, Logical Reasoning, Working Memory, Adaptive Error Correction, Decision-making (minor)
L3: ",L2
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro is a more difficult, more contamination-resistant successor-style benchmark that expands beyond Python and increases task diversity and realism for professional software engineering. Models must diagnose issues, navigate larger codebases, and produce correct multi-file fixes that satisfy automated evaluation criteria.","L1: Language Comprehension
L2: Planning, Logical Reasoning, Working Memory, Adaptive Error Correction
L3: Cognitive Flexibility (minor)",L2
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp evaluates “deep research” behavior where a model must use browsing/search to answer hard questions that require finding and synthesizing information rather than recalling it. It stresses query formulation, evidence selection, and coherent final reporting under tool and time constraints.","L1: Language Production (minor)
L2: Planning, Decision-making, Attention, Semantic Understanding & Context Recognition, Working Memory
L3: ",L2
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench evaluates interactive agent behavior in simulated customer-support domains (e.g., retail/airline/telecom) where the agent must use tools/APIs while following explicit policies. Success requires managing multi-turn dialogue, executing correct tool actions, and maintaining consistent policy compliance across a conversation.","L1: Language Comprehension
L2: Planning, Decision-making
L3: Social Reasoning & Theory of Mind, Inhibitory Control, Empathy (minor)",L3
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,ARC-AGI tests few-shot abstract reasoning on novel grid transformation puzzles where models infer hidden rules from a small number of examples and apply them to new inputs. It is designed to reduce reliance on memorization and to emphasize flexible induction and compositional generalization.,"L1: 
L2: Logical Reasoning, Working Memory, Spatial Representation & Mapping, Attention (minor)
L3: Cognitive Flexibility",L3
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas measures real-world tool-use via Model Context Protocol servers, requiring models to discover, call, and chain tools correctly across multi-step workflows. It emphasizes reliable API interaction (arguments, schemas, error handling) and synthesis of tool outputs into correct final answers.","L1: Language Comprehension (minor)
L2: Planning, Decision-making, Working Memory, Adaptive Error Correction
L3: ",L2
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a high-difficulty, broad-coverage benchmark intended to probe frontier knowledge and reasoning across many domains, often with multimodal elements. Questions are designed to be challenging for models without robust reasoning, tool use, and careful synthesis.","L1: Language Comprehension
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory (minor), Multisensory Integration (minor), Scene Understanding & Visual Reasoning (minor)
L3: ",L2
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,AIME 2025 consists of competition mathematics problems that require multi-step symbolic reasoning and precise final numeric answers. It is commonly used to measure mathematical problem-solving reliability under strict correctness criteria.,"L1: 
L2: Logical Reasoning, Working Memory, Planning (minor), Adaptive Error Correction (minor)
L3: ",L2
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a carefully curated subset of graduate-level, Google-resistant multiple-choice science questions with high-quality filtering to ensure difficulty and discriminative power. It evaluates deep scientific reasoning and domain knowledge rather than surface pattern matching.","L1: Language Comprehension (minor)
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory (minor)
L3: ",L2
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends MMLU-style academic testing to many non-English languages, measuring multilingual knowledge and reasoning across a broad set of subjects. It focuses on whether models preserve competence when comprehension and context must be maintained across languages.","L1: Language Comprehension, Language Production (minor)
L2: Semantic Understanding & Context Recognition, Logical Reasoning (minor)
L3: ",L2
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro is a professionalized variant of multimodal understanding benchmarks where models answer questions that require interpreting images (charts, diagrams, documents) in combination with text. It emphasizes expert-level visual reasoning and cross-modal grounding under more demanding settings than earlier MMMU versions.","L1: Visual Perception, Language Comprehension (minor)
L2: Scene Understanding & Visual Reasoning, Multisensory Integration, Attention (minor)
L3: ",L2
ScreenShot-Pro,https://gui-agent.github.io/grounding-leaderboard/,https://arxiv.org/abs/2504.07981,"ScreenShot-Pro evaluates understanding of high-resolution UI screenshots (and related interface-grounding tasks), often requiring identification of elements, spatial relationships, and correct interpretation of on-screen state. It targets reliable visual grounding in real software interfaces rather than generic image captioning.","L1: Visual Perception, Language Production (minor)
L2: Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Spatial Representation & Mapping (minor)
L3: ",L2
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning tests reasoning over scientific figures from papers, requiring models to extract information from charts/plots and answer questions that depend on correct interpretation of visual evidence. It stresses faithful visual-to-text grounding and multi-step inference from figure content.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Logical Reasoning, Semantic Understanding & Context Recognition (minor), Working Memory (minor)
L3: ",L2
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU extends multimodal understanding to video, requiring models to integrate information across frames and time to answer questions about events and context. It probes temporal understanding (what changed, when) alongside standard visual reasoning.","L1: Visual Perception
L2: Working Memory, Scene Understanding & Visual Reasoning, Multisensory Integration (minor)
L3: Cognitive Timing & Predictive Modeling",L3
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,MRCR v2 (8-needle) evaluates long-context multi-round coreference resolution by embedding multiple similar “needle” interactions into long “haystacks” and asking the model to retrieve the correct referenced response. It primarily measures robust retrieval under interference and distractor-heavy contexts as input length grows.,"L1: 
L2: Working Memory, Attention, Episodic Memory
L3: Inhibitory Control (minor)",L2
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval measures performance on well-specified, economically relevant knowledge-work tasks across many occupations, judged by expert human evaluators via comparisons to professional outputs. Tasks often require producing real artifacts (e.g., spreadsheets, plans, writeups) with correct structure, constraints, and justification.","L1: Language Production
L2: Planning, Decision-making, Semantic Understanding & Context Recognition
L3: Self-reflection (minor), Social Reasoning & Theory of Mind (minor)",L2
SWE-Lancer,https://openai.com/index/swe-lancer/,https://arxiv.org/abs/2502.12115,"SWE-Lancer evaluates advanced software engineering performance on more realistic, end-to-end development and maintenance scenarios, emphasizing robustness beyond short, single-issue patches. It aims to capture agentic coding competence under broader project constraints and more complex task distributions.","L1: 
L2: Planning, Adaptive Error Correction, Logical Reasoning, Working Memory, Decision-making (minor)
L3: ",L2
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks evaluates long-context reasoning over graph-structured data (e.g., following edges/relations) with tasks that require multi-step traversal and consistency across many nodes. It targets systematic compositional reasoning and resistance to losing track of state during extended multi-hop inference.","L1: 
L2: Spatial Representation & Mapping, Working Memory, Logical Reasoning, Attention (minor)
L3: ",L2
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon measures an agent’s ability to solve tasks by selecting and composing tools across multi-step workflows, emphasizing correct tool choice, argument formation, and recovery from tool errors. It is intended to capture practical agentic competence where tool results must be interpreted and integrated into decisions.","L1: 
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory
L3: Inhibitory Control (minor)",L2
HMMT,https://matharena.ai/?view=problem&comp=hmmt--hmmt_feb_2025,https://arxiv.org/abs/2505.23281,"HMMT (as used in MathArena) consists of high-difficulty competition math problems (e.g., Harvard-MIT Math Tournament sets) designed to stress deep multi-step reasoning. It is commonly used to measure robustness on harder-than-AIME olympiad-style problem solving.","L1: 
L2: Logical Reasoning, Working Memory, Planning (minor)
L3: Cognitive Flexibility (minor)",L2
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath is an expert-level mathematics benchmark designed to probe the frontier of formal and semi-formal problem-solving ability, with tiered difficulty and an emphasis on problems that are hard to brute-force via memorization. It targets sustained, precise reasoning, often benefiting from verification-oriented approaches.","L1: 
L2: Logical Reasoning, Working Memory, Planning, Adaptive Error Correction (minor)
L3: Cognitive Flexibility (minor)",L2
