Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 evaluates agentic programming in real command-line environments, where models must navigate repositories, run commands, inspect outputs, and apply fixes under realistic constraints. Success depends on reliably chaining tool calls over multiple steps and recovering from environment or execution errors.","L1: Language Comprehension (minor)
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory
L3: ",L2
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp measures “deep research” performance over a controlled document collection, requiring multi-step information seeking and synthesis rather than a single lookup. Agents must decide what to search for, integrate evidence across sources, and produce a final grounded answer.","L1: 
L2: Planning, Semantic Understanding & Context Recognition, Working Memory, Decision-making, Adaptive Error Correction (minor)
L3: ",L2
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld tests multimodal computer-use agents in a full operating-system setting, where tasks involve interacting with GUIs, files, browsers, and applications. Models must interpret screenshots, choose UI actions, and maintain task state over long horizons to reach a goal.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Sensorimotor Coordination, Planning, Decision-making, Attention (minor)
L3: ",L2
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI evaluates abstraction and fluid reasoning from a few input–output grid examples, where the model must infer the hidden rule and generate the correct output for a new grid. It emphasizes novel pattern induction and systematic generalization rather than memorized knowledge.","L1: Visual Perception (minor)
L2: Logical Reasoning, Working Memory, Spatial Representation & Mapping
L3: Cognitive Flexibility",L3
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 measures long-horizon autonomy by having an agent run a simulated vending-machine business over many turns, managing inventory, pricing, suppliers, and cash flow. It rewards coherent strategy and adaptation across extended time, not just single-step correctness.","L1: 
L2: Planning, Decision-making, Reward Mechanisms, Working Memory, Adaptive Error Correction (minor)
L3: ",L2
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity agents on tasks like finding known vulnerabilities in real software projects and discovering new ones, often requiring code navigation, hypothesis testing, and exploit reasoning. It emphasizes reliable technical problem solving under adversarially flavored constraints and partial information.","L1: 
L2: Logical Reasoning, Planning, Adaptive Error Correction, Semantic Understanding & Context Recognition, Working Memory (minor)
L3: ",L2
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"SpreadsheetBench tests spreadsheet manipulation and analysis workflows (e.g., editing cells, formulas, tables, and producing correct artifacts) that resemble office productivity tasks. Models must track constraints, perform structured transformations, and validate outputs across many dependent cells.","L1: 
L2: Working Memory, Planning, Logical Reasoning, Adaptive Error Correction, Attention (minor)
L3: ",L2
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a broad, frontier-knowledge benchmark spanning challenging academic and professional questions, often including multimodal items. It tests not only recall but also multi-step reasoning and synthesis, especially when tools like search or code are enabled in a specific evaluation setup.","L1: Language Comprehension
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Multisensory Integration (minor), Scene Understanding & Visual Reasoning (minor)
L3: ",L2
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a high-difficulty, graduate-level multiple-choice science benchmark designed to be resistant to shallow pattern matching and casual web lookup. Questions typically require careful reading and multi-step scientific reasoning to select the best option among plausible distractors.","L1: Language Comprehension (minor)
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory
L3: ",L2
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro evaluates expert-level multimodal understanding across many disciplines, combining images (e.g., diagrams, plots, screenshots) with text questions. It emphasizes grounded reasoning over visual evidence, not just captioning or recognition.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Working Memory (minor)
L3: ",L2
MathArena Apex,https://matharena.ai/?view=problem&comp=apex--apex_2025,https://arxiv.org/abs/2505.23281,MathArena Apex aggregates high-difficulty math problems intended to probe frontier mathematical reasoning and solution robustness. It emphasizes correct derivations under competition-style constraints and often highlights failure modes like algebraic slips and brittle heuristics.,"L1: 
L2: Logical Reasoning, Working Memory, Adaptive Error Correction (minor), Planning (minor)
L3: ",L2
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning tests whether models can answer questions about scientific figures and visuals from papers, requiring interpretation of plots, diagrams, and annotations. Success depends on extracting the right visual evidence and integrating it with the textual query to perform scientific reasoning.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Semantic Understanding & Context Recognition, Working Memory (minor), Logical Reasoning (minor)
L3: ",L2
OmniDocBench 1.5,https://github.com/opendatalab/OmniDocBench,https://arxiv.org/abs/2412.07626,"OmniDocBench 1.5 evaluates document understanding and OCR-style reconstruction across heterogeneous layouts, including text blocks, tables, formulas, and reading order. It probes whether a model can parse and reproduce structured documents with high fidelity rather than merely summarize them.","L1: Visual Perception, Language Production
L2: Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Working Memory (minor)
L3: ",L2
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU evaluates multimodal understanding and reasoning over videos, where relevant evidence is distributed across frames and time. It tests whether models can integrate temporal context, track entities/events, and answer questions requiring video-grounded inference.","L1: Visual Perception
L2: Working Memory, Scene Understanding & Visual Reasoning, Attention (minor), Multisensory Integration (minor)
L3: Cognitive Timing & Predictive Modeling",L3
LiveCodeBench Pro,https://livecodebenchpro.com/projects/livecodebench-pro/leaderboard,https://arxiv.org/abs/2506.11928,"LiveCodeBench Pro evaluates competitive coding and algorithmic problem solving on a continuously refreshed set intended to reduce leakage, reporting performance via ratings (e.g., Elo). It measures whether models can design correct algorithms, implement them, and debug edge cases under time-like pressure.","L1: Language Production (minor)
L2: Logical Reasoning, Planning, Adaptive Error Correction, Working Memory
L3: ",L2
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"FACTS Benchmark Suite systematically evaluates factuality by testing whether model outputs remain grounded and correct across varied prompt styles, contexts, and potential pitfalls. It targets hallucination-like behaviors, incorrect extrapolation, and failures to respect evidence constraints.","L1: 
L2: Semantic Understanding & Context Recognition, Logical Reasoning (minor)
L3: Inhibitory Control, Self-reflection (minor)",L3
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA evaluates commonsense and physical reasoning across many languages and cultures, emphasizing whether models preserve meaning and choose sensible actions or explanations in non-parallel multilingual settings. It probes robust understanding beyond English-centric phrasing and familiar idioms.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning (minor)
L3: Cognitive Flexibility (minor)",L2
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 is a long-context evaluation where multiple similar “needle” turns are embedded within long conversational haystacks, and the model must retrieve the correct referenced response for a particular needle. The 8-needle setting stresses interference resistance, precise reference tracking, and faithful recall over long spans.","L1: 
L2: Working Memory, Attention, Episodic Memory, Semantic Understanding & Context Recognition (minor)
L3: ",L2
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval measures performance on well-specified, economically valuable knowledge-work tasks spanning many occupations, judged by expert humans via pairwise comparisons. Outputs are often real work artifacts (e.g., plans, analyses, spreadsheets, presentations), emphasizing end-to-end usefulness and correctness under constraints.","L1: Language Production
L2: Planning, Decision-making, Semantic Understanding & Context Recognition
L3: Self-reflection (minor), Social Reasoning & Theory of Mind (minor)",L2
SWE-Lancer,https://openai.com/index/swe-lancer/,https://arxiv.org/abs/2502.12115,"SWE-Lancer evaluates autonomous software engineering on realistic repositories and tasks, emphasizing the ability to scope work, implement patches, and satisfy hidden tests in a production-like workflow. It targets sustained agentic coding competence beyond isolated algorithmic problems.","L1: 
L2: Planning, Logical Reasoning, Adaptive Error Correction, Working Memory, Decision-making (minor)
L3: ",L2
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks evaluates graph traversal and relational reasoning in long contexts, where the model must follow edges, track paths, and answer queries about nodes/relationships. It stresses structured, stepwise state tracking under distractors and scale.","L1: 
L2: Spatial Representation & Mapping, Working Memory, Logical Reasoning, Attention (minor)
L3: ",L2
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon measures tool-use competence across diverse APIs and multi-step workflows, focusing on selecting the right tools, calling them correctly, and integrating results into a coherent final answer. It exposes failure modes in orchestration, such as premature calls, wrong arguments, and brittle recovery from tool errors.","L1: Language Comprehension (minor)
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory, Reward Mechanisms (minor)
L3: ",L2
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath is a difficult mathematics benchmark aimed at expert-level reasoning, including problems that require nontrivial proof-like insight and careful symbolic manipulation. It is designed to distinguish genuine mathematical problem solving from surface pattern completion, and often evaluates performance with or without tool support (e.g., Python).","L1: 
L2: Logical Reasoning, Working Memory, Planning (minor), Adaptive Error Correction (minor)
L3: ",L2
