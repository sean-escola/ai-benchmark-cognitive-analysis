Benchmark,Website,Paper,Description,Cognitive Functions
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 evaluates autonomous agent performance on real command-line tasks inside sandboxed environments (e.g., configuring tools, manipulating files, running programs, debugging). Success requires selecting and sequencing shell actions under constraints, often with iterative troubleshooting and verification via tool outputs.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Language Comprehension (minor)"
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp measures deep-research ability over a controlled web-like corpus, emphasizing multi-hop information seeking and synthesis rather than single-shot QA. Agents must decide what to search, read, and cite, and then compose a grounded answer from retrieved evidence.","Planning, Decision-making, Attention, Episodic Memory, Semantic Understanding & Context Recognition, Language Production (minor)"
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld evaluates multimodal computer-use agents in realistic desktop OS environments (apps, settings, web, files) with tasks that require UI navigation and stateful interaction. Models must interpret screenshots/GUI elements and execute multi-step action sequences reliably within a step budget.","Visual Perception, Visual Attention & Eye Movements, Sensorimotor Coordination, Planning, Decision-making, Working Memory"
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI tests “fluid intelligence” via novel grid-based pattern transformation problems with only a few input–output examples per task. The benchmark emphasizes abstraction, compositional rule induction, and generalization to unseen patterns rather than memorized domain knowledge.","Logical Reasoning, Cognitive Flexibility, Working Memory, Attention (minor), Spatial Representation & Mapping (minor)"
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon agent coherence by simulating the operation of a vending-machine business over many decision points (pricing, inventory, supplier negotiation, budgeting). It stresses maintaining strategy over extended time, adapting to market dynamics, and avoiding compounding errors.","Planning, Decision-making, Reward Mechanisms, Working Memory, Episodic Memory (minor), Social Reasoning & Theory of Mind (minor)"
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity agent capabilities on real-world vulnerability tasks, including identifying known vulnerabilities and discovering new ones in open-source projects. It requires reading technical descriptions/code, forming hypotheses, testing in constrained environments, and producing a correct exploit/patch outcome under pass@1 settings.","Logical Reasoning, Planning, Adaptive Error Correction, Decision-making, Working Memory, Language Comprehension (minor)"
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"SpreadsheetBench tests whether models can understand, navigate, and modify complex spreadsheets to satisfy realistic tasks (analysis, transformation, formula editing, formatting, and validation). It stresses precise structured manipulation and consistency across linked cells/sheets rather than purely textual reasoning.","Working Memory, Logical Reasoning, Attention, Planning (minor), Adaptive Error Correction (minor)"
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a difficult multimodal benchmark intended to probe frontier knowledge and reasoning across diverse expert domains, often beyond routine textbook problems. Questions can require synthesizing evidence, performing multi-step derivations, and interpreting visual information; tool-enabled variants test grounded research and computation workflows.","Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning, Scene Understanding & Visual Reasoning (minor), Working Memory (minor), Planning (minor)"
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a high-quality subset of GPQA consisting of graduate-level, “Google-proof” multiple-choice science questions that are easy for domain experts but hard for non-experts. It targets rigorous scientific reasoning under uncertainty and discourages shallow pattern matching.","Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Attention (minor), Language Comprehension (minor)"
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro measures expert-level multimodal understanding and reasoning across many disciplines using images paired with questions (charts, diagrams, screenshots, documents, scientific figures). Compared to earlier MMMU settings, the Pro variant emphasizes harder questions and stronger visual reasoning demands.","Visual Perception, Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Working Memory (minor), Attention (minor)"
MathArena Apex,https://matharena.ai/?view=problem&comp=apex--apex_2025,https://arxiv.org/abs/2505.23281,"MathArena Apex is a competitive mathematics benchmark that aggregates difficult problems and evaluates correctness under strict grading, often emphasizing proof-like multi-step reasoning. It is designed to stress robust mathematical problem solving rather than short-form recall.","Logical Reasoning, Working Memory, Cognitive Flexibility (minor), Planning (minor), Attention (minor)"
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning evaluates scientific figure understanding and reasoning using questions grounded in arXiv-style charts/plots and accompanying context. Models must extract quantitative/relational information from figures, connect it to the question, and reason to an answer (often benefiting from tool-based computation).","Visual Perception, Scene Understanding & Visual Reasoning, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory (minor), Attention (minor)"
OmniDocBench 1.5,https://github.com/opendatalab/OmniDocBench,https://arxiv.org/abs/2412.07626,"OmniDocBench 1.5 evaluates document AI systems on end-to-end understanding of complex documents, including OCR text, formulas, tables, and reading order/layout. Scoring emphasizes faithful reconstruction and structured extraction, requiring robust handling of diverse formatting and visual noise.","Visual Perception, Visual Attention & Eye Movements, Language Comprehension, Scene Understanding & Visual Reasoning (minor), Working Memory (minor), Semantic Understanding & Context Recognition (minor)"
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU measures multimodal reasoning over short videos paired with questions that require understanding temporal events, interactions, and sometimes overlaid text/graphics. It stresses integrating information across frames (and possibly audio/transcripts) rather than single-image recognition.","Visual Perception, Scene Understanding & Visual Reasoning, Episodic Memory, Attention, Working Memory (minor), Multisensory Integration (minor)"
LiveCodeBench Pro,https://livecodebenchpro.com/projects/livecodebench-pro/leaderboard,https://arxiv.org/abs/2506.11928,"LiveCodeBench Pro evaluates coding ability on up-to-date programming tasks, typically run in an execution-based harness to verify functional correctness. The Pro setting emphasizes harder problems and robust generalization, including iterative debugging under limited attempts.","Planning, Logical Reasoning, Adaptive Error Correction, Working Memory, Language Production (minor), Decision-making (minor)"
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"FACTS Benchmark Suite systematically evaluates factuality: whether a model’s statements are supported by sources or ground truth, and how it handles uncertainty, citation, and retrieval-grounding. It targets hallucination-resistance and calibration across diverse factuality subtests rather than raw task completion alone.","Semantic Understanding & Context Recognition, Language Comprehension, Inhibitory Control (minor), Self-reflection (minor), Working Memory (minor)"
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA tests physical commonsense and everyday procedural reasoning across multiple languages and cultures, focusing on choosing plausible actions/outcomes in the physical world. It probes whether models can generalize intuitive physics and practical affordances beyond English-centric datasets.","Semantic Understanding & Context Recognition, Logical Reasoning, Spatial Representation & Mapping (minor), Planning (minor), Language Comprehension (minor)"
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) evaluates long-context multi-round coreference and retrieval: multiple similar “needle” interactions are embedded in a long “haystack,” and the model must reproduce the response corresponding to a specified needle. The 8-needle variant stresses accurate tracking of repeated entities and disambiguation across very long contexts.","Working Memory, Attention, Episodic Memory, Language Comprehension, Semantic Understanding & Context Recognition (minor)"
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval measures performance on economically valuable, well-specified knowledge-work tasks spanning many occupations (e.g., producing spreadsheets, plans, presentations, analyses) with human judging against professionals. It stresses end-to-end artifact quality, following constraints, and making appropriate tradeoffs under realistic specifications.","Planning, Decision-making, Language Production, Semantic Understanding & Context Recognition, Working Memory (minor), Self-reflection (minor)"
SWE-Lancer,https://openai.com/index/swe-lancer/,https://arxiv.org/abs/2502.12115,"SWE-Lancer evaluates agentic software engineering on realistic tasks resembling contract work: understanding a codebase, implementing changes, fixing bugs, and producing correct patches under evaluation. Compared with simpler coding sets, it emphasizes practical constraints, robustness, and correct integration into existing systems.","Planning, Logical Reasoning, Adaptive Error Correction, Decision-making, Working Memory, Language Comprehension (minor)"
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks tests long-context algorithmic reasoning over graph-structured data presented in text, requiring models to follow traversal rules (e.g., BFS/parent relationships) and return correct nodes/paths. Performance depends on maintaining state across many steps and resisting distractors over long inputs.","Spatial Representation & Mapping, Working Memory, Attention, Logical Reasoning, Cognitive Timing & Predictive Modeling (minor)"
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon evaluates tool-using agents on multi-step tasks that require selecting appropriate tools, calling them correctly, handling tool errors, and integrating outputs into a final answer. It focuses on reliable orchestration across heterogeneous tools and environments rather than single-model reasoning alone.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Attention (minor), Semantic Understanding & Context Recognition (minor)"
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath evaluates advanced, expert-level mathematics, emphasizing hard problems that require nontrivial multi-step reasoning and are designed to be resistant to memorization. Tool-enabled settings test whether models can combine reasoning with computation while maintaining mathematical rigor.","Logical Reasoning, Working Memory, Cognitive Flexibility (minor), Planning (minor), Attention (minor)"
