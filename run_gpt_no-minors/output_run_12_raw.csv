Benchmark,Website,Paper,Description,Cognitive Functions
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 evaluates agentic coding and system-operation ability in real command-line environments, where the model must plan and execute shell commands, inspect files, and iteratively debug. It emphasizes end-to-end task completion under realistic tooling and environment constraints rather than isolated code writing.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Language Comprehension (minor), Language Production (minor)"
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp measures deep-research performance: answering difficult questions by searching, selecting sources, and synthesizing evidence from a document collection or the web (depending on the setup). Success depends on decomposing the query, managing context over multiple retrieval steps, and producing a grounded final answer.","Planning, Attention, Semantic Understanding & Context Recognition, Working Memory, Decision-making, Language Production (minor)"
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld evaluates multimodal “computer use” agents on realistic OS tasks (e.g., operating applications, navigating GUIs, and completing workflows) with a step limit. It stresses robust perception of interfaces and reliable action sequencing to accomplish goals under partial observability and UI variability.","Visual Perception, Scene Understanding & Visual Reasoning, Attention, Sensorimotor Coordination, Planning, Decision-making"
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,ARC-AGI is a “fluid intelligence” benchmark where models infer a hidden rule from a few input–output grid examples and must apply it to a new grid. It is designed to reduce reliance on memorized knowledge and instead test abstract pattern induction and systematic generalization.,"Logical Reasoning, Working Memory, Cognitive Flexibility, Spatial Representation & Mapping, Planning (minor)"
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon autonomy by asking an agent to run a simulated vending-machine business over an extended time period, making many interdependent decisions (pricing, inventory, suppliers, budgeting). It rewards coherent strategy, adaptation to changing conditions, and avoiding compounding errors across long trajectories.","Planning, Decision-making, Reward Mechanisms, Working Memory, Adaptive Error Correction, Motivational Drives (minor)"
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity agents on large-scale, real-world vulnerability tasks, including identifying known vulnerabilities and discovering previously unknown ones in open-source software. It emphasizes technical reasoning over codebases, iterative testing, and careful exploit construction/verification within realistic constraints.","Logical Reasoning, Planning, Adaptive Error Correction, Working Memory, Inhibitory Control (minor)"
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"SpreadsheetBench tests whether models can navigate, edit, and compute with complex spreadsheets using realistic operations (formulas, formatting, multi-sheet references, and data transforms). It captures practical analytic work where small mistakes propagate and where tool-augmented iteration often matters.","Logical Reasoning, Working Memory, Attention, Planning, Adaptive Error Correction, Visual Perception (minor)"
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam (HLE) is a difficult multimodal benchmark spanning frontier academic and professional questions, intended to probe broad general reasoning and knowledge under hard, often novel prompts. Depending on configuration, models may be evaluated with or without tools such as search and code execution.","Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Scene Understanding & Visual Reasoning (minor)"
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,GPQA Diamond is a high-quality subset of GPQA consisting of very challenging graduate-level science multiple-choice questions designed to be “Google-proof.” It stresses precise scientific reasoning and discrimination among close distractors rather than retrieval of obvious facts.,"Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory (minor), Language Comprehension (minor)"
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro is a multimodal, multi-discipline understanding and reasoning benchmark featuring expert-style questions grounded in images (figures, diagrams, charts) and text. It targets robust visual reasoning fused with domain knowledge and careful option selection under ambiguity.","Visual Perception, Scene Understanding & Visual Reasoning, Language Comprehension, Logical Reasoning, Visual Attention & Eye Movements (minor)"
MathArena Apex,https://matharena.ai/?view=problem&comp=apex--apex_2025,https://arxiv.org/abs/2505.23281,"MathArena Apex evaluates high-level mathematical problem solving, often emphasizing multi-step derivations and correctness under competition-style constraints. Scores are intended to reflect strong quantitative reasoning rather than superficial pattern matching.","Logical Reasoning, Working Memory, Planning, Adaptive Error Correction (minor)"
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning evaluates reasoning over scientific figures and related context from arXiv-style papers, requiring interpretation of plots, diagrams, and experimental visuals. It measures whether a model can connect visual evidence to scientific claims and answer questions that require nontrivial inference.","Scene Understanding & Visual Reasoning, Visual Perception, Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning"
OmniDocBench 1.5,https://github.com/opendatalab/OmniDocBench,https://arxiv.org/abs/2412.07626,"OmniDocBench 1.5 evaluates document understanding and OCR robustness across heterogeneous layouts (text, formulas, tables, and reading order). It focuses on faithfully reconstructing structured content from document images and maintaining layout-sensitive correctness.","Visual Perception, Attention, Scene Understanding & Visual Reasoning, Language Comprehension, Working Memory (minor)"
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU extends multimodal understanding to videos, testing whether models can answer questions that require integrating information across frames and time. It probes temporal event understanding, visual grounding, and multi-step reasoning from dynamic scenes.","Visual Perception, Scene Understanding & Visual Reasoning, Working Memory, Attention, Cognitive Timing & Predictive Modeling (minor), Multisensory Integration (minor)"
LiveCodeBench Pro,https://livecodebenchpro.com/projects/livecodebench-pro/leaderboard,https://arxiv.org/abs/2506.11928,"LiveCodeBench Pro evaluates coding ability on fresh, competitive-programming-style and software tasks, typically scored with pass@k metrics or ELO-style aggregates to reflect reliability. It emphasizes writing correct executable code, handling edge cases, and iterating when initial attempts fail.","Logical Reasoning, Planning, Adaptive Error Correction, Working Memory, Language Production (minor)"
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"FACTS Benchmark Suite systematically evaluates LLM factuality across a battery of tasks that stress groundedness, attribution, and resisting unsupported claims. It aims to separate fluent generation from truthfulness by measuring how often models produce errors or unverified statements under realistic prompts.","Semantic Understanding & Context Recognition, Inhibitory Control, Language Comprehension (minor), Self-reflection (minor)"
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA evaluates physical commonsense and pragmatic reasoning across many languages and cultural contexts, focusing on whether models can choose plausible actions/outcomes in everyday situations. It stresses robust generalization beyond English-centric phrasing and beyond narrow training distributions.","Semantic Understanding & Context Recognition, Logical Reasoning, Language Comprehension, Spatial Representation & Mapping (minor)"
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) tests long-context multi-round coreference resolution by embedding repeated, similar “needle” requests within a long “haystack” of dialogue and asking the model to reproduce the response linked to a specified needle. It strongly probes whether the model can maintain and retrieve the correct referent over long spans with high interference.","Working Memory, Attention, Episodic Memory, Language Comprehension, Semantic Understanding & Context Recognition"
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval evaluates economically valuable knowledge-work outputs across many occupations (e.g., presentations, spreadsheets, plans), typically using expert human judgments in head-to-head comparisons. It emphasizes end-to-end quality, instruction following, and producing usable artifacts rather than only answering questions.","Planning, Decision-making, Language Production, Semantic Understanding & Context Recognition, Social Reasoning & Theory of Mind (minor), Working Memory (minor)"
SWE-Lancer,https://openai.com/index/swe-lancer/,https://arxiv.org/abs/2502.12115,"SWE-Lancer evaluates software engineering agents on realistic repository-level tasks that require implementing fixes or features and producing correct patches. It emphasizes multi-step debugging, navigation of large codebases, and delivering changes that satisfy tests and specifications.","Planning, Logical Reasoning, Adaptive Error Correction, Working Memory, Decision-making, Inhibitory Control (minor)"
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks evaluates structured reasoning over graph problems presented in text, such as following edges, performing BFS-like traversals, or identifying parent/ancestor relationships over long paths. It stresses precise multi-hop computation and resisting distraction from many similar nodes and relations.","Spatial Representation & Mapping, Working Memory, Logical Reasoning, Attention"
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon evaluates tool-using agents on complex tasks that require selecting among many tools, invoking them correctly, recovering from tool errors, and integrating results into a final answer. It targets practical orchestration competence under multi-step, tool-rich workflows.","Planning, Decision-making, Attention, Working Memory, Adaptive Error Correction, Inhibitory Control (minor)"
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath evaluates expert-level mathematics, including problems intended to be near the edge of current model capability and often requiring careful multi-stage derivations. It is designed to better reflect genuine mathematical problem solving than easier competition sets, including robustness to subtle errors.","Logical Reasoning, Working Memory, Planning, Adaptive Error Correction, Cognitive Flexibility (minor)"
