Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates software engineering agents by asking them to generate patches that fix real issues in open-source Python repositories, with tasks filtered/validated to be solvable and reliably testable. Success is measured by running the project’s tests (or task-specific checks) to verify the proposed code change resolves the issue without breaking the codebase.","L1: Language Comprehension (minor)
L2: Logical Reasoning, Planning, Working Memory, Adaptive Error Correction, Semantic Understanding & Context Recognition
L3: ",L2
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro is a harder software engineering benchmark designed to better reflect industrially relevant development work, extending beyond the original setting with broader task diversity and stronger resistance to trivial memorization. Models must produce correct code changes under realistic repository constraints, typically requiring deeper debugging, refactoring, and multi-file modifications.","L1: 
L2: Logical Reasoning, Planning, Working Memory, Adaptive Error Correction, Semantic Understanding & Context Recognition
L3: Cognitive Flexibility",L3
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp evaluates “deep research” style agents that must browse and synthesize information to answer difficult questions, typically requiring multi-step retrieval and cross-checking across sources. It targets end-to-end search behaviors—query formulation, evidence gathering, and final justification—rather than isolated QA from a fixed context.","L1: Language Comprehension, Language Production (minor)
L2: Planning, Decision-making, Attention, Working Memory, Episodic Memory (minor), Semantic Understanding & Context Recognition
L3: ",L2
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench measures how well an agent can complete multi-turn customer-support tasks by interacting with a simulated user and calling domain APIs while obeying policies. It emphasizes robust tool use across long dialogues (e.g., retail orders/returns, airline changes, telecom troubleshooting) and consistent adherence to constraints despite ambiguity and user pressure.","L1: Language Comprehension, Language Production
L2: Decision-making, Planning, Working Memory (minor), Reward Mechanisms (minor)
L3: Social Reasoning & Theory of Mind, Inhibitory Control",L3
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI tests fluid intelligence via novel grid-based pattern transformation tasks: given a few input–output examples, the model must infer the hidden rule and generate the correct output for a new input. The tasks are designed to be out-of-distribution for rote memorization and reward abstraction, compositionality, and systematic generalization.","L1: Visual Perception
L2: Logical Reasoning, Working Memory, Spatial Representation & Mapping, Scene Understanding & Visual Reasoning, Attention (minor)
L3: Cognitive Flexibility",L3
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas evaluates real-world tool use through the Model Context Protocol (MCP), where models must discover the right tools and execute multi-step workflows across production-like tool servers. Tasks require correct API invocation, error handling, and synthesis of tool outputs into a coherent final response.","L1: Language Comprehension (minor)
L2: Planning, Decision-making, Working Memory, Adaptive Error Correction, Semantic Understanding & Context Recognition
L3: ",L2
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a broad, frontier-difficulty benchmark spanning many domains and (often) multiple modalities, intended to stress-test advanced reasoning and expert knowledge. Questions are designed to be challenging for current models, rewarding careful multi-step inference and precise, evidence-based answering (with or without tools depending on the evaluation setup).","L1: Language Comprehension
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory (minor), Planning (minor), Multisensory Integration (minor), Scene Understanding & Visual Reasoning (minor)
L3: ",L2
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 evaluates mathematical problem solving using questions from the American Invitational Mathematics Examination, typically requiring multi-step symbolic reasoning and careful handling of constraints. Performance is usually reported as accuracy over the contest problems, sometimes with optional tool use (e.g., Python) depending on the evaluation protocol.","L1: 
L2: Logical Reasoning, Working Memory, Attention (minor), Semantic Understanding & Context Recognition (minor)
L3: Cognitive Flexibility (minor)",L2
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a high-quality subset of GPQA, consisting of graduate-level multiple-choice questions in science that are deliberately difficult to answer by simple web lookup or shallow pattern matching. It targets deep domain knowledge plus reasoning under distractors, often emphasizing careful reading and elimination.","L1: Language Comprehension (minor)
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory (minor), Attention (minor)
L3: ",L2
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends MMLU into multiple languages, testing broad academic knowledge and reasoning across many subjects in non-English settings. It probes whether models can transfer competencies across languages while maintaining factual accuracy and consistent reasoning behavior.","L1: Language Comprehension, Language Production (minor)
L2: Semantic Understanding & Context Recognition, Logical Reasoning (minor), Working Memory (minor)
L3: ",L2
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro is a challenging multimodal benchmark that requires jointly reasoning over images and text across many disciplines (e.g., diagrams, charts, scientific figures, and technical visuals). It emphasizes expert-level multimodal understanding and structured reasoning beyond simple recognition.","L1: Visual Perception, Language Comprehension (minor)
L2: Multisensory Integration, Scene Understanding & Visual Reasoning, Logical Reasoning, Attention (minor)
L3: ",L2
ScreenShot-Pro,https://gui-agent.github.io/grounding-leaderboard/,https://arxiv.org/abs/2504.07981,"ScreenShot-Pro evaluates GUI understanding from screenshots, where the model must interpret complex user interfaces and answer questions grounded in on-screen elements. It stresses fine-grained visual grounding (buttons, menus, tables, settings) and spatial/layout reasoning typical of professional software workflows.","L1: Visual Perception, Language Comprehension (minor)
L2: Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Spatial Representation & Mapping, Decision-making (minor)
L3: ",L2
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning tests scientific-figure and paper-centric visual reasoning using plots, charts, and visual elements from research-style documents. The goal is to assess whether a model can extract quantitative/relational information from figures and integrate it with the accompanying textual context to answer reasoning-heavy questions.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Logical Reasoning, Multisensory Integration, Working Memory (minor), Attention (minor)
L3: ",L2
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU extends multimodal understanding to the temporal domain by requiring reasoning over videos alongside text questions. Tasks typically require integrating information across frames, tracking changes over time, and combining visual cues with language to answer questions about events, procedures, or interactions.","L1: Visual Perception
L2: Working Memory, Scene Understanding & Visual Reasoning, Multisensory Integration (minor), Attention (minor)
L3: Cognitive Timing & Predictive Modeling",L3
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) evaluates long-context retrieval and multi-round coreference resolution by embedding multiple similar “needle” interactions within a long “haystack” of dialogue. The model must correctly identify and reproduce the response associated with a specific needle, stressing robustness to interference and distractors at long context lengths.","L1: Language Comprehension
L2: Working Memory, Attention, Episodic Memory, Semantic Understanding & Context Recognition (minor)
L3: Inhibitory Control (minor)",L2
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval evaluates well-specified professional “knowledge work” by asking models to produce real workplace artifacts (e.g., spreadsheets, presentations, plans, analyses) across many occupations and industries, judged by expert comparison to human professionals. It targets end-to-end task execution quality—following requirements, producing usable deliverables, and showing practical judgment under constraints.","L1: Language Production
L2: Planning, Decision-making, Semantic Understanding & Context Recognition, Adaptive Error Correction (minor), Working Memory (minor)
L3: Self-reflection (minor)",L2
SWE-Lancer,https://openai.com/index/swe-lancer/,https://arxiv.org/abs/2502.12115,"SWE-Lancer evaluates advanced software engineering performance with an emphasis on realistic developer workflows, such as implementing fixes or changes within larger codebases under constraints. Compared with simpler coding tests, it is intended to better capture practical engineering competence (e.g., navigating projects, applying correct changes, and maintaining overall correctness).","L1: 
L2: Logical Reasoning, Planning, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition
L3: ",L2
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks evaluates a model’s ability to follow and reason over graph-structured data by performing multi-step traversals (e.g., reachability, parent/neighbor queries) within a provided representation. It stresses systematic multi-hop reasoning and maintaining state over long sequences of relational steps, often in long-context settings.","L1: 
L2: Spatial Representation & Mapping, Working Memory, Logical Reasoning, Attention (minor), Planning (minor)
L3: ",L2
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon measures tool-using proficiency across diverse tasks where a model must select appropriate tools, call them correctly, and iteratively refine results to solve multi-step problems. It emphasizes orchestration reliability (including error recovery), not just final-answer quality, reflecting real agent deployments that depend on external APIs and utilities.","L1: 
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition (minor)
L3: Inhibitory Control (minor)",L2
HMMT,https://matharena.ai/?view=problem&comp=hmmt--hmmt_feb_2025,https://arxiv.org/abs/2505.23281,"HMMT evaluates competition-style mathematics using problems from the Harvard-MIT Mathematics Tournament, which often require creative insights, careful case analysis, and multi-step derivations. It is typically harder and more proof/strategy flavored than standard short-form math QA, stressing robustness on novel problem structures.","L1: 
L2: Logical Reasoning, Working Memory, Attention (minor), Semantic Understanding & Context Recognition (minor)
L3: Cognitive Flexibility (minor)",L2
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath is an expert-level mathematics benchmark designed to test advanced reasoning on problems closer to research and high-end olympiad difficulty, sometimes with tiered difficulty levels. It targets deep multi-step derivations, precise symbolic manipulation, and sustained correctness under challenging, unfamiliar problem setups.","L1: 
L2: Logical Reasoning, Working Memory, Planning (minor), Attention (minor)
L3: Cognitive Flexibility (minor)",L2
