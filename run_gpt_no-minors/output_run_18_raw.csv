Benchmark,Website,Paper,Description,Cognitive Functions
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates agentic software engineering by asking a model to generate patches that fix real issues in open-source Python repositories. The “Verified” subset uses problems that have been human-validated as solvable with the provided repo state and tests, making it a widely used measure of practical debugging and code-change reliability.","Planning, Adaptive Error Correction, Logical Reasoning, Working Memory, Decision-making, Language Comprehension (minor), Language Production (minor)"
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro is a larger, more difficult software engineering benchmark spanning multiple programming languages and more complex real-world repo changes. It emphasizes end-to-end problem solving under more contamination-resistant and industrially realistic conditions than many earlier coding benchmarks.","Planning, Adaptive Error Correction, Logical Reasoning, Working Memory, Decision-making, Inhibitory Control (minor), Language Comprehension (minor)"
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp evaluates deep research behavior where models must search, read, and synthesize information from a controlled web-like corpus or browsing setup to answer difficult questions. It stresses multi-step information gathering, source triangulation, and maintaining task goals while navigating many candidate documents.","Planning, Decision-making, Episodic Memory, Working Memory, Attention, Semantic Understanding & Context Recognition, Language Comprehension, Language Production (minor), Self-reflection (minor)"
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench measures interactive agent performance in customer-support style domains (e.g., retail, airline, telecom) where the model must converse with a simulated user and call tools/APIs while following policies. It emphasizes consistency over multi-turn dialogues, correct tool invocation, and adherence to constraints despite user pressure or ambiguity.","Social Reasoning & Theory of Mind, Inhibitory Control, Planning, Decision-making, Working Memory, Language Comprehension, Language Production, Empathy (minor)"
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI tests fluid reasoning by giving a few input–output grid examples and asking the model to infer the transformation rule and apply it to a new grid. It is designed to reduce reliance on memorized knowledge and instead measure generalization to novel, compositional pattern problems.","Logical Reasoning, Cognitive Flexibility, Spatial Representation & Mapping, Scene Understanding & Visual Reasoning, Visual Perception, Working Memory, Attention (minor)"
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas evaluates real-world tool use via the Model Context Protocol, where models must discover, select, and correctly call tools across multi-step workflows and then integrate tool outputs into a final answer. Tasks resemble production API orchestration, including handling tool errors, retries, and cross-tool dependencies.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition, Inhibitory Control (minor), Language Comprehension (minor), Language Production (minor)"
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a large, frontier-difficulty benchmark spanning advanced academic and professional questions, including multimodal items, intended to probe near-expert reasoning and knowledge. It is often evaluated both with and without tools (e.g., search or code execution) to measure end-to-end problem solving under realistic assistance settings.","Logical Reasoning, Semantic Understanding & Context Recognition, Language Comprehension, Working Memory, Language Production (minor), Scene Understanding & Visual Reasoning (minor), Visual Perception (minor)"
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 consists of competition-style mathematics problems that require multi-step derivations and careful symbolic manipulation. Model performance is typically measured by exact final answers, making robustness to small algebraic mistakes and disciplined reasoning central.","Logical Reasoning, Working Memory, Planning, Adaptive Error Correction (minor), Language Comprehension (minor)"
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a high-quality subset of GPQA featuring very difficult graduate-level multiple-choice science questions designed to be “Google-proof.” It aims to measure deep scientific reasoning and comprehension rather than surface pattern matching, with strong controls on question quality.","Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory (minor), Language Comprehension (minor), Decision-making (minor)"
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends broad academic evaluation across many subjects into multiple languages, testing whether models retain knowledge and reasoning competence beyond English. It is typically formatted as standardized QA (often multiple choice), supporting cross-lingual comparison and coverage across domains.","Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning (minor), Working Memory (minor)"
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro is a strengthened, expert-oriented version of multimodal understanding benchmarks, combining text with diagrams, charts, and scientific or technical imagery across disciplines. It measures whether models can integrate visual evidence with textual prompts to answer knowledge-intensive and reasoning-heavy questions.","Visual Perception, Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Language Comprehension, Spatial Representation & Mapping (minor), Attention (minor)"
ScreenShot-Pro,https://gui-agent.github.io/grounding-leaderboard/,https://arxiv.org/abs/2504.07981,"ScreenShot-Pro evaluates screenshot understanding for GUI and software-interface tasks, requiring models to localize relevant UI elements and answer questions or propose actions grounded in the image. It targets fine-grained visual grounding, layout reasoning, and robustness to high-resolution interface details common in real productivity tools.","Visual Perception, Scene Understanding & Visual Reasoning, Spatial Representation & Mapping, Visual Attention & Eye Movements, Decision-making (minor), Sensorimotor Coordination (minor), Language Comprehension (minor)"
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning focuses on reasoning over scientific figures (e.g., plots, tables, schematics) drawn from arXiv-style papers, often requiring quantitative or structural interpretation rather than simple caption reading. It probes the ability to extract evidence from visuals and connect it to domain language and analytical questions.","Visual Perception, Scene Understanding & Visual Reasoning, Logical Reasoning, Multisensory Integration, Semantic Understanding & Context Recognition, Working Memory (minor), Language Comprehension (minor)"
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU extends multimodal understanding to videos, requiring models to integrate information across frames and time to answer questions that depend on temporal context. It evaluates whether models can track events, objects, and state changes and connect them to language prompts.","Visual Perception, Scene Understanding & Visual Reasoning, Cognitive Timing & Predictive Modeling, Attention, Working Memory, Multisensory Integration (minor), Language Comprehension (minor)"
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 is a long-context evaluation where multiple similar “needle” interactions are embedded in a long “haystack” conversation, and the model must retrieve the correct referenced response (the 8-needle variant increases interference). It is designed to stress long-range coreference, distractor resistance, and accurate retrieval across very long inputs.","Working Memory, Attention, Episodic Memory, Semantic Understanding & Context Recognition, Language Comprehension (minor)"
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval measures economically valuable knowledge work by having models produce real workplace artifacts (e.g., spreadsheets, presentations, analyses) across many occupations and comparing them against outputs from industry professionals. Scoring is based on expert judging of task success, emphasizing end-to-end usefulness rather than isolated QA accuracy.","Planning, Decision-making, Language Production, Semantic Understanding & Context Recognition, Working Memory (minor), Self-reflection (minor), Social Reasoning & Theory of Mind (minor)"
SWE-Lancer,https://openai.com/index/swe-lancer/,https://arxiv.org/abs/2502.12115,"SWE-Lancer evaluates software engineering performance in a more job-like setting, emphasizing completing practical coding tasks end-to-end under realistic constraints and quality expectations. It aims to capture aspects of professional engineering such as interpreting specs, implementing changes, and delivering correct patches efficiently.","Planning, Adaptive Error Correction, Logical Reasoning, Decision-making, Working Memory, Language Comprehension (minor), Language Production (minor)"
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks tests algorithmic reasoning over graph-structured inputs, commonly requiring multi-step traversal, parent tracking, or reachability queries that can be embedded in long contexts. It emphasizes systematic state tracking and resistance to shortcuts when solving structured, compositional problems.","Spatial Representation & Mapping, Logical Reasoning, Working Memory, Attention, Planning (minor)"
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon benchmarks evaluate tool-using agents on tasks that require selecting among many tools and composing multi-step tool chains to reach a correct outcome. It stresses reliable tool invocation, intermediate-state management, and recovering from tool errors or unexpected outputs.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Inhibitory Control (minor), Language Comprehension (minor), Language Production (minor)"
HMMT,https://matharena.ai/?view=problem&comp=hmmt--hmmt_feb_2025,https://arxiv.org/abs/2505.23281,"HMMT comprises challenging contest mathematics problems that often require clever insights, multi-stage proofs or computations, and careful case handling. It is used to probe higher-end mathematical reasoning beyond typical school-level benchmarks.","Logical Reasoning, Planning, Working Memory, Adaptive Error Correction (minor), Language Comprehension (minor)"
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath targets expert-level mathematics, including problems intended to be difficult even for strong math solvers, often requiring deep multi-step reasoning and nontrivial techniques. It is designed to better separate top models by focusing on hard, research-adjacent or olympiad-adjacent mathematical problem solving.","Logical Reasoning, Planning, Working Memory, Cognitive Flexibility (minor), Adaptive Error Correction (minor), Language Comprehension (minor)"
