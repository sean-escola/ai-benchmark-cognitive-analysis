Benchmark,Website,Paper,Description,Cognitive Functions
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 evaluates agentic coding and sysadmin-style problem solving in a real command-line environment, where the model must execute shell commands, inspect files, and iteratively fix issues. Success depends on choosing correct tool actions under constraints (timeouts, limited resources) and recovering from mistakes across multi-step tasks.","Planning, Decision-making, Working Memory, Adaptive Error Correction, Attention (minor), Language Comprehension (minor)"
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp measures deep-research question answering that requires searching and synthesizing information from many documents rather than relying on parametric memory alone. Agents must decide what to search, evaluate evidence quality, and integrate scattered facts into a single justified answer.","Planning, Attention, Working Memory, Semantic Understanding & Context Recognition, Decision-making, Self-reflection (minor)"
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld tests multimodal computer-use agents on real desktop tasks (e.g., navigating apps, settings, files, and web UIs) using screenshots and interactive actions. It stresses end-to-end execution: perceiving interface state, choosing UI actions, and maintaining progress over long task horizons.","Visual Perception, Scene Understanding & Visual Reasoning, Sensorimotor Coordination, Planning, Working Memory, Attention (minor)"
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI evaluates fluid reasoning by asking models to infer hidden rules from a few input–output grid examples and apply them to new grids. The tasks are intentionally novel and compositional, rewarding abstraction, hypothesis testing, and robust generalization rather than memorization.","Logical Reasoning, Cognitive Flexibility, Spatial Representation & Mapping, Working Memory, Attention (minor)"
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon agent coherence by simulating a year-long vending-machine business that requires inventory management, supplier negotiation, pricing, and adaptation to market dynamics. The agent must maintain strategy across many sequential decisions and balance short-term cashflow with long-term profit.","Planning, Decision-making, Reward Mechanisms, Working Memory, Cognitive Timing & Predictive Modeling, Social Reasoning & Theory of Mind (minor)"
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity agent competence on tasks involving vulnerability identification and, in some settings, vulnerability discovery in real open-source codebases. It emphasizes actionable reasoning from high-level vulnerability descriptions to concrete code locations, exploit conditions, and patches or reports.","Logical Reasoning, Planning, Working Memory, Adaptive Error Correction, Attention (minor), Semantic Understanding & Context Recognition (minor)"
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"SpreadsheetBench tests a model’s ability to read, manipulate, and reason over complex spreadsheets, often requiring formula edits, data transformations, and verification of outputs. It captures realistic office analytics work where correctness depends on precise multi-step operations and structured data reasoning.","Working Memory, Planning, Logical Reasoning, Attention, Semantic Understanding & Context Recognition (minor), Adaptive Error Correction (minor)"
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a frontier, multi-domain benchmark designed to probe advanced academic reasoning and multimodal understanding across hard questions. Many items require combining domain knowledge with multi-step inference, and tool-enabled settings further stress end-to-end problem solving.","Language Comprehension, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Visual Perception (minor), Planning (minor)"
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,GPQA Diamond is a high-quality subset of GPQA containing very challenging graduate-level multiple-choice science questions intended to be “Google-proof.” It primarily measures deep scientific reasoning and the ability to avoid plausible distractors under uncertainty.,"Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory (minor), Language Comprehension (minor), Inhibitory Control (minor)"
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro is a difficult multimodal understanding and reasoning benchmark spanning many disciplines, where questions require interpreting images (diagrams, charts, screenshots) alongside text. The “Pro” variant emphasizes harder items and stronger evaluation protocols to better distinguish frontier models.","Visual Perception, Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Visual Attention & Eye Movements (minor), Working Memory (minor)"
MathArena Apex,https://matharena.ai/?view=problem&comp=apex--apex_2025,https://arxiv.org/abs/2505.23281,"MathArena Apex is a competitive math evaluation that aggregates hard problems and evaluates solution correctness under standardized settings, often highlighting robustness across problem types. It stresses multi-step symbolic reasoning and careful verification rather than short-form pattern matching.","Logical Reasoning, Working Memory, Planning (minor), Attention (minor), Adaptive Error Correction (minor)"
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning focuses on reasoning about scientific figures and visual evidence from research papers, often requiring chart/plot interpretation and quantitative inference. In tool-augmented settings (e.g., Python), it additionally tests whether models can translate visual understanding into correct computational checks.","Scene Understanding & Visual Reasoning, Visual Perception, Logical Reasoning, Multisensory Integration (minor), Working Memory (minor), Attention (minor)"
OmniDocBench 1.5,https://github.com/opendatalab/OmniDocBench,https://arxiv.org/abs/2412.07626,"OmniDocBench 1.5 evaluates document understanding and OCR quality across heterogeneous layouts including text, tables, formulas, and reading order. It targets end-to-end document perception: detecting structure, transcribing accurately, and preserving layout-consistent sequencing.","Visual Perception, Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Language Comprehension (minor), Working Memory (minor)"
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU evaluates multimodal reasoning over videos, where temporal context and evolving scenes matter for answering questions. It probes whether models can integrate information across frames and align language queries with dynamic visual evidence.","Visual Perception, Cognitive Timing & Predictive Modeling, Working Memory, Scene Understanding & Visual Reasoning, Attention (minor), Multisensory Integration (minor)"
LiveCodeBench Pro,https://livecodebenchpro.com/projects/livecodebench-pro/leaderboard,https://arxiv.org/abs/2506.11928,"LiveCodeBench Pro measures competitive coding performance on recent, difficult programming problems under controlled evaluation, often summarized as an Elo-style rating. It emphasizes algorithmic reasoning, implementation accuracy, and iterative debugging under time/attempt constraints.","Logical Reasoning, Planning, Working Memory, Adaptive Error Correction, Attention (minor)"
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"The FACTS Benchmark Suite systematically evaluates factuality and grounding, measuring whether model outputs stay supported by provided sources or known truths across varied settings. It targets both avoiding hallucinations and correctly representing uncertainty when evidence is insufficient.","Semantic Understanding & Context Recognition, Inhibitory Control, Self-reflection, Language Production (minor), Attention (minor)"
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA extends physical commonsense question answering across many languages, focusing on practical reasoning about everyday interactions with objects and environments. It stresses whether models preserve commonsense constraints and meaning when questions are expressed in diverse linguistic forms.","Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning, Spatial Representation & Mapping (minor), Tactile Perception (minor)"
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,MRCR v2 (8-needle) tests long-context multi-round coreference resolution by embedding multiple similar “needle” requests inside long “haystacks” and asking the model to retrieve the correct referenced response. It primarily measures stable context tracking under distractors and interference.,"Working Memory, Attention, Semantic Understanding & Context Recognition, Language Comprehension (minor)"
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval evaluates economically valuable, well-specified knowledge work across many occupations using side-by-side comparisons against human professionals. Tasks often require producing real artifacts (e.g., plans, analyses, structured documents), so success depends on coherent end-to-end execution rather than isolated QA.","Planning, Decision-making, Language Production, Semantic Understanding & Context Recognition, Working Memory (minor), Social Reasoning & Theory of Mind (minor)"
SWE-Lancer,https://openai.com/index/swe-lancer/,https://arxiv.org/abs/2502.12115,"SWE-Lancer evaluates software engineering agents on realistic tasks that resemble contractor-style work: understanding a repository, proposing changes, and producing correct patches under constraints. It emphasizes reliability on end-to-end engineering workflows beyond small isolated coding puzzles.","Planning, Logical Reasoning, Adaptive Error Correction, Working Memory, Decision-making (minor)"
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks tests reasoning over graph-structured data presented in text, requiring the model to follow edges, answer reachability/parent queries, or simulate traversal procedures. It stresses consistent multi-step state tracking and resistance to distraction over long contexts.","Spatial Representation & Mapping, Working Memory, Logical Reasoning, Attention (minor), Planning (minor)"
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon evaluates tool-using agents on tasks that require selecting, calling, and chaining multiple tools (APIs) correctly to reach a goal. It measures both correct action sequencing and robust handling of tool errors, partial results, and interface constraints.","Planning, Decision-making, Working Memory, Adaptive Error Correction, Attention (minor)"
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath is an expert-level mathematics benchmark designed to be difficult for current frontier models, with tiers reflecting increasing depth and novelty. Many problems require extended derivations, careful case handling, and verification (often in tool-augmented settings).","Logical Reasoning, Working Memory, Cognitive Flexibility, Planning (minor), Attention (minor)"
