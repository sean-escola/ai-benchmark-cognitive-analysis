Benchmark,Website,Paper,Description,Cognitive Functions
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 evaluates agentic performance on real-world command-line tasks inside sandboxed terminal environments, using an agent harness that allows iterative tool use (shell commands) to accomplish goals. It emphasizes end-to-end task completion under realistic constraints like environment state, errors, and the need to inspect files and outputs.","Planning, Decision-making, Adaptive Error Correction, Working Memory (minor), Language Comprehension (minor)"
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp measures deep-research and web-browsing ability: models must locate relevant information in large document collections (or via search) and synthesize correct answers. The benchmark stresses retrieval strategy, evidence aggregation across sources, and maintaining coherence across long multi-step browsing traces.","Planning, Attention, Working Memory, Semantic Understanding & Context Recognition, Language Production (minor)"
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld evaluates multimodal computer-use agents on tasks that require interacting with a desktop operating system via screenshots and UI actions. Success depends on perceiving interface state, executing multi-step procedures (e.g., file operations, settings changes), and recovering from mistakes in a dynamic environment.","Visual Perception, Scene Understanding & Visual Reasoning, Planning, Decision-making, Sensorimotor Coordination (minor), Adaptive Error Correction (minor)"
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI tests abstract, few-shot pattern induction using grid-based input–output examples, where the model must infer the hidden transformation rule and apply it to a new grid. It is designed to emphasize fluid reasoning and generalization to novel problems rather than memorized domain knowledge.","Logical Reasoning, Cognitive Flexibility, Spatial Representation & Mapping, Working Memory (minor), Visual Perception (minor)"
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon agent coherence and strategy in a simulated vending-machine business over an extended period, where the agent manages inventory, pricing, suppliers, and cash flow. Performance is typically measured by final profit/balance, requiring consistent multi-step decisions across many turns.","Planning, Decision-making, Reward Mechanisms, Working Memory (minor), Social Reasoning & Theory of Mind (minor), Adaptive Error Correction (minor)"
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym measures cybersecurity capabilities on real software projects, including identifying known vulnerabilities from descriptions and discovering new vulnerabilities. It stresses code understanding, hypothesis-driven debugging, and iterative testing within realistic tooling and constraints.","Logical Reasoning, Planning, Adaptive Error Correction, Semantic Understanding & Context Recognition, Working Memory (minor)"
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"SpreadsheetBench evaluates an agent’s ability to navigate and manipulate complex spreadsheets (e.g., formulas, tables, formatting, and data transformations) using tools such as programmatic libraries or office backends. Tasks require careful tracking of spreadsheet state and producing correct final artifacts rather than only verbal answers.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Logical Reasoning (minor)"
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a multimodal benchmark intended to probe frontier knowledge and reasoning across diverse academic and professional domains. Questions often require multi-step reasoning and, in tool-enabled settings, effective use of search and code to verify or derive answers.","Language Comprehension, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory (minor), Visual Perception (minor), Planning (minor)"
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a high-difficulty, graduate-level multiple-choice science benchmark curated to be challenging for non-experts and resistant to shallow pattern matching. It emphasizes rigorous scientific reasoning under uncertainty and careful reading of technical problem statements.","Logical Reasoning, Language Comprehension, Semantic Understanding & Context Recognition, Working Memory (minor)"
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro evaluates multimodal expert reasoning over text-plus-image questions spanning many disciplines, often involving charts, diagrams, and visual evidence. It targets robust visual-language integration beyond simple recognition, requiring reasoning grounded in the provided visual context.","Multisensory Integration, Scene Understanding & Visual Reasoning, Visual Perception, Language Comprehension (minor), Logical Reasoning (minor)"
MathArena Apex,https://matharena.ai/?view=problem&comp=apex--apex_2025,https://arxiv.org/abs/2505.23281,"MathArena Apex is a competition-style mathematics benchmark that aggregates challenging problems and evaluates correctness under strict grading. It emphasizes multi-step derivations, formal manipulation, and sustained reasoning; some settings may allow tool assistance such as computation.","Logical Reasoning, Planning, Working Memory, Adaptive Error Correction (minor)"
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning evaluates scientific figure understanding and reasoning, typically requiring models to interpret plots/diagrams from research papers and answer questions that depend on visual evidence. It targets the combination of visual perception, quantitative/relational reasoning, and technical language grounding.","Scene Understanding & Visual Reasoning, Visual Perception, Multisensory Integration, Logical Reasoning, Language Comprehension (minor)"
OmniDocBench 1.5,https://github.com/opendatalab/OmniDocBench,https://arxiv.org/abs/2412.07626,"OmniDocBench 1.5 evaluates document understanding and OCR quality across heterogeneous document elements such as text, tables, formulas, and reading order. Metrics (e.g., edit distance and structure-sensitive measures) emphasize faithful extraction and layout-aware reconstruction from document images.","Visual Perception, Visual Attention & Eye Movements (minor), Scene Understanding & Visual Reasoning, Language Comprehension (minor), Working Memory (minor)"
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU evaluates multimodal reasoning over videos, requiring models to integrate information across time and modalities to answer questions. It probes temporal understanding, event linking across frames, and retaining relevant details from long or information-dense clips.","Visual Perception, Cognitive Timing & Predictive Modeling, Working Memory, Attention, Multisensory Integration (minor)"
LiveCodeBench Pro,https://livecodebenchpro.com/projects/livecodebench-pro/leaderboard,https://arxiv.org/abs/2506.11928,"LiveCodeBench Pro evaluates coding ability with a focus on realistic programming tasks and strong contamination controls, often graded by execution against tests. It emphasizes producing correct, runnable code and adapting solutions under constraints typical of competitive or professional coding settings.","Planning, Logical Reasoning, Adaptive Error Correction, Working Memory (minor), Language Production (minor)"
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"The FACTS Benchmark Suite systematically evaluates factuality-related behaviors, including correctness, grounding, and resistance to hallucination across diverse settings. It focuses on whether outputs stay supported by evidence and whether models properly handle uncertainty and conflicts.","Semantic Understanding & Context Recognition, Inhibitory Control, Logical Reasoning (minor), Self-reflection (minor), Attention (minor)"
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA is a multilingual physical commonsense reasoning benchmark designed to test whether models can answer practical, everyday questions across many languages without relying on parallel translation artifacts. It emphasizes robust semantic understanding and commonsense inference under linguistic variation.","Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning, Cognitive Flexibility (minor)"
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) is a long-context evaluation where multiple similar “needle” turns are embedded in a large “haystack,” and the model must retrieve and reproduce the correct referenced content for a specified needle. It stresses precise long-range dependency handling, interference resistance, and faithful retrieval rather than generic summarization.","Working Memory, Attention, Episodic Memory, Inhibitory Control (minor)"
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval measures performance on well-specified professional knowledge-work tasks across many occupations, judged via comparisons against human industry professionals. Tasks often require generating concrete work products (e.g., spreadsheets, presentations, plans) with quality assessed by expert evaluators.","Planning, Decision-making, Language Production, Semantic Understanding & Context Recognition, Self-reflection (minor)"
SWE-Lancer,https://openai.com/index/swe-lancer/,https://arxiv.org/abs/2502.12115,"SWE-Lancer evaluates advanced software engineering ability on realistic, end-to-end tasks (e.g., understanding a codebase, implementing changes, and producing correct patches) with stronger realism and difficulty than many earlier coding benchmarks. It stresses robust problem decomposition, debugging, and iterative refinement under constraints.","Planning, Adaptive Error Correction, Logical Reasoning, Working Memory (minor), Inhibitory Control (minor)"
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks evaluates long-context algorithmic reasoning on graph-structured problems, where the model must follow traversal rules (e.g., BFS-like procedures) and answer queries that depend on many linked steps. It is designed to test systematic multi-step tracking and resistance to distractors in large inputs.","Logical Reasoning, Working Memory, Attention, Spatial Representation & Mapping (minor), Planning (minor)"
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon evaluates tool-using agents on tasks that require selecting, calling, and composing multiple tools/APIs to achieve a goal, often under multi-turn interaction and partial observability. It emphasizes reliable orchestration (choosing the right tool at the right time), correct parameterization, and recovery from tool or environment errors.","Planning, Decision-making, Adaptive Error Correction, Working Memory (minor), Inhibitory Control (minor)"
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath is an expert-level mathematics benchmark targeting problems beyond routine contest math, intended to measure frontier reasoning and proof-like problem solving. It emphasizes sustained multi-step logical work, careful handling of constraints, and often benefits from tool-assisted verification in supported settings.","Logical Reasoning, Planning, Working Memory, Adaptive Error Correction (minor)"
