Benchmark,Website,Paper,Description,Cognitive Functions
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates patch-based software engineering on real GitHub issues, where a model must produce a code change that makes the project’s tests pass. The “Verified” subset consists of tasks that have been manually checked as solvable and appropriately specified, aiming to reduce noise from ambiguous or unsound issues.","Planning, Logical Reasoning, Working Memory, Adaptive Error Correction, Decision-making (minor), Language Comprehension (minor)"
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro is a more difficult software engineering benchmark designed to better reflect professional, real-world patch generation and reduce contamination and shortcut solving. Compared to SWE-bench Verified, it increases task diversity and difficulty (including broader stacks/languages and more complex repos) and places higher emphasis on robust end-to-end issue resolution.","Planning, Logical Reasoning, Adaptive Error Correction, Working Memory, Cognitive Flexibility (minor), Decision-making (minor)"
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp evaluates “deep research” behavior: answering questions that require searching, reading, and synthesizing information across multiple sources rather than relying on memorized facts. It targets agentic browsing competence, such as deciding what to search, selecting sources, and integrating evidence into a final response.","Planning, Decision-making, Attention, Working Memory, Semantic Understanding & Context Recognition, Language Comprehension (minor)"
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench measures agentic customer-support performance in simulated domains (e.g., retail, airline, telecom) where the agent must follow policies while interacting with APIs and a simulated user over multiple turns. It stresses tool use reliability, long-horizon dialog consistency, and adherence to constraints under realistic conversational pressure.","Social Reasoning & Theory of Mind, Decision-making, Inhibitory Control, Planning, Language Comprehension, Empathy (minor)"
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI tests fluid reasoning by requiring models to infer novel transformation rules from a few input–output grid examples and apply them to new inputs. It is designed to minimize reliance on memorized knowledge, emphasizing abstraction, systematic generalization, and pattern discovery under data scarcity.","Logical Reasoning, Cognitive Flexibility, Working Memory, Spatial Representation & Mapping, Visual Perception (minor), Scene Understanding & Visual Reasoning (minor)"
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas evaluates real-world tool use via the Model Context Protocol (MCP), where models must discover, invoke, and chain multiple tools/services to complete tasks. It emphasizes correct API selection and parameterization, multi-step workflow execution, and robust recovery from tool errors or partial results.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition (minor), Language Comprehension (minor)"
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam (HLE) is a multimodal benchmark intended to probe frontier knowledge and reasoning across many expert-level topics and formats, including questions that benefit from tools like search or code. It targets broad generalization and synthesis—connecting domain knowledge, evidence, and multi-step reasoning into accurate answers.","Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory (minor), Visual Perception (minor), Scene Understanding & Visual Reasoning (minor)"
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 evaluates competition-style mathematics problem solving under strict answer-format constraints. Problems often require multi-step derivations, clever transformations, and careful bookkeeping rather than rote formula recall.","Logical Reasoning, Working Memory, Planning (minor)"
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a high-difficulty, “Google-proof” multiple-choice science benchmark (physics, chemistry, biology) curated to resist simple retrieval and reward genuine reasoning. The Diamond subset focuses on the highest-quality questions where experts succeed and non-experts typically fail.","Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory (minor), Decision-making (minor), Language Comprehension (minor)"
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends MMLU-style academic knowledge testing across multiple languages, probing whether a model’s knowledge and reasoning transfer beyond English. It assesses subject-matter understanding with multilingual prompts and answers, emphasizing cross-lingual robustness and comprehension.","Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning (minor), Language Production (minor)"
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro is a more challenging variant of MMMU for multimodal, expert-level questions spanning multiple disciplines where answers depend on jointly interpreting text and images. It emphasizes fine-grained visual understanding, cross-modal grounding, and multi-step reasoning over diagrams, charts, and scientific figures.","Visual Perception, Scene Understanding & Visual Reasoning, Multisensory Integration, Language Comprehension, Logical Reasoning, Working Memory (minor)"
ScreenShot-Pro,https://gui-agent.github.io/grounding-leaderboard/,https://arxiv.org/abs/2504.07981,"ScreenShot-Pro evaluates understanding of high-resolution UI screenshots, requiring models to locate relevant interface elements and answer questions grounded in the visual layout. It targets GUI comprehension for agentic “computer use,” where success depends on spatially precise interpretation of visual affordances and labels.","Visual Perception, Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Spatial Representation & Mapping, Language Comprehension (minor), Sensorimotor Coordination (minor)"
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning focuses on reasoning over scientific figures and documents (e.g., interpreting plots, diagrams, and figure captions) to answer questions that require more than OCR. It stresses extracting structured information from visuals and combining it with domain context to justify correct answers.","Scene Understanding & Visual Reasoning, Visual Perception, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory (minor)"
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU evaluates multimodal understanding and reasoning over video, requiring integration of temporally distributed visual evidence (and accompanying text prompts) to answer questions. It targets long-range temporal coherence, event understanding, and multi-step inference from dynamic scenes.","Visual Perception, Attention, Cognitive Timing & Predictive Modeling, Working Memory, Scene Understanding & Visual Reasoning (minor), Multisensory Integration (minor)"
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) is a long-context evaluation where multiple similar “needle” exchanges are embedded in a large “haystack,” and the model must retrieve the correct response corresponding to a specified needle. It probes robust long-context retrieval, disambiguation among confusable contexts, and maintaining accuracy as context length grows.","Working Memory, Attention, Episodic Memory, Language Comprehension, Semantic Understanding & Context Recognition (minor)"
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval evaluates economically valuable, well-specified professional knowledge work across many occupations by comparing model outputs to human professional work products (e.g., slides, spreadsheets, plans). It emphasizes end-to-end task execution quality, instruction-following, and producing coherent, useful artifacts under realistic constraints.","Planning, Decision-making, Language Production, Semantic Understanding & Context Recognition, Working Memory (minor), Self-reflection (minor)"
SWE-Lancer,https://openai.com/index/swe-lancer/,https://arxiv.org/abs/2502.12115,"SWE-Lancer targets longer-horizon, agentic software engineering where models must navigate larger codebases and complete multi-step tasks that resemble real engineering work. It stresses sustained problem decomposition, iterative debugging, and coordinating changes across files and components to reach a working solution.","Planning, Adaptive Error Correction, Working Memory, Decision-making, Cognitive Flexibility (minor), Language Comprehension (minor)"
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks evaluates long-context structured reasoning by asking models to traverse or query graphs described in text (often at large scale), such as following edges, finding parents, or performing BFS-like operations. It emphasizes precise state tracking and compositional reasoning over relational structure rather than world knowledge.","Logical Reasoning, Working Memory, Attention, Spatial Representation & Mapping, Planning (minor)"
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon measures an agent’s ability to solve tasks by selecting and composing many tools across multi-step workflows, typically under realistic API constraints and with opportunities for tool failures. It emphasizes orchestration quality: choosing the right tool at the right time, validating outputs, and producing a correct final result.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition (minor)"
HMMT,https://matharena.ai/?view=problem&comp=hmmt--hmmt_feb_2025,https://arxiv.org/abs/2505.23281,"HMMT evaluates high-level competition mathematics performance (e.g., problems from Harvard-MIT Mathematics Tournament settings as packaged in MathArena). Tasks often require multi-step proofs or derivations, strategic insight, and careful symbolic manipulation under tight correctness demands.","Logical Reasoning, Working Memory, Planning (minor)"
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath is designed to measure progress on expert- and research-level mathematics by using hard, carefully curated problems and robust scoring protocols. It targets deep multi-step reasoning and the ability to sustain correct abstractions over long solution chains, often beyond typical contest difficulty tiers.","Logical Reasoning, Working Memory, Planning, Cognitive Flexibility (minor)"
