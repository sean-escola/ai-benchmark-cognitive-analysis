Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates software engineering agents on real GitHub issues where the task is to produce a patch that makes a repository’s test suite pass. The “Verified” subset contains problems curated/validated to be solvable and reliably graded, emphasizing correct code changes over superficial fixes.","L1: Language Comprehension (minor)
L2: Logical Reasoning, Planning, Adaptive Error Correction, Working Memory
L3: ",L2
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro is a harder, more contamination-resistant software engineering benchmark spanning multiple programming languages and more complex real-world tasks. It measures an agent’s ability to understand large codebases, implement robust fixes, and satisfy stringent automated tests under more realistic industrial conditions.","L1: Language Comprehension (minor)
L2: Planning, Adaptive Error Correction, Logical Reasoning, Working Memory
L3: Cognitive Flexibility (minor)",L2
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp evaluates “deep research” agents that must search, read, and synthesize information from the web (or a fixed corpus) to answer difficult questions. It tests end-to-end information acquisition, evidence integration, and citation-quality grounding across multi-step retrieval and reasoning.","L1: Language Production (minor)
L2: Planning, Attention, Working Memory, Semantic Understanding & Context Recognition, Episodic Memory (minor)
L3: ",L2
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench measures agent performance in multi-turn, tool-using customer-support scenarios (e.g., retail, airline, telecom) with policy constraints. Agents must call APIs correctly, maintain consistent dialogue state, and follow domain rules while helping a simulated user reach a resolution.","L1: Language Comprehension (minor), Language Production (minor)
L2: Decision-making, Planning, Working Memory
L3: Social Reasoning & Theory of Mind, Inhibitory Control",L3
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI is a few-shot abstract reasoning benchmark based on grid-world transformation puzzles, designed to probe generalization to novel rules with minimal examples. It emphasizes discovering latent patterns, composing transformations, and avoiding reliance on domain-specific priors.","L1: 
L2: Logical Reasoning, Working Memory, Spatial Representation & Mapping, Attention (minor), Scene Understanding & Visual Reasoning (minor)
L3: Cognitive Flexibility",L3
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas evaluates real-world tool use via the Model Context Protocol (MCP), where models must discover tools, invoke them with correct arguments, handle errors/retries, and synthesize outputs. Tasks reflect production-like multi-step workflows across multiple services rather than single-call function use.","L1: 
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition (minor)
L3: Inhibitory Control (minor)",L2
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a large, difficult, multimodal benchmark intended to approximate frontier “expert-level” questions across many disciplines. It stresses broad knowledge, complex reasoning, and (when tools/search are allowed) reliable grounding and synthesis under open-ended problem statements.","L1: Language Comprehension (minor), Language Production (minor)
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Multisensory Integration (minor), Scene Understanding & Visual Reasoning (minor)
L3: ",L2
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 consists of competition-style mathematics problems that require multi-step derivations and careful symbolic manipulation. It is typically scored by exact final answers, rewarding correctness and robustness over partial progress.","L1: 
L2: Logical Reasoning, Working Memory, Planning (minor), Adaptive Error Correction (minor)
L3: ",L2
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a highly challenging multiple-choice science benchmark (physics, chemistry, biology) curated to be “Google-proof,” with questions that non-experts usually miss. It emphasizes deep conceptual reasoning and precise discrimination among close distractors rather than surface recall.","L1: Language Comprehension (minor)
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory (minor)
L3: ",L2
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,MMMLU extends MMLU to multiple non-English languages and evaluates knowledge and reasoning across many academic subjects. It probes both factual understanding and the ability to apply concepts consistently across linguistic contexts.,"L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning (minor), Working Memory (minor)
L3: ",L2
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro is a professionalized, harder variant of multimodal understanding benchmarks where models answer questions requiring joint interpretation of images and text across many disciplines. It stresses chart/diagram interpretation, visual grounding, and cross-modal reasoning under demanding prompts.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Multisensory Integration, Attention (minor), Spatial Representation & Mapping (minor)
L3: ",L2
ScreenShot-Pro,https://gui-agent.github.io/grounding-leaderboard/,https://arxiv.org/abs/2504.07981,"ScreenShot-Pro evaluates screenshot understanding for real-world GUI and document-like images, often requiring localization of interface elements and correct interpretation of on-screen content. It targets practical vision-language grounding needed for computer-use agents and UI-based workflows.","L1: Visual Perception, Language Comprehension (minor)
L2: Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Spatial Representation & Mapping (minor)
L3: ",L2
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning evaluates scientific-figure reasoning from arXiv-style papers, requiring models to read and interpret charts/plots and answer questions that depend on quantitative and contextual understanding. It often benefits from tool use (e.g., Python) but primarily measures robust figure comprehension and inference.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Logical Reasoning, Working Memory (minor), Attention (minor)
L3: ",L2
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU evaluates multimodal reasoning over video, requiring models to integrate information across frames and time to answer questions about events, actions, and causal/temporal relationships. It emphasizes maintaining coherence over long visual sequences and aligning language outputs to dynamic visual evidence.","L1: Visual Perception
L2: Working Memory, Scene Understanding & Visual Reasoning, Multisensory Integration (minor), Attention (minor)
L3: Cognitive Timing & Predictive Modeling",L3
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (multi-round co-reference resolution) inserts multiple similar “needle” interactions into long “haystack” contexts and asks the model to retrieve the correct referenced response. The 8-needle setting stresses long-context robustness, resistance to distractors, and precise retrieval across many candidate mentions.","L1: Language Comprehension (minor)
L2: Working Memory, Attention, Episodic Memory
L3: Inhibitory Control (minor)",L2
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval measures performance on well-specified professional knowledge-work tasks (e.g., producing spreadsheets, presentations, schedules, or analyses) judged against expert human outputs. It targets end-to-end task execution quality, including constraint satisfaction, structured deliverables, and iterative refinement as needed.","L1: Language Production
L2: Planning, Decision-making, Semantic Understanding & Context Recognition, Adaptive Error Correction (minor)
L3: Social Reasoning & Theory of Mind (minor)",L2
SWE-Lancer,https://openai.com/index/swe-lancer/,https://arxiv.org/abs/2502.12115,"SWE-Lancer evaluates agentic software engineering in a more end-to-end setting, focusing on producing correct patches for realistic tasks aligned with practical developer workflows. It emphasizes reliability, tool-driven iteration, and integrating repository context to deliver changes that satisfy evaluation checks.","L1: 
L2: Planning, Adaptive Error Correction, Logical Reasoning, Working Memory, Decision-making (minor)
L3: ",L2
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks tests reasoning over graph-structured data, such as following paths or performing constrained traversals with many distractors. It probes whether models can maintain and manipulate structured state across many steps without losing track of nodes, edges, or traversal constraints.","L1: 
L2: Spatial Representation & Mapping, Working Memory, Logical Reasoning, Attention (minor), Planning (minor)
L3: ",L2
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon benchmarks multi-tool problem solving, where models must select from many tools, compose tool calls into workflows, and recover from intermediate failures. It emphasizes robust orchestration, correct parameterization, and synthesis of tool outputs into a final, user-facing answer.","L1: 
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory
L3: Inhibitory Control (minor)",L2
HMMT,https://matharena.ai/?view=problem&comp=hmmt--hmmt_feb_2025,https://arxiv.org/abs/2505.23281,"HMMT problems (as used in modern eval suites) draw from high-difficulty competition mathematics, often requiring creative constructions and multi-lemma reasoning. Compared to simpler contests, it typically stresses deeper insight, longer solution chains, and careful handling of edge cases.","L1: 
L2: Logical Reasoning, Working Memory, Planning (minor)
L3: Cognitive Flexibility (minor)",L2
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath is an expert-level mathematics benchmark designed to be challenging for frontier models and to better reflect research-style mathematical reasoning. It emphasizes non-routine problem solving, sustained multi-step proofs/derivations, and high precision in symbolic and quantitative work (often with optional tool support).","L1: 
L2: Logical Reasoning, Working Memory, Planning, Adaptive Error Correction (minor)
L3: Cognitive Flexibility (minor)",L2
