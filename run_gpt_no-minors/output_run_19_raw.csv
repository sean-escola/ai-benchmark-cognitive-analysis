Benchmark,Website,Paper,Description,Cognitive Functions
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates software engineering agents on real GitHub issues where the model must modify a repository to make a hidden test suite pass. The “Verified” subset emphasizes tasks that have been validated as solvable and reduces evaluation noise, focusing on end-to-end patch generation under realistic repository constraints.","Planning, Logical Reasoning, Adaptive Error Correction, Working Memory, Language Comprehension (minor), Language Production (minor)"
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro is a harder, more contamination-resistant software engineering benchmark spanning multiple programming languages and a larger set of professionally curated tasks. It stresses robust codebase understanding, multi-step debugging, and producing correct patches under more diverse real-world conditions than SWE-bench Verified.","Planning, Logical Reasoning, Adaptive Error Correction, Cognitive Flexibility, Working Memory, Language Comprehension (minor)"
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp evaluates “deep research” agent behavior on questions that require browsing and synthesizing information from multiple documents rather than answering from parametric memory alone. It primarily measures whether an agent can plan search steps, extract relevant evidence, and produce a grounded final answer under realistic retrieval constraints.","Planning, Decision-making, Attention, Working Memory, Semantic Understanding & Context Recognition, Language Comprehension, Language Production (minor)"
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench measures an agent’s ability to complete multi-turn customer-support tasks by interacting with simulated users and API/tool calls while following domain-specific policies (e.g., retail, airline, telecom). It emphasizes reliable tool use, policy adherence, and goal-directed dialogue over extended interactions.","Planning, Decision-making, Inhibitory Control, Social Reasoning & Theory of Mind, Language Comprehension, Language Production, Empathy (minor)"
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI is a few-shot “fluid intelligence” benchmark of grid-based transformation puzzles, where models infer latent rules from a small number of input–output examples and apply them to a new input. It targets generalization to novel tasks and systematic reasoning rather than memorized knowledge.","Logical Reasoning, Cognitive Flexibility, Working Memory, Spatial Representation & Mapping, Visual Perception, Attention (minor)"
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas evaluates real-world tool use via the Model Context Protocol (MCP), requiring models to discover available tools, call them with correct schemas, handle errors/retries, and synthesize results. Tasks are workflow-like and stress reliable action sequencing under tool-and-API constraints.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Language Comprehension (minor), Language Production (minor)"
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a frontier, multi-modal benchmark intended to stress difficult academic reasoning and broad expert knowledge, often including questions where tools (search/code) can meaningfully change outcomes. It emphasizes accurate synthesis and problem solving under ambiguity and high difficulty across modalities.","Language Comprehension, Logical Reasoning, Semantic Understanding & Context Recognition, Visual Perception, Scene Understanding & Visual Reasoning, Multisensory Integration (minor), Planning (minor)"
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 evaluates competition-style mathematics problem solving, typically requiring multi-step derivations, algebraic manipulation, and careful case reasoning. Scores reflect correctness on short-answer problems and reward precise reasoning rather than open-ended explanation quality.","Logical Reasoning, Working Memory, Planning (minor), Attention (minor)"
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a difficult multiple-choice science benchmark designed to be “Google-proof,” focusing on graduate-level questions whose correct resolution requires genuine understanding and reasoning. The Diamond subset is a high-quality selection intended to reduce ambiguity and increase evaluative reliability.","Semantic Understanding & Context Recognition, Logical Reasoning, Language Comprehension, Working Memory (minor), Decision-making (minor)"
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends MMLU to many languages, measuring broad academic knowledge and reasoning across subjects under multilingual settings. It primarily probes whether models can comprehend non-English prompts and map them to correct concepts and answers across domains.","Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning (minor), Working Memory (minor)"
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro is an expert-level multimodal benchmark spanning many disciplines, requiring models to answer questions using both images (e.g., diagrams, charts) and text. It focuses on high-fidelity visual understanding combined with domain reasoning and cross-modal grounding.","Visual Perception, Scene Understanding & Visual Reasoning, Multisensory Integration, Language Comprehension, Logical Reasoning, Working Memory (minor)"
ScreenShot-Pro,https://gui-agent.github.io/grounding-leaderboard/,https://arxiv.org/abs/2504.07981,"ScreenShot-Pro evaluates screenshot understanding for GUI agents, where models must interpret user interfaces and ground answers (and often actions) to the correct on-screen elements. It stresses precise spatial grounding and robust interpretation of high-resolution UI layouts that resemble real professional software.","Visual Perception, Visual Attention & Eye Movements, Spatial Representation & Mapping, Scene Understanding & Visual Reasoning, Decision-making (minor), Planning (minor)"
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning tests whether models can correctly answer questions about figures (charts/plots/diagrams) from scientific papers, often benefiting from quantitative extraction and multi-step reasoning. It emphasizes visual-to-symbolic translation and grounded interpretation of scientific visual evidence.","Visual Perception, Scene Understanding & Visual Reasoning, Logical Reasoning, Language Comprehension, Working Memory (minor), Semantic Understanding & Context Recognition (minor)"
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU evaluates multimodal understanding and reasoning over videos paired with natural language questions, requiring models to integrate information across time. It targets temporal event tracking, visual comprehension, and multi-step inference from dynamic scenes.","Visual Perception, Attention, Working Memory, Cognitive Timing & Predictive Modeling, Scene Understanding & Visual Reasoning, Multisensory Integration (minor), Auditory Processing (minor)"
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) is a long-context evaluation where multiple similar “needle” items are embedded in a large “haystack,” and the model must retrieve and reproduce information corresponding to a specified needle. It stresses long-range dependency handling, interference resistance, and accurate retrieval under substantial context length.","Working Memory, Attention, Language Comprehension, Semantic Understanding & Context Recognition, Episodic Memory (minor)"
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval is an evaluation of economically valuable, well-specified knowledge-work tasks across many occupations (e.g., producing spreadsheets, presentations, plans), typically judged via expert comparisons. It measures an agent’s ability to generate useful artifacts and make correct professional decisions under realistic constraints.","Planning, Decision-making, Language Production, Language Comprehension, Semantic Understanding & Context Recognition, Self-reflection (minor), Social Reasoning & Theory of Mind (minor)"
SWE-Lancer,https://openai.com/index/swe-lancer/,https://arxiv.org/abs/2502.12115,"SWE-Lancer evaluates agentic software engineering performance on realistic coding tasks that require understanding a repository, selecting an approach, and producing a working patch, often under tighter correctness and robustness expectations than simpler coding benchmarks. It targets end-to-end autonomy in engineering workflows rather than isolated coding questions.","Planning, Logical Reasoning, Adaptive Error Correction, Decision-making, Working Memory, Language Comprehension (minor)"
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks tests systematic reasoning over graph-structured data presented in text, such as executing traversals (e.g., BFS) or answering relational queries (e.g., parent pointers) across many nodes/edges. It emphasizes faithful algorithmic execution and maintaining correctness over long chains of dependencies.","Logical Reasoning, Working Memory, Spatial Representation & Mapping, Attention, Planning (minor)"
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon evaluates whether models can reliably solve tasks that require orchestrating multiple tools (e.g., retrieval, code execution, calculators, file operations) and integrating their outputs into a correct final response. It stresses tool selection, robust execution under errors, and coherent multi-step workflows.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Inhibitory Control (minor), Language Comprehension (minor)"
HMMT,https://matharena.ai/?view=problem&comp=hmmt--hmmt_feb_2025,https://arxiv.org/abs/2505.23281,"HMMT problems (as used in MathArena) measure advanced competition mathematics skill, typically requiring creative multi-step reasoning, proofs or proof-like argumentation, and careful handling of edge cases. It targets higher difficulty than many standard math benchmarks and is used to differentiate strong mathematical reasoners.","Logical Reasoning, Working Memory, Cognitive Flexibility (minor), Planning (minor)"
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath is an expert-level mathematics benchmark designed to test the frontier of mathematical reasoning, often including problems that require deep insight, long derivations, and (in some settings) tool-assisted computation. It aims to be challenging and contamination-resistant, measuring genuine progress in advanced math problem solving.","Logical Reasoning, Working Memory, Planning, Cognitive Flexibility, Adaptive Error Correction (minor)"
