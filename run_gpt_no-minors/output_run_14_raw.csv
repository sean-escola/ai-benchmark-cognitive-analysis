Benchmark,Website,Paper,Description,Cognitive Functions
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates software-engineering agents on real GitHub issues where the model must generate a code patch that makes tests pass. The Verified split uses human validation to ensure tasks are solvable and correctly specified, emphasizing end-to-end debugging and implementation rather than isolated code generation.","Planning, Adaptive Error Correction, Logical Reasoning, Working Memory, Semantic Understanding & Context Recognition, Decision-making (minor)"
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro is a more difficult software-engineering benchmark designed to be more diverse and contamination-resistant, with tasks spanning multiple programming languages and more complex repos. It stresses robust agentic workflows—understanding requirements, locating relevant files, implementing fixes, and validating via tests.","Planning, Adaptive Error Correction, Logical Reasoning, Working Memory, Cognitive Flexibility, Semantic Understanding & Context Recognition"
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp evaluates “deep research” browsing agents that must search, read, and synthesize information from a web corpus to answer complex questions. It emphasizes retrieval strategy, cross-document evidence integration, and maintaining correctness under noisy or incomplete sources.","Planning, Attention, Episodic Memory, Semantic Understanding & Context Recognition, Logical Reasoning, Language Comprehension"
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench tests interactive customer-support agents in simulated domains (e.g., retail, airline, telecom) that require multi-turn dialogue, tool/API usage, and policy compliance. Success requires coordinating procedural actions with natural-language explanations while avoiding policy violations and handling user constraints.","Social Reasoning & Theory of Mind, Inhibitory Control, Planning, Decision-making, Language Comprehension, Language Production, Working Memory, Empathy (minor)"
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI measures fluid, few-shot abstraction by asking models to infer the underlying rule mapping input grids to output grids from a handful of examples. The tasks are designed to require novel pattern induction and compositional reasoning rather than memorization of domain knowledge.","Logical Reasoning, Cognitive Flexibility, Working Memory, Spatial Representation & Mapping, Scene Understanding & Visual Reasoning, Visual Perception (minor)"
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas evaluates real-world tool use through the Model Context Protocol (MCP), requiring models to discover tools, call them correctly, recover from errors, and synthesize results across multi-step workflows. It targets practical API orchestration skills under realistic constraints (latency, retries, partial failures).","Planning, Decision-making, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition, Language Comprehension (minor), Language Production (minor)"
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam (HLE) is a large, frontier-knowledge benchmark with challenging questions across many domains and modalities, intended to probe expert-level reasoning and generalization. Tasks often require integrating specialized background knowledge with careful multi-step inference and, in tool-enabled settings, disciplined evidence use.","Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Language Comprehension, Scene Understanding & Visual Reasoning (minor), Multisensory Integration (minor), Planning (minor)"
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 consists of competition-style math problems that require structured derivations, algebraic manipulation, and clever transformations under time-like constraints. The benchmark is commonly used to assess mathematical reasoning and reliability without relying on external tools.","Logical Reasoning, Planning, Working Memory, Attention (minor), Language Comprehension (minor)"
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a hard multiple-choice science benchmark designed to be “Google-proof,” emphasizing questions that non-experts typically miss while experts can answer. It stresses precise scientific reasoning, discrimination between close options, and avoidance of plausible-sounding distractors.","Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Decision-making, Inhibitory Control (minor), Language Comprehension (minor)"
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends MMLU to multiple languages, testing broad academic knowledge and reasoning across many subjects in non-English settings. It probes whether a model’s conceptual understanding and test-taking robustness transfer across linguistic contexts.","Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory (minor), Language Production (minor)"
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro is a multimodal benchmark of expert-level questions spanning many disciplines that require joint reasoning over images (e.g., diagrams, plots, tables) and text. It emphasizes compositional visual reasoning, grounding language in visual evidence, and handling complex answer choices.","Multisensory Integration, Scene Understanding & Visual Reasoning, Visual Perception, Visual Attention & Eye Movements (minor), Language Comprehension, Logical Reasoning, Working Memory (minor)"
ScreenShot-Pro,https://gui-agent.github.io/grounding-leaderboard/,https://arxiv.org/abs/2504.07981,"ScreenShot-Pro evaluates screenshot understanding for GUI-centric tasks, such as identifying relevant UI elements and inferring the correct action or answer from the interface state. It targets fine-grained visual grounding, layout/affordance understanding, and robustness to real-world UI variability.","Visual Perception, Scene Understanding & Visual Reasoning, Spatial Representation & Mapping, Visual Attention & Eye Movements, Decision-making (minor), Sensorimotor Coordination (minor)"
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning tests whether models can answer reasoning-heavy questions about figures and visual elements drawn from scientific documents (e.g., charts and plots). It emphasizes extracting quantitative/structural relationships from visuals and mapping them into correct textual conclusions.","Scene Understanding & Visual Reasoning, Visual Perception, Semantic Understanding & Context Recognition, Logical Reasoning, Language Comprehension (minor), Working Memory (minor)"
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU evaluates multimodal understanding and reasoning over video, requiring models to integrate information across frames and time to answer questions. It probes temporal coherence, event understanding, and selective attention to relevant moments in long clips.","Visual Perception, Attention, Episodic Memory, Cognitive Timing & Predictive Modeling, Scene Understanding & Visual Reasoning, Multisensory Integration (minor), Language Comprehension (minor)"
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) is a long-context, multi-round co-reference resolution evaluation where multiple similar “needle” interactions are embedded in a long “haystack,” and the model must retrieve the correct referenced response. It stresses accurate indexing of repeated patterns and robustness to interference across long contexts.","Working Memory, Episodic Memory, Attention, Language Comprehension, Semantic Understanding & Context Recognition"
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval evaluates well-specified professional knowledge-work tasks across many occupations, often requiring creation of concrete work artifacts (e.g., slides, spreadsheets, plans) judged by expert humans. It measures end-to-end task execution quality, including adherence to constraints, organization, and practical decision quality.","Planning, Decision-making, Language Production, Semantic Understanding & Context Recognition, Adaptive Error Correction (minor), Working Memory (minor), Self-reflection (minor)"
SWE-Lancer,https://openai.com/index/swe-lancer/,https://arxiv.org/abs/2502.12115,"SWE-Lancer evaluates software-engineering agents on realistic tasks that more closely resemble contracted engineering work, emphasizing reliability, scope management, and producing correct patches within constraints. It targets longer-horizon engineering competence, including navigating ambiguous requirements and integrating changes safely.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Cognitive Flexibility, Semantic Understanding & Context Recognition"
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks tests systematic multi-step traversal in graph-structured problems described in text, requiring models to follow edges and maintain state across long sequences. It is designed to stress compositional, stepwise reasoning under distractors and long-range dependencies.","Spatial Representation & Mapping, Working Memory, Attention, Logical Reasoning, Language Comprehension (minor)"
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon evaluates general tool-use competence across diverse tasks that require selecting among tools, executing multi-step tool calls, and producing validated final outputs. It emphasizes robust orchestration behaviors such as error handling, iterative refinement, and maintaining task state over extended workflows.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition (minor), Inhibitory Control (minor)"
HMMT,https://matharena.ai/?view=problem&comp=hmmt--hmmt_feb_2025,https://arxiv.org/abs/2505.23281,"HMMT (Harvard-MIT Mathematics Tournament) problems are advanced contest-style questions that demand creative proof/derivation strategies and careful symbolic reasoning. As an evaluation set, it probes high-end mathematical problem solving beyond routine textbook exercises.","Logical Reasoning, Planning, Working Memory, Attention (minor), Language Comprehension (minor)"
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath is an expert-level mathematics benchmark intended to measure progress at the frontier of mathematical reasoning, including problems that resemble research-grade or olympiad-plus difficulty. It stresses deep multi-step derivations, strategy selection, and correctness under high ambiguity and long solution chains.","Logical Reasoning, Planning, Working Memory, Cognitive Flexibility, Attention (minor), Adaptive Error Correction (minor)"
