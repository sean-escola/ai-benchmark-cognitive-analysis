Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates whether a model can solve real GitHub issues by producing a correct code patch that passes the project’s tests, using a curated set of tasks verified by humans to be solvable. It stresses end-to-end software debugging and implementation in a realistic repository setting, including reading code, localizing faults, editing files, and validating changes.","L1: Language Comprehension (minor)
L2: Planning, Logical Reasoning, Adaptive Error Correction, Working Memory
L3: ",L2
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro is a harder, more contamination-resistant software engineering benchmark spanning multiple programming languages and more complex, industry-relevant tasks. Models must navigate larger codebases and produce robust patches under stricter evaluation, emphasizing generalizable engineering competence rather than narrow dataset familiarity.","L1: Language Comprehension (minor)
L2: Planning, Logical Reasoning, Adaptive Error Correction, Working Memory
L3: Cognitive Flexibility",L3
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp evaluates deep-research style web browsing: the model must search, read, cross-check sources, and synthesize an answer grounded in retrieved documents. The benchmark targets agentic information gathering under uncertainty, including query formulation, evidence selection, and final response synthesis.","L1: Language Production (minor)
L2: Planning, Decision-making, Attention, Semantic Understanding & Context Recognition, Working Memory, Episodic Memory (minor)
L3: ",L2
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench measures an agent’s ability to complete multi-turn customer-support tasks in simulated domains (e.g., retail, airline, telecom) while using tools/APIs and complying with policies. It stresses robust dialogue, goal tracking across turns, and consistent adherence to constraints in the face of messy user behavior and edge cases.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Working Memory (minor)
L3: Social Reasoning & Theory of Mind, Inhibitory Control, Empathy (minor)",L3
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,ARC-AGI tests “fluid” abstract reasoning on novel grid-based tasks where the model must infer a latent transformation rule from only a few examples. It emphasizes rapid generalization to unfamiliar patterns and compositional rule induction rather than memorized knowledge.,"L1: Visual Perception
L2: Logical Reasoning, Spatial Representation & Mapping, Working Memory, Attention (minor)
L3: Cognitive Flexibility",L3
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas evaluates real-world tool use through the Model Context Protocol: the model must discover appropriate tools, call them correctly, handle errors/retries, and integrate results into a coherent solution. It stresses multi-step workflow execution with authentic APIs and tool schemas, resembling production agent behavior.","L1: Language Comprehension (minor), Language Production (minor)
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory
L3: ",L2
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam (HLE) is a large, multimodal benchmark intended to probe frontier academic reasoning and knowledge across many disciplines. Questions often require integrating textual and visual information, applying domain knowledge, and performing multi-step reasoning; some evaluation setups also allow tools like search or code.","L1: Language Comprehension, Visual Perception (minor)
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Scene Understanding & Visual Reasoning (minor), Planning (minor), Multisensory Integration (minor)
L3: ",L2
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 evaluates competition-style mathematics problem solving drawn from the American Invitational Mathematics Examination. Problems require constructing solution strategies, performing multi-step symbolic reasoning, and maintaining intermediate results without relying on external factual recall.","L1: Language Comprehension (minor)
L2: Logical Reasoning, Planning, Working Memory
L3: ",L2
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a high-quality subset of graduate-level, Google-proof multiple-choice science questions designed to resist shallow retrieval and test genuine understanding. It emphasizes precise scientific reasoning under distractors, often requiring chaining concepts rather than recalling a single fact.","L1: Language Comprehension (minor)
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory (minor)
L3: ",L2
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends the MMLU-style academic knowledge and reasoning evaluation to many non-English languages, testing whether models can generalize competence across linguistic contexts. It probes breadth across subjects while stressing multilingual comprehension and consistent reasoning across translations.","L1: Language Comprehension, Language Production (minor)
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory (minor)
L3: ",L2
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro is a challenging multimodal benchmark spanning many expert domains where models answer questions grounded in images (diagrams, charts, figures) plus text. It stresses joint visual-text reasoning, handling fine-grained visual evidence and domain-specific interpretation under multiple-choice or structured formats.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Working Memory (minor), Visual Attention & Eye Movements (minor)
L3: ",L2
ScreenShot-Pro,https://gui-agent.github.io/grounding-leaderboard/,https://arxiv.org/abs/2504.07981,"ScreenShot-Pro evaluates screenshot understanding for software/UI contexts, where models must interpret high-resolution interfaces and answer questions that depend on layout, widgets, and on-screen text. It targets visual grounding for professional workflows (e.g., dashboards, settings panels), emphasizing accurate spatial and semantic interpretation of GUIs.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Spatial Representation & Mapping, Semantic Understanding & Context Recognition (minor), Sensorimotor Coordination (minor)
L3: ",L2
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning tests reasoning over scientific figures and accompanying context from academic papers, requiring models to interpret plots/diagrams and answer questions that depend on both visual evidence and scientific semantics. It emphasizes precise extraction of quantitative/structural cues from figures and integrating them into coherent scientific reasoning.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory (minor), Multisensory Integration (minor)
L3: ",L2
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU evaluates multimodal understanding and reasoning over short videos (often with questions requiring temporal integration, event understanding, and context). It probes whether models can track evolving scenes, retain relevant details over time, and combine them with language-based reasoning to answer accurately.","L1: Visual Perception, Auditory Processing (minor)
L2: Attention, Working Memory, Scene Understanding & Visual Reasoning (minor), Multisensory Integration (minor)
L3: Cognitive Timing & Predictive Modeling",L3
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) is a long-context evaluation where multiple similar “needle” requests are embedded within long “haystack” conversations or documents, and the model must retrieve the correct corresponding response. It targets robust long-range reference tracking, interference resistance, and faithful extraction of the right span among distractors.","L1: 
L2: Working Memory, Attention, Episodic Memory, Semantic Understanding & Context Recognition
L3: Inhibitory Control (minor)",L2
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval measures performance on well-specified, economically valuable professional tasks (e.g., creating spreadsheets, slides, schedules, and business artifacts) judged against skilled human outputs. It emphasizes end-to-end task execution quality, including planning deliverables, maintaining constraints, and producing polished, usable work products.","L1: Language Production
L2: Planning, Decision-making, Semantic Understanding & Context Recognition, Working Memory (minor)
L3: Self-reflection (minor)",L2
SWE-Lancer,https://openai.com/index/swe-lancer/,https://arxiv.org/abs/2502.12115,"SWE-Lancer evaluates higher-level software engineering competence beyond single bug fixes, often emphasizing broader coding productivity behaviors such as implementing substantial changes, handling larger repos, and producing patches aligned with real development workflows. It aims to capture more “engineering-like” performance including iterative debugging and coordination with tests/tooling.","L1: Language Comprehension (minor)
L2: Planning, Logical Reasoning, Adaptive Error Correction, Decision-making (minor), Working Memory (minor)
L3: ",L2
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks tests structured reasoning over graph-encoded information, where models must follow edges/relations (often across long contexts) to answer queries about reachability or parent/ancestor relationships. It targets systematic multi-step traversal behavior and resistance to distraction from irrelevant nodes or paths.","L1: 
L2: Logical Reasoning, Spatial Representation & Mapping, Working Memory, Attention (minor), Planning (minor)
L3: ",L2
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon evaluates how well models can solve tasks by selecting, calling, and chaining tools in realistic tool ecosystems, emphasizing correct argument construction and result integration. It stresses reliability of agentic tool use across heterogeneous APIs and the ability to recover from tool errors or unexpected outputs.","L1: Language Comprehension (minor)
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory
L3: Inhibitory Control (minor)",L2
HMMT,https://matharena.ai/?view=problem&comp=hmmt--hmmt_feb_2025,https://arxiv.org/abs/2505.23281,"HMMT evaluates advanced competition mathematics (e.g., Harvard-MIT Mathematics Tournament problems), typically requiring deeper ingenuity than standard curricula. It stresses multi-step proof-like reasoning, strategic problem decomposition, and careful symbolic manipulation under tight precision requirements.","L1: Language Comprehension (minor)
L2: Logical Reasoning, Planning, Working Memory
L3: Cognitive Flexibility (minor)",L2
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath is designed to measure expert-level mathematics and research-adjacent problem solving, including problems that require nontrivial insight and extended derivations. It aims to distinguish frontier reasoning capability by emphasizing difficult, multi-stage mathematical thinking that is hard to solve via shallow pattern matching.","L1: 
L2: Logical Reasoning, Planning, Working Memory
L3: Cognitive Flexibility, Self-reflection (minor)",L3
