Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates patch-based software engineering on real GitHub issues, where a model must produce a code change that makes the project’s tests pass. The “Verified” subset consists of tasks that have been manually checked as solvable and appropriately specified, aiming to reduce noise from ambiguous or unsound issues.","L1: Language Comprehension (minor)
L2: Planning, Logical Reasoning, Working Memory, Adaptive Error Correction, Decision-making (minor)
L3: ",L2
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro is a more difficult software engineering benchmark designed to better reflect professional, real-world patch generation and reduce contamination and shortcut solving. Compared to SWE-bench Verified, it increases task diversity and difficulty (including broader stacks/languages and more complex repos) and places higher emphasis on robust end-to-end issue resolution.","L1: 
L2: Planning, Logical Reasoning, Adaptive Error Correction, Working Memory, Decision-making (minor)
L3: Cognitive Flexibility (minor)",L2
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp evaluates “deep research” behavior: answering questions that require searching, reading, and synthesizing information across multiple sources rather than relying on memorized facts. It targets agentic browsing competence, such as deciding what to search, selecting sources, and integrating evidence into a final response.","L1: Language Comprehension (minor)
L2: Planning, Decision-making, Attention, Working Memory, Semantic Understanding & Context Recognition
L3: ",L2
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench measures agentic customer-support performance in simulated domains (e.g., retail, airline, telecom) where the agent must follow policies while interacting with APIs and a simulated user over multiple turns. It stresses tool use reliability, long-horizon dialog consistency, and adherence to constraints under realistic conversational pressure.","L1: Language Comprehension
L2: Decision-making, Planning
L3: Social Reasoning & Theory of Mind, Inhibitory Control, Empathy (minor)",L3
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI tests fluid reasoning by requiring models to infer novel transformation rules from a few input–output grid examples and apply them to new inputs. It is designed to minimize reliance on memorized knowledge, emphasizing abstraction, systematic generalization, and pattern discovery under data scarcity.","L1: Visual Perception (minor)
L2: Logical Reasoning, Working Memory, Spatial Representation & Mapping, Scene Understanding & Visual Reasoning (minor)
L3: Cognitive Flexibility",L3
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas evaluates real-world tool use via the Model Context Protocol (MCP), where models must discover, invoke, and chain multiple tools/services to complete tasks. It emphasizes correct API selection and parameterization, multi-step workflow execution, and robust recovery from tool errors or partial results.","L1: Language Comprehension (minor)
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition (minor)
L3: ",L2
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam (HLE) is a multimodal benchmark intended to probe frontier knowledge and reasoning across many expert-level topics and formats, including questions that benefit from tools like search or code. It targets broad generalization and synthesis—connecting domain knowledge, evidence, and multi-step reasoning into accurate answers.","L1: Language Comprehension, Visual Perception (minor)
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory (minor), Scene Understanding & Visual Reasoning (minor)
L3: ",L2
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 evaluates competition-style mathematics problem solving under strict answer-format constraints. Problems often require multi-step derivations, clever transformations, and careful bookkeeping rather than rote formula recall.","L1: 
L2: Logical Reasoning, Working Memory, Planning (minor)
L3: ",L2
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a high-difficulty, “Google-proof” multiple-choice science benchmark (physics, chemistry, biology) curated to resist simple retrieval and reward genuine reasoning. The Diamond subset focuses on the highest-quality questions where experts succeed and non-experts typically fail.","L1: Language Comprehension (minor)
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory (minor), Decision-making (minor)
L3: ",L2
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends MMLU-style academic knowledge testing across multiple languages, probing whether a model’s knowledge and reasoning transfer beyond English. It assesses subject-matter understanding with multilingual prompts and answers, emphasizing cross-lingual robustness and comprehension.","L1: Language Comprehension, Language Production (minor)
L2: Semantic Understanding & Context Recognition, Logical Reasoning (minor)
L3: ",L2
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro is a more challenging variant of MMMU for multimodal, expert-level questions spanning multiple disciplines where answers depend on jointly interpreting text and images. It emphasizes fine-grained visual understanding, cross-modal grounding, and multi-step reasoning over diagrams, charts, and scientific figures.","L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Working Memory (minor)
L3: ",L2
ScreenShot-Pro,https://gui-agent.github.io/grounding-leaderboard/,https://arxiv.org/abs/2504.07981,"ScreenShot-Pro evaluates understanding of high-resolution UI screenshots, requiring models to locate relevant interface elements and answer questions grounded in the visual layout. It targets GUI comprehension for agentic “computer use,” where success depends on spatially precise interpretation of visual affordances and labels.","L1: Visual Perception, Language Comprehension (minor)
L2: Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Spatial Representation & Mapping, Sensorimotor Coordination (minor)
L3: ",L2
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning focuses on reasoning over scientific figures and documents (e.g., interpreting plots, diagrams, and figure captions) to answer questions that require more than OCR. It stresses extracting structured information from visuals and combining it with domain context to justify correct answers.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory (minor)
L3: ",L2
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU evaluates multimodal understanding and reasoning over video, requiring integration of temporally distributed visual evidence (and accompanying text prompts) to answer questions. It targets long-range temporal coherence, event understanding, and multi-step inference from dynamic scenes.","L1: Visual Perception
L2: Attention, Working Memory, Scene Understanding & Visual Reasoning (minor), Multisensory Integration (minor)
L3: Cognitive Timing & Predictive Modeling",L3
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) is a long-context evaluation where multiple similar “needle” exchanges are embedded in a large “haystack,” and the model must retrieve the correct response corresponding to a specified needle. It probes robust long-context retrieval, disambiguation among confusable contexts, and maintaining accuracy as context length grows.","L1: Language Comprehension
L2: Working Memory, Attention, Episodic Memory, Semantic Understanding & Context Recognition (minor)
L3: ",L2
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval evaluates economically valuable, well-specified professional knowledge work across many occupations by comparing model outputs to human professional work products (e.g., slides, spreadsheets, plans). It emphasizes end-to-end task execution quality, instruction-following, and producing coherent, useful artifacts under realistic constraints.","L1: Language Production
L2: Planning, Decision-making, Semantic Understanding & Context Recognition, Working Memory (minor)
L3: Self-reflection (minor)",L2
SWE-Lancer,https://openai.com/index/swe-lancer/,https://arxiv.org/abs/2502.12115,"SWE-Lancer targets longer-horizon, agentic software engineering where models must navigate larger codebases and complete multi-step tasks that resemble real engineering work. It stresses sustained problem decomposition, iterative debugging, and coordinating changes across files and components to reach a working solution.","L1: Language Comprehension (minor)
L2: Planning, Adaptive Error Correction, Working Memory, Decision-making
L3: Cognitive Flexibility (minor)",L2
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks evaluates long-context structured reasoning by asking models to traverse or query graphs described in text (often at large scale), such as following edges, finding parents, or performing BFS-like operations. It emphasizes precise state tracking and compositional reasoning over relational structure rather than world knowledge.","L1: 
L2: Logical Reasoning, Working Memory, Attention, Spatial Representation & Mapping, Planning (minor)
L3: ",L2
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon measures an agent’s ability to solve tasks by selecting and composing many tools across multi-step workflows, typically under realistic API constraints and with opportunities for tool failures. It emphasizes orchestration quality: choosing the right tool at the right time, validating outputs, and producing a correct final result.","L1: 
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition (minor)
L3: ",L2
HMMT,https://matharena.ai/?view=problem&comp=hmmt--hmmt_feb_2025,https://arxiv.org/abs/2505.23281,"HMMT evaluates high-level competition mathematics performance (e.g., problems from Harvard-MIT Mathematics Tournament settings as packaged in MathArena). Tasks often require multi-step proofs or derivations, strategic insight, and careful symbolic manipulation under tight correctness demands.","L1: 
L2: Logical Reasoning, Working Memory, Planning (minor)
L3: ",L2
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath is designed to measure progress on expert- and research-level mathematics by using hard, carefully curated problems and robust scoring protocols. It targets deep multi-step reasoning and the ability to sustain correct abstractions over long solution chains, often beyond typical contest difficulty tiers.","L1: 
L2: Logical Reasoning, Working Memory, Planning
L3: Cognitive Flexibility (minor)",L2
