Benchmark,Website,Paper,Description,Cognitive Functions
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates autonomous software engineering by asking a model to generate a code patch that fixes a real GitHub issue in an existing repository, with correctness checked via tests. The “Verified” split contains tasks that have been human-validated as solvable and aims to reduce noise from underspecified or broken tasks.","Planning, Logical Reasoning, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition, Language Comprehension (minor), Language Production (minor)"
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro is a larger, more difficult software engineering benchmark designed to be more contamination-resistant and industrially representative than earlier SWE-bench variants. Models must understand large codebases, implement or repair behavior, and produce patches that pass automated checks across multiple languages and settings.","Planning, Decision-making, Logical Reasoning, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition, Inhibitory Control (minor)"
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp evaluates “deep research” style agents on questions whose answers require multi-step web browsing, evidence gathering, and synthesis rather than single-document lookup. Strong performance depends on choosing useful queries, iterating based on findings, and consolidating information into a final answer with minimal hallucination.","Planning, Decision-making, Attention, Working Memory, Episodic Memory (minor), Semantic Understanding & Context Recognition, Language Comprehension, Language Production"
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench evaluates interactive agent behavior in simulated customer-support domains (e.g., retail, airline, telecom), where the agent must use tools/APIs, follow policies, and respond to a simulated user over multiple turns. It emphasizes reliable tool use under constraints, consistent policy adherence, and coherent multi-turn resolution of user goals.","Social Reasoning & Theory of Mind, Decision-making, Planning, Inhibitory Control, Working Memory, Language Comprehension, Language Production, Empathy (minor)"
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI measures abstract, few-shot reasoning on novel grid-based tasks where models infer a latent transformation rule from a handful of examples and apply it to a new input. It is intended to probe “fluid intelligence” and generalization to unfamiliar pattern problems with minimal task-specific training signal.","Logical Reasoning, Cognitive Flexibility, Working Memory, Spatial Representation & Mapping, Scene Understanding & Visual Reasoning, Visual Perception, Attention (minor)"
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas evaluates real-world tool use via the Model Context Protocol (MCP), requiring models to discover appropriate tools, invoke them with correct parameters, handle errors/retries, and synthesize results across multi-step workflows. The benchmark focuses on execution reliability and correct integration of tool outputs into final responses.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition, Language Comprehension (minor), Language Production (minor)"
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam (HLE) is a difficult multimodal benchmark spanning expert-level questions across many domains, intended to stress broad knowledge and reasoning near the frontier of human expertise. Items often require multi-step derivations or careful interpretation of provided context (and, in some settings, tool use such as search/code) to reach a correct answer.","Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Language Comprehension, Language Production (minor), Visual Perception (minor), Scene Understanding & Visual Reasoning (minor), Multisensory Integration (minor)"
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 evaluates mathematical problem solving on contest-style questions that typically require creative algebraic, geometric, combinatorial, or number-theoretic reasoning. Solutions demand maintaining intermediate symbolic constraints and selecting appropriate strategies under time/complexity pressure.","Logical Reasoning, Working Memory, Planning (minor), Adaptive Error Correction (minor)"
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a high-quality subset of GPQA consisting of very challenging graduate-level multiple-choice questions in biology, chemistry, and physics that are designed to be difficult to answer via superficial pattern matching. It probes deep scientific reasoning, careful reading, and domain knowledge integration.","Language Comprehension, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory (minor)"
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends MMLU-style multiple-choice evaluation to many non-English languages across a broad range of subjects. It measures multilingual academic knowledge and reasoning, and stresses whether competence transfers across languages rather than relying on English-only cues.","Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory (minor), Language Production (minor)"
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro is a harder, more robust version of MMMU that evaluates multimodal understanding and reasoning across many disciplines using images combined with text questions. Tasks often require interpreting diagrams, tables, plots, or instrument readouts and integrating them with domain knowledge to answer accurately.","Visual Perception, Scene Understanding & Visual Reasoning, Multisensory Integration, Language Comprehension, Logical Reasoning, Working Memory (minor)"
ScreenShot-Pro,https://gui-agent.github.io/grounding-leaderboard/,https://arxiv.org/abs/2504.07981,"ScreenShot-Pro evaluates GUI understanding from high-resolution screenshots, where models must ground language instructions/questions in interface elements and layout. It stresses reading screen content, understanding spatial relationships among widgets, and mapping intent to UI-relevant answers or actions.","Visual Perception, Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Spatial Representation & Mapping, Decision-making (minor), Planning (minor), Language Comprehension (minor)"
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning evaluates reasoning over charts and figures (often sourced from scientific documents), requiring models to extract quantitative/structural information from visuals and answer questions that depend on that evidence. It emphasizes faithful visual grounding and multi-step inference from plotted data rather than generic world knowledge.","Visual Perception, Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Working Memory (minor), Semantic Understanding & Context Recognition (minor)"
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU evaluates multimodal understanding and reasoning over videos paired with text questions, often requiring integration of events across time. It stresses temporal coherence, selective attention to key frames, and combining visual evidence with general knowledge to answer correctly.","Visual Perception, Cognitive Timing & Predictive Modeling, Attention, Working Memory, Scene Understanding & Visual Reasoning, Multisensory Integration, Language Comprehension (minor)"
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,MRCR v2 (multi-round coreference resolution) tests long-context robustness by inserting multiple similar “needle” interactions into a large “haystack” of dialogue and asking the model to recover the correct response corresponding to a specified needle. The 8-needle setting increases distractors and demands precise retrieval rather than vague summarization.,"Working Memory, Attention, Language Comprehension, Semantic Understanding & Context Recognition, Episodic Memory (minor), Inhibitory Control (minor)"
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval evaluates well-specified professional knowledge-work tasks spanning many occupations, using expert human judging to compare model outputs against industry professionals. Tasks frequently require producing real work artifacts (e.g., plans, analyses, spreadsheets/slides), emphasizing end-to-end execution quality, constraint satisfaction, and practical decision-making.","Planning, Decision-making, Language Production, Language Comprehension, Semantic Understanding & Context Recognition, Working Memory (minor), Self-reflection (minor)"
SWE-Lancer,https://openai.com/index/swe-lancer/,https://arxiv.org/abs/2502.12115,"SWE-Lancer evaluates higher-level, more “agentic” software engineering work where models must make progress on realistic engineering tasks that can include ambiguity, prioritization, and longer-horizon execution compared with patch-only benchmarks. It emphasizes selecting the right changes, coordinating subtasks, and delivering a correct and maintainable outcome.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition, Language Production (minor)"
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks evaluates algorithmic reasoning over graph-structured problems presented in textual form, such as following edges, performing traversals, or identifying relationships like parents/ancestors. It stresses step-by-step state tracking and resisting distractions in long sequences of graph navigation operations.","Logical Reasoning, Working Memory, Spatial Representation & Mapping, Attention, Inhibitory Control (minor)"
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon evaluates tool-using agents across diverse tasks that require selecting among tools, calling them correctly, and composing multi-step solutions from tool outputs. It emphasizes robustness to tool errors, correct parameterization, and coherent orchestration over longer workflows than single-call tool tests.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Language Comprehension (minor), Language Production (minor)"
HMMT,https://matharena.ai/?view=problem&comp=hmmt--hmmt_feb_2025,https://arxiv.org/abs/2505.23281,"HMMT is a competition-math benchmark drawn from Harvard-MIT Mathematics Tournament style problems, which tend to be proof-like or puzzle-like and often require creative multi-step reasoning. It probes advanced problem decomposition, maintaining invariants, and executing precise calculations under complex constraints.","Logical Reasoning, Working Memory, Planning (minor), Adaptive Error Correction (minor)"
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath evaluates extremely challenging research-level mathematics, with tiers reflecting increasing difficulty and often requiring novel insights rather than routine techniques. It stresses long-horizon symbolic reasoning, careful control of intermediate assumptions, and high precision in multi-step derivations (sometimes aided by external computation tools).","Logical Reasoning, Working Memory, Planning, Adaptive Error Correction (minor)"
