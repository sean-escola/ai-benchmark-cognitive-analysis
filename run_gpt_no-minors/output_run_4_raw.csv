Benchmark,Website,Paper,Description,Cognitive Functions
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 evaluates agentic software engineering in a real command-line environment, where models must execute shell commands, inspect files, run programs, and iteratively fix issues to complete tasks. It emphasizes end-to-end autonomy: choosing actions, interpreting tool outputs, and recovering from errors under realistic constraints.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Attention (minor), Language Comprehension (minor), Language Production (minor)"
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp measures deep-research performance over a controlled web-like corpus, requiring models to locate, verify, and synthesize information to answer difficult questions. It tests whether a model can plan search strategies, manage evidence across many documents, and produce grounded responses rather than plausible-sounding guesses.","Planning, Decision-making, Semantic Understanding & Context Recognition, Working Memory, Episodic Memory (minor), Attention (minor), Language Comprehension (minor), Language Production (minor)"
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld evaluates computer-use agents on tasks completed within an operating-system desktop (e.g., launching apps, navigating GUIs, editing files, and configuring settings). Success requires integrating visual perception of UI state with sequential action selection, handling long-horizon workflows and recovering from misclicks or wrong navigation steps.","Visual Perception, Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Sensorimotor Coordination, Planning, Decision-making, Adaptive Error Correction, Working Memory (minor)"
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI is a few-shot abstract reasoning benchmark using grid-based input–output examples, where the model must infer the hidden transformation rule and apply it to a new grid. It is designed to probe “fluid intelligence” by minimizing reliance on memorized knowledge and emphasizing novel pattern induction and generalization.","Logical Reasoning, Cognitive Flexibility, Working Memory, Scene Understanding & Visual Reasoning, Visual Perception (minor), Planning (minor)"
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon agent coherence and strategy in a simulated year-long vending-machine business, including pricing, inventory, supplier negotiation, and reacting to market dynamics. The score reflects sustained goal pursuit over many steps with delayed outcomes and the need to balance risk, profit, and operational constraints.","Planning, Decision-making, Reward Mechanisms, Working Memory, Cognitive Timing & Predictive Modeling (minor), Adaptive Error Correction (minor), Self-reflection (minor)"
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym assesses cybersecurity agent capabilities on real-world vulnerability tasks, including identifying known weaknesses from descriptions and attempting discovery of new vulnerabilities in open-source codebases. It stresses systematic exploration, hypothesis-driven debugging, and careful verification to avoid false positives.","Logical Reasoning, Planning, Adaptive Error Correction, Decision-making (minor), Working Memory (minor), Attention (minor), Language Comprehension (minor)"
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"SpreadsheetBench evaluates models on manipulating complex spreadsheets derived from real workflows, often requiring tool-based interaction (e.g., editing cells, formulas, formats, and producing derived tables). It measures whether an agent can maintain task state across many steps, apply arithmetic/logic correctly, and verify outputs against constraints.","Planning, Working Memory, Logical Reasoning, Adaptive Error Correction, Decision-making (minor), Attention (minor), Language Comprehension (minor), Language Production (minor)"
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam (HLE) is a large, frontier-level benchmark spanning academic and professional knowledge, with many questions requiring multi-step reasoning and, in some settings, tool use (search/code). It is designed to stress models on hard, real-world-style problems where shallow pattern matching and hallucination are penalized by grounded evaluation.","Language Comprehension, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Visual Perception (minor), Scene Understanding & Visual Reasoning (minor), Planning (minor)"
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a high-quality subset of graduate-level, “Google-proof” science multiple-choice questions across physics, chemistry, and biology. It targets deep conceptual understanding and careful elimination under adversarially difficult distractors, making superficial retrieval strategies unreliable.","Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory (minor), Language Comprehension (minor), Inhibitory Control (minor)"
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro is a multimodal benchmark covering expert-level questions that combine images (e.g., diagrams, charts, figures) with text across many disciplines. It probes whether models can integrate visual evidence with domain knowledge and execute multi-step reasoning grounded in the image content.","Visual Perception, Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Language Comprehension (minor), Working Memory (minor)"
MathArena Apex,https://matharena.ai/?view=problem&comp=apex--apex_2025,https://arxiv.org/abs/2505.23281,"MathArena Apex aggregates challenging competition-style mathematics problems and reports performance in a way that emphasizes robust problem solving over memorized templates. It stresses multi-step derivations, precise symbolic manipulation, and consistency under tightly specified answers.","Logical Reasoning, Working Memory, Adaptive Error Correction (minor), Planning (minor), Attention (minor)"
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning evaluates reasoning over scientific figures and accompanying context from arXiv-style papers, often requiring extracting quantitative relations from plots/diagrams. It stresses grounding: correct answers depend on faithfully interpreting visual structure and linking it to scientific language and calculations.","Scene Understanding & Visual Reasoning, Visual Perception, Multisensory Integration, Logical Reasoning, Working Memory (minor), Language Comprehension (minor), Attention (minor)"
OmniDocBench 1.5,https://github.com/opendatalab/OmniDocBench,https://arxiv.org/abs/2412.07626,"OmniDocBench 1.5 evaluates document understanding and OCR across heterogeneous layouts, including text blocks, tables, formulas, and reading order. It measures whether models can accurately transcribe and structure content from visually complex documents rather than only recognizing isolated text.","Visual Perception, Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Language Comprehension (minor), Language Production (minor), Working Memory (minor), Attention (minor)"
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU is a multimodal benchmark requiring understanding of video content and answering questions that may depend on temporal events, interactions, and visual cues across frames. It tests whether a model can maintain and integrate evidence over time instead of relying on single-frame recognition.","Visual Perception, Scene Understanding & Visual Reasoning, Episodic Memory, Attention, Working Memory (minor), Multisensory Integration (minor), Auditory Processing (minor)"
LiveCodeBench Pro,https://livecodebenchpro.com/projects/livecodebench-pro/leaderboard,https://arxiv.org/abs/2506.11928,"LiveCodeBench Pro evaluates coding ability on realistic, time-indexed programming tasks designed to reduce leakage and better reflect contemporary software work. It emphasizes producing correct solutions under constraints, often with iterative debugging and test-driven refinement in an execution environment.","Planning, Logical Reasoning, Adaptive Error Correction, Working Memory, Decision-making (minor), Language Comprehension (minor), Language Production (minor)"
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"The FACTS Benchmark Suite systematically evaluates factuality: whether a model’s outputs are supported by the provided context or by reliable world knowledge, and how often it produces unsupported claims. It targets failure modes like hallucination, overconfidence, and drifting away from source evidence during generation.","Semantic Understanding & Context Recognition, Inhibitory Control, Self-reflection (minor), Attention (minor), Language Production (minor), Working Memory (minor)"
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA evaluates commonsense and practical reasoning across many languages and cultures, aiming to test whether models can generalize intuitive physical and everyday reasoning beyond English. It stresses robust understanding under linguistic variation and culturally diverse framing of similar underlying situations.","Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning, Cognitive Flexibility (minor), Spatial Representation & Mapping (minor), Working Memory (minor)"
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 is a long-context, multi-round co-reference resolution benchmark where multiple similar “needle” interactions are embedded in long “haystacks,” and the model must retrieve the correct referenced response. The 8-needle variant increases interference, stressing precision under distractors and robustness to long-range dependencies.","Working Memory, Attention, Episodic Memory, Language Comprehension, Semantic Understanding & Context Recognition (minor), Inhibitory Control (minor)"
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval evaluates economically valuable, well-specified knowledge work by comparing model-produced artifacts (e.g., spreadsheets, slides, plans) against outputs from industry professionals, using expert judging. It emphasizes end-to-end task execution quality: interpreting requirements, producing usable deliverables, and making sound tradeoffs under real constraints.","Planning, Decision-making, Language Production, Semantic Understanding & Context Recognition, Working Memory (minor), Self-reflection (minor), Social Reasoning & Theory of Mind (minor)"
SWE-Lancer,https://openai.com/index/swe-lancer/,https://arxiv.org/abs/2502.12115,"SWE-Lancer evaluates software engineering agents on realistic repository-level tasks that require generating patches, navigating large codebases, and satisfying tests and specifications. It targets reliability in end-to-end engineering workflows, including diagnosing failures, implementing fixes, and validating behavior.","Planning, Logical Reasoning, Adaptive Error Correction, Working Memory, Decision-making (minor), Language Comprehension (minor), Language Production (minor)"
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks measures reasoning over graph-structured data presented in text, requiring models to follow edges, perform traversals (e.g., BFS-style), and answer queries about reachability or parent relations. It stresses systematic multi-step state tracking and resistance to getting lost amid many similar entities.","Spatial Representation & Mapping, Working Memory, Logical Reasoning, Attention (minor), Planning (minor)"
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon evaluates general tool-use competence across diverse APIs and environments, focusing on whether models can select appropriate tools, call them correctly, and combine results into a final solution. It stresses reliable orchestration under multi-step dependencies, error handling, and correct interpretation of tool outputs.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Attention (minor), Language Comprehension (minor), Language Production (minor)"
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath evaluates advanced, expert-level mathematics problems intended to be difficult for both humans and models, with tiers that emphasize deep reasoning and proof-like derivations. It stresses rigorous multi-step inference, quantitative accuracy, and robustness to brittle heuristic shortcuts.","Logical Reasoning, Working Memory, Planning (minor), Adaptive Error Correction (minor), Attention (minor)"
