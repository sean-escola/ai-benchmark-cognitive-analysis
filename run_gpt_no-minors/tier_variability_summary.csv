Benchmark,L1_count,L2_count,L3_count,Total_runs,Mode_tier,Distinct_tiers
SWE-bench Verified,0,25,0,25,L2,1
SWE-bench Pro,0,17,8,25,L2,2
BrowseComp,0,25,0,25,L2,1
τ2-bench,0,0,25,25,L3,1
ARC-AGI,0,0,25,25,L3,1
MCP-Atlas,0,25,0,25,L2,1
Humanity’s Last Exam,0,25,0,25,L2,1
AIME 2025,0,25,0,25,L2,1
GPQA Diamond,0,25,0,25,L2,1
MMMLU,0,24,1,25,L2,2
MMMU-Pro,0,25,0,25,L2,1
ScreenShot-Pro,0,25,0,25,L2,1
CharXiv Reasoning,0,25,0,25,L2,1
Video-MMMU,0,4,21,25,L3,2
MRCR v2 (8-needle),0,25,0,25,L2,1
GDPval,0,25,0,25,L2,1
SWE-Lancer,0,24,1,25,L2,2
Graphwalks,0,25,0,25,L2,1
Toolathon,0,25,0,25,L2,1
HMMT,0,24,1,25,L2,2
FrontierMath,0,16,9,25,L2,2
