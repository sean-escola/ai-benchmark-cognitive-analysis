Benchmark,Website,Paper,Description,Cognitive Functions
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates end-to-end software engineering by giving a model a real GitHub issue and repository snapshot, requiring it to generate a patch that makes all tests pass. The “Verified” subset uses tasks that have been human-validated as solvable, emphasizing correctness under realistic repository constraints and tooling.","Planning, Logical Reasoning, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition, Language Comprehension (minor)"
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro is a harder successor-style software engineering benchmark designed to be more industrially relevant and contamination-resistant, spanning multiple languages and more complex project structures. Models must propose code changes that satisfy tests and implicit engineering constraints, often requiring multi-file edits and careful dependency reasoning.","Planning, Logical Reasoning, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition, Decision-making (minor)"
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp evaluates deep-research agents on information-seeking questions that require multi-step web navigation and synthesis rather than single-document lookup. Success depends on forming a search strategy, tracking evidence across sources, and producing a supported final answer.","Planning, Attention, Working Memory, Language Comprehension, Semantic Understanding & Context Recognition, Decision-making (minor)"
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench measures interactive agent performance in simulated customer-support domains (e.g., retail, airline, telecom) where the model must converse with a user simulator and call APIs/tools while following policies. It stresses robust multi-turn control: keeping goals aligned with policy constraints while resolving the user’s problem.","Social Reasoning & Theory of Mind, Decision-making, Planning, Inhibitory Control, Language Comprehension, Language Production"
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI tests few-shot “fluid reasoning” on novel grid-transformation puzzles, where models infer latent rules from a small set of input–output examples and apply them to a new input. It targets generalization to unseen concepts and compositional rule induction rather than recall of world knowledge.","Logical Reasoning, Cognitive Flexibility, Spatial Representation & Mapping, Working Memory, Attention (minor)"
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas evaluates real-world tool use through the Model Context Protocol by requiring models to discover tools, invoke them correctly, handle errors, and integrate tool outputs into a coherent answer. Tasks are workflow-like and often require multiple dependent calls with state carried across steps.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition"
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam (HLE) is a difficult, expert-oriented benchmark spanning many domains and modalities, intended to probe frontier knowledge and reasoning rather than routine QA. Questions commonly require chaining evidence, using specialist concepts, and (in some setups) leveraging tools like search or code to verify claims.","Language Comprehension, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Scene Understanding & Visual Reasoning (minor), Visual Perception (minor)"
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 evaluates competition-style mathematical problem solving with short final answers, emphasizing precise symbolic reasoning and careful case handling. Problems typically require multi-step derivations and managing intermediate constraints without external references.","Logical Reasoning, Working Memory, Planning (minor), Attention (minor)"
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a high-quality subset of graduate-level science multiple-choice questions designed to be “Google-proof,” emphasizing deep domain reasoning over superficial recall. The Diamond subset filters for questions where experts reliably agree on the correct answer and non-experts struggle, increasing signal for genuine scientific understanding.","Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory (minor), Language Comprehension (minor)"
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends broad academic knowledge testing across many subjects into multiple languages, measuring whether models retain competence under multilingual prompts and culturally varied formulations. It emphasizes robust comprehension and knowledge access rather than tool use or long-horizon action.","Language Comprehension, Semantic Understanding & Context Recognition, Working Memory (minor), Language Production (minor)"
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro is an expert-level multimodal understanding benchmark that combines images (e.g., diagrams, charts, scientific visuals) with text questions across many disciplines. It stresses integrating visual evidence with domain knowledge and performing multi-step reasoning to select or produce correct answers.","Multisensory Integration, Scene Understanding & Visual Reasoning, Visual Perception, Language Comprehension, Logical Reasoning (minor), Working Memory (minor)"
ScreenShot-Pro,https://gui-agent.github.io/grounding-leaderboard/,https://arxiv.org/abs/2504.07981,"ScreenShot-Pro evaluates screenshot understanding and GUI grounding: models must interpret high-resolution interface screenshots and answer questions that require locating elements, reading fine text, and understanding layout and affordances. It targets practical visual grounding for agentic computer use rather than generic image captioning.","Visual Perception, Scene Understanding & Visual Reasoning, Spatial Representation & Mapping, Visual Attention & Eye Movements, Planning (minor)"
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,CharXiv Reasoning tests reasoning over figures and technical content from scientific papers (often requiring extracting quantitative or relational information from charts/diagrams). It emphasizes connecting visual evidence to textual scientific context and performing structured inference over that combined information.,"Scene Understanding & Visual Reasoning, Visual Perception, Multisensory Integration, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory (minor)"
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU evaluates multimodal reasoning over video, requiring models to integrate information across frames and time to answer questions about actions, events, and visual details. It probes temporal integration and attention control under long visual contexts, often combined with language-based explanations or choices.","Visual Perception, Cognitive Timing & Predictive Modeling, Attention, Working Memory, Multisensory Integration (minor), Scene Understanding & Visual Reasoning (minor)"
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) is a long-context evaluation based on multi-round co-reference and retrieval: multiple similar “needle” interactions are embedded in a long “haystack,” and the model must reproduce the correct referenced response. It stresses maintaining and selecting the right discourse state amid distractors over long contexts.","Working Memory, Attention, Episodic Memory, Language Comprehension, Semantic Understanding & Context Recognition"
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval evaluates economically valuable, well-specified professional knowledge-work tasks across many occupations, often requiring producing realistic work artifacts (e.g., analyses, plans, structured documents). Judging is performed by expert humans comparing outputs, so success depends on goal satisfaction, clarity, and reliability under practical constraints.","Planning, Decision-making, Language Production, Semantic Understanding & Context Recognition, Self-reflection (minor), Social Reasoning & Theory of Mind (minor)"
SWE-Lancer,https://openai.com/index/swe-lancer/,https://arxiv.org/abs/2502.12115,"SWE-Lancer evaluates agentic software engineering in a more end-to-end, contractor-like setting, focusing on completing realistic coding tasks that resemble professional work rather than isolated algorithm problems. It emphasizes producing correct patches while managing ambiguity, requirements interpretation, and iterative debugging.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition"
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks tests multi-step reasoning over graph-structured data where models must follow traversal rules (e.g., BFS-like walks, parent pointers) and return correct nodes/paths. It targets systematic state tracking and resisting distraction across many incremental transitions.","Working Memory, Logical Reasoning, Spatial Representation & Mapping, Attention (minor)"
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon evaluates general tool-use competence across diverse tools and tasks, typically requiring models to decide which tools to call, execute correct arguments, recover from failures, and synthesize results. It is designed to reflect real agent workflows where tool selection and coordination matter as much as raw reasoning.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Language Comprehension (minor)"
HMMT,https://matharena.ai/?view=problem&comp=hmmt--hmmt_feb_2025,https://arxiv.org/abs/2505.23281,"HMMT is a high-level math competition benchmark (here via MathArena) composed of challenging contest problems that require creative problem decomposition and rigorous derivations. Compared with routine math QA, it emphasizes deeper strategy selection and maintaining correctness over long chains of inference.","Logical Reasoning, Working Memory, Planning (minor), Attention (minor)"
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath is an expert-level mathematics benchmark intended to be difficult for current models and to better reflect frontier research-style problem solving than standard contests. It emphasizes robust multi-step reasoning, careful handling of edge cases, and (in some settings) verification with computational tools.","Logical Reasoning, Planning, Working Memory, Cognitive Flexibility, Adaptive Error Correction (minor)"
