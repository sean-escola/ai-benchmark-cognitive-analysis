Benchmark,Website,Paper,Description,Cognitive Functions
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 evaluates agentic software engineering in real command-line environments, requiring models to navigate files, run programs, install dependencies, and debug using shell tools. Tasks emphasize end-to-end autonomy under realistic constraints (stateful environment, tool errors, and long action sequences) rather than isolated code generation.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Language Comprehension (minor)"
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp measures “deep research” ability: agents must answer complex questions by searching a fixed web-like corpus and synthesizing evidence across multiple documents. It stresses retrieval strategy, source triage, and faithful synthesis under long, multi-step browsing trajectories.","Planning, Attention, Semantic Understanding & Context Recognition, Working Memory, Language Production (minor)"
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld evaluates multimodal computer-use agents on realistic desktop tasks (e.g., navigating apps, webpages, and system dialogs) with step limits and high-resolution screenshots. Success depends on visually grounding UI elements, choosing correct actions, and adapting to dynamic interface states and errors.","Visual Perception, Scene Understanding & Visual Reasoning, Sensorimotor Coordination, Planning, Attention (minor)"
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI tests fluid intelligence via few-shot induction of hidden rules from small grid input–output examples, requiring generalization to a new grid. It is designed to reduce reliance on memorized knowledge and instead probe abstract pattern discovery and compositional reasoning.","Logical Reasoning, Cognitive Flexibility, Spatial Representation & Mapping, Working Memory"
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon autonomous business management in a simulated environment, where agents run a vending machine company over many in-game days. Agents must plan inventory, pricing, procurement, and communications while adapting to changing market conditions to maximize final balance.","Planning, Decision-making, Reward Mechanisms, Working Memory, Self-reflection (minor)"
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity agent capability across large-scale tasks involving identifying known vulnerabilities and discovering new ones in real open-source codebases. It stresses multi-step investigation, hypothesis testing, and iterative debugging under realistic tooling and adversarial failure modes.","Logical Reasoning, Planning, Adaptive Error Correction, Working Memory, Inhibitory Control (minor)"
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"SpreadsheetBench measures the ability to understand and manipulate complex spreadsheets using tools (e.g., editing cells, formulas, tables, and formatting) to produce correct outputs. Tasks often require structured reasoning over tabular data and iterative verification across multiple steps.","Logical Reasoning, Working Memory, Planning, Attention (minor)"
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam (HLE) is a large multimodal benchmark intended to probe frontier knowledge and reasoning across diverse expert-level domains. Questions frequently require integrating text with figures or diagrams and producing well-justified answers, sometimes with optional tool use in evaluated setups.","Language Comprehension, Logical Reasoning, Multisensory Integration, Semantic Understanding & Context Recognition, Visual Perception (minor)"
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a high-quality subset of graduate-level, “Google-proof” multiple-choice science questions where non-experts tend to fail. It emphasizes precise scientific reasoning and discrimination among closely related answer choices under high knowledge and rigor demands.","Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory (minor), Language Comprehension (minor)"
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro is a challenging multimodal understanding and reasoning benchmark spanning many disciplines, designed for expert-level vision-language reasoning. It tests interpreting images (charts, diagrams, scenes) alongside text prompts to answer questions that require grounded inference rather than surface recognition.","Visual Perception, Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning"
MathArena Apex,https://matharena.ai/?view=problem&comp=apex--apex_2025,https://arxiv.org/abs/2505.23281,"MathArena Apex aggregates difficult competition-style mathematics problems and evaluates models under standardized settings, often emphasizing rigorous solution finding rather than explanation quality. It is used to compare high-end mathematical reasoning and reliability across strong models.","Logical Reasoning, Working Memory, Planning (minor)"
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning evaluates reasoning over figures and visual content from scientific papers, requiring models to answer questions that depend on correctly interpreting plots, schematics, and annotated graphics. It stresses visual grounding, cross-referencing captions/context, and multi-step inference.","Scene Understanding & Visual Reasoning, Visual Perception, Multisensory Integration, Logical Reasoning, Attention (minor)"
OmniDocBench 1.5,https://github.com/opendatalab/OmniDocBench,https://arxiv.org/abs/2412.07626,"OmniDocBench 1.5 evaluates document understanding and OCR across heterogeneous layouts, including text, formulas, tables, and reading order. Metrics emphasize faithful extraction and structural correctness, reflecting real-world document pipelines where layout parsing and symbol accuracy matter.","Visual Perception, Visual Attention & Eye Movements, Language Comprehension, Working Memory (minor)"
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU extends multimodal reasoning to video, requiring models to integrate information across frames and time to answer questions about events, actions, and causal structure. It targets temporal understanding beyond single-image perception, often requiring tracking entities and state changes.","Visual Perception, Cognitive Timing & Predictive Modeling, Working Memory, Scene Understanding & Visual Reasoning, Attention (minor)"
LiveCodeBench Pro,https://livecodebenchpro.com/projects/livecodebench-pro/leaderboard,https://arxiv.org/abs/2506.11928,"LiveCodeBench Pro evaluates coding ability on competitive programming and practical coding tasks with strong controls against leakage and a leaderboard-style rating (e.g., Elo). It measures not only writing code but also selecting algorithms, handling edge cases, and iterating toward correct solutions.","Logical Reasoning, Planning, Adaptive Error Correction, Working Memory, Language Production (minor)"
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"FACTS Benchmark Suite systematically evaluates factuality across multiple sub-benchmarks, focusing on whether model outputs are supported by evidence and avoid hallucinated claims. It targets faithfulness under pressure from plausible-sounding completions and tests robustness across domains and formats.","Semantic Understanding & Context Recognition, Inhibitory Control, Self-reflection (minor), Language Production (minor)"
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA is a multilingual/non-parallel extension of physical commonsense question answering, probing whether models can reason about everyday interactions with objects and environments across languages and cultures. Items emphasize plausible physical affordances and outcomes rather than specialist knowledge.","Logical Reasoning, Semantic Understanding & Context Recognition, Spatial Representation & Mapping (minor), Language Comprehension (minor)"
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 is a long-context, multi-round coreference-style retrieval benchmark where many similar “needle” requests are embedded in a long “haystack” of dialogue, and the model must reproduce the correct referenced response. The 8-needle setting stresses sustained attention and interference resistance across long contexts.","Working Memory, Attention, Episodic Memory, Language Comprehension (minor)"
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval evaluates well-specified professional knowledge-work tasks (e.g., producing spreadsheets, slides, schedules, and other artifacts) across many occupations, judged by expert humans in head-to-head comparisons. It probes end-to-end task execution quality, including planning, adherence to constraints, and producing usable deliverables.","Planning, Decision-making, Language Production, Semantic Understanding & Context Recognition, Self-reflection (minor)"
SWE-Lancer,https://openai.com/index/swe-lancer/,https://arxiv.org/abs/2502.12115,"SWE-Lancer evaluates software engineering agents on realistic repository-level tasks, emphasizing autonomous patch generation and robustness across diverse problem types. It is designed to better reflect industrial workflows than short-form coding problems, rewarding correct integration with existing code and tests.","Planning, Adaptive Error Correction, Working Memory, Logical Reasoning, Language Comprehension (minor)"
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks tests the ability to follow graph-structured relations embedded in text (e.g., BFS traversals or parent-pointer reasoning) under long-context settings. It stresses systematic state tracking and consistent multi-step traversal rather than open-ended generation.","Working Memory, Logical Reasoning, Attention, Spatial Representation & Mapping (minor)"
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon evaluates tool-using agents on multi-step tasks requiring correct tool selection, parameterization, error handling, and synthesis of results across heterogeneous tools. It targets practical agent reliability, including recovering from tool failures and maintaining a coherent execution plan.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Inhibitory Control (minor)"
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath is a hard mathematics benchmark designed to measure progress near the frontier of expert-level problem solving, with tiers reflecting increasing difficulty. It emphasizes rigorous multi-step reasoning and precision, often benefiting from careful decomposition and verification rather than pattern matching.","Logical Reasoning, Working Memory, Planning, Adaptive Error Correction (minor)"
