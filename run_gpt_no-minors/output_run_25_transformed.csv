Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates an agent’s ability to fix real issues in open-source GitHub repositories by generating code patches that pass the project’s tests. The “Verified” subset consists of tasks that have been validated as solvable and reliably graded, emphasizing end-to-end debugging rather than toy coding puzzles.","L1: Language Comprehension (minor), Language Production (minor)
L2: Planning, Logical Reasoning, Working Memory, Adaptive Error Correction, Semantic Understanding & Context Recognition (minor)
L3: ",L2
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro is a harder software engineering benchmark with more tasks and stronger emphasis on real-world engineering complexity (e.g., broader codebases, more varied requirements, and more demanding evaluation). It targets agentic development skills such as diagnosing failures, implementing correct fixes, and avoiding regressions across a rigorous suite.","L1: Language Comprehension (minor), Language Production (minor)
L2: Planning, Logical Reasoning, Working Memory, Adaptive Error Correction, Decision-making (minor)
L3: ",L2
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp evaluates “deep research” style browsing agents on questions that require multi-step web search, reading, cross-checking, and synthesis rather than recalling a single fact. Performance depends on navigating information sources, selecting relevant evidence, and maintaining a coherent research plan across tool calls.","L1: Language Comprehension (minor)
L2: Planning, Attention, Working Memory, Semantic Understanding & Context Recognition, Decision-making (minor)
L3: Self-reflection (minor)",L2
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench measures an agent’s ability to complete multi-turn customer-support tasks in simulated domains (e.g., retail, airline, telecom) while using APIs and following policies. It emphasizes robust tool use under constraints, consistency across turns, and policy-compliant decision-making under realistic interaction dynamics.","L1: Language Comprehension, Language Production
L2: Decision-making, Planning, Working Memory (minor)
L3: Inhibitory Control, Social Reasoning & Theory of Mind",L3
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI tests fluid intelligence by requiring models to infer latent transformation rules from a few input–output grid examples and apply them to new inputs. Tasks are designed to be novel and compositional, stressing generalization, abstraction, and systematic reasoning on spatial patterns.","L1: Visual Perception
L2: Logical Reasoning, Spatial Representation & Mapping, Working Memory, Planning (minor), Attention (minor)
L3: Cognitive Flexibility",L3
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas evaluates real-world tool use through the Model Context Protocol by testing whether models can discover appropriate tools, call them correctly, handle errors, and compose multi-step workflows across servers. It emphasizes practical agent reliability in production-like API environments rather than single-shot Q&A.","L1: Language Comprehension (minor), Language Production (minor)
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory
L3: ",L2
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam (HLE) is a broad, frontier difficulty benchmark spanning complex questions across many domains, often requiring deep reasoning, synthesis, and sometimes multimodal interpretation. It is intended to stress near-boundary capabilities on challenging, knowledge-intensive and reasoning-intensive items.","L1: Language Comprehension, Visual Perception (minor)
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory (minor), Scene Understanding & Visual Reasoning (minor)
L3: ",L2
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 evaluates competition-style mathematics problem solving, typically requiring multi-step symbolic reasoning and careful constraint handling. It targets precise derivations and error-free arithmetic/algebra under tight problem statements.","L1: 
L2: Logical Reasoning, Working Memory, Planning, Adaptive Error Correction (minor), Attention (minor)
L3: ",L2
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a high-difficulty multiple-choice science QA benchmark curated to be challenging for non-experts and resistant to simple web lookup. It stresses rigorous reasoning over complex scientific content across physics, chemistry, and biology.","L1: Language Comprehension (minor)
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory (minor)
L3: ",L2
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends MMLU-style knowledge and reasoning evaluation into multiple languages, testing performance across many subjects and diverse linguistic contexts. It probes multilingual understanding and the ability to apply learned knowledge consistently across languages.","L1: Language Comprehension, Language Production (minor)
L2: Semantic Understanding & Context Recognition, Logical Reasoning (minor), Working Memory (minor)
L3: ",L2
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,MMMU-Pro is a multimodal benchmark that evaluates expert-level understanding and reasoning over images paired with text across many disciplines. It emphasizes integrating visual evidence with domain knowledge to answer multiple-choice or structured questions.,"L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Multisensory Integration, Working Memory (minor)
L3: ",L2
ScreenShot-Pro,https://gui-agent.github.io/grounding-leaderboard/,https://arxiv.org/abs/2504.07981,"ScreenShot-Pro evaluates GUI and screenshot understanding, where models must interpret high-resolution interfaces and answer questions or take actions grounded in on-screen elements. It targets reliable visual grounding, layout reasoning, and selecting correct targets in complex UIs.","L1: Visual Perception
L2: Visual Attention & Eye Movements, Spatial Representation & Mapping, Scene Understanding & Visual Reasoning, Sensorimotor Coordination (minor), Planning (minor)
L3: ",L2
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning tests scientific-figure and paper-content reasoning using arXiv-style figures and associated questions, often requiring quantitative and diagrammatic interpretation. It emphasizes extracting structured information from visual scientific artifacts and integrating it with textual context.","L1: Visual Perception, Language Comprehension (minor)
L2: Scene Understanding & Visual Reasoning, Semantic Understanding & Context Recognition, Logical Reasoning
L3: ",L2
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU evaluates multimodal understanding over videos, requiring temporal integration to answer questions about events, actions, and context. It stresses tracking changing scenes and maintaining coherence across frames rather than single-image interpretation.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Working Memory, Attention (minor), Multisensory Integration (minor)
L3: Cognitive Timing & Predictive Modeling",L3
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,MRCR v2 (8-needle) evaluates long-context multi-round coreference resolution by embedding multiple similar “needle” interactions inside long “haystacks” and asking the model to retrieve the correct referenced response. It primarily measures robust retrieval and disambiguation under heavy context interference.,"L1: Language Comprehension
L2: Working Memory, Episodic Memory, Attention, Semantic Understanding & Context Recognition (minor)
L3: ",L2
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval measures performance on well-specified professional knowledge-work tasks across many occupations, judged by expert evaluators on output quality (often including artifacts like documents, spreadsheets, or plans). It targets end-to-end task execution quality under realistic workplace specifications.","L1: Language Production
L2: Planning, Decision-making, Semantic Understanding & Context Recognition, Working Memory (minor)
L3: Self-reflection (minor)",L2
SWE-Lancer,https://openai.com/index/swe-lancer/,https://arxiv.org/abs/2502.12115,"SWE-Lancer evaluates agentic software engineering in a more “job-like” setting, emphasizing longer-horizon execution and realistic constraints for completing engineering tasks end-to-end. It targets the ability to understand a codebase, implement correct changes, and iteratively resolve failures in a production-like workflow.","L1: Language Comprehension (minor), Language Production (minor)
L2: Planning, Adaptive Error Correction, Logical Reasoning, Working Memory, Decision-making (minor)
L3: ",L2
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks evaluates long-context reasoning over graph-structured data, such as following edges, recovering paths, or answering queries that require consistent traversal and bookkeeping. It stresses systematic multi-step reasoning where small errors compound across long dependency chains.","L1: 
L2: Spatial Representation & Mapping, Working Memory, Logical Reasoning, Attention (minor), Planning (minor)
L3: ",L2
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon benchmarks an agent’s ability to use many tools/APIs across heterogeneous tasks, emphasizing correct tool selection, parameterization, and integration of results into final answers. It is designed to probe robustness of tool-use policies under multi-step workflows and partial failures.","L1: Language Comprehension (minor), Language Production (minor)
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory
L3: ",L2
HMMT,https://matharena.ai/?view=problem&comp=hmmt--hmmt_feb_2025,https://arxiv.org/abs/2505.23281,"HMMT (as used in MathArena) evaluates high-difficulty contest mathematics problems that often require creative problem decomposition, proof-style reasoning, and careful symbolic manipulation. It is generally harder and more varied than standard competition sets, stressing reliability under complexity.","L1: 
L2: Logical Reasoning, Planning, Working Memory, Attention (minor)
L3: Cognitive Flexibility (minor)",L2
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath is an expert-level mathematics evaluation intended to measure progress on problems beyond routine contest math, including more research-adjacent and technically demanding items. It stresses deep multi-step reasoning, precision, and the ability to manage complex intermediate states over long solutions.","L1: 
L2: Logical Reasoning, Planning, Working Memory, Adaptive Error Correction (minor)
L3: Cognitive Flexibility (minor)",L2
