Benchmark,Website,Paper,Description,Cognitive Functions
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,SWE-bench Verified evaluates software engineering by giving a real GitHub repository plus an issue and asking the model to produce a patch that makes the test suite pass. The Verified subset consists of problems validated by human experts to be solvable and is commonly used to assess end-to-end debugging and code change quality under realistic constraints.,"Logical Reasoning, Planning, Working Memory, Adaptive Error Correction, Language Comprehension (minor), Language Production (minor), Semantic Understanding & Context Recognition"
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro is a more difficult, larger-scale software engineering benchmark intended to better reflect industrial complexity and reduce shortcuts from superficial pattern matching. It expands scope and difficulty versus SWE-bench Verified, requiring robust repository understanding, multi-step debugging, and correct patch generation across varied codebases.","Logical Reasoning, Planning, Working Memory, Adaptive Error Correction, Cognitive Flexibility, Semantic Understanding & Context Recognition, Language Comprehension (minor)"
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp evaluates deep-research agents on information-seeking tasks where the model must locate and synthesize answers from a constrained web or document corpus, typically via search and browsing tools. It emphasizes long-horizon decomposition, evidence gathering, and faithful synthesis under limited attempts.","Planning, Decision-making, Attention, Working Memory, Episodic Memory (minor), Language Comprehension, Language Production, Semantic Understanding & Context Recognition"
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench evaluates interactive agent behavior in simulated customer-support settings (e.g., retail/airline/telecom), where the agent must converse with a user and call APIs while following domain policies. Success depends on multi-turn state tracking, correct tool invocation, and adhering to constraints despite user pressure or ambiguity.","Social Reasoning & Theory of Mind, Decision-making, Planning, Inhibitory Control, Working Memory, Language Comprehension, Language Production, Empathy (minor)"
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI measures fluid intelligence via few-shot abstract pattern induction on grid transformation puzzles: the model infers a hidden rule from a handful of input-output examples and applies it to a new input. It is designed to reward generalizable reasoning over memorization, with strong emphasis on novel task adaptation.","Logical Reasoning, Cognitive Flexibility, Working Memory, Spatial Representation & Mapping, Visual Perception, Scene Understanding & Visual Reasoning, Attention (minor)"
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas measures real-world tool use through the Model Context Protocol by testing whether a model can discover relevant tools, call them correctly, handle failures, and integrate results into an answer. It targets multi-step workflows and robustness to API/interface details rather than purely text-only reasoning.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition, Language Comprehension (minor)"
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a frontier-level benchmark spanning difficult, often multimodal questions intended to probe advanced reasoning and knowledge at the edge of what models can reliably solve. It typically requires integrating domain knowledge with careful reasoning and, in some settings, effective use of tools such as search or code execution.","Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Visual Perception (minor), Scene Understanding & Visual Reasoning (minor), Multisensory Integration (minor)"
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 evaluates competition-style mathematics problem solving drawn from the American Invitational Mathematics Examination. Problems require multi-step derivations and careful symbolic reasoning, with answers typically being short integers that reduce reward for verbose but incorrect explanations.","Logical Reasoning, Working Memory, Planning, Cognitive Flexibility (minor)"
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a high-difficulty, multiple-choice science QA benchmark designed to be “Google-proof,” emphasizing questions that non-experts usually miss while experts solve reliably. It probes deep scientific understanding and precise reasoning under distractors across biology, chemistry, and physics.","Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory (minor), Language Comprehension (minor)"
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends broad academic knowledge testing to multiple languages, measuring whether a model can answer subject-matter questions across many domains in non-English settings. It stresses multilingual generalization and robust semantic understanding across varied cultural and linguistic contexts.","Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning (minor), Working Memory (minor), Language Production (minor)"
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro is a harder, more diagnostic variant of multimodal expert understanding that combines images (e.g., diagrams, charts, screenshots) with domain questions across many disciplines. It emphasizes grounded multimodal reasoning and careful interpretation of visual evidence alongside textual context.","Visual Perception, Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Visual Attention & Eye Movements (minor), Working Memory (minor), Semantic Understanding & Context Recognition"
ScreenShot-Pro,https://gui-agent.github.io/grounding-leaderboard/,https://arxiv.org/abs/2504.07981,"ScreenShot-Pro evaluates GUI understanding from screenshots by requiring models to identify interface elements, interpret layout, and (in many setups) select grounded actions such as clicks or form fills. It targets fine-grained visual grounding and spatial reasoning over real-world UI designs rather than natural-image recognition alone.","Visual Perception, Visual Attention & Eye Movements, Spatial Representation & Mapping, Scene Understanding & Visual Reasoning, Decision-making (minor), Sensorimotor Coordination (minor)"
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning tests whether models can answer reasoning questions grounded in scientific paper figures (charts, plots, tables, and rendered content) and associated context. It stresses extracting quantitative/structural information from visuals and performing multi-step inference rather than merely describing images.","Scene Understanding & Visual Reasoning, Visual Perception, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory (minor), Attention (minor)"
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU evaluates multimodal understanding and reasoning over videos paired with questions that require integrating information across time. It probes event tracking, temporal coherence, and inference from dynamic scenes rather than single-frame perception.","Visual Perception, Scene Understanding & Visual Reasoning, Working Memory, Cognitive Timing & Predictive Modeling, Attention, Multisensory Integration (minor)"
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 is a long-context evaluation where multiple similar “needle” requests are embedded in a long “haystack,” and the model must retrieve the correct referenced response for a specified needle. The 8-needle variant increases interference, stressing robust co-reference resolution and retrieval over extended contexts.","Working Memory, Episodic Memory, Attention, Language Comprehension, Semantic Understanding & Context Recognition (minor)"
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval measures performance on well-specified, economically valuable knowledge-work tasks spanning many occupations, often judged by experts via pairwise comparisons. Tasks include producing professional artifacts (e.g., analyses, plans, spreadsheets/slides), emphasizing end-to-end task execution quality and usefulness.","Planning, Decision-making, Language Production, Semantic Understanding & Context Recognition, Working Memory (minor), Self-reflection (minor), Social Reasoning & Theory of Mind (minor)"
SWE-Lancer,https://openai.com/index/swe-lancer/,https://arxiv.org/abs/2502.12115,"SWE-Lancer evaluates agentic software engineering on realistic coding tasks, emphasizing end-to-end patch production with stronger incentives for solving the actual problem in-repo rather than producing plausible-looking text. It is commonly used to assess robustness on larger, messier engineering workflows and constraints.","Planning, Logical Reasoning, Working Memory, Adaptive Error Correction, Cognitive Flexibility (minor), Semantic Understanding & Context Recognition"
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks evaluates long-context structured reasoning by presenting graph-structured data (often serialized as text) and asking the model to perform navigation or relational queries (e.g., BFS-style steps or parent tracing). It stresses consistent multi-step traversal under context length and distractor pressure.","Spatial Representation & Mapping, Working Memory, Logical Reasoning, Attention (minor), Semantic Understanding & Context Recognition (minor)"
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon evaluates tool-using agents on sequences of tasks that require selecting appropriate tools, executing multi-step tool calls, and recovering from tool or parameter errors. It is designed to probe reliability and compositionality in real tool-use workflows rather than single-call demos.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Language Comprehension (minor), Inhibitory Control (minor)"
HMMT,https://matharena.ai/?view=problem&comp=hmmt--hmmt_feb_2025,https://arxiv.org/abs/2505.23281,"HMMT problems (e.g., the February 2025 set) come from the Harvard-MIT Mathematics Tournament and are typically harder, more proof- and insight-oriented than standard school math. The benchmark probes deep multi-step reasoning, strategic problem decomposition, and careful algebraic/number-theoretic manipulation.","Logical Reasoning, Planning, Working Memory, Cognitive Flexibility (minor)"
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath is an expert-level mathematics benchmark designed to measure progress on problems closer to research or advanced competition difficulty, often requiring nontrivial insights and long derivations. It is intended to be challenging even for strong models, emphasizing correctness on hard, multi-step mathematical reasoning.","Logical Reasoning, Planning, Working Memory, Cognitive Flexibility, Self-reflection (minor)"
