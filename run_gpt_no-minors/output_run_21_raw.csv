Benchmark,Website,Paper,Description,Cognitive Functions
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates LLM-based software engineering by asking the model to produce code patches that fix real GitHub issues in Python repositories. The “Verified” split consists of problems that have been validated by human reviewers to be solvable and reliably testable, emphasizing correctness under repository tests and realistic debugging workflows.","Logical Reasoning, Planning, Adaptive Error Correction, Working Memory, Language Comprehension (minor)"
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro is a harder successor-style benchmark that measures end-to-end software engineering over a broader and more challenging set of real-world tasks, spanning multiple programming languages. It aims to be more industrially relevant and contamination-resistant, stressing robust patch generation under complex project structure and test suites.","Logical Reasoning, Planning, Adaptive Error Correction, Working Memory, Decision-making (minor), Language Comprehension (minor)"
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp evaluates “deep research” behavior: answering difficult questions that require multi-step web search, reading, and synthesis rather than recalling a single fact. It stresses the agentic loop of query planning, evidence gathering, and consolidating sources into a supported final answer.","Planning, Attention, Episodic Memory, Semantic Understanding & Context Recognition, Decision-making, Language Production (minor)"
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench measures interactive agent performance in simulated customer-service domains (e.g., retail, airline, telecom) where the model must converse with a user, call tools/APIs, and follow domain policies. The benchmark emphasizes multi-turn policy adherence, tool reliability, and resolving tasks under constraints.","Social Reasoning & Theory of Mind, Inhibitory Control, Decision-making, Planning, Language Comprehension, Language Production (minor), Empathy (minor)"
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI tests “fluid intelligence” on novel abstract pattern tasks: given a few input–output grid examples, the model must infer the underlying rule and produce the correct output grid for a new input. It is designed to reduce reliance on memorized knowledge and instead probe generalization to unseen task types.","Logical Reasoning, Cognitive Flexibility, Working Memory, Spatial Representation & Mapping, Visual Perception (minor), Scene Understanding & Visual Reasoning (minor)"
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas evaluates real-world tool use via the Model Context Protocol (MCP), requiring models to discover tools, invoke them with correct schemas, manage multi-step workflows, and integrate results into an answer. It stresses robustness to tool errors, sequencing across services, and correct synthesis from tool outputs.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition, Language Comprehension (minor)"
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a large, difficult benchmark spanning frontier academic and professional questions, often requiring multi-step reasoning and (in some settings) tool use such as search or code. It is intended to probe broad expert-level competence across domains rather than narrow skill fitting.","Language Comprehension, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Multisensory Integration (minor), Decision-making (minor)"
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 uses problems from the American Invitational Mathematics Examination to assess competition-style mathematical reasoning. Questions typically require multi-step symbolic manipulation, careful case analysis, and error-free arithmetic/logical deductions under time-like constraints.","Logical Reasoning, Planning, Working Memory, Adaptive Error Correction (minor)"
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a very challenging multiple-choice science benchmark designed to be difficult to answer via simple web search, targeting graduate-level reasoning in biology, chemistry, and physics. The “Diamond” subset is curated for high-quality questions that reliably separate experts from non-experts.","Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory (minor), Language Comprehension (minor)"
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends MMLU to multilingual settings, testing broad academic knowledge and reasoning across many subjects and multiple languages. It probes whether models can maintain consistent competence and comprehension beyond English, often under multiple-choice formats.","Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory (minor)"
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro is a harder, more evaluation-focused variant of MMMU for expert-level multimodal understanding and reasoning, combining text with images such as diagrams, plots, and scientific figures. It emphasizes cross-domain visual reasoning and precise grounding of answers in visual evidence.","Visual Perception, Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Visual Attention & Eye Movements (minor), Language Comprehension (minor)"
ScreenShot-Pro,https://gui-agent.github.io/grounding-leaderboard/,https://arxiv.org/abs/2504.07981,"ScreenShot-Pro evaluates GUI understanding from high-resolution screenshots, often requiring models to localize relevant UI elements and answer questions grounded in interface layout. It targets visually grounded reasoning needed for computer-use agents (e.g., interpreting menus, settings, dashboards) rather than generic image captioning.","Visual Perception, Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Spatial Representation & Mapping (minor), Language Comprehension (minor), Sensorimotor Coordination (minor)"
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning focuses on answering reasoning questions about figures and content drawn from arXiv-style scientific papers, emphasizing chart/figure interpretation and grounded inference. It tests whether a model can extract the right visual evidence and connect it to the accompanying scientific context.","Scene Understanding & Visual Reasoning, Visual Perception, Logical Reasoning, Multisensory Integration, Language Comprehension (minor), Working Memory (minor)"
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU extends multimodal understanding to video, requiring temporal integration of visual information across frames to answer questions. Tasks stress tracking events, attributes, and relations over time rather than relying on a single static image.","Visual Perception, Working Memory, Attention, Cognitive Timing & Predictive Modeling (minor), Multisensory Integration (minor), Scene Understanding & Visual Reasoning (minor), Language Comprehension (minor)"
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (multi-round coreference resolution) evaluates long-context retrieval and reference tracking by embedding multiple similar “needle” interactions inside long distractor “haystacks,” then asking the model to reproduce the correct response for a specified needle. The 8-needle variant stresses maintaining and retrieving the correct referent among many near-duplicates over long contexts.","Working Memory, Episodic Memory, Attention, Language Comprehension, Semantic Understanding & Context Recognition (minor)"
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval measures performance on well-specified professional “knowledge work” tasks (e.g., producing presentations, spreadsheets, plans, analyses) across many occupations, often judged by experts in head-to-head comparisons. It aims to capture applied competence: producing usable artifacts, following constraints, and making practical decisions under realistic instructions.","Planning, Decision-making, Language Production, Semantic Understanding & Context Recognition, Working Memory (minor), Self-reflection (minor), Social Reasoning & Theory of Mind (minor)"
SWE-Lancer,https://openai.com/index/swe-lancer/,https://arxiv.org/abs/2502.12115,"SWE-Lancer evaluates advanced software engineering performance on realistic repository tasks, emphasizing reliability and end-to-end patch quality under constraints similar to professional workflows. Compared to simpler coding benchmarks, it more strongly stresses autonomous debugging, refactoring, and integrating changes that satisfy tests and requirements.","Planning, Adaptive Error Correction, Logical Reasoning, Working Memory, Decision-making (minor), Language Comprehension (minor)"
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks evaluates multi-step reasoning over graph-structured data presented in textual form, such as following parent pointers or performing breadth-first-like traversals. It tests whether models can reliably maintain and update an internal representation of a graph path/state across many steps without losing track.","Spatial Representation & Mapping, Working Memory, Logical Reasoning, Attention (minor)"
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon evaluates complex tool-using agents on multi-step tasks that require selecting tools, composing calls, handling failures, and synthesizing results into a final deliverable. It emphasizes robustness and orchestration in realistic tool ecosystems rather than isolated single-call function usage.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition (minor), Inhibitory Control (minor)"
HMMT,https://matharena.ai/?view=problem&comp=hmmt--hmmt_feb_2025,https://arxiv.org/abs/2505.23281,"HMMT problems (e.g., from Harvard-MIT Mathematics Tournament sets) assess high-difficulty contest mathematics that typically requires creative multi-step reasoning and careful proof-like structure. Compared with standard exam items, these tasks more often demand strategic problem decomposition and non-routine insights.","Logical Reasoning, Planning, Working Memory, Cognitive Flexibility (minor)"
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath targets expert-level mathematics problems intended to be challenging even for strong automated reasoning systems, often requiring deep multi-step derivations and careful control of intermediate results. It is designed to measure progress on difficult, research-adjacent mathematical reasoning rather than routine computation.","Logical Reasoning, Planning, Working Memory, Adaptive Error Correction (minor), Cognitive Flexibility (minor)"
