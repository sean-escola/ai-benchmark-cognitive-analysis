Benchmark,Website,Paper,Description,Cognitive Functions
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 evaluates agentic coding and sysadmin-style problem solving in real terminal environments. Models must navigate a filesystem, run commands, inspect outputs, and iteratively repair their approach under execution constraints to complete end-to-end tasks.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Attention (minor), Language Comprehension (minor)"
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp evaluates “deep research” style question answering where the model must search and synthesize evidence from a controlled document collection or browsing setup. Success depends on formulating effective queries, integrating information across sources, and producing a supported final answer rather than a single-step response.","Planning, Semantic Understanding & Context Recognition, Episodic Memory (minor), Working Memory, Attention, Language Production (minor)"
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld benchmarks multimodal “computer use” agents that operate a real desktop-like OS to accomplish tasks across applications. Agents must interpret screenshots/UI state, take multi-step actions (click/type/open menus), and recover from mistakes to reach goal states.","Visual Perception, Visual Attention & Eye Movements, Sensorimotor Coordination, Planning, Decision-making, Adaptive Error Correction"
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI measures fluid intelligence via abstract grid transformation puzzles with only a few input–output examples per task. Models must infer latent rules and generalize them to a new input, emphasizing systematic generalization over memorization.","Logical Reasoning, Cognitive Flexibility, Working Memory, Attention, Spatial Representation & Mapping"
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon autonomous agency in a simulated year-long vending-machine business. Agents must manage inventory, pricing, supplier interactions, and cashflow across thousands of steps, optimizing for final profit under uncertainty.","Planning, Decision-making, Reward Mechanisms, Working Memory, Adaptive Error Correction, Self-reflection (minor)"
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity capabilities on large suites of tasks involving vulnerability discovery and exploitation reasoning in real-world software contexts. Agents must interpret bug descriptions or codebases, form hypotheses about weaknesses, and validate fixes or exploits via tool-driven workflows.","Logical Reasoning, Planning, Adaptive Error Correction, Semantic Understanding & Context Recognition, Working Memory (minor)"
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"SpreadsheetBench measures the ability to understand and manipulate complex spreadsheets to answer questions or produce correct transformed files. It tests structured data reasoning, formula/aggregation understanding, and multi-step editing actions that must be consistent across a workbook.","Logical Reasoning, Planning, Working Memory, Attention, Adaptive Error Correction (minor), Semantic Understanding & Context Recognition (minor)"
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a frontier, multi-domain (often multimodal) test intended to probe advanced reasoning and expert knowledge across many fields. Questions are designed to be difficult for models by requiring careful reading, multi-step inference, and integration of specialized concepts.","Language Comprehension, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Visual Perception (minor), Scene Understanding & Visual Reasoning (minor)"
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a high-quality subset of GPQA consisting of very challenging multiple-choice questions in biology, chemistry, and physics. It targets deep scientific reasoning and knowledge that resists shallow pattern matching or simple web lookup.","Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Language Comprehension"
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro evaluates expert-level multimodal understanding across disciplines, requiring models to answer questions grounded in images (diagrams, plots, tables) alongside text. It emphasizes visual reasoning, cross-modal grounding, and robust interpretation of technical visuals.","Visual Perception, Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Attention (minor)"
MathArena Apex,https://matharena.ai/?view=problem&comp=apex--apex_2025,https://arxiv.org/abs/2505.23281,"MathArena Apex is a competitive mathematics benchmark designed to stress advanced problem solving with difficult, often olympiad-style questions. It typically rewards coherent multi-step derivations and precise final answers under single-attempt grading.","Logical Reasoning, Working Memory, Attention (minor), Planning (minor)"
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning evaluates reasoning over scientific figures and charts drawn from research papers, often requiring extraction of plotted relationships and quantitative/qualitative conclusions. It probes whether models can align textual questions to visual evidence and perform structured inference from that evidence.","Visual Perception, Scene Understanding & Visual Reasoning, Semantic Understanding & Context Recognition, Logical Reasoning, Attention (minor)"
OmniDocBench 1.5,https://github.com/opendatalab/OmniDocBench,https://arxiv.org/abs/2412.07626,"OmniDocBench 1.5 evaluates document understanding and OCR across heterogeneous elements such as paragraphs, tables, formulas, and reading order. Systems must accurately recover text and structure from document images, reflecting robustness to layout complexity.","Visual Perception, Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Language Comprehension, Semantic Understanding & Context Recognition (minor)"
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU extends multimodal understanding to videos, requiring models to answer questions that depend on temporal events, actions, and evolving visual context. It emphasizes integrating cues across frames rather than relying on a single snapshot.","Visual Perception, Attention, Working Memory, Cognitive Timing & Predictive Modeling, Scene Understanding & Visual Reasoning, Multisensory Integration (minor)"
LiveCodeBench Pro,https://livecodebenchpro.com/projects/livecodebench-pro/leaderboard,https://arxiv.org/abs/2506.11928,"LiveCodeBench Pro evaluates coding ability on contemporary programming tasks with an emphasis on realistic developer workflows and up-to-date problems. Models must generate correct solutions under execution-based verification, often requiring debugging and iterative refinement within a single attempt.","Logical Reasoning, Planning, Adaptive Error Correction, Working Memory, Language Production (minor)"
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"The FACTS Benchmark Suite systematically evaluates factuality and grounding, measuring whether model outputs remain faithful to provided evidence and avoid unsupported claims. It aggregates multiple factuality-oriented tests to characterize error patterns beyond a single QA metric.","Semantic Understanding & Context Recognition, Language Comprehension, Inhibitory Control (minor), Self-reflection (minor)"
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA evaluates physical commonsense reasoning in a multilingual/non-parallel setting, probing whether models can generalize intuitive physics and everyday action plausibility across languages and cultures. It reduces reliance on memorized English-only artifacts by varying linguistic realizations.","Logical Reasoning, Semantic Understanding & Context Recognition, Language Comprehension, Cognitive Flexibility (minor)"
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,MRCR v2 (8-needle) tests long-context multi-round coreference resolution by embedding multiple similar “needle” requests within long “haystack” dialogues and asking the model to reproduce the correct referenced answer. It stresses robust retrieval of the right mention under heavy interference.,"Working Memory, Attention, Episodic Memory, Language Comprehension"
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval evaluates economically valuable, well-specified knowledge work across many occupations using human expert judging (often via pairwise comparisons). Tasks include creating professional artifacts (e.g., slides, spreadsheets, plans) where quality depends on meeting constraints and producing coherent deliverables.","Planning, Decision-making, Language Production, Semantic Understanding & Context Recognition, Self-reflection (minor)"
SWE-Lancer,https://openai.com/index/swe-lancer/,https://arxiv.org/abs/2502.12115,"SWE-Lancer evaluates software engineering agents on real repository tasks that resemble practical bug-fixing and feature implementation work. Models must reason over codebases, produce patches, and satisfy tests/constraints that reflect production-like engineering requirements.","Logical Reasoning, Planning, Adaptive Error Correction, Working Memory, Language Comprehension (minor)"
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks measures a model’s ability to perform algorithmic traversal and relational reasoning over graphs represented in long textual contexts. Typical tasks require following edges, tracking parents/paths, or executing BFS-like reasoning without losing state across many nodes.","Spatial Representation & Mapping, Working Memory, Attention, Logical Reasoning"
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon evaluates tool-using agents on multi-step tasks that require selecting, sequencing, and correctly invoking external tools/APIs, then composing results into a final response. It targets reliability under tool errors, partial observability, and long action chains.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Attention (minor), Language Comprehension (minor)"
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath is an expert-level mathematics benchmark intended to measure progress on research-grade problem solving rather than routine competition items. Problems often require sustained, rigorous multi-step reasoning and careful handling of formal definitions and edge cases.","Logical Reasoning, Working Memory, Cognitive Flexibility (minor), Planning (minor)"
