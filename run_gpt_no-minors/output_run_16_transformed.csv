Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 evaluates agentic coding and sysadmin-style problem solving in real command-line environments. Models must interpret task instructions, run shell commands, inspect files/processes, and iteratively fix issues to reach a verifiable terminal state.","L1: Language Comprehension (minor)
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory, Logical Reasoning
L3: ",L2
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp measures research-style browsing: finding and synthesizing answers that require navigating multiple web pages rather than recalling facts. Tasks emphasize search strategy, evidence gathering, and producing a final grounded response under realistic browsing constraints.","L1: Language Comprehension
L2: Planning, Semantic Understanding & Context Recognition, Working Memory, Decision-making (minor), Attention (minor)
L3: ",L2
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld evaluates computer-use agents on end-to-end tasks inside a desktop operating system, such as editing files, configuring settings, and using applications. Success requires perceiving UI state, choosing actions across many steps, and recovering from interaction errors.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Sensorimotor Coordination, Planning, Attention, Adaptive Error Correction (minor)
L3: ",L2
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,ARC-AGI is a few-shot abstract reasoning benchmark where models infer latent rules mapping input grids to output grids from a small set of examples. It targets generalization to novel patterns with minimal prior task-specific training signals.,"L1: Visual Perception (minor)
L2: Logical Reasoning, Working Memory, Spatial Representation & Mapping, Planning (minor)
L3: Cognitive Flexibility",L3
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 tests long-horizon agent coherence in a simulated business management setting over an extended (e.g., year-long) timeline. Agents must handle inventory, pricing, supplier interactions, and strategy adaptation to maximize final cash balance.","L1: 
L2: Planning, Decision-making, Working Memory, Reward Mechanisms, Adaptive Error Correction (minor)
L3: Self-reflection (minor)",L2
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity agent capabilities on real-world vulnerability tasks at scale, including identifying known weaknesses and discovering new issues in software. It emphasizes multi-step technical reasoning, code/artefact inspection, and precise exploit-relevant conclusions.","L1: Language Comprehension (minor)
L2: Logical Reasoning, Planning, Adaptive Error Correction, Decision-making, Working Memory (minor)
L3: ",L2
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"SpreadsheetBench tests an agent’s ability to read, edit, and compute over complex spreadsheets resembling real workplace artifacts. Tasks require understanding table structure, applying formulas/transformations, and producing correct updated files or values.","L1: Visual Perception (minor)
L2: Working Memory, Logical Reasoning, Planning, Semantic Understanding & Context Recognition, Adaptive Error Correction (minor)
L3: ",L2
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam (HLE) is a multimodal benchmark of difficult, frontier-knowledge questions designed to stress advanced reasoning beyond common exam-style datasets. It often rewards synthesis across domains and careful handling of uncertainty and evidence (with or without tool use, depending on the eval setup).","L1: Language Comprehension, Visual Perception (minor)
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory (minor), Multisensory Integration (minor)
L3: ",L2
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,GPQA Diamond is a high-quality subset of extremely challenging graduate-level science multiple-choice questions that are designed to be “Google-proof.” It probes deep conceptual understanding and multi-step reasoning under strict answer selection constraints.,"L1: Language Comprehension (minor)
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory (minor), Decision-making (minor)
L3: ",L2
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro measures expert-level multimodal understanding and reasoning across many disciplines, using images (e.g., diagrams, charts, figures) paired with questions. It emphasizes interpreting visual evidence and integrating it with domain knowledge to select correct answers.","L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Attention (minor)
L3: ",L2
MathArena Apex,https://matharena.ai/?view=problem&comp=apex--apex_2025,https://arxiv.org/abs/2505.23281,MathArena Apex is a competitive mathematics benchmark that aggregates hard problems and evaluates solution accuracy under standardized rules. It stresses rigorous multi-step derivations and robustness to tricky formulations rather than memorized patterns.,"L1: Language Comprehension (minor)
L2: Logical Reasoning, Working Memory, Planning (minor), Adaptive Error Correction (minor)
L3: ",L2
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,CharXiv Reasoning evaluates multimodal scientific figure understanding by asking models to answer questions that require reading and reasoning over paper figures and associated context. It targets faithful extraction of evidence from plots/diagrams and correct scientific interpretation.,"L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Semantic Understanding & Context Recognition, Logical Reasoning, Multisensory Integration (minor), Attention (minor)
L3: ",L2
OmniDocBench 1.5,https://github.com/opendatalab/OmniDocBench,https://arxiv.org/abs/2412.07626,"OmniDocBench 1.5 benchmarks document understanding and OCR across heterogeneous layouts, including text blocks, formulas, tables, and reading order. It emphasizes structured extraction quality and robustness to complex formatting found in real documents.","L1: Visual Perception, Language Comprehension (minor)
L2: Visual Attention & Eye Movements, Semantic Understanding & Context Recognition, Working Memory (minor)
L3: ",L2
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,Video-MMMU extends multimodal reasoning to the temporal domain by requiring understanding of videos plus text questions. It probes whether models can integrate events across frames and use temporal context to answer complex queries.,"L1: Visual Perception
L2: Working Memory, Scene Understanding & Visual Reasoning, Attention (minor), Multisensory Integration (minor)
L3: Cognitive Timing & Predictive Modeling",L3
LiveCodeBench Pro,https://livecodebenchpro.com/projects/livecodebench-pro/leaderboard,https://arxiv.org/abs/2506.11928,"LiveCodeBench Pro evaluates coding ability on competitive-programming and practical coding tasks under more controlled, time-aware curation than many legacy code benchmarks. It emphasizes producing correct programs and handling edge cases, often under single-attempt constraints.","L1: Language Comprehension (minor)
L2: Logical Reasoning, Planning, Adaptive Error Correction, Working Memory, Decision-making (minor)
L3: ",L2
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"The FACTS Benchmark Suite systematically evaluates LLM factuality across multiple settings, including grounding, attribution, and resistance to producing unsupported claims. It focuses on whether outputs remain faithful to evidence and avoid hallucinations under realistic prompting.","L1: Language Production (minor)
L2: Semantic Understanding & Context Recognition, Logical Reasoning (minor)
L3: Inhibitory Control, Self-reflection",L3
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA extends physical commonsense question answering to a multilingual, non-parallel setting, aiming to test generalization across languages and cultural/linguistic variations. It measures whether models can choose plausible actions or tools for everyday physical goals described in text.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Decision-making (minor)
L3: Cognitive Flexibility (minor)",L2
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) tests long-context multi-round coreference by embedding multiple similar “needle” interactions inside long “haystacks,” then asking the model to reproduce the response associated with a specific needle. It stresses precise retrieval of the right referent amid distractors across long contexts.","L1: Language Comprehension (minor)
L2: Working Memory, Attention, Episodic Memory
L3: Inhibitory Control (minor)",L2
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval evaluates economically valuable, well-specified professional knowledge work across many occupations using expert human judging of produced artifacts (e.g., slides, spreadsheets, plans). It probes end-to-end execution quality, instruction following, and practical decision quality under realistic constraints.","L1: Language Production
L2: Planning, Decision-making, Semantic Understanding & Context Recognition
L3: Self-reflection (minor), Social Reasoning & Theory of Mind (minor)",L2
SWE-Lancer,https://openai.com/index/swe-lancer/,https://arxiv.org/abs/2502.12115,"SWE-Lancer evaluates software engineering agents on realistic repository-level tasks that resemble professional engineering work, often requiring multi-step edits and integration with project context. It emphasizes correct patch generation, debugging, and adherence to task requirements at scale.","L1: 
L2: Planning, Adaptive Error Correction, Logical Reasoning, Working Memory, Decision-making (minor)
L3: ",L2
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks evaluates structured reasoning over graphs by asking models to perform algorithmic traversals (e.g., BFS-like walks) and track relationships across nodes. It stresses maintaining and updating a mental representation of graph structure across long sequences.","L1: 
L2: Spatial Representation & Mapping, Working Memory, Logical Reasoning, Attention (minor)
L3: ",L2
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon measures an agent’s ability to select, call, and coordinate multiple tools to solve tasks end-to-end, including handling tool failures and composing intermediate results. It emphasizes reliable tool orchestration and correct final synthesis from tool outputs.","L1: Language Comprehension (minor), Language Production (minor)
L2: Planning, Decision-making, Working Memory, Adaptive Error Correction
L3: ",L2
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath targets expert-level mathematics by evaluating whether models can solve problems near the frontier of current automated math performance. It emphasizes rigorous reasoning, multi-step derivations, and (in tool-enabled setups) correct use of computation to support proofs or calculations.","L1: 
L2: Logical Reasoning, Working Memory, Planning (minor), Adaptive Error Correction (minor)
L3: ",L2
