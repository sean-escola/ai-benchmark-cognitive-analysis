Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates real-world software engineering by asking a model to generate a code patch that resolves a GitHub issue and passes project tests in a repository-based harness. The Verified split adds human validation that tasks are solvable and that evaluation is reliable, making it a common reference point for agentic coding performance.","L1: Language Comprehension (minor)
L2: Planning, Logical Reasoning, Working Memory, Adaptive Error Correction, Decision-making (minor)
L3: ",L2
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro is a harder, more contamination-resistant software engineering benchmark that expands task diversity and difficulty and includes multiple programming languages and more realistic engineering constraints. It emphasizes end-to-end codebase understanding, robust patch generation, and higher failure tolerance than SWE-bench Verified.","L1: 
L2: Planning, Logical Reasoning, Working Memory, Adaptive Error Correction, Decision-making (minor)
L3: Cognitive Flexibility",L3
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp evaluates “deep research” style web-browsing agents on questions that typically require searching, reading, and synthesizing evidence from multiple sources rather than recalling a single fact. It stresses iterative retrieval, context management over long browsing trajectories, and citation/grounding discipline when forming final answers.","L1: Language Comprehension (minor)
L2: Planning, Attention, Semantic Understanding & Context Recognition, Working Memory, Decision-making (minor)
L3: ",L2
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench measures interactive, policy-constrained customer-support agent performance in simulated domains (e.g., retail, airline, telecom) where the model must follow rules while calling APIs/tools over multiple turns. It highlights robustness to user pressure, tool/API correctness, and consistent policy adherence across long dialogues.","L1: Language Production, Language Comprehension (minor)
L2: Decision-making, Planning
L3: Social Reasoning & Theory of Mind, Inhibitory Control",L3
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,ARC-AGI tests few-shot abstraction and pattern induction on grid-based tasks where the underlying rule must be inferred from only a handful of input–output examples. It is designed to emphasize novel reasoning and generalization to unseen transformations rather than memorization or broad factual knowledge.,"L1: Visual Perception (minor)
L2: Logical Reasoning, Working Memory, Spatial Representation & Mapping, Attention (minor)
L3: Cognitive Flexibility",L3
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas evaluates tool-use via the Model Context Protocol by requiring models to discover, invoke, and chain real tools across multi-step workflows while handling errors and retries. The benchmark focuses on correctness of tool selection and calling, and on synthesizing final outputs from tool results in production-like settings.","L1: Language Comprehension (minor), Language Production (minor)
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory
L3: ",L2
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam (HLE) is a frontier-level, often multimodal benchmark intended to probe advanced reasoning and knowledge across difficult, expert-facing questions. It is commonly evaluated both with and without external tools (e.g., search, code execution) to separate core reasoning from tool-augmented problem solving.","L1: Language Comprehension
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Scene Understanding & Visual Reasoning (minor), Multisensory Integration (minor), Working Memory (minor)
L3: ",L2
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 consists of competition-style math problems that require multi-step derivations, algebraic manipulation, and careful error checking under time-like constraints. It is widely used to assess mathematical reasoning without relying on tool execution (though some reports also include tool-enabled variants).","L1: 
L2: Logical Reasoning, Working Memory, Planning (minor), Adaptive Error Correction (minor)
L3: ",L2
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a high-quality subset of challenging, “Google-proof” graduate-level science multiple-choice questions curated to be difficult for non-experts and resistant to shallow retrieval. It measures scientific reasoning and careful reading more than broad trivia recall, with strong emphasis on selecting the best-supported option.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory (minor), Decision-making (minor)
L3: ",L2
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends MMLU-style knowledge-and-reasoning questions to many non-English languages, testing whether a model can maintain competence across languages and subjects. It probes multilingual generalization, instruction/question understanding, and consistent reasoning under language shifts.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning (minor), Working Memory (minor)
L3: Cognitive Flexibility",L3
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro is a harder, more evaluation-focused variant of MMMU that tests expert-level multimodal understanding across disciplines, typically combining images/diagrams with text questions. It emphasizes grounded interpretation of visual evidence, cross-domain reasoning, and robust answer selection under challenging distractors.","L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning (minor), Working Memory (minor)
L3: ",L2
ScreenShot-Pro,https://gui-agent.github.io/grounding-leaderboard/,https://arxiv.org/abs/2504.07981,"ScreenShot-Pro evaluates screenshot understanding for GUI-centric tasks, requiring models to interpret high-resolution interfaces and answer questions or identify targets based on layout and visual cues. It is commonly used to assess visual grounding relevant to computer-use agents (e.g., locating buttons, fields, menus) rather than generic image captioning.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Spatial Representation & Mapping, Decision-making (minor), Sensorimotor Coordination (minor)
L3: ",L2
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning evaluates reasoning over scientific figures/charts and associated context drawn from arXiv-style papers, often rewarding precise interpretation of plotted values, trends, and experimental comparisons. Some settings allow auxiliary computation (e.g., Python) to test tool-augmented quantitative analysis grounded in the visual artifact.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Logical Reasoning, Semantic Understanding & Context Recognition (minor), Working Memory (minor)
L3: ",L2
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU extends multimodal understanding to video, asking questions that require integrating information across frames and sometimes combining visual cues with text context. It stresses temporal integration, tracking events, and resolving references over longer audiovisual narratives than single-image benchmarks.","L1: Visual Perception, Auditory Processing (minor)
L2: Scene Understanding & Visual Reasoning, Working Memory, Attention (minor)
L3: Cognitive Timing & Predictive Modeling",L3
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) tests long-context retrieval in a multi-round co-reference setting by inserting multiple similar “needle” interactions into a long “haystack” and asking the model to reproduce the response associated with a particular needle. It emphasizes robust context tracking, disambiguation among near-duplicates, and resistance to interference across long contexts.","L1: Language Comprehension
L2: Working Memory, Attention, Episodic Memory, Semantic Understanding & Context Recognition (minor)
L3: ",L2
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval evaluates economically valuable, well-specified knowledge-work tasks across many occupations, with expert human judging of work-product quality (e.g., spreadsheets, presentations, schedules, analyses). It targets end-to-end professional execution—turning requirements into polished artifacts—rather than isolated Q&A accuracy.","L1: Language Production
L2: Planning, Decision-making, Semantic Understanding & Context Recognition, Adaptive Error Correction (minor)
L3: Self-reflection (minor)",L2
SWE-Lancer,https://openai.com/index/swe-lancer/,https://arxiv.org/abs/2502.12115,"SWE-Lancer evaluates software engineering ability in a more “realistic work” frame, often using longer, less templated tasks and reward- or incentive-shaped success criteria that resemble paid contracting outcomes. It stresses sustained problem decomposition, correct implementation under ambiguity, and iterative debugging to completion.","L1: 
L2: Planning, Logical Reasoning, Adaptive Error Correction, Working Memory, Decision-making (minor), Reward Mechanisms (minor)
L3: ",L2
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks tests whether models can follow explicit graph-structured constraints (e.g., traversals like BFS, parent pointers, or multi-step walks) described within text, often at long context lengths. It is designed to be sensitive to state tracking errors and shortcutting, making it a strong probe of structured long-horizon reasoning.","L1: 
L2: Spatial Representation & Mapping, Working Memory, Attention, Logical Reasoning, Planning (minor)
L3: ",L2
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon measures general tool-using agent ability across diverse APIs and multi-step tasks, emphasizing correct tool selection, argument construction, and recovery from tool failures. It targets reliable orchestration of external actions in realistic workflows rather than purely linguistic problem solving.","L1: Language Comprehension (minor)
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory
L3: Inhibitory Control (minor)",L2
HMMT,https://matharena.ai/?view=problem&comp=hmmt--hmmt_feb_2025,https://arxiv.org/abs/2505.23281,"HMMT problems (as used in MathArena) reflect high-difficulty contest mathematics, typically requiring creative insights, multi-step derivations, and careful handling of edge cases. Compared with more standardized exams, it often places greater weight on novel problem decomposition and flexible strategy choice.","L1: 
L2: Logical Reasoning, Working Memory, Planning (minor)
L3: Cognitive Flexibility (minor)",L2
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath is a benchmark of expert-level mathematics designed to be difficult for both humans and models, with an emphasis on nontrivial proofs, derivations, and advanced problem-solving patterns. It is used to measure progress near the frontier of mathematical reasoning, including tool-augmented settings for computation checking.","L1: 
L2: Logical Reasoning, Working Memory, Planning, Adaptive Error Correction (minor)
L3: Cognitive Flexibility",L3
