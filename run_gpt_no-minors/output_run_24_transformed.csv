Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates software engineering agents by asking them to generate code patches that fix real GitHub issues in Python repositories, with correctness determined by running the project’s tests. The “Verified” subset filters to tasks that have been manually validated as solvable and reliably testable, reducing noise from flaky or underspecified issues.","L1: Language Comprehension (minor), Language Production (minor)
L2: Logical Reasoning, Planning, Working Memory, Adaptive Error Correction, Decision-making
L3: ",L2
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro is a larger, more difficult software engineering benchmark designed to be more realistic and contamination-resistant than earlier SWE-bench variants, spanning multiple programming languages. Models must understand a repository and issue description, implement a correct fix, and pass evaluation tests under stricter task curation.","L1: Language Comprehension (minor)
L2: Logical Reasoning, Planning, Working Memory, Adaptive Error Correction, Decision-making
L3: Cognitive Flexibility (minor)",L2
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp evaluates “deep research” style agents that must search, read, and synthesize information from the web (or a controlled corpus) to answer difficult questions. Performance depends on iterative querying, evidence gathering, and consolidating findings into a final grounded answer.","L1: Language Comprehension
L2: Planning, Decision-making, Attention, Working Memory, Semantic Understanding & Context Recognition, Adaptive Error Correction (minor), Episodic Memory (minor)
L3: ",L2
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench measures interactive agent performance in customer-service style simulations (e.g., retail, airline, telecom) where the model must converse with a user simulator and call APIs while adhering to domain policies. Success requires multi-turn state tracking, policy compliance, and robust tool-mediated problem resolution.","L1: Language Comprehension, Language Production
L2: Decision-making, Planning, Working Memory
L3: Social Reasoning & Theory of Mind, Inhibitory Control, Empathy (minor)",L3
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI is a grid-based “fluid intelligence” benchmark in which models infer abstract transformation rules from a few input–output examples and apply them to a novel input. It emphasizes generalization to new tasks with minimal examples, rather than recalling domain facts.","L1: Visual Perception
L2: Logical Reasoning, Working Memory, Spatial Representation & Mapping, Scene Understanding & Visual Reasoning, Attention (minor)
L3: Cognitive Flexibility",L3
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas evaluates real-world tool use through the Model Context Protocol (MCP), requiring models to discover appropriate tools, invoke them correctly, and complete multi-step workflows across APIs and servers. Tasks stress reliable orchestration (including handling errors/retries) and synthesizing tool outputs into correct final responses.","L1: Language Comprehension (minor), Language Production (minor)
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition
L3: ",L2
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam (HLE) is a large, difficult multimodal benchmark intended to probe frontier knowledge and reasoning across many subjects, often requiring synthesis beyond straightforward recall. It is commonly reported both with and without tools (e.g., search or code), highlighting differences between pure reasoning and tool-augmented problem solving.","L1: Language Comprehension, Visual Perception (minor)
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory (minor), Planning (minor), Scene Understanding & Visual Reasoning (minor), Multisensory Integration (minor)
L3: ",L2
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 evaluates competition-style mathematics problem solving, typically requiring multi-step derivations, algebraic manipulation, and careful case handling. Scores are usually reported without tools (pure reasoning) and sometimes with tools (e.g., Python) to separate reasoning skill from calculation support.","L1: 
L2: Logical Reasoning, Working Memory, Planning, Adaptive Error Correction (minor), Semantic Understanding & Context Recognition (minor)
L3: ",L2
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a high-quality subset of GPQA consisting of very challenging graduate-level multiple-choice questions in physics, chemistry, and biology, designed to be hard to answer by superficial web search. It targets deep scientific understanding and careful reasoning under distractor options.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory (minor), Decision-making (minor)
L3: ",L2
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends MMLU to a multilingual setting, testing knowledge and reasoning across many academic subjects and multiple languages. It probes whether models maintain subject competence and reasoning consistency when operating outside English.","L1: Language Comprehension, Language Production (minor)
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory (minor)
L3: ",L2
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro is a more challenging, expert-oriented version of MMMU for multimodal understanding, where models answer questions that require jointly reasoning over images (charts, diagrams, figures) and text across diverse disciplines. It emphasizes robust visual reasoning and domain-grounded interpretation rather than simple recognition.","L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Multisensory Integration, Visual Attention & Eye Movements (minor), Logical Reasoning, Spatial Representation & Mapping (minor)
L3: ",L2
ScreenShot-Pro,https://gui-agent.github.io/grounding-leaderboard/,https://arxiv.org/abs/2504.07981,"ScreenShot-Pro evaluates GUI understanding from screenshots, requiring models to interpret screen layouts, read UI text, and answer questions that depend on visual grounding in real interfaces. It is commonly used to measure how well agents can understand software screens as a prerequisite for reliable computer-use automation.","L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Spatial Representation & Mapping, Working Memory (minor), Sensorimotor Coordination (minor)
L3: ",L2
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning tests whether models can answer questions about scientific figures and content from arXiv-style papers, often requiring extracting quantitative/structural information from charts and linking it to textual context. It emphasizes higher-order interpretation of technical visuals rather than generic image captioning.","L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Visual Attention & Eye Movements (minor), Semantic Understanding & Context Recognition (minor)
L3: ",L2
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU extends multimodal understanding to videos, requiring models to integrate information across frames and time to answer questions about events, interactions, and visual evidence. It targets temporal integration and cross-modal reasoning under long-context visual inputs.","L1: Visual Perception, Language Comprehension (minor)
L2: Scene Understanding & Visual Reasoning, Multisensory Integration, Working Memory, Attention
L3: Cognitive Timing & Predictive Modeling (minor)",L2
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,MRCR v2 (8-needle) evaluates long-context multi-round coreference resolution by inserting multiple similar “needle” interactions into long “haystacks” and asking the model to retrieve the correct referenced response. It stresses precise attention control and retrieval over long sequences with high distractor density.,"L1: Language Comprehension
L2: Working Memory, Attention, Episodic Memory, Semantic Understanding & Context Recognition
L3: Inhibitory Control (minor)",L2
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval measures performance on well-specified professional knowledge-work tasks across many occupations, where models produce real work artifacts such as spreadsheets, presentations, schedules, or analyses. It emphasizes end-to-end task execution quality judged against human professional outputs, often including tool use and iterative refinement.","L1: Language Production
L2: Planning, Decision-making, Semantic Understanding & Context Recognition, Adaptive Error Correction (minor), Working Memory (minor)
L3: Self-reflection (minor)",L2
SWE-Lancer,https://openai.com/index/swe-lancer/,https://arxiv.org/abs/2502.12115,"SWE-Lancer evaluates agentic software engineering on longer, more realistic engineering work that resembles “freelance” tasks, often spanning multiple files, constraints, and higher-level requirements. It is designed to better capture end-to-end engineering competence beyond narrow bug fixes.","L1: Language Comprehension (minor), Language Production (minor)
L2: Planning, Logical Reasoning, Decision-making, Working Memory, Adaptive Error Correction
L3: ",L2
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks tests structured reasoning over graph-like data embedded in text, requiring models to perform multi-step traversals (e.g., BFS-style reasoning) and answer queries that depend on correct intermediate states. It targets systematic compositional reasoning and resistance to distraction over long, algorithmic contexts.","L1: 
L2: Logical Reasoning, Working Memory, Attention, Spatial Representation & Mapping, Planning (minor), Adaptive Error Correction (minor)
L3: ",L2
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon evaluates an agent’s ability to solve tasks by selecting, sequencing, and correctly calling tools across multi-step workflows, often under realistic constraints (API schemas, partial failures, or multi-turn requirements). It emphasizes reliable tool orchestration and integrating tool outputs into coherent final results.","L1: Language Comprehension (minor), Language Production (minor)
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory, Attention (minor)
L3: ",L2
HMMT,https://matharena.ai/?view=problem&comp=hmmt--hmmt_feb_2025,https://arxiv.org/abs/2505.23281,"HMMT (Harvard-MIT Mathematics Tournament) problems in MathArena-style evaluations target advanced contest mathematics, typically requiring inventive multi-step reasoning, proofs or proof-like derivations, and careful handling of edge cases. Compared to easier contest sets, it is used to probe depth and robustness of mathematical reasoning.","L1: 
L2: Logical Reasoning, Working Memory, Planning, Adaptive Error Correction (minor)
L3: Cognitive Flexibility (minor)",L2
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath is an expert-level mathematics benchmark designed to be substantially harder than standard contest math, targeting multi-step reasoning that often benefits from sustained exploration and verification. It is frequently reported with tool assistance (e.g., Python) to separate reasoning quality from arithmetic burden while still stressing correctness.","L1: 
L2: Logical Reasoning, Planning, Working Memory, Adaptive Error Correction
L3: Cognitive Flexibility (minor)",L2
