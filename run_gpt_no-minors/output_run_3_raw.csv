Benchmark,Website,Paper,Description,Cognitive Functions
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates software engineering agents by asking them to produce patches for real GitHub issues, with success determined by running the project’s test suite in a standardized environment. The “Verified” subset uses tasks validated by humans as solvable and aims to reduce noisy or ambiguous cases, emphasizing reliable end-to-end bug fixing and feature implementation.","Planning, Logical Reasoning, Working Memory, Adaptive Error Correction, Semantic Understanding & Context Recognition, Decision-making (minor), Language Comprehension (minor), Language Production (minor)"
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro is a more difficult software engineering benchmark designed to be more contamination-resistant and industrially relevant, including a larger set of tasks and broader language/ecosystem coverage than SWE-bench Verified. It tests an agent’s ability to understand a codebase, implement correct changes, and pass automated tests under realistic constraints.","Planning, Logical Reasoning, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition, Decision-making (minor), Language Comprehension (minor), Language Production (minor)"
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp evaluates deep-research and browsing agents on questions that require locating, synthesizing, and verifying information from a controlled web-like document collection (or a browsing setup, depending on the variant). It emphasizes multi-step information seeking, source triangulation, and maintaining a coherent research thread across many retrieval and reasoning steps.","Planning, Attention, Episodic Memory, Working Memory, Semantic Understanding & Context Recognition, Logical Reasoning, Decision-making (minor), Language Production (minor)"
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench measures an agent’s ability to handle multi-turn customer-support tasks by interacting with simulated users and tool/API surfaces while adhering to domain policies (e.g., retail, airline, telecom). It probes whether the agent can coordinate dialogue, tool calls, and policy constraints over extended interactions to reach correct, compliant resolutions.","Social Reasoning & Theory of Mind, Planning, Decision-making, Inhibitory Control, Language Comprehension, Language Production, Working Memory (minor), Reward Mechanisms (minor)"
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI evaluates fluid reasoning via small grid-based puzzles where the model must infer latent transformation rules from a few input-output examples and apply them to a new input. It is designed to reduce reliance on memorized knowledge and instead emphasize generalization, abstraction, and compositional rule induction.","Logical Reasoning, Cognitive Flexibility, Spatial Representation & Mapping, Working Memory, Visual Perception (minor), Scene Understanding & Visual Reasoning (minor)"
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas benchmarks real-world tool use through the Model Context Protocol (MCP), where models must discover appropriate tools, call them with correct arguments, handle tool errors, and synthesize outputs into final answers. Tasks typically require multi-step workflows across heterogeneous services, making robustness and tool-chaining central.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition, Language Production (minor)"
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam (HLE) is a multimodal benchmark intended to stress-test frontier knowledge and reasoning across a wide range of difficult, expert-level questions. It emphasizes careful interpretation of problem statements, cross-domain reasoning, and (in tool-enabled settings) reliable use of external resources like search or code execution.","Semantic Understanding & Context Recognition, Logical Reasoning, Language Comprehension, Working Memory, Scene Understanding & Visual Reasoning (minor), Visual Perception (minor)"
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 consists of competition-style mathematics problems that require multi-step derivations and precise final numeric answers. The benchmark stresses symbolic manipulation, algebraic/number-theoretic reasoning, and the ability to sustain long solution chains without errors.","Logical Reasoning, Working Memory, Planning, Adaptive Error Correction (minor), Cognitive Flexibility (minor)"
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a high-quality subset of graduate-level, “Google-proof” multiple-choice science questions where non-experts typically fail and domain experts succeed. It emphasizes deep scientific reasoning and disambiguation rather than shallow recall, with careful question construction to reduce easy lookup shortcuts.","Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Decision-making, Language Comprehension (minor)"
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends broad subject testing into many languages, measuring whether models retain knowledge and reasoning ability under multilingual prompts. It probes robustness to linguistic variation and whether semantic understanding and task competence transfer across languages and scripts.","Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory (minor), Language Production (minor)"
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro is a harder, updated version of MMMU for expert-level multimodal understanding, combining text with complex images such as diagrams, charts, tables, and scientific figures. It stresses integrated reasoning over visual evidence and language instructions, often requiring multi-step inference rather than direct recognition.","Multisensory Integration, Scene Understanding & Visual Reasoning, Visual Perception, Language Comprehension, Logical Reasoning, Working Memory (minor)"
ScreenShot-Pro,https://gui-agent.github.io/grounding-leaderboard/,https://arxiv.org/abs/2504.07981,"ScreenShot-Pro evaluates GUI understanding and grounding from high-resolution screenshots, where the model must interpret interface elements and answer questions or identify actionable targets. It emphasizes spatially grounded perception of UIs (layout, labels, affordances) and reasoning about how a user would operate software.","Visual Perception, Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Spatial Representation & Mapping, Language Comprehension (minor), Sensorimotor Coordination (minor)"
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning tests a model’s ability to answer questions that require reasoning over scientific paper content, especially figure- and chart-centric evidence common in arXiv-style documents. It stresses extracting structured information from visual plots and linking it to technical text to produce correct, grounded conclusions.","Scene Understanding & Visual Reasoning, Visual Perception, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory (minor), Language Production (minor)"
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU evaluates multimodal understanding over video, requiring models to integrate information across frames and time to answer questions about events, actions, and causal relationships. It targets temporal coherence, event tracking, and reasoning about visually observed dynamics rather than single-image recognition.","Visual Perception, Scene Understanding & Visual Reasoning, Working Memory, Attention, Cognitive Timing & Predictive Modeling (minor), Auditory Processing (minor)"
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) is a long-context evaluation where multiple similar “needle” requests are embedded within a long, distracting “haystack,” and the model must retrieve and reproduce the correct response corresponding to the specified needle. It emphasizes long-range dependency tracking, robust retrieval under interference, and maintaining correctness as context length scales.","Working Memory, Episodic Memory, Attention, Language Comprehension, Semantic Understanding & Context Recognition, Adaptive Error Correction (minor)"
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval measures performance on well-specified professional knowledge-work tasks across many occupations, judged by expert humans via head-to-head comparisons against industry professionals. Tasks often require producing real work artifacts (e.g., spreadsheets, presentations, plans) and reflecting practical execution quality, not just answering questions.","Planning, Decision-making, Language Production, Semantic Understanding & Context Recognition, Adaptive Error Correction, Self-reflection (minor), Social Reasoning & Theory of Mind (minor)"
SWE-Lancer,https://openai.com/index/swe-lancer/,https://arxiv.org/abs/2502.12115,"SWE-Lancer evaluates agentic software engineering in settings that reward completing engineering work reliably, often emphasizing longer-horizon task execution than isolated bug fixes. It probes whether models can navigate codebases, follow project conventions, and deliver correct patches that satisfy tests and task requirements under realistic workflows.","Planning, Logical Reasoning, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition, Decision-making (minor)"
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks tests algorithmic reasoning over graph-structured data, where the model must follow traversal rules (e.g., BFS-style walks) and answer queries that depend on multi-step navigation through nodes and edges. It emphasizes systematic state tracking and faithful execution of discrete procedures under distractors.","Spatial Representation & Mapping, Working Memory, Logical Reasoning, Attention (minor), Planning (minor)"
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon evaluates tool-using agents on complex tasks that require selecting appropriate tools, composing multi-step toolchains, and recovering from tool errors to reach correct end results. It focuses on reliable orchestration (when to call which tool), argument correctness, and robust integration of tool outputs into the final solution.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition, Inhibitory Control (minor)"
HMMT,https://matharena.ai/?view=problem&comp=hmmt--hmmt_feb_2025,https://arxiv.org/abs/2505.23281,"HMMT problems are high-difficulty competition mathematics questions that require deep multi-step reasoning, creative insight, and careful error checking to reach exact answers. As an evaluation, it stresses sustained symbolic reasoning and robustness on non-routine math beyond standard textbook patterns.","Logical Reasoning, Working Memory, Planning, Cognitive Flexibility, Adaptive Error Correction (minor)"
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath is a benchmark of expert-level mathematics designed to be challenging for frontier models and more resistant to memorization, spanning multiple tiers of difficulty. It emphasizes rigorous multi-step derivations and the ability to combine advanced concepts while maintaining precision over long solution paths.","Logical Reasoning, Working Memory, Planning, Cognitive Flexibility, Adaptive Error Correction (minor)"
