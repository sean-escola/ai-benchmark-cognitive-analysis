Benchmark,Website,Paper,Description,Cognitive Functions
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates software engineering by asking a model/agent to generate patches that resolve real GitHub issues in Python repositories, with correctness checked by running the project’s tests. The “Verified” subset uses tasks validated to be solvable and aims to reduce noisy labels and ambiguous problems compared with earlier SWE-bench variants.","Planning, Logical Reasoning, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition, Language Comprehension (minor), Language Production (minor)"
SWE-bench Multilingual,https://www.swebench.com/multilingual.html,https://arxiv.org/abs/2504.21798,"SWE-bench Multilingual extends SWE-bench-style issue-to-patch evaluation beyond Python, covering multiple programming languages and ecosystems. It tests whether agents can understand repository context, implement fixes, and satisfy test suites across varied language/tooling conventions.","Planning, Logical Reasoning, Adaptive Error Correction, Cognitive Flexibility, Working Memory, Semantic Understanding & Context Recognition, Language Comprehension (minor), Language Production (minor)"
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro is a harder, more industrially oriented software engineering benchmark where models must produce correct code changes for a larger and more diverse set of real-world tasks across multiple languages. It is designed to be more challenging and more resistant to superficial shortcuts, emphasizing robust end-to-end problem solving in repositories.","Planning, Logical Reasoning, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition, Decision-making (minor), Cognitive Flexibility (minor)"
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 evaluates autonomous performance on real tasks in a command-line environment (e.g., diagnosing issues, running programs, manipulating files, and producing correct artifacts). It emphasizes iterative action, tool invocation, and recovering from errors under realistic execution constraints.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Language Comprehension (minor), Language Production (minor), Inhibitory Control (minor)"
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp measures deep-research and browsing capability by requiring models/agents to find, integrate, and justify answers using information from the web (or a controlled corpus, depending on the setup). It stresses multi-step information seeking, synthesis across sources, and maintaining coherence over long interactions.","Planning, Attention, Episodic Memory, Semantic Understanding & Context Recognition, Logical Reasoning, Language Comprehension (minor), Language Production (minor)"
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench evaluates agentic tool use in multi-turn customer-support-like environments where the model must interact with simulated users and APIs while adhering to domain policies. It tests reliable tool calling, policy-following behavior, and resolving tasks over extended dialogues with stateful constraints.","Decision-making, Planning, Inhibitory Control, Social Reasoning & Theory of Mind, Working Memory, Language Comprehension, Language Production, Empathy (minor)"
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld evaluates multimodal “computer use” agents that must complete tasks on an operating system by perceiving screens and interacting via actions such as clicking, typing, and navigation. Success depends on interpreting UI state, executing long-horizon action sequences, and correcting mistakes under step limits.","Visual Perception, Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Spatial Representation & Mapping, Planning, Decision-making, Adaptive Error Correction, Working Memory, Sensorimotor Coordination (minor), Language Comprehension (minor)"
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI is a few-shot abstract reasoning benchmark where systems infer hidden rules from small sets of grid-based input–output examples and must apply them to new inputs. It targets “fluid” generalization to novel patterns rather than knowledge recall, with strong penalties for brittle heuristics.","Logical Reasoning, Cognitive Flexibility, Working Memory, Spatial Representation & Mapping, Visual Perception, Scene Understanding & Visual Reasoning, Planning (minor), Attention (minor)"
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon agent coherence and strategy in a simulated vending-machine business over an extended period, with performance measured by final financial outcomes. Agents must manage inventory, negotiate, adapt to market dynamics, and sustain consistent goals across many steps.","Planning, Decision-making, Reward Mechanisms, Working Memory, Episodic Memory (minor), Adaptive Error Correction (minor), Motivational Drives (minor)"
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas measures real-world tool use via the Model Context Protocol (MCP), requiring models to discover appropriate tools, invoke them correctly, and compose multi-step workflows across external services. It emphasizes reliable API/tool interaction, error handling, and synthesizing results into correct outputs.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Language Comprehension (minor), Language Production (minor), Inhibitory Control (minor)"
FinanceAgent,https://www.vals.ai/benchmarks/finance_agent,,"FinanceAgent evaluates agent performance on tasks typical of an entry-level financial analyst, such as retrieving relevant information, performing calculations, and producing structured, decision-relevant outputs. The benchmark stresses applied quantitative reasoning, domain constraints, and multi-step analysis rather than simple fact recall.","Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Planning, Decision-making (minor), Language Production (minor)"
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity agents on real vulnerability tasks, including identifying known vulnerabilities from descriptions and discovering new issues in open-source codebases. It tests iterative technical investigation, hypothesis-driven debugging, and precise patch or exploit-relevant reasoning under realistic constraints.","Logical Reasoning, Planning, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition, Inhibitory Control (minor), Decision-making (minor)"
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"SpreadsheetBench evaluates the ability to understand, edit, and compute over complex spreadsheets derived from realistic tasks, often requiring multi-step transformations and correct formula or data operations. It stresses structured manipulation, consistency across dependencies, and producing verifiably correct final files.","Planning, Decision-making, Working Memory, Adaptive Error Correction, Logical Reasoning (minor), Scene Understanding & Visual Reasoning (minor), Language Comprehension (minor)"
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a difficult multimodal benchmark intended to probe frontier-level knowledge and reasoning across many domains using challenging questions (often requiring deep inference rather than memorization). Depending on evaluation setup, models may be tested with or without tools like web search or code execution, highlighting both core reasoning and agentic augmentation.","Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Language Comprehension, Language Production (minor), Scene Understanding & Visual Reasoning (minor), Visual Perception (minor), Planning (minor)"
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 consists of competition-style math problems that require multi-step symbolic and quantitative reasoning, typically without reliance on external tools. It is widely used to measure mathematical problem solving, compositional reasoning, and robustness under exact-answer scoring.","Logical Reasoning, Working Memory, Planning (minor), Adaptive Error Correction (minor), Language Comprehension (minor)"
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a high-quality subset of GPQA composed of very challenging, “Google-proof” graduate-level science multiple-choice questions. It aims to measure deep scientific reasoning and understanding, with questions selected so that non-experts typically fail while experts succeed.","Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Decision-making (minor), Language Comprehension (minor)"
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends MMLU-style evaluation to multiple languages, testing broad academic knowledge and reasoning across many subjects with standardized multiple-choice questions. It probes cross-lingual generalization and whether capabilities transfer beyond English without task-specific fine-tuning.","Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning, Cognitive Flexibility, Working Memory (minor), Language Production (minor)"
MMMU,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2311.16502,"MMMU is a multidisciplinary multimodal benchmark where models answer questions requiring joint understanding of text and images, including diagrams, charts, and figures across many domains. It emphasizes integrating visual evidence with textual context to support correct reasoning.","Multisensory Integration, Visual Perception, Scene Understanding & Visual Reasoning, Language Comprehension, Logical Reasoning, Attention (minor), Working Memory (minor)"
LAB-Bench FigQA,https://huggingface.co/datasets/futurehouse/lab-bench,https://arxiv.org/abs/2407.10362,"LAB-Bench FigQA evaluates whether models can correctly interpret complex scientific figures from biology papers and answer figure-grounded questions. It stresses precise extraction of information from plots/diagrams, linking visual elements to domain concepts, and avoiding hallucinated figure content.","Visual Perception, Scene Understanding & Visual Reasoning, Multisensory Integration, Semantic Understanding & Context Recognition, Logical Reasoning, Attention (minor), Working Memory (minor)"
WebArena,https://webarena.dev/,https://arxiv.org/abs/2307.13854,"WebArena evaluates autonomous web agents on realistic multi-step tasks across several web apps (e-commerce, CMS, forums, collaboration), requiring navigation, form filling, and state tracking. It probes end-to-end agent competence: perceiving UI state, planning action sequences, and recovering from mistakes while meeting task objectives.","Visual Perception, Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Planning, Decision-making, Adaptive Error Correction, Working Memory, Sensorimotor Coordination (minor), Inhibitory Control (minor)"
