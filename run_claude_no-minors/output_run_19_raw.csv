Benchmark,Website,Paper,Description,Cognitive Functions
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates whether a model can resolve real GitHub issues by generating a correct patch in a Python codebase, with tasks curated/validated to be solvable and reliably graded. It emphasizes end-to-end software engineering: understanding problem statements, navigating code, implementing fixes, and satisfying tests under a single-attempt setting.","Language Comprehension, Language Production, Planning, Adaptive Error Correction, Working Memory, Logical Reasoning"
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 measures an agent’s ability to complete real tasks inside a command-line environment (e.g., debugging, build steps, data processing), typically via iterative tool use and verification. It stresses robust action sequences under noisy execution conditions (errors, missing dependencies, resource limits) and rewards correct final outcomes rather than fluent text.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Language Comprehension (minor), Logical Reasoning"
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp evaluates “deep research” behavior: searching/browsing a fixed corpus (or web-like index), gathering evidence, and producing a correct, supported answer. It probes whether models can manage long-horizon information seeking—querying, filtering sources, cross-checking, and synthesizing—without losing the task objective.","Planning, Attention, Semantic Understanding & Context Recognition, Episodic Memory, Language Production, Decision-making"
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench measures an agent’s ability to interact with simulated users and backend APIs in multi-turn customer-support-style environments (e.g., retail, airline, telecom) while following domain policies. Success requires consistent state tracking, tool calling, and policy adherence under conversational pressure and ambiguity.","Language Comprehension, Language Production, Social Reasoning & Theory of Mind, Decision-making, Planning, Inhibitory Control, Working Memory"
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld evaluates multimodal computer-use agents that operate graphical desktop environments to complete tasks across applications (web, files, settings, productivity tools). It tests whether models can perceive UI state from screenshots, plan sequences of interactions, and robustly recover from mistakes across many steps.","Visual Perception, Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Spatial Representation & Mapping (minor), Planning, Decision-making, Sensorimotor Coordination, Adaptive Error Correction"
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,ARC-AGI presents small grid-based puzzles where a model must infer a latent transformation rule from a few input-output examples and apply it to a new input. The benchmark targets novelty and abstraction: high scores require discovering new rules rather than recalling memorized templates.,"Cognitive Flexibility, Logical Reasoning, Working Memory, Visual Perception, Scene Understanding & Visual Reasoning, Spatial Representation & Mapping"
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon autonomy by placing an agent in a simulated year-long vending-machine business that requires thousands of decisions (inventory, pricing, supplier negotiation). The score reflects sustained coherence, strategy, and adaptation to changing conditions over extended trajectories.","Planning, Decision-making, Reward Mechanisms, Episodic Memory, Working Memory (minor), Self-reflection (minor), Adaptive Error Correction (minor)"
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas evaluates real-world tool use via the Model Context Protocol by requiring models to discover tools, call them correctly, handle errors/retries, and synthesize results across multi-step workflows. It emphasizes robust API/tool interaction rather than purely textual problem solving.","Planning, Decision-making, Working Memory, Adaptive Error Correction, Language Comprehension (minor), Language Production (minor)"
FinanceAgent,https://www.vals.ai/benchmarks/finance_agent,,"FinanceAgent measures performance on tasks typical of an entry-level financial analyst (e.g., extracting signals from financial documents, reasoning about metrics, producing justified recommendations). It stresses domain-grounded reasoning, numerical consistency, and producing structured professional outputs.","Semantic Understanding & Context Recognition, Logical Reasoning, Decision-making, Working Memory, Planning (minor), Language Production (minor)"
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity agent capabilities on large-scale, real-software vulnerability tasks, including identifying known weaknesses and finding new ones. It stresses systematic investigation, hypothesis testing, and correcting course when exploitation or reproduction attempts fail.","Planning, Logical Reasoning, Adaptive Error Correction, Working Memory, Inhibitory Control (minor), Decision-making"
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"SpreadsheetBench evaluates whether a model can navigate and manipulate complex spreadsheets to solve realistic tasks (formula creation, restructuring, analysis, formatting, and consistency checks). It targets structured, multi-step reasoning over semi-structured artifacts, where small mistakes can cascade into incorrect outputs.","Planning, Working Memory, Logical Reasoning, Adaptive Error Correction, Attention (minor), Language Production (minor)"
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a large, frontier-level benchmark spanning many expert domains, designed to probe difficult reasoning and advanced knowledge, often with multimodal components. It aims to test model performance near the limits of current systems, where shallow pattern matching is less reliable.","Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Visual Perception (minor), Scene Understanding & Visual Reasoning (minor)"
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 evaluates competition-style mathematics requiring multi-step symbolic reasoning and careful algebraic/combinatorial manipulation. It is typically scored by exact final answers, rewarding correctness under tight logical constraints rather than explanatory quality.","Logical Reasoning, Working Memory, Planning (minor), Adaptive Error Correction (minor)"
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,GPQA Diamond is a difficult multiple-choice science QA benchmark constructed to be resistant to superficial search and to separate expert from non-expert performance. It tests whether models can integrate scientific knowledge with multi-step reasoning to select the correct option among plausible distractors.,"Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory (minor), Decision-making (minor), Language Comprehension (minor)"
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends broad academic knowledge testing across many languages, evaluating both subject-matter competence and multilingual understanding. It probes whether a model can maintain consistent reasoning and factual recall when the prompt language changes and when cultural/linguistic variation affects framing.","Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning (minor), Working Memory (minor), Language Production (minor)"
MMMU,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2311.16502,"MMMU is a multidisciplinary multimodal benchmark where questions require combining images (diagrams, charts, scenes) with text to answer expert-level problems. It stresses visual understanding, cross-modal grounding, and multi-step reasoning rather than pure captioning.","Visual Perception, Scene Understanding & Visual Reasoning, Multisensory Integration, Language Comprehension, Logical Reasoning, Working Memory (minor)"
LAB-Bench FigQA,https://huggingface.co/datasets/futurehouse/lab-bench,https://arxiv.org/abs/2407.10362,"LAB-Bench FigQA evaluates whether models can interpret and reason over complex figures from biology papers (plots, microscopy panels, multi-part schematics) to answer targeted questions. It probes scientific visual literacy: extracting the right visual evidence and mapping it to domain concepts.","Visual Perception, Scene Understanding & Visual Reasoning, Semantic Understanding & Context Recognition, Logical Reasoning, Attention (minor), Working Memory (minor)"
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning evaluates reasoning over figures and visual elements from scientific documents (e.g., charts/plots/diagrams) where answering requires reading and integrating visual cues with the question context. It emphasizes precise interpretation of scientific visual encodings and quantitative/relational reasoning from the figure.","Visual Perception, Scene Understanding & Visual Reasoning, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory (minor), Attention (minor)"
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU extends multimodal understanding to video, requiring temporal integration of events, actions, and scene changes to answer questions. It probes whether models can maintain coherent representations over time and use sparse visual evidence to support correct inferences.","Visual Perception, Visual Attention & Eye Movements, Working Memory, Cognitive Timing & Predictive Modeling (minor), Scene Understanding & Visual Reasoning, Multisensory Integration (minor)"
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"The FACTS Benchmark Suite systematically evaluates LLM factuality across diverse settings, including resisting hallucination, maintaining consistency, and grounding answers when evidence is present. It targets reliability of stated claims under realistic question-answering and generation workloads.","Semantic Understanding & Context Recognition, Language Production, Episodic Memory (minor), Inhibitory Control (minor), Adaptive Error Correction (minor)"
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA evaluates physical commonsense reasoning across many languages, focusing on selecting plausible solutions to everyday interaction problems. It probes whether models preserve intuitive physics and affordance reasoning under multilingual paraphrase and culturally varied framing.","Language Comprehension, Logical Reasoning, Semantic Understanding & Context Recognition, Spatial Representation & Mapping (minor), Working Memory (minor)"
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) is a long-context retrieval and multi-round coreference-style evaluation where multiple similar “needle” interactions are embedded in long “haystacks,” and the model must reproduce the correct referenced response. It stresses faithful context tracking under interference from many near-duplicate distractors.","Working Memory, Attention, Episodic Memory, Language Comprehension, Inhibitory Control (minor)"
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval evaluates economically meaningful, well-specified professional work products (e.g., presentations, spreadsheets, schedules) across dozens of occupations, judged by expert humans in head-to-head comparisons. It targets end-to-end task completion quality, including following constraints, structuring deliverables, and making reasonable professional decisions.","Planning, Decision-making, Language Production, Semantic Understanding & Context Recognition, Working Memory (minor), Adaptive Error Correction (minor), Social Reasoning & Theory of Mind (minor)"
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks tests algorithmic reasoning over graph-structured data embedded in text, such as following parent pointers, BFS-style traversals, or multi-hop path queries across large contexts. It stresses correct symbolic-like manipulation and state tracking rather than broad world knowledge.","Logical Reasoning, Working Memory, Planning (minor), Spatial Representation & Mapping, Attention (minor)"
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon evaluates general tool-using competence across heterogeneous APIs and tasks, emphasizing correct tool selection, argument construction, multi-step orchestration, and recovery from tool errors. It is designed to reflect practical agent workflows where correctness depends on external actions and intermediate results.","Planning, Decision-making, Working Memory, Adaptive Error Correction, Language Comprehension (minor), Language Production (minor)"
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath evaluates expert-level mathematics with problems intended to be difficult for current models and less susceptible to memorization, often requiring deep multi-step reasoning and careful verification. It is used to track progress near the frontier of formal and quantitative problem solving, frequently with optional tool (e.g., Python) assistance in some settings.","Logical Reasoning, Working Memory, Planning (minor), Adaptive Error Correction (minor)"
