Benchmark,Website,Paper,Description,Cognitive Functions
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates software engineering agents by asking them to fix real bugs in GitHub repositories so that hidden unit tests pass. The “Verified” subset focuses on issues that have been confirmed solvable and reduces noise from ambiguous or broken tasks, emphasizing reliable end-to-end patch generation.","Planning, Decision-making, Logical Reasoning, Working Memory, Adaptive Error Correction, Language Comprehension (minor), Semantic Understanding & Context Recognition"
SWE-bench Multilingual,https://www.swebench.com/multilingual.html,https://arxiv.org/abs/2504.21798,"SWE-bench Multilingual extends SWE-bench-style repository bug-fixing to multiple programming languages, testing whether agents can generalize beyond Python to diverse ecosystems and tooling. It emphasizes cross-language code understanding, patch synthesis, and debugging under realistic repo constraints.","Language Comprehension (minor), Semantic Understanding & Context Recognition, Language Production (minor), Logical Reasoning, Cognitive Flexibility, Planning, Adaptive Error Correction, Working Memory"
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro is a larger, more difficult software engineering benchmark intended to be more contamination-resistant and more representative of professional development work. Tasks generally require deeper repository understanding, longer-horizon debugging, and more robust tool-and-test loops than SWE-bench Verified.","Planning, Decision-making, Logical Reasoning, Working Memory, Adaptive Error Correction, Semantic Understanding & Context Recognition, Inhibitory Control (minor)"
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 evaluates agents on real command-line tasks in containerized environments (e.g., installing dependencies, manipulating files, running programs, diagnosing failures). It measures practical tool use, iterative debugging, and the ability to translate natural-language goals into correct terminal actions.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Attention, Language Comprehension (minor)"
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp measures “deep research” capability by requiring models to locate, read, and synthesize information from a controlled web or document collection to answer difficult questions. It stresses evidence gathering, multi-step retrieval, and maintaining coherence across long browsing trajectories.","Planning, Decision-making, Language Comprehension, Semantic Understanding & Context Recognition, Working Memory, Episodic Memory (minor), Attention"
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench evaluates interactive customer-support agents that must use APIs and follow domain policies in multi-turn conversations (e.g., retail, airline, telecom). It emphasizes policy adherence under pressure, correct tool invocation, and socially appropriate responses while resolving user goals.","Social Reasoning & Theory of Mind, Language Comprehension, Language Production, Decision-making, Planning, Inhibitory Control, Empathy (minor)"
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld is a multimodal computer-use benchmark where agents perceive screenshots and interact with an operating-system-like environment to complete tasks. It tests UI understanding, sequential action execution, and robust recovery from mistakes in realistic workflows.","Visual Perception, Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Planning, Decision-making, Sensorimotor Coordination, Working Memory"
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI presents few-shot abstract reasoning puzzles over colored grids, requiring inference of latent rules from a handful of examples and application to a new input. It aims to probe fluid intelligence and systematic generalization rather than memorized domain knowledge.","Logical Reasoning, Cognitive Flexibility, Working Memory, Spatial Representation & Mapping, Attention, Planning (minor)"
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon agent coherence by having the model run a simulated vending-machine business over extended time, optimizing profit under changing conditions. Success requires sustained strategy, iterative decisions, and adapting to feedback across many steps.","Planning, Decision-making, Reward Mechanisms, Working Memory, Episodic Memory, Self-reflection (minor), Cognitive Timing & Predictive Modeling (minor)"
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas evaluates real-world tool use via the Model Context Protocol by testing whether a model can discover, call, and chain tools correctly to solve multi-step tasks. It emphasizes robust workflow execution, handling tool errors, and integrating tool outputs into final answers.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Language Comprehension (minor), Semantic Understanding & Context Recognition"
FinanceAgent,https://www.vals.ai/benchmarks/finance_agent,,"FinanceAgent assesses agent performance on tasks typical of an entry-level financial analyst, such as interpreting financial documents, producing analyses, and creating defensible recommendations. It stresses domain-grounded reasoning, structured outputs, and multi-step problem solving under realistic constraints.","Semantic Understanding & Context Recognition, Logical Reasoning, Planning, Decision-making, Working Memory, Language Production (minor)"
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity agents on identifying known vulnerabilities from descriptions and, in some settings, discovering new weaknesses in real codebases. It tests systematic debugging, hypothesis-driven investigation, and careful verification under adversarial or failure-prone conditions.","Logical Reasoning, Planning, Adaptive Error Correction, Working Memory, Attention, Inhibitory Control (minor)"
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"SpreadsheetBench evaluates whether models can navigate and manipulate complex spreadsheets to solve realistic problems, often requiring formula edits, data transformations, and correctness checks. It stresses procedural task execution, structured reasoning over tabular data, and error correction.","Planning, Decision-making, Working Memory, Adaptive Error Correction, Attention, Semantic Understanding & Context Recognition (minor)"
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a frontier, multimodal benchmark designed to probe advanced reasoning and broad academic/professional knowledge across hard questions. Tasks often require integrating information, resisting shallow pattern-matching, and producing precise answers under ambiguity.","Language Comprehension, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Visual Perception (minor), Multisensory Integration (minor), Planning (minor)"
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 consists of competition-style mathematics problems (typically short-answer) that require multi-step derivations and careful algebraic or combinatorial reasoning. It targets mathematical problem solving more than factual recall, with strong penalties for small logical slips.","Logical Reasoning, Working Memory, Attention, Planning (minor), Adaptive Error Correction (minor)"
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a high-quality subset of graduate-level, “Google-proof” multiple-choice questions in biology, chemistry, and physics. It is designed so that superficial retrieval is insufficient, emphasizing deep conceptual understanding and careful reasoning.","Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Attention (minor), Language Comprehension (minor)"
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends MMLU-style academic questions to multiple languages, measuring multilingual knowledge and reasoning across diverse subjects. It probes whether models preserve competency and calibration when the same kinds of tasks are presented in different linguistic forms.","Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory (minor), Language Production (minor)"
MMMU,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2311.16502,"MMMU is a multimodal benchmark spanning many disciplines where models must answer questions that require understanding images alongside text (e.g., diagrams, plots, screenshots). It emphasizes multimodal grounding, visual reasoning, and cross-domain generalization.","Visual Perception, Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Spatial Representation & Mapping (minor), Visual Attention & Eye Movements (minor)"
LAB-Bench FigQA,https://huggingface.co/datasets/futurehouse/lab-bench,https://arxiv.org/abs/2407.10362,LAB-Bench FigQA focuses on interpreting complex scientific figures from biology papers and answering questions that depend on correct figure reading. It tests whether models can extract quantitative/relational information from plots and panels and connect it to scientific context.,"Visual Perception, Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Attention, Semantic Understanding & Context Recognition (minor)"
WebArena,https://webarena.dev/,https://arxiv.org/abs/2307.13854,"WebArena evaluates autonomous web agents performing realistic tasks across multiple web apps (e-commerce, CMS, forums, repositories), requiring navigation, form filling, and multi-step workflows. It stresses robust sequential decision-making under partial observability and UI variability.","Visual Perception, Visual Attention & Eye Movements, Planning, Decision-making, Sensorimotor Coordination, Working Memory, Inhibitory Control (minor)"
