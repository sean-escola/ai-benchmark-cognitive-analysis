Benchmark,Website,Paper,Description,Cognitive Functions
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates whether a model can solve real GitHub issues by producing a patch that passes the project’s tests, using a curated set of tasks that are verified as solvable. It measures end-to-end software engineering ability: understanding requirements, locating relevant code, implementing fixes, and validating them under a deterministic test harness.","Language Comprehension, Planning, Logical Reasoning, Adaptive Error Correction, Working Memory, Decision-making (minor)"
SWE-bench Multilingual,https://www.swebench.com/multilingual.html,https://arxiv.org/abs/2504.21798,"SWE-bench Multilingual extends the SWE-bench paradigm beyond Python, evaluating issue-to-patch performance across multiple programming languages. It stresses cross-language transfer and codebase understanding under differing ecosystems, tooling, and conventions.","Language Comprehension, Planning, Logical Reasoning, Adaptive Error Correction, Semantic Understanding & Context Recognition, Working Memory (minor), Language Production (minor)"
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro is a harder, more contamination-resistant software engineering benchmark designed to better reflect industrial workflows, often requiring deeper debugging, broader code changes, and more robust validation. Compared with Verified, it places more emphasis on reliably navigating ambiguity and complex dependency interactions across repositories.","Planning, Logical Reasoning, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition, Decision-making (minor), Self-reflection (minor)"
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 evaluates agentic performance in command-line environments, where the model must accomplish practical tasks by issuing shell commands and interpreting outputs under resource and tool constraints. Success depends on iterative experimentation, error diagnosis, and correct sequencing of actions to reach a goal state.","Planning, Adaptive Error Correction, Decision-making, Working Memory, Language Comprehension (minor), Sensorimotor Coordination (minor)"
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp evaluates “deep research” behavior where a model must search and synthesize information from a controlled document corpus to answer questions, often requiring multi-step retrieval and evidence integration. It targets reproducible search-and-reason pipelines by standardizing the underlying index, reducing dependence on external web variability.","Planning, Semantic Understanding & Context Recognition, Language Comprehension, Working Memory, Logical Reasoning (minor), Decision-making (minor)"
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench measures an agent’s ability to interact with simulated users and APIs in multi-turn customer-support scenarios while adhering to domain policies (e.g., retail, airline, telecom). It emphasizes policy-following under conversational pressure, tool/API correctness, and maintaining consistent state across turns.","Language Comprehension, Language Production, Decision-making, Planning, Inhibitory Control, Social Reasoning & Theory of Mind (minor), Empathy (minor)"
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld evaluates multimodal computer-use agents performing tasks on operating-system-like interfaces, requiring navigation, UI understanding, and action execution over many steps. It tests whether the agent can ground instructions in visual observations and carry out reliable sequences of interactions to complete real workflows.","Visual Perception, Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements (minor), Planning, Decision-making, Adaptive Error Correction, Working Memory (minor), Sensorimotor Coordination (minor)"
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI tests fluid reasoning by requiring models to infer the rule behind a few input–output grid examples and produce the correct output for a novel grid. It is designed to minimize reliance on memorized knowledge and instead emphasize abstraction, compositional pattern discovery, and generalization from sparse evidence.","Logical Reasoning, Cognitive Flexibility, Spatial Representation & Mapping, Working Memory, Attention (minor)"
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon agent coherence and strategy in a simulated business management setting (e.g., inventory, supplier negotiation, pricing, and adaptation over extended time). It rewards sustained goal pursuit, consistent bookkeeping/state tracking, and robust decision-making under changing conditions.","Planning, Decision-making, Working Memory, Reward Mechanisms (minor), Social Reasoning & Theory of Mind (minor), Self-reflection (minor)"
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas evaluates real-world tool use via the Model Context Protocol (MCP), focusing on selecting appropriate tools, executing multi-step workflows across servers, handling errors/retries, and synthesizing outputs. It targets agent reliability in production-like API environments rather than single-shot question answering.","Planning, Decision-making, Language Comprehension, Adaptive Error Correction, Working Memory (minor), Semantic Understanding & Context Recognition (minor)"
FinanceAgent,https://www.vals.ai/benchmarks/finance_agent,,"FinanceAgent assesses capabilities expected of an entry-level financial analyst, typically involving multi-step reasoning over financial documents, spreadsheet-like computations, and the production of justified conclusions. It stresses correctness, structured analysis, and maintaining assumptions and intermediate results consistently.","Logical Reasoning, Planning, Decision-making, Working Memory, Semantic Understanding & Context Recognition, Language Production (minor)"
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity agent skills at scale, including reproducing known vulnerabilities from descriptions and discovering previously unknown issues in real open-source codebases. It emphasizes iterative debugging, hypothesis-driven exploration, and producing concrete, verifiable exploit/fix artifacts under realistic constraints.","Logical Reasoning, Planning, Adaptive Error Correction, Working Memory, Inhibitory Control (minor), Semantic Understanding & Context Recognition (minor)"
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"SpreadsheetBench evaluates an agent’s ability to navigate and manipulate complex spreadsheets to solve realistic tasks, often requiring formula edits, data cleaning, structured transformations, and verification. It probes procedural reliability and the ability to track dependencies across cells, sheets, and linked calculations.","Planning, Decision-making, Working Memory, Adaptive Error Correction, Semantic Understanding & Context Recognition (minor), Language Comprehension (minor)"
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a frontier, multi-domain (often multimodal) benchmark intended to stress advanced reasoning and expert knowledge across many topics and question styles. It is frequently used to compare tool-free reasoning versus tool-augmented performance (e.g., search/code), highlighting integration of evidence and robust synthesis.","Language Comprehension, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory (minor), Planning (minor), Multisensory Integration (minor), Visual Perception (minor)"
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 measures competition-level mathematical problem solving, typically requiring multi-step derivations and careful handling of constraints. It is commonly used to gauge symbolic reasoning accuracy and the ability to avoid subtle algebraic or combinatorial errors under time/step pressure.","Logical Reasoning, Working Memory, Attention (minor), Adaptive Error Correction (minor)"
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a high-quality subset of extremely difficult graduate-level science multiple-choice questions designed to be hard to answer via superficial pattern matching. It tests deep scientific reasoning and precise reading, with distractors that punish shallow heuristics.","Logical Reasoning, Semantic Understanding & Context Recognition, Language Comprehension, Working Memory (minor), Inhibitory Control (minor)"
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends broad academic knowledge and reasoning evaluation into multiple languages across many subjects, probing whether competence transfers beyond English. It is used to measure multilingual understanding, instruction following in different languages, and consistency of reasoning across linguistic contexts.","Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning (minor), Working Memory (minor), Language Production (minor)"
MMMU,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2311.16502,"MMMU evaluates multimodal, multi-discipline understanding by requiring models to answer questions grounded in images (e.g., diagrams, charts, scientific figures) plus text. It emphasizes visual reasoning and cross-modal integration needed to interpret technical visuals and relate them to problem statements.","Visual Perception, Scene Understanding & Visual Reasoning, Multisensory Integration, Language Comprehension, Logical Reasoning (minor), Working Memory (minor)"
LAB-Bench FigQA,https://huggingface.co/datasets/futurehouse/lab-bench,https://arxiv.org/abs/2407.10362,"LAB-Bench FigQA measures whether models can correctly interpret complex scientific figures from biology papers and answer associated questions, often requiring careful extraction of evidence from plots, schematics, and multi-panel visuals. It targets figure-grounded reasoning rather than general text-only biological knowledge.","Visual Perception, Scene Understanding & Visual Reasoning, Semantic Understanding & Context Recognition, Logical Reasoning, Attention (minor), Multisensory Integration (minor)"
WebArena,https://webarena.dev/,https://arxiv.org/abs/2307.13854,"WebArena evaluates autonomous web agents completing realistic tasks across multiple web apps (e-commerce, CMS, forums, code hosting) through interactive browsing. It probes long-horizon planning, robust UI grounding from screenshots/DOM, and recovery from errors or unexpected page states while pursuing task objectives.","Planning, Decision-making, Adaptive Error Correction, Visual Perception, Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements (minor), Working Memory (minor), Language Comprehension (minor), Sensorimotor Coordination (minor)"
