Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,SWE-bench Verified evaluates an LLM’s ability to solve real-world software engineering issues by generating patches that make a repository’s tests pass. The “Verified” subset consists of tasks that have been validated as solvable and is typically run with a fixed agent scaffold to measure end-to-end coding reliability.,"L1: Language Comprehension
L2: Planning, Logical Reasoning, Working Memory, Adaptive Error Correction, Decision-making (minor)
L3: ",L2
SWE-bench Multilingual,https://www.swebench.com/multilingual.html,https://arxiv.org/abs/2504.21798,"SWE-bench Multilingual extends SWE-bench-style patch generation to multiple programming languages, assessing whether an agent can read, modify, and validate code across diverse language ecosystems. It stresses generalization of debugging and fix synthesis beyond a single language/tooling stack.","L1: Language Comprehension, Language Production (minor)
L2: Planning, Logical Reasoning, Adaptive Error Correction, Working Memory
L3: Cognitive Flexibility",L3
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro is a larger and harder software engineering benchmark designed to be more challenging and more contamination-resistant, spanning multiple languages and more industrially realistic tasks. It requires sustained tool-driven iteration (edit–run tests–diagnose–repair) and robust handling of complex repos.","L1: 
L2: Planning, Decision-making, Logical Reasoning, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition
L3: Cognitive Flexibility (minor), Inhibitory Control (minor)",L2
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 evaluates agentic performance on real command-line tasks (e.g., using shell tools, editing files, running programs, and diagnosing failures) in constrained environments. It emphasizes practical problem-solving loops under tool feedback rather than pure text-only reasoning.","L1: Language Comprehension (minor)
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition
L3: ",L2
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp measures deep-research performance where models must search, read, and synthesize information from a controlled document collection (or browsing-like setup) to answer questions. It targets long-horizon information gathering, source integration, and error recovery when evidence is incomplete or misleading.","L1: Language Comprehension
L2: Planning, Attention, Working Memory, Semantic Understanding & Context Recognition, Logical Reasoning (minor), Adaptive Error Correction (minor)
L3: ",L2
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench evaluates interactive agent behavior in customer-support-like environments where the model must follow domain policies while using tools/APIs over multi-turn dialogs. It probes reliability under policy constraints, maintaining state across turns, and balancing user goals with rules.","L1: Language Comprehension, Language Production
L2: Decision-making, Planning, Working Memory
L3: Social Reasoning & Theory of Mind, Inhibitory Control, Empathy (minor)",L3
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld evaluates multimodal “computer use” agents that must complete tasks by interacting with a graphical desktop environment (e.g., apps, web pages, dialogs) within step limits. It measures grounded perception-to-action competence: interpreting UI state, choosing actions, and recovering from mistakes.","L1: Visual Perception
L2: Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Planning, Decision-making, Sensorimotor Coordination, Working Memory, Adaptive Error Correction
L3: ",L2
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI (Abstraction and Reasoning Corpus) tests few-shot induction of novel rules from small sets of input–output grid examples, aiming to measure fluid reasoning rather than memorized knowledge. Solutions typically require discovering compositional transformations and applying them to new instances.","L1: Visual Perception
L2: Logical Reasoning, Working Memory, Spatial Representation & Mapping, Scene Understanding & Visual Reasoning, Planning (minor)
L3: Cognitive Flexibility",L3
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 measures long-horizon autonomous business management in a simulated setting, where an agent runs a vending machine business over an extended period. Success depends on sustained coherence, strategy under uncertainty, and adapting decisions based on outcomes and market dynamics.","L1: 
L2: Planning, Decision-making, Reward Mechanisms, Working Memory, Adaptive Error Correction (minor)
L3: Cognitive Timing & Predictive Modeling, Self-reflection (minor)",L3
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas evaluates real-world tool use via the Model Context Protocol, emphasizing multi-step workflows that require tool discovery, correct invocation, and synthesis of outputs. It stresses robust orchestration across heterogeneous tools and handling errors/retries in production-like settings.","L1: Language Comprehension (minor)
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition (minor)
L3: ",L2
FinanceAgent,https://www.vals.ai/benchmarks/finance_agent,,"FinanceAgent evaluates an agent’s ability to perform tasks resembling entry-level financial analyst work, such as extracting information, performing calculations, and producing finance-relevant analyses and artifacts. It targets accuracy, structured reasoning with domain conventions, and multi-step tool-assisted workflows when available.","L1: Language Production (minor)
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Planning, Decision-making
L3: ",L2
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity agent capabilities on tasks like identifying known vulnerabilities in real open-source projects from descriptions and, in some settings, discovering new issues. It stresses code/trace comprehension, hypothesis-driven debugging, and iterative validation under realistic constraints.","L1: 
L2: Logical Reasoning, Planning, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition
L3: Inhibitory Control (minor)",L2
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"SpreadsheetBench measures an agent’s ability to navigate, edit, and compute over complex spreadsheets derived from realistic scenarios. It emphasizes procedural task execution (often via tools/libraries), maintaining consistency across many dependent cells, and producing correctly formatted outputs.","L1: 
L2: Planning, Decision-making, Working Memory, Adaptive Error Correction, Semantic Understanding & Context Recognition, Sensorimotor Coordination (minor)
L3: ",L2
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a frontier, multi-domain benchmark intended to probe advanced knowledge and reasoning at or beyond typical expert difficulty, including multimodal questions. It is designed to stress robust reasoning, careful reading, and (in tool-enabled settings) effective use of external resources while avoiding hallucinations.","L1: Language Comprehension
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Scene Understanding & Visual Reasoning (minor), Multisensory Integration (minor)
L3: ",L2
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 evaluates competition-level mathematical problem solving on the 2025 American Invitational Mathematics Examination. Problems require multi-step derivations and careful symbolic manipulation, with accuracy typically measured by exact final answers.","L1: 
L2: Logical Reasoning, Working Memory, Adaptive Error Correction (minor), Planning (minor)
L3: ",L2
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a high-quality subset of graduate-level, “Google-proof” multiple-choice science questions designed to be difficult for non-experts and resistant to simple retrieval. It targets deep scientific reasoning and precise selection among plausible distractors.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory (minor), Decision-making (minor)
L3: ",L2
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends the MMLU-style subject test to multiple languages, evaluating broad academic knowledge and reasoning across many domains in non-English settings. It is commonly used to assess multilingual generalization and consistency of knowledge across languages.","L1: Language Comprehension, Language Production (minor)
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory (minor)
L3: Cognitive Flexibility",L3
MMMU,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2311.16502,"MMMU is a multidisciplinary multimodal benchmark that requires combining text with images (e.g., diagrams, charts, screenshots) to answer expert-level questions across many fields. It probes integrated visual-text reasoning, not just OCR or generic captioning.","L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Visual Attention & Eye Movements (minor), Working Memory (minor)
L3: ",L2
LAB-Bench FigQA,https://huggingface.co/datasets/futurehouse/lab-bench,https://arxiv.org/abs/2407.10362,"LAB-Bench FigQA evaluates whether models can correctly interpret and reason over complex scientific figures from biology papers, answering questions that require extracting relationships and experimental results from visuals. It stresses fine-grained figure understanding, cross-referencing labels/legends, and drawing correct inferences.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Semantic Understanding & Context Recognition, Logical Reasoning, Attention, Working Memory (minor)
L3: ",L2
WebArena,https://webarena.dev/,https://arxiv.org/abs/2307.13854,"WebArena evaluates autonomous web agents on realistic multi-step tasks across self-hosted web apps (e-commerce, CMS, forums, repos, maps), requiring navigation, form filling, and stateful interaction. It emphasizes grounded decision-making under UI uncertainty, tool/action sequencing, and recovery from partial failures.","L1: Visual Perception, Language Comprehension (minor), Language Production (minor)
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory, Visual Attention & Eye Movements, Sensorimotor Coordination
L3: Social Reasoning & Theory of Mind (minor)",L2
