Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates agentic software engineering by asking a model to produce code patches that fix real issues in open-source Python repositories, with correctness checked by running the project’s tests in a controlled harness. The “Verified” subset emphasizes tasks that have been validated as solvable and aims to reduce noise from ambiguous or broken tasks.","L1: Language Comprehension (minor), Language Production (minor)
L2: Planning, Logical Reasoning, Adaptive Error Correction, Working Memory
L3: ",L2
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 measures an agent’s ability to complete real command-line tasks (e.g., debugging, installing dependencies, editing files, running programs) in sandboxed terminal environments. Success depends on selecting correct sequences of shell actions, interpreting tool outputs, and iteratively correcting mistakes under resource constraints.","L1: Language Comprehension (minor)
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory
L3: ",L2
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp evaluates deep research agents on questions that require searching and synthesizing information from documents, emphasizing end-to-end browsing behavior rather than purely parametric recall. It stresses reliable information gathering, citation-grounded synthesis, and robustness to distractors or partially relevant sources.","L1: Language Comprehension, Language Production (minor)
L2: Planning, Attention, Semantic Understanding & Context Recognition, Episodic Memory (minor), Working Memory
L3: ",L2
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench measures interactive agent performance in multi-turn customer-support-like domains (e.g., retail, airline, telecom) that combine natural language dialogue with API/tool actions under explicit policies. It emphasizes consistent policy adherence, accurate state tracking across turns, and successful task completion with simulated users.","L1: Language Comprehension, Language Production
L2: Decision-making, Planning, Working Memory
L3: Social Reasoning & Theory of Mind, Inhibitory Control, Empathy (minor)",L3
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld evaluates multimodal computer-use agents that operate graphical desktop environments to complete realistic tasks across applications (e.g., web, documents, settings) within a step budget. Models must perceive UI screenshots, locate actionable elements, and execute sequences of actions while handling errors and unexpected UI states.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Planning, Decision-making, Sensorimotor Coordination, Adaptive Error Correction (minor), Working Memory (minor)
L3: ",L2
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,ARC-AGI tests few-shot abstraction and reasoning over grid-based puzzles where models infer latent rules from a small number of examples and must generalize to a new input. It is designed to emphasize novelty and compositional pattern discovery rather than memorization of domain facts.,"L1: 
L2: Logical Reasoning, Working Memory, Spatial Representation & Mapping, Scene Understanding & Visual Reasoning, Attention (minor)
L3: Cognitive Flexibility",L3
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon autonomous management in a simulated business setting, where an agent runs a vending machine operation over extended time, making thousands of interconnected decisions. Performance is based on long-term outcomes (e.g., final balance), requiring sustained coherence, adaptation to market dynamics, and strategic planning.","L1: 
L2: Planning, Decision-making, Reward Mechanisms, Working Memory, Episodic Memory (minor), Adaptive Error Correction (minor)
L3: Motivational Drives (minor)",L2
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas evaluates real-world tool use via the Model Context Protocol, stressing discovery and correct invocation of tools across multi-step workflows and heterogeneous servers. It measures whether models can orchestrate sequences of API calls, manage failures/retries, and synthesize tool outputs into correct final results.","L1: Language Comprehension (minor), Language Production (minor)
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory
L3: ",L2
FinanceAgent,https://www.vals.ai/benchmarks/finance_agent,,"FinanceAgent assesses agentic performance on tasks resembling entry-level financial analyst work, such as analyzing documents, performing computations, and producing structured deliverables. It emphasizes multi-step reasoning with domain constraints, numerical consistency, and the ability to operationalize instructions into analysis workflows.","L1: Language Production (minor)
L2: Logical Reasoning, Decision-making, Planning, Semantic Understanding & Context Recognition, Working Memory, Adaptive Error Correction (minor)
L3: ",L2
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity capabilities in realistic settings, including identifying known vulnerabilities from descriptions and discovering new issues in open-source projects. It stresses technical reasoning over codebases, iterative hypothesis testing, and careful validation of exploitability or bug presence.","L1: 
L2: Logical Reasoning, Planning, Adaptive Error Correction, Semantic Understanding & Context Recognition, Working Memory
L3: Inhibitory Control (minor)",L2
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"SpreadsheetBench measures an agent’s ability to navigate, edit, and compute within complex spreadsheets that mirror real-world business artifacts. It stresses structured data manipulation, formula/logic consistency, and executing multi-step transformations while maintaining global sheet integrity.","L1: Visual Perception (minor)
L2: Planning, Working Memory, Logical Reasoning, Adaptive Error Correction, Attention (minor)
L3: ",L2
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a frontier-level benchmark spanning many difficult questions intended to probe broad academic knowledge and reasoning, including multimodal items. It emphasizes synthesizing evidence, handling unfamiliar problems, and avoiding overconfident hallucinations when information is uncertain or incomplete.","L1: Language Comprehension, Visual Perception (minor)
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory (minor)
L3: Inhibitory Control (minor)",L2
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,AIME 2025 evaluates mathematical problem solving on competition-style questions that typically require multi-step derivations and careful symbolic manipulation. It emphasizes correctness under tight constraints and rewards structured reasoning rather than relying on external knowledge retrieval.,"L1: 
L2: Logical Reasoning, Working Memory, Planning (minor), Attention (minor)
L3: ",L2
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a high-difficulty multiple-choice benchmark of graduate-level science questions designed to be resistant to simple web search and superficial pattern matching. The “Diamond” subset targets questions where experts succeed and non-experts commonly fail, emphasizing deep understanding and precise reasoning.","L1: 
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory (minor)
L3: Inhibitory Control (minor)",L2
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends the MMLU-style format to multiple languages, testing knowledge and reasoning across many subjects and non-English settings. It probes whether models can generalize their competence across linguistic contexts, not only in English.","L1: Language Comprehension, Language Production (minor)
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory (minor)
L3: ",L2
MMMU,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2311.16502,"MMMU evaluates expert-level multimodal understanding across diverse academic domains, requiring models to answer questions grounded in images (e.g., diagrams, charts, scientific figures) and text. It stresses integration of visual evidence with domain reasoning and careful interpretation of fine-grained visual details.","L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Attention (minor)
L3: ",L2
LAB-Bench FigQA,https://huggingface.co/datasets/futurehouse/lab-bench,https://arxiv.org/abs/2407.10362,"LAB-Bench FigQA tests whether models can correctly read and reason over complex biological research figures, including plots, multi-panel diagrams, and experimental schematics. It emphasizes extracting the right variables/relationships from the figure and combining them with domain knowledge to answer targeted questions.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Semantic Understanding & Context Recognition, Logical Reasoning, Attention
L3: ",L2
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning evaluates reasoning over scientific charts and figures derived from arXiv-style papers, typically requiring quantitative and relational interpretation rather than generic caption summarization. It stresses robust extraction of plotted trends/axes/legends and translating them into correct answers.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Logical Reasoning, Semantic Understanding & Context Recognition (minor), Attention (minor)
L3: ",L2
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU extends multimodal understanding to video, requiring models to integrate information across time to answer questions about events, actions, and context. It stresses temporal aggregation, maintaining coherent representations across frames, and (when present) aligning audio/text cues with visual evidence.","L1: Visual Perception, Auditory Processing (minor)
L2: Scene Understanding & Visual Reasoning, Working Memory, Attention (minor), Multisensory Integration (minor)
L3: Cognitive Timing & Predictive Modeling",L3
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"The FACTS Benchmark Suite systematically evaluates factuality-related behaviors in language models, including whether outputs remain grounded in provided sources and whether models avoid introducing unsupported claims. It targets reliability failures such as hallucination, misattribution, and overconfident fabrication across varied prompting setups.","L1: 
L2: Semantic Understanding & Context Recognition, Logical Reasoning (minor), Attention (minor)
L3: Inhibitory Control, Self-reflection (minor)",L3
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA evaluates physical commonsense reasoning across languages and cultural contexts, typically using paired-choice questions that require selecting the more plausible physical interaction outcome. It stresses robust commonsense generalization and sensitivity to linguistic and contextual variation.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Spatial Representation & Mapping (minor), Working Memory (minor)
L3: ",L2
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) is a long-context evaluation where multiple similar “needle” requests and responses are embedded within large “haystack” transcripts, and the model must retrieve and reproduce the correct response corresponding to a specified needle. It stresses accurate multi-round co-reference resolution and resistance to interference from near-duplicate contexts.","L1: Language Comprehension
L2: Working Memory, Attention, Episodic Memory
L3: Inhibitory Control (minor)",L2
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval evaluates economically meaningful, well-specified professional tasks (e.g., building spreadsheets, creating presentations, drafting operational artifacts) across many occupations, using expert human judging for quality comparisons. It emphasizes end-to-end task execution quality under real deliverable constraints (format, correctness, completeness) rather than only answering questions.","L1: Language Production
L2: Planning, Decision-making, Semantic Understanding & Context Recognition, Adaptive Error Correction (minor), Working Memory (minor)
L3: Social Reasoning & Theory of Mind (minor)",L2
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks tests whether models can follow algorithmic graph-traversal instructions and maintain consistency over many steps (e.g., BFS-style expansions or parent-pointer reasoning) when the graph structure is specified in text. It stresses systematic multi-hop reasoning and robustness to compounding errors over long chains.","L1: 
L2: Logical Reasoning, Working Memory, Spatial Representation & Mapping, Attention (minor), Adaptive Error Correction (minor)
L3: ",L2
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon evaluates general tool-use competence across heterogeneous APIs and tasks, emphasizing correct tool selection, parameterization, and multi-step orchestration to reach a final answer. It stresses reliable agentic execution under constraints where tool outputs must be interpreted and integrated, often with intermediate failures or ambiguities.","L1: Language Comprehension (minor), Language Production (minor)
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory
L3: ",L2
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath evaluates advanced, expert-level mathematics, including problems intended to resist superficial pattern matching and require substantial derivations. It stresses deep multi-step reasoning, precise symbolic manipulation, and maintaining correctness across long proofs or calculations (often improved by verified computation when tools are allowed).","L1: 
L2: Logical Reasoning, Working Memory, Planning (minor), Attention (minor)
L3: Cognitive Flexibility (minor)",L2
