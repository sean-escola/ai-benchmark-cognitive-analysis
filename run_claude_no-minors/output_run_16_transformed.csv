Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates an agent’s ability to solve real software engineering issues in open-source repositories by generating a correct code patch that passes tests. The “Verified” set focuses on tasks that are confirmed solvable and scored end-to-end via automated checks, emphasizing practical debugging and implementation under realistic constraints.","L1: Language Comprehension (minor), Language Production (minor)
L2: Planning, Logical Reasoning, Adaptive Error Correction, Working Memory
L3: ",L2
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 measures how well an agent completes real command-line tasks (e.g., building, configuring, debugging, or data processing) by issuing sequences of terminal commands. It stresses iterative interaction with a live environment, where mistakes require recovery and tool feedback must be interpreted correctly.","L1: Language Comprehension (minor)
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory, Attention (minor)
L3: ",L2
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp evaluates “deep research” performance where a model must search a fixed web corpus (or controlled index) and synthesize an answer grounded in retrieved documents. The benchmark emphasizes query formulation, evidence tracking, and correctly integrating information across multiple sources under time and context limits.","L1: Language Comprehension, Language Production (minor)
L2: Planning, Decision-making, Semantic Understanding & Context Recognition, Episodic Memory (minor), Working Memory
L3: ",L2
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench evaluates interactive, tool-using agents in customer-support style environments (e.g., retail, airline, telecom) with policies and APIs. Success requires correct multi-turn dialogue management, adherence to constraints, and reliable execution of actions via tools while keeping the user’s goal in view.","L1: Language Comprehension, Language Production
L2: Decision-making, Planning, Working Memory
L3: Social Reasoning & Theory of Mind, Inhibitory Control, Empathy (minor)",L3
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld benchmarks multimodal “computer use” where an agent completes tasks by operating within an OS-like graphical environment. It tests perception of UI state (screenshots), action selection (click/type/navigation), and long-horizon task completion across many steps with frequent error recovery.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements (minor), Planning, Decision-making, Sensorimotor Coordination, Adaptive Error Correction, Working Memory
L3: ",L2
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI tests fluid reasoning on novel grid-based puzzles where the model must infer a transformation rule from a few examples and apply it to a new input. It is designed to reduce reliance on memorized knowledge and instead emphasize abstraction, compositional pattern discovery, and generalization.","L1: Visual Perception (minor)
L2: Logical Reasoning, Spatial Representation & Mapping, Working Memory, Scene Understanding & Visual Reasoning (minor)
L3: Cognitive Flexibility",L3
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon autonomous management in a simulated vending-machine business over many decisions (e.g., procurement, pricing, inventory, negotiation). It emphasizes sustained coherence, strategic planning under uncertainty, and adapting policies based on outcomes and changing conditions.","L1: 
L2: Planning, Decision-making, Reward Mechanisms, Working Memory, Episodic Memory (minor)
L3: Cognitive Timing & Predictive Modeling, Social Reasoning & Theory of Mind (minor)",L3
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas evaluates tool use via the Model Context Protocol, focusing on whether a model can discover tools, call them correctly, handle errors, and compose multi-step workflows across real APIs. It measures practical agent reliability: schema understanding, sequencing calls, and synthesizing outputs into correct final answers.","L1: Language Comprehension (minor), Language Production (minor)
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition
L3: ",L2
FinanceAgent,https://www.vals.ai/benchmarks/finance_agent,,"FinanceAgent assesses tasks expected of an entry-level financial analyst, such as reading financial documents, building analyses, and producing decision-support outputs. It stresses structured reasoning with domain constraints, correct use of quantitative/financial concepts, and producing professional, auditable deliverables.","L1: Language Production (minor)
L2: Logical Reasoning, Decision-making, Planning (minor), Working Memory, Semantic Understanding & Context Recognition
L3: ",L2
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity agent capability on tasks such as identifying known vulnerabilities in real projects and (in some settings) discovering new ones. It emphasizes careful code and system reasoning, hypothesis-driven debugging, and safely iterating based on tool feedback and test outcomes.","L1: Language Comprehension (minor)
L2: Logical Reasoning, Planning, Adaptive Error Correction, Working Memory, Attention (minor)
L3: ",L2
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"SpreadsheetBench measures an agent’s ability to understand, manipulate, and compute with complex spreadsheets using realistic tasks and artifacts. It stresses structured transformation of tabular data, formula reasoning, and careful multi-step editing where small mistakes can cascade.","L1: 
L2: Logical Reasoning, Planning, Working Memory, Adaptive Error Correction, Attention (minor), Sensorimotor Coordination (minor)
L3: ",L2
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam (HLE) is a large, difficult multimodal benchmark spanning expert-level questions across many subjects, intended to probe frontier reasoning beyond routine test prep. Depending on the setup, it may allow tools like web search or code execution, emphasizing both deep understanding and reliable synthesis.","L1: Language Comprehension, Visual Perception (minor)
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Scene Understanding & Visual Reasoning (minor), Planning (minor)
L3: ",L2
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 is a competition-math benchmark based on the American Invitational Mathematics Examination, featuring problems that require multi-step symbolic reasoning and careful constraint handling. It primarily tests deductive problem solving under tight correctness requirements, with little reliance on external knowledge.","L1: 
L2: Logical Reasoning, Working Memory, Planning, Adaptive Error Correction (minor)
L3: ",L2
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,GPQA Diamond is a high-quality subset of graduate-level science multiple-choice questions designed to be difficult for non-experts and resistant to shallow pattern matching. It probes deep scientific reasoning and precise discrimination among closely related answer options.,"L1: Language Comprehension (minor)
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory (minor), Decision-making (minor)
L3: ",L2
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends broad academic knowledge testing across many subjects into multiple languages, evaluating both knowledge and reasoning under multilingual prompts. It probes whether competence transfers across languages and whether models maintain consistent conceptual understanding under translation and linguistic variation.","L1: Language Comprehension, Language Production (minor)
L2: Semantic Understanding & Context Recognition, Logical Reasoning (minor), Working Memory (minor)
L3: ",L2
MMMU,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2311.16502,"MMMU evaluates multimodal understanding across many disciplines, requiring models to combine text with images such as charts, diagrams, or figures to answer questions. It stresses visual grounding, cross-modal integration, and reasoning from structured visual evidence rather than purely textual cues.","L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning (minor), Visual Attention & Eye Movements (minor)
L3: ",L2
LAB-Bench FigQA,https://huggingface.co/datasets/futurehouse/lab-bench,https://arxiv.org/abs/2407.10362,"LAB-Bench FigQA tests whether models can accurately interpret complex scientific figures from biology papers, including plots, multi-panel layouts, and annotated diagrams. It emphasizes extracting evidence from visuals, mapping it to the question, and performing domain-relevant reasoning grounded in the figure.","L1: Visual Perception, Language Comprehension (minor)
L2: Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements (minor), Logical Reasoning, Semantic Understanding & Context Recognition (minor)
L3: ",L2
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning evaluates reasoning over scientific charts and figures drawn from arXiv-style documents, often requiring quantitative interpretation and linking captions/axes to conclusions. The benchmark stresses robust chart reading, multi-step inference, and reducing hallucinations by grounding in visual evidence.","L1: Visual Perception, Language Comprehension (minor)
L2: Scene Understanding & Visual Reasoning, Logical Reasoning, Visual Attention & Eye Movements (minor), Working Memory (minor)
L3: ",L2
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU extends multimodal understanding to video, requiring models to answer questions that depend on events unfolding over time rather than a single static image. It probes temporal integration, attention over long sequences, and reasoning that connects earlier and later visual evidence.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Working Memory, Attention, Multisensory Integration (minor)
L3: Cognitive Timing & Predictive Modeling (minor)",L2
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,The FACTS Benchmark Suite systematically evaluates factuality by testing whether model outputs are supported by provided sources and whether the model avoids unsupported claims across diverse settings. It emphasizes faithfulness/grounding and reliability rather than raw task completion alone.,"L1: Language Production
L2: Semantic Understanding & Context Recognition, Working Memory (minor)
L3: Inhibitory Control, Self-reflection (minor)",L3
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA evaluates physical commonsense reasoning across languages and cultural contexts, focusing on whether models can choose plausible actions or explanations in everyday physical scenarios. It probes robustness of commonsense inference beyond a single language or region-specific phrasing.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning
L3: Social Reasoning & Theory of Mind (minor)",L2
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) is a long-context benchmark where multiple similar “needle” requests are embedded within a large “haystack” dialogue or document, and the model must retrieve the correct referenced response. It tests whether models can maintain and use long-range dependencies without confusion among near-duplicates.","L1: Language Comprehension
L2: Working Memory, Attention, Episodic Memory (minor), Semantic Understanding & Context Recognition (minor)
L3: ",L2
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval evaluates well-specified professional knowledge-work tasks across many occupations, judged by expert humans based on the quality of produced work artifacts (e.g., spreadsheets, presentations, plans). It emphasizes end-to-end execution quality, adherence to constraints, and practical decision support rather than short-form answers.","L1: Language Production
L2: Planning, Decision-making, Semantic Understanding & Context Recognition, Adaptive Error Correction (minor), Working Memory (minor)
L3: Social Reasoning & Theory of Mind (minor)",L2
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks tests multi-step reasoning over graph-structured problems presented in textual form, such as following edges, tracking paths, or answering reachability/parent queries. It probes whether models can reliably simulate discrete graph traversal and maintain intermediate state across many hops.","L1: 
L2: Logical Reasoning, Working Memory, Spatial Representation & Mapping, Attention (minor), Planning (minor)
L3: ",L2
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon evaluates complex tool-using workflows where a model must select among tools, call them correctly, recover from failures, and integrate results into a coherent final output. It emphasizes robustness in multi-step orchestration, including correct parameterization, sequencing, and verification.","L1: Language Comprehension (minor)
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition (minor)
L3: ",L2
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath is a challenging mathematics benchmark designed to test advanced problem solving beyond standard contest math, with tiers spanning increasing difficulty. It probes sustained multi-step reasoning, novel strategy construction, and error checking under high brittleness where partial progress is not sufficient.","L1: 
L2: Logical Reasoning, Working Memory, Planning, Adaptive Error Correction
L3: Cognitive Flexibility",L3
