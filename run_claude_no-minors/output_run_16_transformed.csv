Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,SWE-bench Verified evaluates software engineering agents on real GitHub issues where the model must generate a code patch that passes the repository’s tests. The “Verified” subset focuses on tasks that have been confirmed solvable and are graded by whether the produced patch resolves the issue under the benchmark harness.,"L1: Language Comprehension, Language Production (minor)
L2: Planning, Logical Reasoning, Working Memory, Adaptive Error Correction
L3: ",L2
SWE-bench Multilingual,https://www.swebench.com/multilingual.html,https://arxiv.org/abs/2504.21798,"SWE-bench Multilingual extends SWE-bench-style issue-to-patch evaluation beyond Python, covering multiple programming languages and ecosystems. It tests whether models can understand issues, navigate unfamiliar codebases, and implement fixes across diverse toolchains and language conventions.","L1: Language Comprehension
L2: Planning, Logical Reasoning, Working Memory, Adaptive Error Correction, Semantic Understanding & Context Recognition
L3: Cognitive Flexibility",L3
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro is a harder, more contamination-resistant software engineering benchmark with a larger and more industrially realistic task set. Models must produce correct multi-file patches in real repositories, often requiring deeper repo understanding, dependency reasoning, and more robust debugging than standard SWE-bench tasks.","L1: Language Comprehension (minor)
L2: Planning, Decision-making, Logical Reasoning, Working Memory, Adaptive Error Correction, Semantic Understanding & Context Recognition
L3: ",L2
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 evaluates agentic performance on real tasks in a command-line environment, such as debugging, building, configuring software, and manipulating files via shell commands. Success depends on choosing effective command sequences, interpreting tool outputs, and iterating when errors occur under resource and time constraints.","L1: Language Comprehension (minor)
L2: Planning, Decision-making, Working Memory, Adaptive Error Correction, Logical Reasoning, Attention (minor)
L3: ",L2
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp evaluates deep-research style agents that must browse and synthesize information from a controlled document index to answer complex questions reproducibly. It emphasizes search strategy, evidence aggregation, and faithful synthesis from retrieved sources rather than open-web variability.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Working Memory, Semantic Understanding & Context Recognition
L3: Self-reflection (minor)",L2
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench measures how well agents complete multi-turn customer-support tasks by combining natural language interaction with API/tool use while following domain policies (e.g., retail, airline, telecom). It stresses consistent policy adherence, state tracking across turns, and pragmatic tool decisions under conversational pressure.","L1: Language Comprehension, Language Production
L2: Decision-making, Planning, Working Memory
L3: Social Reasoning & Theory of Mind, Inhibitory Control, Empathy (minor)",L3
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld benchmarks multimodal “computer use” agents operating in realistic desktop environments, requiring navigation across GUIs to complete tasks in applications and browsers. It tests whether an agent can perceive screen state, decide next interactions, and recover from mistakes over long action sequences.","L1: Visual Perception
L2: Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Sensorimotor Coordination, Planning, Decision-making, Working Memory, Adaptive Error Correction
L3: ",L2
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI is a fluid-intelligence benchmark based on abstract grid transformation puzzles with only a few demonstrations per task. Models must infer latent rules and generalize them to new inputs, emphasizing novel pattern discovery rather than memorized domain knowledge.","L1: Visual Perception (minor)
L2: Scene Understanding & Visual Reasoning, Spatial Representation & Mapping, Logical Reasoning, Working Memory, Attention
L3: Cognitive Flexibility",L3
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon autonomy by having an agent run a simulated vending-machine business over many steps, making pricing, purchasing, and negotiation decisions. Performance is measured by final business outcomes, requiring sustained coherence, strategic planning, and adaptation to changing conditions.","L1: Language Production (minor)
L2: Planning, Decision-making, Reward Mechanisms, Working Memory, Episodic Memory (minor)
L3: Motivational Drives (minor), Social Reasoning & Theory of Mind (minor)",L2
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas evaluates real-world tool use via the Model Context Protocol (MCP), where agents must discover tools, call them correctly, handle errors, and compose multi-step workflows across services. It emphasizes reliable orchestration across multiple tool calls and robust recovery when tool outputs are imperfect.","L1: Language Comprehension, Language Production (minor)
L2: Planning, Decision-making, Working Memory, Adaptive Error Correction
L3: Inhibitory Control (minor)",L2
FinanceAgent,https://www.vals.ai/benchmarks/finance_agent,,"FinanceAgent measures performance on tasks representative of an entry-level financial analyst, such as analysis, modeling, and report-style reasoning over financial documents and constraints. It stresses numerate reasoning, correct application of finance concepts, and producing actionable outputs under professional standards.","L1: Language Production (minor)
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Decision-making, Planning (minor)
L3: ",L2
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity agents on identifying known vulnerabilities and discovering new ones in real open-source software given task descriptions and codebases. It probes systematic investigation, exploit-relevant reasoning, and iterative debugging under realistic constraints.","L1: 
L2: Logical Reasoning, Planning, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition
L3: Inhibitory Control (minor)",L2
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"SpreadsheetBench tests an agent’s ability to understand, edit, and compute within complex spreadsheets derived from real-world scenarios. Tasks often require locating relevant cells, applying correct formulas or transformations, and maintaining consistency across multiple sheets and constraints.","L1: Visual Perception (minor)
L2: Working Memory, Logical Reasoning, Planning, Adaptive Error Correction, Semantic Understanding & Context Recognition, Scene Understanding & Visual Reasoning (minor)
L3: ",L2
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam (HLE) is a large, frontier-level benchmark spanning many domains (and often multiple modalities) intended to probe advanced reasoning and breadth of knowledge. Questions are designed to be difficult for models without careful multi-step reasoning and, in tool-enabled settings, effective use of search and computation.","L1: Language Comprehension, Visual Perception (minor)
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Planning (minor), Multisensory Integration (minor)
L3: ",L2
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,AIME 2025 consists of competition-style math problems that require precise symbolic manipulation and multi-step derivations under time-like constraints. It primarily measures mathematical reasoning and the ability to maintain correctness across long chains of intermediate steps.,"L1: 
L2: Logical Reasoning, Working Memory, Planning, Attention (minor), Adaptive Error Correction (minor)
L3: ",L2
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,GPQA Diamond is a high-quality subset of GPQA containing extremely challenging graduate-level science multiple-choice questions intended to be “Google-proof.” It emphasizes deep conceptual understanding and careful reasoning rather than shallow recall or pattern matching.,"L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Decision-making (minor)
L3: ",L2
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends broad academic knowledge evaluation to many languages, testing whether models can answer subject-matter questions across disciplines in multilingual settings. It probes cross-lingual robustness, instruction following in non-English languages, and domain knowledge expressed through varied linguistic forms.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning (minor), Working Memory (minor)
L3: Cognitive Flexibility",L3
MMMU,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2311.16502,"MMMU is a multimodal benchmark requiring reasoning over images and text across many professional and academic topics. Tasks often involve interpreting diagrams, charts, tables, and screenshots and combining visual evidence with language instructions to select or generate correct answers.","L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Working Memory (minor), Attention (minor)
L3: ",L2
LAB-Bench FigQA,https://huggingface.co/datasets/futurehouse/lab-bench,https://arxiv.org/abs/2407.10362,"LAB-Bench FigQA evaluates whether models can correctly interpret complex scientific figures from biology papers, including reading axes, legends, and experimental relationships. It stresses figure-grounded reasoning and extracting precise claims from visual evidence rather than relying on general biology knowledge alone.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Attention, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory (minor), Multisensory Integration (minor)
L3: ",L2
WebArena,https://webarena.dev/,https://arxiv.org/abs/2307.13854,"WebArena benchmarks autonomous web agents that must complete realistic tasks across multiple web apps (e-commerce, CMS, forums, collaboration tools) through multi-step navigation and interaction. Agents need to interpret changing web states, plan action sequences, and recover from partial failures while satisfying task objectives.","L1: Visual Perception, Language Comprehension (minor), Language Production (minor)
L2: Planning, Decision-making, Working Memory, Adaptive Error Correction, Visual Attention & Eye Movements, Sensorimotor Coordination
L3: ",L2
