Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates software-engineering agents on real GitHub issues by requiring them to produce a patch that makes an existing test suite pass. The Verified subset focuses on tasks that have been manually checked to be solvable and correctly specified, emphasizing end-to-end debugging, code editing, and iterative test-driven fixing.","L1: Language Comprehension (minor), Language Production (minor)
L2: Planning, Logical Reasoning, Adaptive Error Correction, Working Memory
L3: ",L2
SWE-bench Multilingual,https://www.swebench.com/multilingual.html,https://arxiv.org/abs/2504.21798,"SWE-bench Multilingual extends the SWE-bench paradigm beyond Python to multiple programming languages, testing whether agents can understand unfamiliar codebases and apply language-specific fixes. It stresses generalization of debugging and patch generation across different ecosystems, tooling conventions, and build/test behaviors.","L1: Language Comprehension (minor)
L2: Planning, Logical Reasoning, Adaptive Error Correction, Working Memory
L3: Cognitive Flexibility",L3
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro is a larger, more challenging software-engineering benchmark designed to be more contamination-resistant and industrially realistic. Agents must resolve complex repository issues that often require deeper investigation, longer dependency chains, and careful validation of fixes under constraints.","L1: Language Comprehension (minor)
L2: Planning, Logical Reasoning, Adaptive Error Correction, Working Memory
L3: Inhibitory Control (minor)",L2
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 measures agent performance on real tasks performed through a command-line interface, such as installing dependencies, running programs, inspecting logs, and editing files. Success typically requires choosing correct commands, interpreting tool output, and recovering from errors in a closed-loop workflow.","L1: Language Comprehension (minor)
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory, Attention (minor)
L3: ",L2
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp evaluates research-style browsing agents by requiring them to answer questions using information found across many web documents under constrained search/browse interactions. It tests multi-step retrieval, evidence synthesis, and the ability to manage context as the agent gathers and filters sources.","L1: Language Production (minor)
L2: Planning, Attention, Episodic Memory, Semantic Understanding & Context Recognition, Decision-making (minor)
L3: ",L2
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench evaluates interactive customer-support agents that must follow domain policies while using tools/APIs and conversing over multiple turns with a simulated user. It emphasizes policy-consistent decision making, robust tool calling, and handling long, branching dialogues in realistic service settings.","L1: Language Comprehension, Language Production
L2: Decision-making, Planning
L3: Inhibitory Control, Social Reasoning & Theory of Mind, Empathy (minor)",L3
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld benchmarks multimodal computer-use agents that must complete tasks by operating a real or realistic desktop environment (e.g., navigating apps, clicking UI elements, and entering text). It probes grounded perception-to-action loops: understanding screenshots, tracking state across steps, and executing reliable sequences of UI actions.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Planning, Decision-making, Visual Attention & Eye Movements (minor), Sensorimotor Coordination (minor), Working Memory (minor)
L3: ",L2
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI is a few-shot abstract reasoning benchmark where models infer latent rules from a small number of grid-based input–output examples and produce the correct output for a new input. It is designed to reduce reliance on memorized knowledge and instead measure pattern induction, compositional generalization, and rule transfer.","L1: 
L2: Logical Reasoning, Spatial Representation & Mapping, Working Memory, Attention (minor)
L3: Cognitive Flexibility, Cognitive Timing & Predictive Modeling (minor)",L3
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon agent coherence and strategy in a simulated year-long vending-machine business, tracking performance over many sequential decisions. It requires planning under uncertainty, adapting to market dynamics, and integrating information from ongoing interactions such as procurement and pricing.","L1: 
L2: Planning, Decision-making, Reward Mechanisms, Episodic Memory, Semantic Understanding & Context Recognition (minor)
L3: Motivational Drives (minor), Social Reasoning & Theory of Mind (minor)",L2
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas measures real-world tool use through the Model Context Protocol, where agents must discover and invoke appropriate tools across multi-step workflows and integrate results into correct final outputs. The benchmark emphasizes API reasoning, error handling, and orchestration across heterogeneous services.","L1: Language Comprehension (minor), Language Production (minor)
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory
L3: ",L2
FinanceAgent,https://www.vals.ai/benchmarks/finance_agent,,"FinanceAgent evaluates performance on tasks expected of an entry-level financial analyst, such as interpreting documents, extracting figures, building analyses, and producing written recommendations. It stresses quantitative reasoning, structured reporting, and the ability to follow domain conventions and constraints.","L1: Language Production (minor)
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Decision-making, Planning (minor), Working Memory (minor)
L3: ",L2
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity agents on finding known vulnerabilities and discovering new ones in real open-source software, typically requiring code inspection, exploitation reasoning, and patch/mitigation thinking. It emphasizes realistic workflows: interpreting vulnerability descriptions, locating relevant code paths, and validating hypotheses with tooling.","L1: 
L2: Logical Reasoning, Planning, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition (minor)
L3: Inhibitory Control (minor)",L2
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"SpreadsheetBench tests an agent’s ability to understand, manipulate, and generate complex spreadsheets drawn from real-world-like scenarios. Tasks commonly require multi-step transformations, formula reasoning, and careful bookkeeping to maintain consistency across many interdependent cells.","L1: Language Comprehension (minor)
L2: Working Memory, Logical Reasoning, Planning, Adaptive Error Correction, Attention (minor)
L3: ",L2
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a frontier, multi-modal benchmark spanning many domains, intended to probe expert-level reasoning and knowledge synthesis under challenging question distributions. It typically rewards careful interpretation of prompts, multi-step inference, and (when enabled) effective use of external tools like search or code to verify claims.","L1: Language Comprehension, Visual Perception (minor)
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory (minor), Planning (minor)
L3: ",L2
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 evaluates competition-style mathematics problem solving under time-independent conditions, with problems that typically require multi-step derivations and clever insights. It stresses symbolic reasoning, maintaining intermediate constraints, and avoiding small algebraic or combinatorial mistakes.","L1: 
L2: Logical Reasoning, Working Memory, Attention (minor)
L3: Cognitive Flexibility (minor)",L2
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a high-quality subset of graduate-level multiple-choice questions in biology, chemistry, and physics designed to be difficult for non-experts and resistant to shallow retrieval. It tests deep scientific reasoning and the ability to discriminate between closely related answer options based on technical details.","L1: Language Comprehension (minor)
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory (minor)
L3: ",L2
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends broad academic knowledge testing into many languages, covering dozens of subjects with a consistent multiple-choice format. It primarily assesses multilingual comprehension and transfer of learned concepts across linguistic contexts, with a secondary emphasis on basic reasoning within each subject area.","L1: Language Comprehension, Language Production (minor)
L2: Semantic Understanding & Context Recognition, Logical Reasoning (minor)
L3: ",L2
MMMU,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2311.16502,"MMMU is a multimodal benchmark spanning many disciplines where models must answer questions requiring joint understanding of images (e.g., diagrams, plots, screenshots) and text. It probes visual reasoning grounded in domain knowledge and the ability to integrate cross-modal evidence into a single coherent decision.","L1: Visual Perception
L2: Multisensory Integration, Scene Understanding & Visual Reasoning, Logical Reasoning, Attention (minor)
L3: ",L2
LAB-Bench FigQA,https://huggingface.co/datasets/futurehouse/lab-bench,https://arxiv.org/abs/2407.10362,"LAB-Bench FigQA focuses on scientific-figure question answering, requiring models to interpret complex figures from biology papers and answer targeted reasoning questions. It stresses extracting quantitative/structural information from visuals, aligning it with accompanying text, and drawing correct inferences under domain-specific conventions.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Attention, Multisensory Integration (minor), Logical Reasoning (minor), Semantic Understanding & Context Recognition (minor)
L3: ",L2
WebArena,https://webarena.dev/,https://arxiv.org/abs/2307.13854,"WebArena evaluates autonomous web agents on realistic tasks across multiple websites (e-commerce, CMS, forums, developer tools), requiring navigation, form filling, and multi-step task completion. It tests long-horizon action selection, handling of dynamic interfaces, and recovery from partial failures during web interaction.","L1: Visual Perception (minor)
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory, Visual Attention & Eye Movements (minor), Sensorimotor Coordination (minor)
L3: ",L2
