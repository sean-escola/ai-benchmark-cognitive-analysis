Benchmark,Website,Paper,Description,Cognitive Functions
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,SWE-bench Verified evaluates LLM-based coding agents on real GitHub issues by requiring them to generate patches that make a repository’s tests pass. The “Verified” subset uses tasks that have been human-validated as solvable and includes stricter evaluation and filtering to reduce ambiguous cases.,"Planning, Logical Reasoning, Adaptive Error Correction, Working Memory (minor), Language Comprehension (minor)"
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 tests agentic performance in command-line environments, where models must complete real tasks by issuing shell commands, manipulating files, and inspecting outputs. It emphasizes robust iterative interaction with a live environment rather than one-shot code generation.","Planning, Decision-making, Adaptive Error Correction, Working Memory (minor), Language Comprehension (minor)"
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp measures an agent’s ability to answer complex questions by browsing and synthesizing information from the web (or a controlled corpus), typically requiring multi-step retrieval and verification. It evaluates whether agents can decompose research problems, gather evidence, and produce a justified final answer.","Planning, Attention, Semantic Understanding & Context Recognition, Episodic Memory (minor), Decision-making (minor), Language Production (minor)"
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench evaluates interactive, policy-constrained agent behavior in simulated customer service domains (e.g., retail, airline, telecom) where agents use tools/APIs while conversing with a user simulator. It probes policy adherence, multi-turn state tracking, and robust task completion under constraints.","Social Reasoning & Theory of Mind, Inhibitory Control, Decision-making, Planning (minor), Language Comprehension (minor), Language Production (minor)"
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld benchmarks multimodal “computer use” agents that must complete tasks in realistic operating system environments using screenshots and UI interactions under step limits. It stresses perception-to-action grounding: identifying UI elements, navigating apps, and executing correct sequences of actions.","Visual Perception, Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements (minor), Planning, Decision-making, Sensorimotor Coordination (minor), Adaptive Error Correction (minor)"
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI is a few-shot abstract reasoning benchmark where models infer transformation rules from small sets of input-output grid examples and apply them to new inputs. It is designed to emphasize generalization to novel concepts over memorization, often requiring compositional rule induction.","Cognitive Flexibility, Logical Reasoning, Working Memory, Spatial Representation & Mapping, Visual Perception (minor), Cognitive Timing & Predictive Modeling (minor)"
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon agent coherence and strategy in a simulated vending-machine business over many decisions (e.g., inventory, pricing, supplier negotiation). Success requires maintaining goals over time, adapting to feedback, and optimizing outcomes in a dynamic environment.","Planning, Decision-making, Reward Mechanisms, Working Memory (minor), Episodic Memory (minor), Cognitive Timing & Predictive Modeling (minor)"
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas assesses real-world tool use through the Model Context Protocol by requiring models to discover tools, invoke them correctly, handle errors, and compose multi-step workflows across services. It emphasizes reliable API interaction and orchestration rather than purely linguistic answers.","Planning, Decision-making, Adaptive Error Correction, Working Memory (minor), Language Comprehension (minor), Language Production (minor)"
FinanceAgent,https://www.vals.ai/benchmarks/finance_agent,,"FinanceAgent measures performance on tasks representative of an entry-level financial analyst, such as financial modeling, document-driven analysis, and synthesis of investment-relevant insights. It typically stresses quantitative reasoning, domain knowledge, and producing structured professional outputs.","Logical Reasoning, Decision-making, Planning (minor), Semantic Understanding & Context Recognition, Working Memory (minor), Language Production (minor)"
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity capabilities by testing agents on identifying known vulnerabilities and discovering new ones in real open-source projects, often from high-level descriptions. It emphasizes precise technical reasoning, iterative investigation, and correct exploitation/patch understanding within constraints.","Logical Reasoning, Planning, Adaptive Error Correction, Decision-making (minor), Inhibitory Control (minor), Working Memory (minor)"
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"SpreadsheetBench evaluates an agent’s ability to understand, edit, and compute over complex spreadsheets, including transforming data, fixing formulas, and producing correct outputs. It targets end-to-end productivity workflows where correctness depends on careful state tracking and structured manipulation.","Working Memory, Logical Reasoning, Planning, Decision-making (minor), Adaptive Error Correction (minor)"
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam (HLE) is a difficult multimodal benchmark intended to probe frontier knowledge and reasoning across many domains with expert-level questions. It often requires integrating text with figures or other modalities and producing precise, justified answers under uncertainty.","Language Comprehension, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory (minor), Visual Perception (minor), Scene Understanding & Visual Reasoning (minor)"
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 evaluates competition-style mathematics problem solving, typically requiring multi-step derivations and careful symbolic manipulation. Scores reflect exact-answer accuracy on challenging short-form problems rather than tool-based computation alone.","Logical Reasoning, Working Memory, Planning (minor), Adaptive Error Correction (minor)"
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,GPQA Diamond is a high-quality subset of graduate-level science multiple-choice questions designed to resist simple web search and reward deep understanding. It probes scientific reasoning and the ability to discriminate between plausible distractors.,"Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory (minor), Decision-making (minor)"
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends MMLU to multiple languages, testing knowledge and reasoning across many academic subjects with multilingual prompts and answers. It highlights cross-lingual generalization and robustness of conceptual understanding under translation and cultural variation.","Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning (minor), Working Memory (minor)"
MMMU,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2311.16502,"MMMU is a broad multimodal benchmark spanning many disciplines where models answer questions requiring both textual understanding and image-based reasoning (e.g., diagrams, charts, screenshots). It emphasizes grounded multimodal inference and compositional reasoning over heterogeneous inputs.","Visual Perception, Scene Understanding & Visual Reasoning, Multisensory Integration, Language Comprehension, Logical Reasoning (minor), Working Memory (minor)"
LAB-Bench FigQA,https://huggingface.co/datasets/futurehouse/lab-bench,https://arxiv.org/abs/2407.10362,"LAB-Bench FigQA evaluates whether models can interpret scientific figures from biology papers (plots, schematics, multi-panel figures) and answer questions that require careful visual and contextual reading. It targets practical research skills such as extracting quantitative/qualitative claims from figures.","Scene Understanding & Visual Reasoning, Visual Perception, Visual Attention & Eye Movements (minor), Semantic Understanding & Context Recognition, Logical Reasoning (minor), Working Memory (minor)"
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning focuses on reasoning over charts and figures from scientific documents, requiring models to read axes/legends, interpret trends, and answer questions grounded in the visual evidence. It stresses chart literacy and robustness to dense scientific visual formats.","Scene Understanding & Visual Reasoning, Visual Perception, Semantic Understanding & Context Recognition, Logical Reasoning (minor), Working Memory (minor)"
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU extends multimodal reasoning to video, requiring models to integrate information across frames and time to answer questions about events, procedures, or scene changes. It probes temporal grounding and the ability to maintain and update hypotheses as new visual evidence appears.","Visual Perception, Cognitive Timing & Predictive Modeling, Attention, Working Memory, Scene Understanding & Visual Reasoning (minor), Multisensory Integration (minor)"
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,The FACTS Benchmark Suite systematically evaluates factuality by testing whether model outputs remain consistent with available evidence and avoid unsupported claims across diverse settings. It is designed to measure factual correctness more robustly than single-dataset QA by covering multiple factuality-related behaviors.,"Semantic Understanding & Context Recognition, Language Production, Self-reflection (minor), Inhibitory Control (minor), Working Memory (minor)"
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,Global PIQA is a multilingual commonsense/physical reasoning benchmark that tests whether models can choose the more plausible solution to everyday situations across many languages. It emphasizes robustness of pragmatic and physical-intuition reasoning under linguistic variation.,"Semantic Understanding & Context Recognition, Logical Reasoning, Language Comprehension, Decision-making (minor)"
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) is a long-context evaluation where multiple similar “needle” interactions are embedded in long conversational “haystacks,” and the model must retrieve and reproduce the correct referenced content. It stresses sustained context tracking, coreference resolution, and resistance to distractors over very long inputs.","Working Memory, Attention, Episodic Memory, Language Comprehension, Semantic Understanding & Context Recognition (minor)"
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval evaluates economically valuable, well-specified professional tasks across many occupations, where outputs are compared against industry professionals by expert judges. Tasks often require producing real work artifacts (e.g., spreadsheets, presentations) and following detailed constraints end-to-end.","Planning, Decision-making, Language Production, Semantic Understanding & Context Recognition, Working Memory (minor), Social Reasoning & Theory of Mind (minor)"
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks tests a model’s ability to perform multi-step reasoning over graph-structured data described in text, such as walking paths, tracing parents, or answering reachability questions. It emphasizes compositional state updates across many hops and sensitivity to small structural changes.","Logical Reasoning, Working Memory, Attention, Spatial Representation & Mapping (minor), Planning (minor)"
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon evaluates general tool-use competence across diverse APIs and workflows, emphasizing correct tool selection, parameterization, and multi-step orchestration. It is designed to reflect real agent pipelines where errors compound across calls and recovery strategies matter.","Planning, Decision-making, Adaptive Error Correction, Working Memory (minor), Language Comprehension (minor)"
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath is an expert-level mathematics benchmark intended to be difficult even for strong models, targeting deep problem solving rather than routine computation. It emphasizes rigorous multi-step reasoning and careful handling of definitions and edge cases, often benefitting from verification tools when allowed.","Logical Reasoning, Working Memory, Planning (minor), Adaptive Error Correction (minor)"
