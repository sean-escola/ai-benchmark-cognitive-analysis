Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,SWE-bench Verified evaluates software engineering agents on real GitHub issues by requiring them to generate a patch that makes a failing test suite pass. The “Verified” subset uses human validation and stricter evaluation infrastructure to ensure tasks are solvable and that proposed fixes are correctly assessed under reproducible conditions.,"L1: Language Comprehension (minor)
L2: Planning, Logical Reasoning, Adaptive Error Correction, Working Memory, Decision-making (minor)
L3: ",L2
SWE-bench Multilingual,https://www.swebench.com/multilingual.html,https://arxiv.org/abs/2504.21798,"SWE-bench Multilingual extends SWE-bench-style issue-to-patch tasks beyond Python, testing agents across multiple programming languages and ecosystems. It probes whether an agent can transfer debugging, test-driven fixing, and repository navigation skills across varied syntax, tooling, and library conventions.","L1: Language Comprehension (minor)
L2: Planning, Logical Reasoning, Adaptive Error Correction, Working Memory
L3: Cognitive Flexibility",L3
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro is a larger, more difficult software engineering benchmark designed to be more industrially relevant and contamination-resistant, spanning multiple languages and complex repos. Agents must identify root causes from issue descriptions and repository state, implement robust fixes, and satisfy evaluation tests under realistic constraints.","L1: Language Comprehension (minor)
L2: Planning, Logical Reasoning, Adaptive Error Correction, Working Memory, Decision-making (minor)
L3: ",L2
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 evaluates agentic performance on real command-line tasks (e.g., debugging, building, data processing, environment configuration) executed inside terminal environments. Success depends on choosing correct sequences of shell commands, interpreting tool output, and iteratively fixing errors under resource and time constraints.","L1: 
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory, Logical Reasoning (minor)
L3: ",L2
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp evaluates “deep research” agents by requiring them to answer difficult questions using browsing/search over a fixed document collection, emphasizing reproducibility across systems. It tests whether an agent can formulate search strategies, gather evidence from multiple sources, and synthesize a final grounded response.","L1: Language Comprehension (minor), Language Production (minor)
L2: Planning, Semantic Understanding & Context Recognition, Working Memory, Decision-making
L3: ",L2
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench measures an agent’s ability to resolve multi-turn customer support tasks in simulated domains (e.g., retail, airline, telecom) while using tools/APIs and following policies. It stresses instruction adherence under interaction, including handling conflicting user demands, tool errors, and domain-specific rules.","L1: Language Production
L2: Decision-making, Planning, Adaptive Error Correction (minor)
L3: Inhibitory Control, Social Reasoning & Theory of Mind, Empathy (minor)",L3
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld evaluates multimodal computer-use agents that must complete tasks on an operating system (e.g., navigating applications, configuring settings, manipulating files) using visual observations and action APIs. The benchmark emphasizes robust GUI understanding and long-horizon task completion with realistic interface variability.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Planning, Decision-making, Sensorimotor Coordination (minor), Working Memory (minor)
L3: ",L2
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI (Abstraction and Reasoning Corpus) tests fluid reasoning on novel grid-based pattern transformation tasks given only a few demonstrations. Models must infer latent rules and generalize to new inputs, emphasizing abstraction, compositional reasoning, and robust generalization beyond memorization.","L1: 
L2: Logical Reasoning, Working Memory, Spatial Representation & Mapping, Scene Understanding & Visual Reasoning, Planning (minor)
L3: Cognitive Flexibility",L3
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon autonomous business operation in a simulated vending-machine company over an extended time period. Agents must plan inventory and pricing, interact with suppliers, adapt to changing conditions, and remain coherent across many sequential decisions to maximize final balance.","L1: 
L2: Planning, Decision-making, Working Memory, Reward Mechanisms, Adaptive Error Correction (minor)
L3: Self-reflection (minor)",L2
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas evaluates real-world tool use via the Model Context Protocol, where agents must discover appropriate tools, call them with correct arguments, handle failures, and integrate results. It targets multi-step workflow execution across heterogeneous services in production-like environments.","L1: Language Comprehension (minor), Language Production (minor)
L2: Planning, Decision-making, Working Memory, Adaptive Error Correction
L3: ",L2
FinanceAgent,https://www.vals.ai/benchmarks/finance_agent,,"FinanceAgent evaluates whether models can perform tasks typical of an entry-level financial analyst, such as extracting information from documents, building analyses, and producing well-structured outputs with appropriate assumptions. It emphasizes domain-grounded reasoning, numerical consistency, and procedural task execution over multi-step workflows.","L1: 
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Planning, Decision-making, Working Memory (minor), Adaptive Error Correction (minor)
L3: ",L2
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates agentic cybersecurity capabilities on real-world vulnerability tasks at scale, including identifying known vulnerabilities and discovering new ones in open-source projects. Agents must interpret vulnerability descriptions or codebases, generate exploits or patches in some settings, and navigate realistic security workflows.","L1: 
L2: Logical Reasoning, Planning, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition (minor)
L3: Inhibitory Control (minor)",L2
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"SpreadsheetBench evaluates an agent’s ability to understand, edit, and compute with complex spreadsheets drawn from realistic tasks, often requiring multi-step transformations and formula logic. It stresses structured manipulation, consistency across cells/sheets, and correct end-state artifacts rather than single-turn answers.","L1: 
L2: Planning, Working Memory, Logical Reasoning, Adaptive Error Correction, Decision-making (minor), Sensorimotor Coordination (minor)
L3: ",L2
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a difficult frontier benchmark spanning advanced questions across many domains, including multimodal items, intended to probe expert-level reasoning and knowledge. It is typically evaluated in configurations with and without tools (e.g., search, code), highlighting both core reasoning and agentic augmentation.","L1: Language Comprehension, Language Production, Visual Perception (minor)
L2: Logical Reasoning, Working Memory, Scene Understanding & Visual Reasoning (minor)
L3: ",L2
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 is a competition-math benchmark derived from the American Invitational Mathematics Examination problems, focusing on non-routine mathematical problem solving. Performance reflects multi-step symbolic reasoning, careful constraint handling, and error avoidance under tight answer formats.","L1: 
L2: Logical Reasoning, Working Memory, Planning (minor)
L3: Cognitive Flexibility (minor)",L2
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is the highest-quality subset of GPQA, consisting of very difficult graduate-level science multiple-choice questions designed to be resistant to superficial lookup. It tests deep conceptual understanding and reasoning in physics, chemistry, and biology under adversarially hard distractors.","L1: 
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory (minor), Decision-making (minor)
L3: ",L2
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends MMLU-style breadth testing to multiple languages, measuring academic knowledge and reasoning across many subjects in non-English settings. It probes whether models can preserve competency under linguistic variation, translation ambiguity, and culturally varied phrasing.","L1: Language Comprehension, Language Production (minor)
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory (minor)
L3: Cognitive Flexibility",L3
MMMU,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2311.16502,"MMMU is a multimodal, multi-discipline benchmark requiring joint reasoning over images and text across expert-level domains (e.g., science, engineering, charts, diagrams). It tests whether models can integrate visual evidence with language instructions to answer complex questions that often require multi-step inference.","L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning (minor), Working Memory (minor)
L3: ",L2
LAB-Bench FigQA,https://huggingface.co/datasets/futurehouse/lab-bench,https://arxiv.org/abs/2407.10362,"LAB-Bench FigQA evaluates whether models can correctly interpret and reason about complex scientific figures from biology papers, including plots, multi-panel diagrams, and experimental schematics. It emphasizes extracting quantitative/structural information from visuals and mapping that evidence to precise scientific answers.","L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Logical Reasoning, Working Memory (minor)
L3: ",L2
WebArena,https://webarena.dev/,https://arxiv.org/abs/2307.13854,"WebArena evaluates autonomous web agents performing realistic tasks across multiple web apps (e-commerce, forums, CMS, code hosting), requiring navigation, form filling, and stateful multi-step interaction. It stresses robustness to dynamic interfaces and long-horizon planning with tool-mediated actions and verification loops.","L1: Visual Perception
L2: Planning, Decision-making, Visual Attention & Eye Movements, Adaptive Error Correction, Working Memory, Sensorimotor Coordination (minor)
L3: Inhibitory Control (minor)",L2
