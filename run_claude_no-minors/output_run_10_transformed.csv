Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates whether a model can solve real-world GitHub issues by producing a code patch that makes the repository’s tests pass. The “Verified” split uses tasks that have been manually checked to be solvable and correctly specified, reducing noise from ambiguous or flaky problems.","L1: Language Comprehension
L2: Planning, Logical Reasoning, Working Memory, Adaptive Error Correction
L3: ",L2
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 measures agentic performance on command-line tasks such as debugging, environment setup, data processing, and system operations inside a sandboxed terminal. Success requires iterative tool use, interpreting error output, and managing state across multiple steps.","L1: Language Comprehension (minor)
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory
L3: ",L2
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp evaluates deep-research style web browsing where models must search, read, and synthesize information to answer questions that are hard to solve without multi-step retrieval. It emphasizes navigating evidence, resolving ambiguity, and producing grounded final answers from sources.","L1: 
L2: Planning, Attention, Semantic Understanding & Context Recognition, Working Memory, Decision-making, Adaptive Error Correction (minor)
L3: ",L2
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench evaluates interactive agent behavior in simulated customer-support domains (e.g., retail, airline, telecom) where the agent must follow policies while using tools/APIs over multi-turn dialogues. It stresses policy compliance, conversation management, and correct tool invocation under realistic constraints.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making
L3: Social Reasoning & Theory of Mind, Inhibitory Control (minor)",L3
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld benchmarks autonomous computer-use agents on end-to-end tasks within real operating system environments, requiring GUI navigation, form filling, app switching, and multi-step execution. It tests whether models can perceive screens, decide next actions, and reliably complete workflows under step limits.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Spatial Representation & Mapping, Sensorimotor Coordination, Planning, Attention (minor)
L3: ",L2
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,ARC-AGI tests fluid reasoning on novel grid-based pattern tasks where the model must infer a hidden transformation rule from a few examples and apply it to a new input. The benchmark is designed to reduce reliance on memorized knowledge and emphasize abstraction and generalization.,"L1: Visual Perception (minor)
L2: Logical Reasoning, Working Memory, Spatial Representation & Mapping
L3: Cognitive Flexibility",L3
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon autonomy by having an agent run a simulated vending machine business over an extended time period, optimizing decisions like procurement, pricing, inventory, and negotiation. Performance is typically measured by final financial outcome, requiring sustained coherence and strategy.","L1: 
L2: Planning, Decision-making, Reward Mechanisms, Working Memory, Adaptive Error Correction (minor)
L3: ",L2
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas measures real-world tool use via the Model Context Protocol, where models must discover relevant tools, execute multi-step API workflows, handle errors, and synthesize results. It targets practical agent reliability across heterogeneous services and authentic tool interfaces.","L1: Language Comprehension (minor)
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory
L3: ",L2
FinanceAgent,https://www.vals.ai/benchmarks/finance_agent,,"FinanceAgent evaluates how well models perform tasks resembling entry-level financial analyst work, such as extracting information, performing calculations, building or checking analyses, and producing finance-oriented deliverables. It emphasizes correctness, reasoning with numbers, and professional decision support behavior.","L1: 
L2: Logical Reasoning, Decision-making, Semantic Understanding & Context Recognition, Working Memory, Planning (minor)
L3: ",L2
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates AI agents on cybersecurity tasks at scale, including reproducing known vulnerabilities from descriptions and, in some settings, discovering new weaknesses in real open-source projects. It stresses technical reasoning, iterative testing, and robust debugging in adversarial problem spaces.","L1: 
L2: Logical Reasoning, Planning, Adaptive Error Correction, Working Memory
L3: Inhibitory Control (minor)",L2
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"SpreadsheetBench measures an agent’s ability to navigate, edit, and compute over complex spreadsheets derived from realistic scenarios. Tasks often require multi-step transformations, formula reasoning, and careful state tracking to produce the correct final workbook outputs.","L1: 
L2: Planning, Working Memory, Logical Reasoning, Adaptive Error Correction, Attention (minor)
L3: ",L2
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a challenging multimodal benchmark spanning frontier academic and professional topics, designed to measure broad reasoning and knowledge under difficult, often research-like questions. It can include image-based components and rewards rigorous, well-justified answers.","L1: Language Comprehension, Visual Perception (minor)
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Scene Understanding & Visual Reasoning (minor)
L3: ",L2
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 is a competition-math evaluation based on problems from the American Invitational Mathematics Examination. It tests multi-step symbolic reasoning, precision, and the ability to derive exact numeric answers under tight problem constraints.","L1: 
L2: Logical Reasoning, Working Memory, Planning (minor), Adaptive Error Correction (minor)
L3: ",L2
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,GPQA Diamond is a high-quality subset of GPQA consisting of very difficult graduate-level science multiple-choice questions designed to be “Google-proof.” It emphasizes deep scientific reasoning and careful discrimination among plausible distractors.,"L1: 
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Decision-making (minor)
L3: ",L2
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends MMLU to multiple languages, testing knowledge and reasoning across many academic subjects in non-English settings. It emphasizes multilingual understanding, transfer of learned concepts across languages, and consistent subject-matter competence.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning
L3: Cognitive Flexibility (minor)",L2
MMMU,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2311.16502,"MMMU is a multidisciplinary multimodal benchmark where models answer questions requiring joint reasoning over images (e.g., diagrams, charts, figures) and text across many expert domains. It stresses visual understanding integrated with domain knowledge and multi-step inference.","L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Visual Attention & Eye Movements (minor)
L3: ",L2
LAB-Bench FigQA,https://huggingface.co/datasets/futurehouse/lab-bench,https://arxiv.org/abs/2407.10362,"LAB-Bench FigQA evaluates whether models can correctly interpret and answer questions about complex scientific figures from biology papers, including plots, microscopy images, and multi-panel diagrams. It targets practical scientific visual literacy and evidence-based reasoning from figures.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Semantic Understanding & Context Recognition, Logical Reasoning, Attention (minor)
L3: ",L2
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning tests reasoning over scientific charts and figures (often from arXiv-style documents), requiring extraction of quantitative/structural information and answering associated questions. It emphasizes robust chart understanding, cross-referencing captions/labels, and multi-step figure-based inference.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory (minor)
L3: ",L2
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU evaluates multimodal understanding and reasoning over videos, where models must integrate information across time to answer questions about events, actions, and context. It stresses temporal grounding, tracking entities and state changes, and synthesizing video evidence with language.","L1: Visual Perception, Auditory Processing (minor)
L2: Scene Understanding & Visual Reasoning, Working Memory, Attention
L3: Cognitive Timing & Predictive Modeling",L3
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"FACTS Benchmark Suite systematically evaluates factuality and grounding, measuring whether models produce statements supported by reliable evidence and whether they avoid hallucinations across diverse settings. It is designed to probe robustness of factual claims under different prompting and information conditions.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Adaptive Error Correction (minor)
L3: Self-reflection (minor), Inhibitory Control (minor)",L2
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA evaluates commonsense physical reasoning across many languages using non-parallel or language-diverse formulations, stressing robust understanding beyond English-centric patterns. It tests whether models can choose plausible actions or explanations grounded in everyday physics and affordances.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning
L3: Cognitive Flexibility (minor)",L2
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) is a long-context multi-round coreference and retrieval test where multiple similar “needle” interactions are embedded in long “haystacks,” and the model must reproduce the correct response for a specified needle. It targets precise long-range dependency handling under distractor-heavy contexts.","L1: 
L2: Working Memory, Attention, Semantic Understanding & Context Recognition
L3: Inhibitory Control (minor)",L2
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval measures performance on economically relevant, well-specified professional knowledge-work tasks across many occupations, judged by expert humans via pairwise comparisons. Tasks often require producing real deliverables (e.g., spreadsheets, presentations, plans) under realistic constraints.","L1: Language Production
L2: Planning, Decision-making, Semantic Understanding & Context Recognition
L3: Self-reflection (minor)",L2
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks tests long-context, algorithmic-style reasoning over graph-structured data represented in text, such as performing traversals or following parent/neighbor relationships across many nodes. It stresses consistent state tracking and correct multi-step navigation through abstract structures.","L1: 
L2: Working Memory, Logical Reasoning, Spatial Representation & Mapping, Attention (minor)
L3: ",L2
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon evaluates tool-using agents across diverse tasks requiring selecting the right tools, calling them correctly, and composing multi-step tool chains to reach an accurate final outcome. It emphasizes reliability under realistic tool errors, schema constraints, and multi-turn execution.","L1: Language Comprehension (minor)
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory
L3: ",L2
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath is a challenging benchmark of expert-level mathematics problems intended to measure progress near the research frontier, with strong emphasis on difficult multi-step derivations and exactness. It is designed to be hard to solve by memorization and to reward genuine mathematical reasoning.","L1: 
L2: Logical Reasoning, Working Memory, Planning, Adaptive Error Correction (minor)
L3: ",L2
