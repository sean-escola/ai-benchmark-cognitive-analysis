Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates an agent’s ability to solve real-world software engineering issues by generating patches in a Python repository that satisfy tests and task descriptions. The “Verified” subset contains problems manually checked to be solvable and to have reliable evaluation signals, emphasizing end-to-end bug fixing rather than code-only microtasks.","L1: 
L2: Planning, Adaptive Error Correction, Logical Reasoning, Working Memory, Attention (minor), Semantic Understanding & Context Recognition (minor), Decision-making (minor)
L3: ",L2
SWE-bench Multilingual,https://www.swebench.com/multilingual.html,https://arxiv.org/abs/2504.21798,"SWE-bench Multilingual extends SWE-bench-style repository patching to multiple programming languages, testing whether agents can diagnose and repair issues across diverse code ecosystems. It stresses transferring debugging strategies across languages and tooling while still meeting test-based acceptance criteria.","L1: Language Comprehension (minor)
L2: Planning, Adaptive Error Correction, Logical Reasoning, Working Memory, Semantic Understanding & Context Recognition (minor)
L3: Cognitive Flexibility",L3
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro is a larger, more difficult software engineering benchmark intended to be more robust and industrially representative, including tasks beyond the easiest verified fixes and spanning multiple languages. It emphasizes sustained, correct code changes under realistic repository constraints and aims to reduce shortcut solutions via stronger task construction and evaluation.","L1: 
L2: Planning, Adaptive Error Correction, Logical Reasoning, Decision-making, Working Memory, Attention (minor)
L3: Cognitive Flexibility (minor)",L2
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 measures agent performance on real command-line tasks in sandboxed environments, such as installing tools, manipulating files, running programs, and debugging failures. Success typically requires iterative tool use with feedback from command outputs and careful state tracking across many steps.","L1: 
L2: Planning, Adaptive Error Correction, Working Memory, Decision-making, Attention (minor), Spatial Representation & Mapping (minor), Semantic Understanding & Context Recognition (minor)
L3: ",L2
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp evaluates deep-research and browsing agents on information-seeking problems that require searching, reading, cross-checking sources, and synthesizing an answer. It is designed to test multi-step web navigation and evidence aggregation rather than single-shot recall.","L1: Language Comprehension
L2: Planning, Attention, Working Memory, Semantic Understanding & Context Recognition, Decision-making (minor), Episodic Memory (minor)
L3: ",L2
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench evaluates interactive customer-support agents that must follow domain policies while using tools/APIs over multi-turn conversations with simulated users. It stresses policy adherence, robust dialogue management, and correct execution of operational workflows (e.g., refunds, bookings, troubleshooting).","L1: Language Comprehension, Language Production
L2: Decision-making, Planning
L3: Social Reasoning & Theory of Mind, Inhibitory Control, Empathy (minor)",L3
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld is a multimodal computer-use benchmark where agents complete tasks in desktop environments by perceiving screens and interacting with applications through GUI actions. It measures practical tool use under partial observability, requiring navigation, error recovery, and long-horizon execution across heterogeneous software.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Planning, Decision-making, Visual Attention & Eye Movements (minor), Spatial Representation & Mapping (minor), Adaptive Error Correction (minor)
L3: ",L2
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI tests fluid reasoning on novel grid-based pattern transformation tasks given only a few demonstrations, aiming to minimize reliance on memorized knowledge. Models must infer latent rules, generalize to a new input, and output the correct transformed grid under strong distribution shift.","L1: Visual Perception (minor)
L2: Logical Reasoning, Working Memory, Scene Understanding & Visual Reasoning, Planning (minor)
L3: Cognitive Flexibility",L3
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon agent coherence and strategy by having agents run a simulated vending machine business over many decisions (inventory, pricing, suppliers, messaging). Performance depends on sustained goal tracking, adapting to changing conditions, and compounding gains over time.","L1: 
L2: Planning, Decision-making, Reward Mechanisms, Working Memory, Episodic Memory (minor)
L3: Social Reasoning & Theory of Mind (minor), Self-reflection (minor)",L2
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas measures real-world tool-use via the Model Context Protocol, where models must discover appropriate tools, call them with correct schemas, handle failures, and integrate results into final answers. Tasks typically involve multi-step workflows with authentic API-like interactions rather than purely textual reasoning.","L1: 
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition (minor)
L3: Inhibitory Control (minor)",L2
FinanceAgent,https://www.vals.ai/benchmarks/finance_agent,,"FinanceAgent evaluates agentic performance on tasks representative of an entry-level financial analyst, such as building analyses, extracting figures, making assumptions explicit, and producing structured outputs with citations. It emphasizes combining domain knowledge with multi-step artifact creation and verification-like behavior.","L1: 
L2: Logical Reasoning, Decision-making, Planning, Working Memory, Semantic Understanding & Context Recognition, Attention (minor)
L3: ",L2
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym tests cybersecurity capabilities at scale, including finding known vulnerabilities from descriptions and discovering new ones in real open-source codebases. It stresses iterative hypothesis testing, reading code precisely, and using tool feedback to refine an exploit or patch strategy.","L1: 
L2: Logical Reasoning, Adaptive Error Correction, Planning, Working Memory, Attention (minor), Semantic Understanding & Context Recognition (minor)
L3: ",L2
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"SpreadsheetBench measures whether models can navigate and manipulate complex spreadsheets to answer questions or produce correct transformed files, often mirroring real office workflows. It requires accurate referencing, multi-step transformations, and careful checking to avoid subtle formula and formatting errors.","L1: 
L2: Working Memory, Attention, Planning, Adaptive Error Correction, Logical Reasoning (minor), Spatial Representation & Mapping (minor)
L3: ",L2
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a challenging multimodal benchmark spanning frontier academic and professional questions, designed to probe deep reasoning beyond routine test-taking. Items often require synthesizing information, handling ambiguity, and (in tool-enabled settings) integrating external evidence responsibly.","L1: Language Comprehension
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Multisensory Integration (minor), Scene Understanding & Visual Reasoning (minor), Planning (minor)
L3: ",L2
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 is a set of competition-style mathematics problems used to evaluate mathematical reasoning without relying on domain-specific tooling. It emphasizes multi-step derivations, symbolic manipulation, and maintaining correctness across long solution chains.","L1: 
L2: Logical Reasoning, Working Memory, Planning, Attention (minor)
L3: ",L2
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,GPQA Diamond is a high-quality subset of GPQA consisting of difficult graduate-level science multiple-choice questions intended to be resistant to superficial search and pattern matching. It probes precise scientific reasoning and understanding under tight answer constraints.,"L1: 
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory (minor), Attention (minor)
L3: ",L2
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends MMLU-style broad academic testing to multiple languages, assessing knowledge and reasoning across many subjects under multilingual prompts. It highlights cross-lingual robustness, translation-like understanding, and consistency of factual and conceptual knowledge.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory (minor)
L3: Cognitive Flexibility (minor)",L2
MMMU,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2311.16502,"MMMU is a multidisciplinary multimodal benchmark where models answer questions that require jointly reasoning over images and text across many expert domains. It targets visual grounding, diagram/table understanding, and integrating visual evidence with language-based inference.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Attention (minor), Working Memory (minor)
L3: ",L2
LAB-Bench FigQA,https://huggingface.co/datasets/futurehouse/lab-bench,https://arxiv.org/abs/2407.10362,"LAB-Bench FigQA evaluates whether models can correctly interpret complex scientific figures (often from biology papers) and answer questions requiring precise visual extraction and scientific reasoning. It emphasizes chart/figure literacy, visual localization, and disciplined interpretation over plausible-sounding guesses.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Multisensory Integration, Semantic Understanding & Context Recognition, Logical Reasoning, Visual Attention & Eye Movements (minor)
L3: ",L2
WebArena,https://webarena.dev/,https://arxiv.org/abs/2307.13854,"WebArena measures autonomous web agent performance on realistic tasks across multiple web apps (e-commerce, forums, code hosting, CMS), requiring navigation, form-filling, and multi-step completion. It probes robustness to dynamic interfaces, long-horizon planning, and recovering from mistakes under a functional grader.","L1: Visual Perception
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory, Scene Understanding & Visual Reasoning, Spatial Representation & Mapping (minor)
L3: Inhibitory Control (minor)",L2
