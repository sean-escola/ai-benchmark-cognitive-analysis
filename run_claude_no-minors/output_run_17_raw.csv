Benchmark,Website,Paper,Description,Cognitive Functions
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates software engineering agents by asking them to fix real GitHub issues by producing patches that make a repository’s tests pass. The “Verified” subset consists of tasks that have been validated as solvable and reliably graded, emphasizing end-to-end debugging and codebase navigation rather than isolated coding problems.","Planning, Logical Reasoning, Adaptive Error Correction, Working Memory, Language Comprehension (minor), Decision-making (minor)"
SWE-bench Multilingual,https://www.swebench.com/multilingual.html,https://arxiv.org/abs/2504.21798,"SWE-bench Multilingual extends SWE-bench-style issue resolution beyond Python to multiple programming languages and ecosystems. It tests whether an agent can interpret issues, understand unfamiliar project conventions, and apply correct edits across diverse tooling and language-specific patterns.","Cognitive Flexibility, Planning, Logical Reasoning, Adaptive Error Correction, Semantic Understanding & Context Recognition, Working Memory (minor)"
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro is a larger and more difficult software engineering benchmark designed to be more industrially relevant and more resistant to contamination than earlier variants. It emphasizes solving realistic engineering tasks across multiple languages, requiring robust patch generation, testing, and iterative debugging under stricter evaluation conditions.","Planning, Decision-making, Logical Reasoning, Adaptive Error Correction, Working Memory, Language Comprehension (minor)"
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 evaluates agents on real command-line workflows such as installing dependencies, manipulating files, running programs, and diagnosing failures in a shell environment. Success requires selecting appropriate commands, interpreting outputs/errors, and iterating until the task goal is satisfied.","Planning, Adaptive Error Correction, Decision-making, Attention (minor), Working Memory (minor), Motor Coordination (minor)"
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp measures how well agents perform research-style question answering using web browsing and retrieval, often requiring multi-step search, source comparison, and synthesis. It focuses on whether an agent can find relevant information in noisy contexts and produce grounded, correct answers under tool-use constraints.","Planning, Attention, Semantic Understanding & Context Recognition, Episodic Memory (minor), Logical Reasoning (minor), Language Production (minor)"
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench evaluates interactive tool-using agents in customer-support-like environments (e.g., retail, airline, telecom) where the model must communicate with a simulated user and call APIs while following policies. It stresses consistent multi-turn behavior, correct API sequencing, and adherence to domain constraints despite user pressure or ambiguity.","Social Reasoning & Theory of Mind, Decision-making, Planning, Inhibitory Control, Language Comprehension (minor), Empathy (minor)"
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld benchmarks multimodal computer-use agents on tasks performed inside an operating-system-like desktop environment, requiring interaction with GUIs over many steps. It tests whether an agent can perceive screen state, navigate applications, and execute correct sequences of actions to accomplish practical goals.","Visual Perception, Scene Understanding & Visual Reasoning, Planning, Decision-making, Visual Attention & Eye Movements (minor), Sensorimotor Coordination (minor)"
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,ARC-AGI evaluates “fluid intelligence” through abstract grid-based tasks where models infer a transformation rule from a few input–output examples and apply it to a new input. The benchmark emphasizes generalization to novel patterns and compositional reasoning rather than domain knowledge.,"Cognitive Flexibility, Logical Reasoning, Working Memory, Spatial Representation & Mapping, Attention (minor)"
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon agent coherence by simulating the operation of a vending-machine business over extended time, where the agent must manage inventory, pricing, suppliers, and cashflow. High performance requires sustained strategy, adapting to changing conditions, and avoiding compounding mistakes across many decisions.","Planning, Decision-making, Reward Mechanisms, Working Memory (minor), Episodic Memory (minor), Self-reflection (minor)"
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas measures tool-use competence via the Model Context Protocol, requiring models to discover and invoke real tools/APIs, chain multi-step calls, handle failures, and synthesize outputs into correct final responses. It focuses on practical orchestration and robustness in production-like tool ecosystems.","Planning, Decision-making, Adaptive Error Correction, Working Memory (minor), Language Comprehension (minor), Language Production (minor)"
FinanceAgent,https://www.vals.ai/benchmarks/finance_agent,,"FinanceAgent evaluates agents on tasks typical of entry-level financial analysts, such as extracting information from documents, performing structured calculations, building assumptions, and producing justified conclusions. It emphasizes domain-grounded reasoning, consistency with financial constraints, and producing usable professional artifacts.","Logical Reasoning, Semantic Understanding & Context Recognition, Planning (minor), Decision-making (minor), Working Memory (minor), Language Production (minor)"
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity agents on tasks involving locating known vulnerabilities and discovering new ones in real open-source codebases. It tests the ability to interpret vulnerability descriptions, analyze code, reproduce conditions, and propose fixes or exploitation-relevant findings under realistic constraints.","Logical Reasoning, Planning, Adaptive Error Correction, Attention (minor), Working Memory (minor), Inhibitory Control (minor)"
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"SpreadsheetBench measures an agent’s ability to navigate, edit, and compute within complex spreadsheets, often mirroring business workflows with formulas, tables, and formatting constraints. It stresses structured manipulation, error diagnosis, and producing correct spreadsheet states rather than just explaining what to do.","Working Memory, Planning, Attention, Logical Reasoning (minor), Adaptive Error Correction (minor), Motor Coordination (minor)"
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a challenging multimodal benchmark spanning questions at the frontier of human knowledge, often requiring careful reasoning and integration of evidence across modalities. It is designed to stress general problem solving, scientific/technical understanding, and robustness beyond memorized facts.","Language Comprehension, Logical Reasoning, Semantic Understanding & Context Recognition, Scene Understanding & Visual Reasoning (minor), Multisensory Integration (minor), Working Memory (minor)"
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,AIME 2025 evaluates mathematical reasoning on competition-style problems that require multi-step derivations and precise final answers. It emphasizes symbolic manipulation and strategy selection under tight correctness criteria rather than tool-heavy workflows.,"Logical Reasoning, Working Memory, Planning (minor), Attention (minor)"
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a subset of graduate-level, “Google-proof” multiple-choice science questions chosen for high quality and difficulty. It targets deep scientific reasoning and disambiguation, where superficial pattern matching or memorized trivia is insufficient to reliably select the correct option.","Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory (minor), Decision-making (minor), Language Comprehension (minor)"
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends broad academic evaluation to multiple languages, probing whether models retain knowledge and reasoning competence across diverse linguistic contexts. It stresses cross-lingual generalization, instruction following in different languages, and consistent subject-matter understanding.","Language Comprehension, Semantic Understanding & Context Recognition, Cognitive Flexibility, Logical Reasoning (minor), Language Production (minor)"
MMMU,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2311.16502,"MMMU evaluates multimodal understanding across many disciplines using problems that require combining text with images such as charts, diagrams, and figures. It probes whether models can ground language in visual evidence and perform reasoning that depends on visual details.","Visual Perception, Scene Understanding & Visual Reasoning, Multisensory Integration, Language Comprehension (minor), Working Memory (minor), Attention (minor)"
LAB-Bench FigQA,https://huggingface.co/datasets/futurehouse/lab-bench,https://arxiv.org/abs/2407.10362,"LAB-Bench FigQA focuses on scientific figure question answering from biology papers, requiring careful interpretation of plots, labels, and experimental results presented visually. It emphasizes precise visual-to-text grounding and domain-relevant reasoning about what the figure implies.","Scene Understanding & Visual Reasoning, Visual Perception, Semantic Understanding & Context Recognition, Attention (minor), Logical Reasoning (minor), Multisensory Integration (minor)"
WebArena,https://webarena.dev/,https://arxiv.org/abs/2307.13854,"WebArena evaluates autonomous web agents on realistic tasks across multiple web apps (e-commerce, CMS, forums, collaboration tools) requiring navigation, form filling, and multi-step workflows. It stresses robust interaction with dynamic interfaces, long-horizon task execution, and recovery from partial failures.","Planning, Decision-making, Visual Perception, Scene Understanding & Visual Reasoning, Adaptive Error Correction, Working Memory (minor), Visual Attention & Eye Movements (minor)"
