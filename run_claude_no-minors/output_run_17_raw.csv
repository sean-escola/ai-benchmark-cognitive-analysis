Benchmark,Website,Paper,Description,Cognitive Functions
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates agentic software engineering by having a model modify real GitHub repositories to fix issues, with success determined by running tests on a verified subset of tasks that humans confirmed are solvable. It stresses end-to-end patch generation: understanding requirements, navigating codebases, implementing changes, and ensuring the solution passes automated checks in a single attempt setting.","Planning, Logical Reasoning, Working Memory, Adaptive Error Correction, Semantic Understanding & Context Recognition (minor)"
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 measures how well an agent can complete real command-line tasks in a sandboxed terminal environment, often requiring iterative diagnosis, file manipulation, and tool usage (shell utilities, package managers, etc.). It emphasizes robust execution under constraints, including handling errors, stateful environments, and multi-step workflows.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition (minor)"
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp evaluates a model’s ability to do “deep research” style web browsing: searching, reading multiple sources, and synthesizing a grounded answer. It tests whether agents can navigate information discovery, resolve conflicting evidence, and maintain a coherent line of inquiry across multiple steps and documents.","Planning, Attention, Episodic Memory, Semantic Understanding & Context Recognition, Language Comprehension (minor)"
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench evaluates interactive tool-using agents in simulated customer-service domains where the agent must follow policies while using APIs over multi-turn dialogues. It emphasizes policy adherence, procedural correctness, and consistent behavior across long conversations with changing user goals and constraints.","Social Reasoning & Theory of Mind, Decision-making, Planning, Inhibitory Control, Language Comprehension (minor), Language Production (minor)"
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld evaluates multimodal “computer use” agents that operate graphical desktop environments to accomplish realistic tasks (e.g., navigating apps, settings, and web pages). It probes whether models can perceive UI states, plan action sequences, and reliably execute step-by-step interactions under partial observability and long horizons.","Visual Perception, Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements (minor), Planning, Decision-making, Sensorimotor Coordination"
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI measures fluid reasoning via novel grid-based tasks: given a few input-output examples, the model must infer the hidden transformation rule and apply it to a new input. The benchmark targets generalization to unseen concepts, compositional pattern discovery, and robust reasoning with very little data per task.","Logical Reasoning, Cognitive Flexibility, Working Memory, Spatial Representation & Mapping, Visual Perception (minor)"
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon autonomous agent performance in a simulated business setting (running a vending machine operation over an extended period). It requires strategic planning, adapting to market dynamics, negotiating and managing inventory, and maintaining coherent goals across many decisions.","Planning, Decision-making, Reward Mechanisms, Working Memory (minor), Episodic Memory (minor), Self-reflection (minor)"
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas evaluates real-world tool use through the Model Context Protocol by testing multi-step workflows across authentic APIs and tool servers. It probes whether models can discover relevant tools, call them with correct parameters, recover from failures, and synthesize results into an accurate final response.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition (minor)"
FinanceAgent,https://www.vals.ai/benchmarks/finance_agent,,"FinanceAgent evaluates performance on tasks representative of an entry-level financial analyst, such as analysis, summarization, and producing work artifacts from financial data and documents. It stresses quantitative and domain reasoning, structured reporting, and maintaining correctness under professional constraints.","Logical Reasoning, Semantic Understanding & Context Recognition, Planning (minor), Decision-making (minor), Working Memory (minor), Language Production (minor)"
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity capability on a large suite of tasks involving identifying known vulnerabilities and, in some settings, discovering new ones in real open-source projects. It tests multi-step technical reasoning, reading and understanding code, and producing actionable vulnerability findings under time and tool constraints.","Logical Reasoning, Planning, Adaptive Error Correction, Inhibitory Control (minor), Working Memory (minor), Semantic Understanding & Context Recognition (minor)"
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"SpreadsheetBench evaluates an agent’s ability to manipulate complex spreadsheets and produce correct outputs (e.g., formulas, transformations, and data organization) using software tools. It probes procedural accuracy, multi-step editing, and robustness to errors in structured, stateful artifacts.","Planning, Logical Reasoning, Working Memory, Adaptive Error Correction, Semantic Understanding & Context Recognition (minor), Visual Perception (minor)"
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a large, frontier-level benchmark of difficult questions intended to probe broad expert knowledge and reasoning, including multimodal items. It emphasizes integrating information, avoiding hallucinations, and producing grounded, high-precision answers under adversarially hard coverage of domains.","Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory (minor), Visual Perception (minor), Multisensory Integration (minor)"
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,AIME 2025 evaluates mathematical problem solving on competition-grade questions that require multi-step derivations and careful symbolic reasoning. It stresses correctness under nontrivial constraints and the ability to maintain long chains of dependencies without losing intermediate information.,"Logical Reasoning, Working Memory, Planning (minor), Adaptive Error Correction (minor)"
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a high-quality subset of extremely difficult, “Google-proof” multiple-choice science questions designed to resist superficial retrieval. It probes deep scientific reasoning, precise reading of technical statements, and selecting among close distractors that require careful discrimination.","Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory (minor), Inhibitory Control (minor)"
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends the classic MMLU format to multiple languages, testing knowledge and reasoning across many academic subjects in non-English settings. It probes whether models can transfer conceptual understanding across languages and maintain consistent reasoning under multilingual variation.","Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning (minor), Language Production (minor)"
MMMU,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2311.16502,"MMMU measures multidisciplinary multimodal understanding by requiring reasoning over images plus text across many expert domains (e.g., diagrams, plots, and scientific figures). It emphasizes interpreting visual evidence, integrating it with instructions, and producing answers that depend on both modalities.","Visual Perception, Scene Understanding & Visual Reasoning, Multisensory Integration, Attention (minor), Logical Reasoning (minor)"
LAB-Bench FigQA,https://huggingface.co/datasets/futurehouse/lab-bench,https://arxiv.org/abs/2407.10362,"LAB-Bench FigQA evaluates whether models can answer questions about complex scientific figures from biology papers, often requiring careful reading of plots, labels, and experimental structure. It stresses accurate extraction of visual evidence and chaining that evidence into correct scientific conclusions.","Scene Understanding & Visual Reasoning, Visual Perception, Attention, Logical Reasoning (minor), Semantic Understanding & Context Recognition (minor)"
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning evaluates reasoning over charts and figures from scientific documents, emphasizing interpretation rather than surface caption reading. It probes whether models can extract quantitative or relational information from visualizations and combine it with contextual text to answer questions.","Visual Perception, Scene Understanding & Visual Reasoning, Working Memory (minor), Logical Reasoning (minor), Semantic Understanding & Context Recognition (minor)"
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU extends multimodal academic reasoning into the video setting, requiring models to interpret temporally unfolding visual content along with text prompts. It probes tracking objects and events over time and integrating dispersed evidence across frames to answer questions.","Visual Perception, Cognitive Timing & Predictive Modeling, Attention, Working Memory, Multisensory Integration (minor)"
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"The FACTS Benchmark Suite evaluates factuality by measuring how reliably a model’s outputs remain consistent with provided sources or ground truth across multiple factuality-focused tasks. It targets hallucination resistance, evidence tracking, and careful calibration about what is and is not supported.","Semantic Understanding & Context Recognition, Inhibitory Control, Working Memory (minor), Self-reflection (minor), Language Comprehension (minor)"
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA evaluates physical commonsense reasoning and practical knowledge in a multilingual, non-parallel setting, emphasizing generalization beyond English-only phrasing. It probes understanding of everyday physical interactions, constraints, and plausible action outcomes as expressed in diverse languages.","Semantic Understanding & Context Recognition, Logical Reasoning, Spatial Representation & Mapping (minor), Sensorimotor Coordination (minor), Language Comprehension (minor)"
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,MRCR v2 (8-needle) measures long-context multi-round coreference and retrieval by inserting multiple similar “needle” requests into long “haystacks” and asking the model to reproduce the response to a specific needle. It probes whether models can maintain accurate dependency tracking across large contexts without confusing near-duplicate items.,"Working Memory, Attention, Episodic Memory, Semantic Understanding & Context Recognition (minor)"
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval evaluates economically valuable, well-specified professional knowledge work across many occupations, judged by expert humans via head-to-head comparisons with professional outputs. It probes end-to-end task execution: producing structured artifacts (e.g., slides, spreadsheets, schedules) that meet real workplace constraints and quality bars.","Planning, Decision-making, Language Production, Semantic Understanding & Context Recognition, Self-reflection (minor), Social Reasoning & Theory of Mind (minor)"
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks evaluates multi-step reasoning over graph-structured data, such as following edges, tracking paths, and answering questions that require systematic traversal. It stresses maintaining intermediate states across many steps and avoiding shortcut heuristics that fail on larger or adversarial graphs.","Logical Reasoning, Working Memory, Spatial Representation & Mapping, Attention (minor)"
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon evaluates general tool-use competence across heterogeneous APIs and tasks, emphasizing reliable function selection, correct parameterization, and iterative refinement when tools fail. It probes agentic orchestration under realistic tool noise and the ability to integrate tool outputs into a coherent final answer.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Language Comprehension (minor)"
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath evaluates advanced, expert-level mathematics, including problems that require novel insight and long derivations rather than routine computation. It probes deep multi-step reasoning, abstraction, and the ability to sustain correct intermediate structure over extended solutions.","Logical Reasoning, Working Memory, Planning (minor), Cognitive Flexibility (minor)"
