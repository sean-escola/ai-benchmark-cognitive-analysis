Benchmark,Website,Paper,Description,Cognitive Functions
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates an agent’s ability to solve real-world GitHub issues by generating code patches that make failing tests pass. The “Verified” subset contains tasks that have been manually checked to be solvable and to have reliable evaluation via unit tests, emphasizing end-to-end bug fixing in realistic repositories.","Planning, Logical Reasoning, Adaptive Error Correction, Working Memory, Language Comprehension (minor), Language Production (minor)"
SWE-bench Multilingual,https://www.swebench.com/multilingual.html,https://arxiv.org/abs/2504.21798,"SWE-bench Multilingual extends the SWE-bench paradigm to multiple programming languages, requiring models to understand diverse codebases, toolchains, and ecosystem conventions. It measures whether software-engineering competence transfers beyond Python while maintaining realistic repo-level constraints and test-based verification.","Planning, Logical Reasoning, Cognitive Flexibility, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition (minor)"
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro is a larger, harder software engineering benchmark designed to be more industrially representative and more resistant to shortcutting via memorization. It evaluates end-to-end repo modification across multiple languages and more complex issue settings, stressing robustness under realistic constraints.","Planning, Decision-making, Logical Reasoning, Adaptive Error Correction, Working Memory, Cognitive Flexibility (minor)"
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 measures how well agentic systems accomplish practical tasks in a command-line environment (e.g., debugging, environment setup, data processing) using shell commands and program outputs. It emphasizes iterative trial-and-error, tool execution, and recovery from mistakes under real system constraints.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition, Inhibitory Control (minor)"
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp evaluates “deep research” agents that must search and read web content to answer challenging questions, typically requiring multi-step retrieval, source comparison, and synthesis. It probes whether an agent can manage context over long browsing trajectories and avoid brittle or hallucinated conclusions when evidence is distributed across documents.","Planning, Decision-making, Semantic Understanding & Context Recognition, Episodic Memory (minor), Working Memory, Self-reflection (minor)"
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench evaluates interactive customer-support agents in simulated domains (e.g., retail, airline, telecom) that must use APIs/tools while following policies and maintaining a coherent multi-turn dialogue. It tests reliable tool use, policy adherence, and pragmatic communication under user pressure and ambiguous constraints.","Social Reasoning & Theory of Mind, Decision-making, Planning, Inhibitory Control, Language Comprehension, Empathy (minor)"
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld evaluates multimodal computer-use agents that operate within a real or realistic operating-system desktop, perceiving screenshots and executing mouse/keyboard actions to complete tasks. It stresses long-horizon GUI navigation, error recovery, and grounding actions in visual state changes across many applications.","Visual Perception, Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Planning, Decision-making, Sensorimotor Coordination, Working Memory (minor), Spatial Representation & Mapping (minor)"
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,ARC-AGI tests fluid reasoning on abstract grid-based puzzles where a system must infer transformation rules from a few input-output examples and apply them to a new input. The benchmark is designed to emphasize novel pattern induction and compositional generalization rather than rote knowledge.,"Logical Reasoning, Cognitive Flexibility, Spatial Representation & Mapping, Visual Perception, Working Memory (minor)"
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 measures long-term coherence and strategy in a simulated “run a vending machine business” setting over many decision steps. Agents must plan purchases, pricing, inventory, and communications while adapting to changing conditions to maximize end-of-horizon profit.","Planning, Decision-making, Reward Mechanisms, Working Memory, Episodic Memory (minor), Self-reflection (minor)"
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas evaluates real-world tool use via the Model Context Protocol (MCP), requiring models to discover tools, call them correctly across multiple steps, handle failures, and integrate results into accurate final outputs. It focuses on reliability of structured actions in production-like API environments rather than pure text generation.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition, Language Comprehension (minor)"
FinanceAgent,https://www.vals.ai/benchmarks/finance_agent,,"FinanceAgent assesses performance on tasks typical of an entry-level financial analyst, such as interpreting financial documents, performing calculations, constructing analyses, and producing decision-ready summaries. It emphasizes grounded quantitative reasoning and multi-step workflows that mirror professional finance deliverables.","Logical Reasoning, Semantic Understanding & Context Recognition, Planning, Decision-making, Working Memory, Language Production (minor)"
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity agent capabilities on real-world vulnerability tasks, including identifying known weaknesses from descriptions and discovering new vulnerabilities in open-source projects. It stresses systematic exploration, hypothesis testing, and iterative debugging under realistic code and system constraints.","Logical Reasoning, Planning, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition, Inhibitory Control (minor)"
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"SpreadsheetBench tests whether models can understand, edit, and compute within complex spreadsheets derived from real-world scenarios. Tasks often require multi-step manipulation (formulas, formatting, table operations) and consistency checks, emphasizing reliable execution over brittle pattern matching.","Planning, Decision-making, Working Memory, Adaptive Error Correction, Semantic Understanding & Context Recognition, Visual Perception (minor)"
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a difficult, multimodal academic benchmark spanning many domains and question styles, intended to probe frontier reasoning and knowledge at the edge of current systems. It rewards integrating evidence, performing multi-step reasoning, and (when tools are allowed) using external resources without introducing unsupported claims.","Language Comprehension, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory (minor), Visual Perception (minor)"
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 consists of competition-style math problems that require precise symbolic reasoning, careful case analysis, and nontrivial algebra/geometry/number theory insights. The benchmark is typically scored by exact final answers, making it sensitive to small reasoning errors.","Logical Reasoning, Working Memory, Planning (minor), Cognitive Flexibility (minor)"
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a subset of graduate-level, “Google-proof” science multiple-choice questions curated to be difficult for non-experts and resistant to superficial retrieval. It evaluates deep scientific understanding and reasoning under distractors that penalize shallow pattern matching.","Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory (minor), Language Comprehension (minor)"
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends MMLU-style academic questions into multiple languages, measuring whether a model’s knowledge and reasoning transfer across linguistic settings. It probes multilingual comprehension under domain-specific terminology and varied cultural/linguistic phrasing.","Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning, Cognitive Flexibility, Working Memory (minor)"
MMMU,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2311.16502,"MMMU is a multidisciplinary multimodal benchmark where questions combine text with images (charts, diagrams, scenes) across many expert domains. It evaluates whether models can ground reasoning in visual evidence and integrate it with domain knowledge to answer complex questions.","Visual Perception, Scene Understanding & Visual Reasoning, Multisensory Integration, Language Comprehension, Logical Reasoning, Working Memory (minor)"
LAB-Bench FigQA,https://huggingface.co/datasets/futurehouse/lab-bench,https://arxiv.org/abs/2407.10362,"LAB-Bench FigQA focuses on question answering over complex scientific figures from biology papers, requiring careful extraction of values, trends, and experimental relationships from visualizations. It emphasizes accurate figure interpretation and evidence-based scientific reasoning rather than generic vision recognition.","Visual Perception, Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Multisensory Integration, Semantic Understanding & Context Recognition, Logical Reasoning"
WebArena,https://webarena.dev/,https://arxiv.org/abs/2307.13854,"WebArena evaluates autonomous web agents performing realistic multi-step tasks across several web apps (e-commerce, CMS, forums, code hosting, maps) using a browser interface. Success requires planning and executing sequences of grounded interactions while tracking task state, forms, navigation, and dynamic page feedback.","Planning, Decision-making, Visual Perception, Scene Understanding & Visual Reasoning (minor), Sensorimotor Coordination, Working Memory, Inhibitory Control (minor), Semantic Understanding & Context Recognition (minor)"
