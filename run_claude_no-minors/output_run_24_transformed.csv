Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates agentic software engineering by asking a model to generate patches that resolve real issues in open-source Python repositories, with correctness judged by unit/integration tests. The “Verified” split uses human-vetted tasks intended to reduce ambiguity and ensure each task is solvable as specified, emphasizing reliable end-to-end coding behavior rather than only code synthesis.","L1: Language Comprehension (minor), Language Production (minor)
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition
L3: ",L2
SWE-bench Multilingual,https://www.swebench.com/multilingual.html,https://arxiv.org/abs/2504.21798,"SWE-bench Multilingual extends SWE-bench-style repository repair tasks beyond Python to multiple programming languages, requiring models to understand heterogeneous toolchains, test harnesses, and language idioms. It stresses cross-language transfer and robustness of the same agent loop (read context → plan → edit → test → iterate) across diverse ecosystems.","L1: Language Comprehension (minor), Language Production (minor)
L2: Planning, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition
L3: Cognitive Flexibility",L3
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro is a larger, more difficult software engineering benchmark designed to be more industrially representative and more resistant to contamination, spanning multiple languages and more complex change requirements. It emphasizes sustained multi-step problem solving in real codebases—triaging the failure, localizing the bug, implementing a fix, and satisfying tests—under realistic constraints.","L1: 
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition, Logical Reasoning
L3: Inhibitory Control (minor)",L2
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 evaluates agents operating in a command-line environment on practical tasks such as installing dependencies, inspecting files, running programs, debugging errors, and producing correct artifacts. Success requires reliable tool use, iterative troubleshooting, and maintaining state across many terminal interactions rather than answering a single prompt.","L1: 
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition
L3: Inhibitory Control (minor)",L2
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp measures deep-research and browsing agents that must locate and synthesize information from a large document collection, emphasizing search strategy and evidence-backed answers. It probes long-context behaviors such as tracking leads, reconciling conflicting sources, and compiling a coherent final response after multiple retrieval steps.","L1: Language Comprehension, Language Production
L2: Planning, Attention, Episodic Memory, Semantic Understanding & Context Recognition
L3: Self-reflection (minor)",L2
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench evaluates tool-using customer-support agents in simulated domains (e.g., retail, airline, telecom) where models must interact with APIs and a user while adhering to domain policies. It stresses multi-turn workflow execution, policy compliance under pressure, and producing user-facing resolutions that are both correct and procedurally appropriate.","L1: Language Production
L2: Decision-making, Planning, Semantic Understanding & Context Recognition
L3: Inhibitory Control, Social Reasoning & Theory of Mind, Empathy (minor)",L3
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld evaluates multimodal computer-use agents that must complete tasks in a realistic operating-system environment using screenshots and actions like clicking, typing, and navigating applications. It probes grounded perception-to-action loops, where success depends on interpreting UI state, planning sequences of interactions, and recovering from errors over many steps.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Sensorimotor Coordination, Planning, Decision-making, Adaptive Error Correction (minor)
L3: ",L2
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI presents few-shot grid transformation puzzles intended to measure fluid reasoning: models infer latent rules from a small set of input–output examples and apply them to a new input. It targets systematic generalization, compositional pattern discovery, and robustness to novelty rather than memorized knowledge.","L1: 
L2: Logical Reasoning, Spatial Representation & Mapping, Working Memory, Attention
L3: Cognitive Flexibility, Cognitive Timing & Predictive Modeling (minor)",L3
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon autonomy by placing an agent in a simulated year-long vending-machine business that requires sourcing inventory, setting prices, negotiating with suppliers, and adapting to market dynamics. The score reflects cumulative strategic choices across thousands of decisions, emphasizing coherence, long-term planning, and goal maintenance.","L1: 
L2: Planning, Decision-making, Reward Mechanisms, Working Memory, Episodic Memory
L3: Motivational Drives (minor), Self-reflection (minor)",L2
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas evaluates real-world tool use via the Model Context Protocol, requiring models to discover tools, call them with correct schemas, manage errors/retries, and integrate results across multi-step workflows. It emphasizes reliable agentic orchestration across heterogeneous services rather than single-call function execution.","L1: 
L2: Planning, Decision-making, Working Memory, Adaptive Error Correction, Semantic Understanding & Context Recognition
L3: Inhibitory Control (minor)",L2
FinanceAgent,https://www.vals.ai/benchmarks/finance_agent,,"FinanceAgent evaluates agents on tasks typical of an entry-level financial analyst, such as extracting information from documents, performing calculations, building analyses, and drafting investment or reporting outputs. It stresses domain-grounded reasoning and accurate synthesis across multiple sources and steps, often with structured outputs.","L1: Language Production (minor)
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Planning, Decision-making
L3: ",L2
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity capabilities by testing whether an agent can identify known vulnerabilities from descriptions and also discover previously unknown vulnerabilities in real open-source projects. It probes systematic debugging and adversarial thinking: reading code, hypothesizing failure modes, crafting exploits or proofs, and iterating based on feedback.","L1: 
L2: Logical Reasoning, Planning, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition
L3: Inhibitory Control (minor)",L2
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"SpreadsheetBench evaluates the ability to understand, navigate, and modify complex spreadsheets using realistic tasks (e.g., formulas, formatting, tables, and data transformations). It stresses precise multi-step manipulation and consistency across a structured artifact, where small mistakes can cascade into incorrect final outputs.","L1: 
L2: Working Memory, Planning, Decision-making, Logical Reasoning, Adaptive Error Correction (minor), Semantic Understanding & Context Recognition
L3: ",L2
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam (HLE) is a difficult, frontier-focused benchmark spanning advanced questions across many domains (often multimodal), intended to test broad expert-level reasoning and knowledge under realistic ambiguity. It emphasizes integrating evidence, performing multi-step inference, and producing justified answers when tasks exceed typical exam difficulty.","L1: Language Comprehension, Language Production, Visual Perception (minor)
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Multisensory Integration (minor), Working Memory
L3: ",L2
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 is a set of competition-style mathematics problems that require nontrivial, multi-step derivations and careful symbolic manipulation. It primarily tests deliberate mathematical reasoning and error-free intermediate steps under a compact answer format.","L1: Language Comprehension (minor)
L2: Logical Reasoning, Working Memory, Planning (minor), Attention (minor)
L3: ",L2
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a subset of challenging graduate-level multiple-choice science questions curated to be “Google-proof” and to separate experts from non-experts. It probes precise scientific reasoning, disambiguation of closely related concepts, and resisting superficial pattern-matching.","L1: Language Comprehension (minor)
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Decision-making (minor)
L3: ",L2
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends MMLU to multiple languages, testing broad academic knowledge and reasoning across many subjects in non-English settings. It emphasizes multilingual comprehension and the ability to apply the same concepts under varied linguistic and cultural surface forms.","L1: Language Comprehension, Language Production (minor)
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory (minor)
L3: Cognitive Flexibility",L3
MMMU,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2311.16502,"MMMU is a multimodal benchmark covering many disciplines where questions require reasoning over images (e.g., diagrams, plots, photos) together with text. It probes whether models can extract relevant visual evidence and integrate it with domain knowledge to answer expert-style questions.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Attention (minor), Semantic Understanding & Context Recognition (minor)
L3: ",L2
LAB-Bench FigQA,https://huggingface.co/datasets/futurehouse/lab-bench,https://arxiv.org/abs/2407.10362,"LAB-Bench FigQA evaluates whether models can correctly interpret and reason about complex scientific figures from biology papers, including plots, panels, and annotated visual elements. It stresses extracting quantitative/relational information from figures and mapping it to precise answers grounded in the visual evidence.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Attention, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory (minor)
L3: ",L2
WebArena,https://webarena.dev/,https://arxiv.org/abs/2307.13854,"WebArena evaluates autonomous web agents on realistic tasks across multiple web apps (e.g., shopping, CMS, forums, Git hosting, maps), requiring navigation, form filling, and multi-step completion under dynamic UIs. It probes end-to-end planning and grounded interaction, including recovering from mistakes and maintaining task state over long action sequences.","L1: Visual Perception
L2: Planning, Decision-making, Scene Understanding & Visual Reasoning, Sensorimotor Coordination, Working Memory
L3: Inhibitory Control (minor)",L2
