Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates software engineering agents by asking them to fix real GitHub issues by producing patches that make a repository’s tests pass. The “Verified” subset consists of tasks that have been validated as solvable and reliably graded, emphasizing end-to-end debugging and codebase navigation rather than isolated coding problems.","L1: Language Comprehension (minor)
L2: Planning, Logical Reasoning, Adaptive Error Correction, Working Memory, Decision-making (minor)
L3: ",L2
SWE-bench Multilingual,https://www.swebench.com/multilingual.html,https://arxiv.org/abs/2504.21798,"SWE-bench Multilingual extends SWE-bench-style issue resolution beyond Python to multiple programming languages and ecosystems. It tests whether an agent can interpret issues, understand unfamiliar project conventions, and apply correct edits across diverse tooling and language-specific patterns.","L1: 
L2: Planning, Logical Reasoning, Adaptive Error Correction, Semantic Understanding & Context Recognition, Working Memory (minor)
L3: Cognitive Flexibility",L3
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro is a larger and more difficult software engineering benchmark designed to be more industrially relevant and more resistant to contamination than earlier variants. It emphasizes solving realistic engineering tasks across multiple languages, requiring robust patch generation, testing, and iterative debugging under stricter evaluation conditions.","L1: Language Comprehension (minor)
L2: Planning, Decision-making, Logical Reasoning, Adaptive Error Correction, Working Memory
L3: ",L2
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 evaluates agents on real command-line workflows such as installing dependencies, manipulating files, running programs, and diagnosing failures in a shell environment. Success requires selecting appropriate commands, interpreting outputs/errors, and iterating until the task goal is satisfied.","L1: 
L2: Planning, Adaptive Error Correction, Decision-making, Attention (minor), Working Memory (minor), Motor Coordination (minor)
L3: ",L2
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp measures how well agents perform research-style question answering using web browsing and retrieval, often requiring multi-step search, source comparison, and synthesis. It focuses on whether an agent can find relevant information in noisy contexts and produce grounded, correct answers under tool-use constraints.","L1: Language Production (minor)
L2: Planning, Attention, Semantic Understanding & Context Recognition, Episodic Memory (minor), Logical Reasoning (minor)
L3: ",L2
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench evaluates interactive tool-using agents in customer-support-like environments (e.g., retail, airline, telecom) where the model must communicate with a simulated user and call APIs while following policies. It stresses consistent multi-turn behavior, correct API sequencing, and adherence to domain constraints despite user pressure or ambiguity.","L1: Language Comprehension (minor)
L2: Decision-making, Planning
L3: Social Reasoning & Theory of Mind, Inhibitory Control, Empathy (minor)",L3
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld benchmarks multimodal computer-use agents on tasks performed inside an operating-system-like desktop environment, requiring interaction with GUIs over many steps. It tests whether an agent can perceive screen state, navigate applications, and execute correct sequences of actions to accomplish practical goals.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Planning, Decision-making, Visual Attention & Eye Movements (minor), Sensorimotor Coordination (minor)
L3: ",L2
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,ARC-AGI evaluates “fluid intelligence” through abstract grid-based tasks where models infer a transformation rule from a few input–output examples and apply it to a new input. The benchmark emphasizes generalization to novel patterns and compositional reasoning rather than domain knowledge.,"L1: 
L2: Logical Reasoning, Working Memory, Spatial Representation & Mapping, Attention (minor)
L3: Cognitive Flexibility",L3
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon agent coherence by simulating the operation of a vending-machine business over extended time, where the agent must manage inventory, pricing, suppliers, and cashflow. High performance requires sustained strategy, adapting to changing conditions, and avoiding compounding mistakes across many decisions.","L1: 
L2: Planning, Decision-making, Reward Mechanisms, Working Memory (minor), Episodic Memory (minor)
L3: Self-reflection (minor)",L2
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas measures tool-use competence via the Model Context Protocol, requiring models to discover and invoke real tools/APIs, chain multi-step calls, handle failures, and synthesize outputs into correct final responses. It focuses on practical orchestration and robustness in production-like tool ecosystems.","L1: Language Comprehension (minor), Language Production (minor)
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory (minor)
L3: ",L2
FinanceAgent,https://www.vals.ai/benchmarks/finance_agent,,"FinanceAgent evaluates agents on tasks typical of entry-level financial analysts, such as extracting information from documents, performing structured calculations, building assumptions, and producing justified conclusions. It emphasizes domain-grounded reasoning, consistency with financial constraints, and producing usable professional artifacts.","L1: Language Production (minor)
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Planning (minor), Decision-making (minor), Working Memory (minor)
L3: ",L2
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity agents on tasks involving locating known vulnerabilities and discovering new ones in real open-source codebases. It tests the ability to interpret vulnerability descriptions, analyze code, reproduce conditions, and propose fixes or exploitation-relevant findings under realistic constraints.","L1: 
L2: Logical Reasoning, Planning, Adaptive Error Correction, Attention (minor), Working Memory (minor)
L3: Inhibitory Control (minor)",L2
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"SpreadsheetBench measures an agent’s ability to navigate, edit, and compute within complex spreadsheets, often mirroring business workflows with formulas, tables, and formatting constraints. It stresses structured manipulation, error diagnosis, and producing correct spreadsheet states rather than just explaining what to do.","L1: 
L2: Working Memory, Planning, Attention, Logical Reasoning (minor), Adaptive Error Correction (minor), Motor Coordination (minor)
L3: ",L2
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a challenging multimodal benchmark spanning questions at the frontier of human knowledge, often requiring careful reasoning and integration of evidence across modalities. It is designed to stress general problem solving, scientific/technical understanding, and robustness beyond memorized facts.","L1: Language Comprehension
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Scene Understanding & Visual Reasoning (minor), Multisensory Integration (minor), Working Memory (minor)
L3: ",L2
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,AIME 2025 evaluates mathematical reasoning on competition-style problems that require multi-step derivations and precise final answers. It emphasizes symbolic manipulation and strategy selection under tight correctness criteria rather than tool-heavy workflows.,"L1: 
L2: Logical Reasoning, Working Memory, Planning (minor), Attention (minor)
L3: ",L2
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a subset of graduate-level, “Google-proof” multiple-choice science questions chosen for high quality and difficulty. It targets deep scientific reasoning and disambiguation, where superficial pattern matching or memorized trivia is insufficient to reliably select the correct option.","L1: Language Comprehension (minor)
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory (minor), Decision-making (minor)
L3: ",L2
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends broad academic evaluation to multiple languages, probing whether models retain knowledge and reasoning competence across diverse linguistic contexts. It stresses cross-lingual generalization, instruction following in different languages, and consistent subject-matter understanding.","L1: Language Comprehension, Language Production (minor)
L2: Semantic Understanding & Context Recognition, Logical Reasoning (minor)
L3: Cognitive Flexibility",L3
MMMU,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2311.16502,"MMMU evaluates multimodal understanding across many disciplines using problems that require combining text with images such as charts, diagrams, and figures. It probes whether models can ground language in visual evidence and perform reasoning that depends on visual details.","L1: Visual Perception, Language Comprehension (minor)
L2: Scene Understanding & Visual Reasoning, Multisensory Integration, Working Memory (minor), Attention (minor)
L3: ",L2
LAB-Bench FigQA,https://huggingface.co/datasets/futurehouse/lab-bench,https://arxiv.org/abs/2407.10362,"LAB-Bench FigQA focuses on scientific figure question answering from biology papers, requiring careful interpretation of plots, labels, and experimental results presented visually. It emphasizes precise visual-to-text grounding and domain-relevant reasoning about what the figure implies.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Semantic Understanding & Context Recognition, Attention (minor), Logical Reasoning (minor), Multisensory Integration (minor)
L3: ",L2
WebArena,https://webarena.dev/,https://arxiv.org/abs/2307.13854,"WebArena evaluates autonomous web agents on realistic tasks across multiple web apps (e-commerce, CMS, forums, collaboration tools) requiring navigation, form filling, and multi-step workflows. It stresses robust interaction with dynamic interfaces, long-horizon task execution, and recovery from partial failures.","L1: Visual Perception
L2: Planning, Decision-making, Scene Understanding & Visual Reasoning, Adaptive Error Correction, Working Memory (minor), Visual Attention & Eye Movements (minor)
L3: ",L2
