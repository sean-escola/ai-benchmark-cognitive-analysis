Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,SWE-bench Verified evaluates whether a model can solve real-world GitHub issues by producing a code patch that makes the project’s tests pass. The “Verified” subset uses tasks that have been validated by humans to be solvable and aims to reduce ambiguity in problem statements and evaluation.,"L1: Language Comprehension (minor)
L2: Planning, Logical Reasoning, Adaptive Error Correction, Working Memory, Decision-making (minor)
L3: ",L2
SWE-bench Multilingual,https://www.swebench.com/multilingual.html,https://arxiv.org/abs/2504.21798,SWE-bench Multilingual extends SWE-bench-style software engineering tasks beyond Python to multiple programming languages. It tests whether models can understand issue reports and repository context and then generate correct patches in the appropriate language ecosystems and tooling conventions.,"L1: Language Comprehension, Language Production
L2: Planning, Adaptive Error Correction, Working Memory (minor)
L3: Cognitive Flexibility",L3
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro is a larger, harder software engineering benchmark designed to be more challenging and contamination-resistant than prior SWE-bench variants. It emphasizes realistic, multi-language codebase modifications that must satisfy test suites and repository constraints.","L1: 
L2: Planning, Logical Reasoning, Adaptive Error Correction, Working Memory, Decision-making (minor)
L3: Cognitive Flexibility (minor)",L2
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 evaluates agentic performance in command-line environments, where models must execute sequences of shell commands and file edits to accomplish tasks. It stresses robustness to tool errors and the ability to iteratively diagnose and fix problems under realistic environment constraints.","L1: Language Comprehension (minor)
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory
L3: Inhibitory Control (minor)",L2
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp evaluates “deep research” behavior: models must search a fixed web-like corpus and synthesize answers that require multiple evidence-gathering steps. It targets long-horizon information seeking, source integration, and answer verification rather than pure closed-book recall.","L1: 
L2: Planning, Semantic Understanding & Context Recognition, Decision-making, Working Memory, Episodic Memory (minor), Logical Reasoning (minor)
L3: ",L2
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench measures how well an agent can complete multi-turn customer-support-style tasks by interacting with simulated users and APIs while adhering to domain policies. It probes reliable tool use within constraints, maintaining dialogue state, and resisting actions that violate policy even when user pressure is present.","L1: 
L2: Decision-making, Planning, Working Memory (minor)
L3: Social Reasoning & Theory of Mind, Inhibitory Control, Empathy (minor)",L3
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld evaluates multimodal computer-use agents on real OS tasks (e.g., using apps, settings, browsers) with screenshots and interactive actions. Success requires perceiving UI state, selecting correct action sequences, and recovering from UI and environment variability across many steps.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Planning, Decision-making, Attention (minor), Sensorimotor Coordination (minor)
L3: ",L2
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI measures fluid, abstract reasoning from a few demonstrations: given a small set of grid input-output examples, the model must infer the underlying transformation and apply it to new inputs. It is designed to emphasize generalization to novel rules over memorization of domain knowledge.","L1: Visual Perception (minor)
L2: Logical Reasoning, Working Memory, Spatial Representation & Mapping, Planning (minor)
L3: Cognitive Flexibility",L3
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon autonomous agent performance in a year-long simulated business management setting (inventory, pricing, supplier negotiation, budgeting). It emphasizes sustained coherence, strategy under delayed outcomes, and iterative adaptation to market dynamics over thousands of decisions.","L1: 
L2: Planning, Decision-making, Reward Mechanisms, Working Memory, Adaptive Error Correction (minor)
L3: Motivational Drives (minor)",L2
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas evaluates real-world tool use via the Model Context Protocol (MCP), where models must discover and correctly call tools across multi-step workflows and integrate tool outputs. It focuses on correct tool selection, argument formation, error handling, and synthesis across heterogeneous APIs.","L1: Language Comprehension (minor)
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory (minor)
L3: ",L2
FinanceAgent,https://www.vals.ai/benchmarks/finance_agent,,"FinanceAgent tests agent performance on tasks typical of an entry-level financial analyst, often requiring multi-step analysis, structured outputs, and careful handling of constraints and assumptions. It emphasizes applied reasoning over financial concepts and producing decision-ready artifacts from messy inputs.","L1: 
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Planning, Decision-making, Working Memory (minor), Adaptive Error Correction (minor)
L3: ",L2
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity agent capabilities at scale, including finding known vulnerabilities in real open-source projects and discovering new issues. It stresses systematic investigation, hypothesis-driven debugging, and careful validation under realistic code and tooling constraints.","L1: 
L2: Logical Reasoning, Planning, Adaptive Error Correction, Working Memory
L3: Inhibitory Control (minor)",L2
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"SpreadsheetBench evaluates the ability to understand, navigate, and modify complex spreadsheets drawn from realistic tasks. It probes structured manipulation (formulas, tables, formatting) and multi-step workflows where intermediate states must remain consistent to achieve a correct final artifact.","L1: 
L2: Working Memory, Planning, Decision-making, Logical Reasoning, Adaptive Error Correction (minor)
L3: ",L2
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a difficult multimodal benchmark intended to sit near the frontier of human knowledge, spanning many domains and requiring careful reasoning and synthesis. Questions often demand integrating textual and visual information and avoiding superficial pattern-matching by using deep conceptual understanding.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Scene Understanding & Visual Reasoning (minor), Working Memory (minor)
L3: ",L2
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 evaluates competition-style mathematical problem solving, typically requiring multi-step derivations and exact final numeric answers. It emphasizes symbolic reasoning, constraint tracking, and the ability to carry intermediate results without losing correctness.","L1: 
L2: Logical Reasoning, Working Memory, Planning (minor), Attention (minor)
L3: ",L2
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,GPQA Diamond is a subset of especially challenging graduate-level science multiple-choice questions designed to be difficult to solve via simple web search or shallow recall. It targets deep domain understanding and multi-step reasoning under tightly specified answer options.,"L1: Language Comprehension (minor)
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory (minor)
L3: ",L2
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,MMMLU extends broad academic knowledge-and-reasoning evaluation across many subjects and multiple languages. It probes whether models can understand questions and answer choices across languages while retaining the underlying conceptual and test-taking reasoning required by MMLU-style tasks.,"L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning (minor), Working Memory (minor)
L3: Cognitive Flexibility (minor)",L2
MMMU,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2311.16502,"MMMU evaluates multimodal expert-level understanding by requiring models to answer questions that combine images (e.g., charts, diagrams) with text across many disciplines. It emphasizes interpreting visual information, integrating it with textual context, and performing multi-step reasoning to reach the correct choice.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Working Memory (minor), Attention (minor)
L3: ",L2
LAB-Bench FigQA,https://huggingface.co/datasets/futurehouse/lab-bench,https://arxiv.org/abs/2407.10362,"LAB-Bench FigQA tests whether models can correctly interpret and reason over complex scientific figures from biology papers, including plots and multi-panel graphics. It targets precise extraction of visually grounded evidence and translating it into correct, context-sensitive scientific answers.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Multisensory Integration, Semantic Understanding & Context Recognition, Logical Reasoning (minor), Attention (minor)
L3: ",L2
WebArena,https://webarena.dev/,https://arxiv.org/abs/2307.13854,"WebArena evaluates autonomous web agents completing realistic tasks across multiple web apps (e-commerce, CMS, forums, code hosting, maps) using interactive browsing. It stresses planning and decision-making over long action sequences, robust UI understanding from screenshots/DOM signals, and iterative recovery from mistakes.","L1: Visual Perception (minor)
L2: Planning, Decision-making, Adaptive Error Correction, Scene Understanding & Visual Reasoning, Attention (minor), Sensorimotor Coordination (minor)
L3: ",L2
