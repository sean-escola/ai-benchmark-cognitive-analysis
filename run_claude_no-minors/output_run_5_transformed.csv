Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates agentic software engineering by having a model generate patches for real GitHub issues, validated by unit tests in a standardized harness. The “Verified” subset focuses on problems confirmed solvable and aims to better measure end-to-end bug fixing (understanding code, editing, and test-driven validation) under realistic constraints.","L1: Language Comprehension, Language Production
L2: Logical Reasoning, Planning, Working Memory, Adaptive Error Correction
L3: ",L2
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 measures how well models can complete real command-line tasks in a sandboxed environment, such as file manipulation, dependency setup, debugging, and scripting. Success depends on choosing correct shell actions, iterating from feedback, and maintaining state across multi-step workflows.","L1: Language Comprehension (minor), Language Production (minor)
L2: Planning, Decision-making, Working Memory, Adaptive Error Correction
L3: ",L2
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp evaluates deep research and browsing agents by testing whether they can find and synthesize answers that require searching, reading, and integrating information across multiple sources. It emphasizes tool-augmented reasoning under evidence constraints and penalizes unsupported conclusions.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Attention, Working Memory, Semantic Understanding & Context Recognition
L3: Self-reflection (minor)",L2
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench assesses interactive customer-support agents that must follow domain policies while using tools/APIs in multi-turn conversations with simulated users. It stresses robust policy adherence, correct tool invocation, and coherent dialogue management over long trajectories.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Working Memory, Reward Mechanisms (minor)
L3: Social Reasoning & Theory of Mind, Inhibitory Control",L3
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld evaluates multimodal computer-use agents on realistic desktop tasks (e.g., web apps, file operations, forms, and productivity tools) via screenshots and action APIs. It measures an agent’s ability to perceive UI state, plan multi-step interactions, recover from errors, and complete goals within step limits.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements (minor), Planning, Decision-making, Working Memory, Adaptive Error Correction, Sensorimotor Coordination (minor)
L3: ",L2
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI tests fluid, few-shot abstract reasoning using small grid-based puzzles where models infer transformation rules from a handful of examples. It is designed to reduce reliance on memorized knowledge and emphasize systematic generalization and compositional rule induction.","L1: Visual Perception (minor)
L2: Logical Reasoning, Working Memory, Spatial Representation & Mapping, Scene Understanding & Visual Reasoning, Planning (minor)
L3: Cognitive Flexibility",L3
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon agent coherence by simulating operation of a vending-machine business over extended time, requiring repeated decisions about purchasing, pricing, inventory, and negotiation. High scores require consistent strategy, adaptation to feedback, and avoidance of compounding mistakes across many steps.","L1: 
L2: Planning, Decision-making, Working Memory, Reward Mechanisms, Adaptive Error Correction
L3: Motivational Drives (minor), Social Reasoning & Theory of Mind (minor)",L2
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas benchmarks real-world tool use over the Model Context Protocol by requiring models to discover tools, call them with correct schemas, handle failures, and synthesize results into correct final outputs. It targets multi-step orchestration across heterogeneous, production-like tool servers rather than single-call function use.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Working Memory, Adaptive Error Correction, Semantic Understanding & Context Recognition (minor)
L3: ",L2
FinanceAgent,https://www.vals.ai/benchmarks/finance_agent,,"FinanceAgent evaluates whether a model can perform tasks typical of an entry-level financial analyst, such as extracting figures, building analyses, and producing decision-relevant summaries. Tasks often require structured reasoning, careful numerical handling, and tool-augmented workflows with documents or spreadsheets.","L1: Language Production (minor)
L2: Logical Reasoning, Planning, Decision-making, Working Memory, Semantic Understanding & Context Recognition
L3: ",L2
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym measures agentic cybersecurity capabilities, including finding known vulnerabilities from descriptions and discovering new issues in real open-source codebases at scale. Success requires multi-step reasoning about programs, experimenting with hypotheses, and iterating based on test or exploit feedback.","L1: 
L2: Logical Reasoning, Planning, Decision-making, Adaptive Error Correction, Working Memory
L3: Inhibitory Control (minor)",L2
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"SpreadsheetBench evaluates an agent’s ability to read, modify, and compute over complex spreadsheets derived from realistic tasks, often requiring multi-step operations and correct formulas. It stresses structured manipulation, maintaining constraints (formats/links), and verifying results under tool-based execution.","L1: 
L2: Planning, Working Memory, Logical Reasoning, Adaptive Error Correction, Decision-making (minor)
L3: ",L2
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam (HLE) is a large, difficult set of expert-level questions intended to probe frontier knowledge and reasoning, including multimodal items. It rewards accurate, well-grounded answers across diverse domains and tends to be sensitive to tool use, verification, and careful synthesis.","L1: Language Comprehension, Visual Perception (minor)
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory (minor), Scene Understanding & Visual Reasoning (minor)
L3: ",L2
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,AIME 2025 comprises competition-style mathematics problems that require multi-step symbolic reasoning under time-like constraints and minimal context. It primarily measures precise mathematical problem solving and robustness against small reasoning errors without relying on external knowledge.,"L1: 
L2: Logical Reasoning, Working Memory, Planning (minor), Adaptive Error Correction (minor)
L3: ",L2
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a high-quality subset of graduate-level, “Google-proof” multiple-choice science questions designed to be difficult for non-experts while answerable by experts. It measures deep scientific reasoning and careful discrimination among plausible distractors.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory (minor)
L3: Inhibitory Control (minor)",L2
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends MMLU-style academic questions across many subjects and multiple languages to evaluate multilingual knowledge and reasoning. It probes whether models preserve competence when understanding and answering outside English, including culturally and linguistically varied phrasing.","L1: Language Comprehension, Language Production
L2: Semantic Understanding & Context Recognition, Logical Reasoning (minor), Working Memory (minor)
L3: ",L2
MMMU,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2311.16502,"MMMU is a multimodal benchmark spanning many disciplines where models must answer questions requiring joint reasoning over images (diagrams, plots, screenshots) and text. It stresses visual understanding, cross-modal grounding, and multi-step inference rather than pure text recall.","L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Working Memory (minor), Attention (minor)
L3: ",L2
LAB-Bench FigQA,https://huggingface.co/datasets/futurehouse/lab-bench,https://arxiv.org/abs/2407.10362,"LAB-Bench FigQA evaluates scientific figure understanding in biology papers by asking questions that require reading complex plots, annotations, and experimental schematics. It targets research-relevant visual reasoning, careful interpretation of evidence, and avoidance of overconfident extrapolation from figures.","L1: Visual Perception, Language Comprehension (minor)
L2: Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning
L3: Inhibitory Control (minor)",L2
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning focuses on reasoning over charts and figures from scientific documents, requiring models to extract quantitative/relational information and answer targeted questions. It emphasizes accurate visual parsing of axes/legends and logically consistent, numerically grounded explanations.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Logical Reasoning, Working Memory (minor), Attention (minor)
L3: ",L2
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU extends multimodal evaluation to temporal video understanding, requiring models to integrate information across frames and align it with textual questions. It probes temporal coherence, event understanding, and cross-modal reasoning under long-context video inputs.","L1: Visual Perception, Language Comprehension (minor)
L2: Scene Understanding & Visual Reasoning, Multisensory Integration, Attention, Working Memory
L3: Cognitive Timing & Predictive Modeling (minor)",L2
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"The FACTS Benchmark Suite systematically evaluates factuality by testing whether model outputs are supported by provided sources or widely accepted references, and by measuring different failure modes (hallucination, misattribution, inconsistency). It emphasizes reliable grounding, calibration, and resisting the urge to invent details when evidence is insufficient.","L1: 
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory (minor)
L3: Inhibitory Control, Self-reflection (minor)",L3
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA evaluates physical commonsense reasoning across languages and locales, focusing on how actions and objects interact in everyday situations. It measures robust understanding of practical causality and whether models generalize such reasoning beyond English-centric phrasing.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory (minor)
L3: Cognitive Flexibility (minor)",L2
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,MRCR v2 (8-needle) tests long-context multi-round co-reference and retrieval by inserting multiple similar “needle” requests into lengthy conversations and asking for the correct associated response. It measures whether a model can maintain and retrieve precise referents amid interference and distractors over very long contexts.,"L1: Language Comprehension (minor)
L2: Working Memory, Attention, Episodic Memory
L3: Inhibitory Control (minor)",L2
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval evaluates economically relevant knowledge work by comparing model-produced professional artifacts (e.g., presentations, schedules, analyses) against outputs from industry professionals under well-specified tasks. It targets end-to-end project execution quality, including following constraints, producing usable deliverables, and making sound judgments.","L1: Language Production
L2: Planning, Decision-making, Semantic Understanding & Context Recognition, Working Memory (minor)
L3: Social Reasoning & Theory of Mind (minor), Self-reflection (minor)",L2
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks evaluates reasoning over graph-structured problems expressed in text, such as traversals, reachability, and parent/neighbor queries across many steps. It probes whether models can maintain consistent state and perform systematic multi-hop inference rather than shortcutting with surface heuristics.","L1: 
L2: Spatial Representation & Mapping, Working Memory, Logical Reasoning, Attention (minor), Planning (minor)
L3: ",L2
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon benchmarks general tool-use competency across diverse tools and tasks, stressing correct tool selection, parameterization, and integration of tool outputs into final answers. It is designed to measure reliability under multi-step workflows, including recovery from tool errors and ambiguous intermediate results.","L1: Language Comprehension (minor), Language Production (minor)
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory
L3: ",L2
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath is a hard set of expert-level mathematics problems intended to measure genuine mathematical progress and reduce gains from memorization through careful curation and release practices. It emphasizes long chains of correct reasoning, precise abstractions, and robustness against subtle logical slips that derail proofs or computations.","L1: 
L2: Logical Reasoning, Working Memory, Planning (minor), Adaptive Error Correction (minor)
L3: ",L2
