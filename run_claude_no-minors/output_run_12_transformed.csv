Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates LLMs as software engineering agents by giving real GitHub issue reports (plus repository context) and scoring whether the model produces a correct code patch that makes the project’s tests pass. The “Verified” subset contains tasks that have been checked by humans to be solvable and well-specified, emphasizing reliable end-to-end bug fixing and feature implementation.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Logical Reasoning, Working Memory, Adaptive Error Correction, Semantic Understanding & Context Recognition
L3: ",L2
SWE-bench Multilingual,https://www.swebench.com/multilingual.html,https://arxiv.org/abs/2504.21798,"SWE-bench Multilingual extends the SWE-bench paradigm beyond Python, measuring whether models can resolve real-world issues across multiple programming languages and ecosystems. It stresses cross-language generalization in debugging, patch construction, and aligning changes to project-specific conventions and tests.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Logical Reasoning, Working Memory, Adaptive Error Correction, Semantic Understanding & Context Recognition
L3: Cognitive Flexibility",L3
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro is a harder, larger software-engineering benchmark designed to be more realistic and contamination-resistant, requiring models to make nontrivial edits across repositories and languages. It targets robustness under ambiguity and longer dependency chains, where small design choices can determine whether integration tests succeed.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Logical Reasoning, Working Memory, Adaptive Error Correction, Semantic Understanding & Context Recognition
L3: Cognitive Flexibility (minor)",L2
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 evaluates agentic performance in command-line environments where models must use shell tools to complete real tasks (e.g., debugging, data wrangling, building, testing) under execution constraints. Success depends on iteratively forming hypotheses, running commands, inspecting outputs, and correcting course until the objective is met.","L1: Language Comprehension (minor), Language Production (minor)
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory, Logical Reasoning
L3: ",L2
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp tests “deep research” style browsing: models must search a fixed or controlled web corpus/index, gather evidence, and synthesize answers to information-seeking questions. It measures the ability to plan search strategies, integrate scattered sources, and maintain correctness despite noisy or misleading documents.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Attention, Episodic Memory (minor), Working Memory, Semantic Understanding & Context Recognition, Logical Reasoning
L3: ",L2
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench evaluates interactive tool-using agents in customer-service style domains (e.g., retail, airline, telecom) with multi-turn simulated users and policy constraints. Models must decide what to ask, what tools/APIs to call, and how to stay within rules while resolving cases end-to-end.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Working Memory, Reward Mechanisms (minor)
L3: Social Reasoning & Theory of Mind, Inhibitory Control, Empathy (minor)",L3
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld benchmarks multimodal “computer use” in realistic operating-system environments, where models must complete tasks by interacting with GUIs and applications within step limits. It emphasizes robust perception of screens, mapping intent to interface actions, and recovering from misclicks or unexpected UI states.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements (minor), Sensorimotor Coordination, Planning, Decision-making, Adaptive Error Correction, Working Memory
L3: ",L2
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI measures fluid reasoning on novel grid-transformation problems with only a few input–output examples, aiming to reduce reliance on memorized knowledge. Models must infer latent rules and generalize them to new instances, penalizing brittle pattern-matching.","L1: 
L2: Logical Reasoning, Spatial Representation & Mapping, Working Memory, Attention
L3: Cognitive Flexibility, Cognitive Timing & Predictive Modeling (minor)",L3
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon autonomous agency in a simulated business setting (running a vending machine operation over an extended period). Models must manage inventory, pricing, supplier interactions, and budget under delayed consequences, testing sustained coherence and strategy.","L1: 
L2: Planning, Decision-making, Reward Mechanisms, Working Memory, Episodic Memory (minor), Adaptive Error Correction (minor)
L3: Self-reflection (minor)",L2
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas measures real-world tool use via the Model Context Protocol (MCP), where models must discover appropriate tools, invoke them with correct schemas, and compose multi-step workflows across services. It focuses on reliability under tool errors, retries, and dependency between calls.","L1: Language Comprehension (minor), Language Production (minor)
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition
L3: ",L2
FinanceAgent,https://www.vals.ai/benchmarks/finance_agent,,"FinanceAgent evaluates agentic performance on tasks representative of an entry-level financial analyst, such as extracting facts from documents, doing calculations, building analyses, and producing well-supported outputs. It stresses disciplined reasoning with domain constraints (assumptions, formatting, and consistency) rather than only recall.","L1: Language Production (minor)
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Planning, Decision-making, Working Memory
L3: ",L2
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity capabilities by asking agents to identify, reproduce, and/or discover vulnerabilities in real open-source codebases at scale. It requires iterating between code inspection, hypothesis formation, exploit reasoning, and validation under a pass@1 setting.","L1: 
L2: Logical Reasoning, Planning, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition
L3: Inhibitory Control (minor)",L2
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"SpreadsheetBench measures how well models can navigate and manipulate complex spreadsheets drawn from realistic tasks, often requiring multi-step editing and computation. It emphasizes structured state updates (cells, formulas, references) and catching cascading errors introduced by earlier edits.","L1: Visual Perception (minor), Language Comprehension (minor)
L2: Planning, Decision-making, Working Memory, Logical Reasoning, Adaptive Error Correction
L3: ",L2
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a difficult, frontier-knowledge benchmark spanning many domains and including multimodal questions, intended to probe near-expert reasoning under limited attempts. It measures how well models integrate specialized knowledge with careful reasoning and, when enabled, tool-mediated retrieval and computation.","L1: Language Comprehension, Language Production, Visual Perception (minor)
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Scene Understanding & Visual Reasoning (minor), Multisensory Integration (minor)
L3: ",L2
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 consists of competition-style mathematics problems that require multi-step derivations, algebraic manipulation, and precise final answers. It probes disciplined symbolic reasoning and the ability to maintain intermediate states without drifting or hallucinating steps.","L1: Language Comprehension (minor)
L2: Logical Reasoning, Working Memory, Planning, Attention (minor)
L3: ",L2
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a curated subset of very hard graduate-level science multiple-choice questions designed to be “Google-proof,” emphasizing reasoning over superficial retrieval. The Diamond split targets high-quality items where experts reliably answer correctly and non-experts struggle, making it sensitive to deep scientific understanding.","L1: Language Comprehension, Language Production (minor)
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Decision-making
L3: ",L2
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends the MMLU-style academic knowledge and reasoning evaluation to many languages, covering diverse subjects and testing cross-lingual robustness. It emphasizes comprehension of domain questions in different linguistic forms and consistent selection/production of correct answers across languages.","L1: Language Comprehension, Language Production (minor)
L2: Semantic Understanding & Context Recognition, Logical Reasoning (minor), Working Memory (minor)
L3: ",L2
MMMU,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2311.16502,"MMMU evaluates expert-level multimodal understanding and reasoning across many disciplines using paired text-and-image problems (e.g., diagrams, charts, figures). It stresses aligning visual evidence with textual constraints and performing multi-step reasoning grounded in the image content.","L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Working Memory (minor), Attention (minor)
L3: ",L2
LAB-Bench FigQA,https://huggingface.co/datasets/futurehouse/lab-bench,https://arxiv.org/abs/2407.10362,"LAB-Bench FigQA tests whether models can correctly interpret and reason about scientific figures from biology papers, including dense plots and multi-panel diagrams. It targets grounded scientific visual reasoning—extracting signals from the figure and connecting them to domain concepts and questions.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Semantic Understanding & Context Recognition, Logical Reasoning, Attention, Multisensory Integration (minor), Working Memory (minor)
L3: ",L2
WebArena,https://webarena.dev/,https://arxiv.org/abs/2307.13854,"WebArena evaluates autonomous web agents completing realistic tasks across multiple web apps (e-commerce, CMS, forums, code hosting) in a controlled environment with an official grader. Models must navigate dynamic pages, fill forms, follow multi-step objectives, and recover from partial failures while staying on task.","L1: Visual Perception
L2: Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Sensorimotor Coordination, Planning, Decision-making, Working Memory, Adaptive Error Correction
L3: Inhibitory Control (minor)",L2
