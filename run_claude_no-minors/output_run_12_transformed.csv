Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates software engineering agents on real GitHub issues by requiring them to produce a patch that makes a failing test suite pass. The “Verified” subset uses tasks that have been human-checked for solvability and robust evaluation, emphasizing end-to-end debugging and code changes rather than isolated code generation.","L1: Language Comprehension (minor), Language Production (minor)
L2: Planning, Logical Reasoning, Adaptive Error Correction, Working Memory
L3: ",L2
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 measures an agent’s ability to complete realistic tasks inside a command-line environment (e.g., navigating files, installing dependencies, running programs, and diagnosing failures). It stresses iterative experimentation and recovery from errors under tool constraints typical of developer workflows.","L1: Language Comprehension (minor)
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory
L3: ",L2
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp evaluates browsing-and-research agents on questions that require finding and synthesizing information from the web, typically through multiple search and reading steps. It targets reliability in information acquisition and synthesis, including managing context as evidence accumulates.","L1: Language Comprehension, Language Production (minor)
L2: Planning, Decision-making, Semantic Understanding & Context Recognition, Working Memory
L3: ",L2
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench evaluates interactive customer-support agents that must use tools/APIs and follow domain policies across multi-turn dialogues (e.g., retail, airline, telecom). It stresses consistent policy adherence, conversational grounding, and robust tool-mediated task completion over extended interactions.","L1: Language Comprehension, Language Production
L2: Decision-making, Planning
L3: Social Reasoning & Theory of Mind, Inhibitory Control, Empathy (minor)",L3
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld tests multimodal computer-use agents on tasks performed in a full desktop operating system, requiring screenshot understanding, UI navigation, and multi-step actions. Success depends on correctly perceiving UI state, choosing actions, and handling unexpected interface outcomes or errors.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Sensorimotor Coordination, Planning, Adaptive Error Correction (minor), Decision-making
L3: ",L2
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,ARC-AGI measures “fluid” abstraction and generalization on grid-based puzzles where models infer latent rules from a few input–output examples. It emphasizes novel pattern induction and transfer to unseen tasks with minimal data rather than memorized domain knowledge.,"L1: 
L2: Logical Reasoning, Spatial Representation & Mapping, Working Memory, Scene Understanding & Visual Reasoning (minor)
L3: Cognitive Flexibility",L3
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon agent coherence by simulating management of a vending-machine business over many decisions (inventory, pricing, supplier negotiation, and adaptation). Performance reflects sustained strategy, error recovery, and consistent goal pursuit over extended time.","L1: 
L2: Planning, Decision-making, Working Memory, Adaptive Error Correction, Reward Mechanisms (minor)
L3: Motivational Drives (minor)",L2
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas evaluates real-world tool use via the Model Context Protocol, requiring models to discover tools, call APIs correctly, and chain multiple calls to complete workflows. It emphasizes robust orchestration, handling tool errors/retries, and integrating tool outputs into final answers.","L1: Language Comprehension (minor), Language Production (minor)
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory
L3: ",L2
FinanceAgent,https://www.vals.ai/benchmarks/finance_agent,,"FinanceAgent assesses agent performance on tasks typical of an entry-level financial analyst, such as interpreting filings, building analyses, and producing grounded financial outputs. It stresses domain reasoning, careful interpretation of constraints, and producing structured deliverables.","L1: Language Production (minor)
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Planning, Decision-making, Working Memory
L3: ",L2
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity agent capabilities on large suites of tasks including vulnerability identification in real software and, in some settings, discovery of previously unknown vulnerabilities. It stresses systematic investigation, hypothesis testing, and iterative correction when exploitation attempts fail.","L1: 
L2: Logical Reasoning, Planning, Adaptive Error Correction, Decision-making, Working Memory (minor), Semantic Understanding & Context Recognition (minor)
L3: ",L2
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"SpreadsheetBench measures an agent’s ability to understand, edit, and compute with complex spreadsheets (often using programmatic or office-like tooling). It targets multi-step transformation, formula reasoning, and maintaining consistency across a structured artifact.","L1: 
L2: Logical Reasoning, Planning, Working Memory, Adaptive Error Correction (minor), Decision-making (minor)
L3: ",L2
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam (HLE) is a multimodal benchmark intended to probe frontier academic and expert-level knowledge and reasoning across many domains. It includes questions where success often requires careful synthesis, multi-step reasoning, and (in tool-enabled settings) controlled information retrieval.","L1: Language Comprehension, Visual Perception (minor)
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory (minor), Scene Understanding & Visual Reasoning (minor)
L3: ",L2
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 evaluates competition-style mathematics problem solving, typically requiring clever multi-step derivations and accurate symbolic manipulation. It favors robust reasoning over rote computation, with answers usually requiring exact final results.","L1: 
L2: Logical Reasoning, Working Memory, Planning (minor), Adaptive Error Correction (minor)
L3: ",L2
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a subset of exceptionally difficult, “Google-proof” graduate-level science multiple-choice questions curated for quality. It targets deep scientific reasoning and the ability to avoid superficial pattern matching in favor of coherent inference under uncertainty.","L1: 
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory (minor), Decision-making (minor)
L3: ",L2
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends MMLU-style subject tests to multiple languages, assessing broad academic knowledge and reasoning beyond English. It emphasizes multilingual comprehension and consistent reasoning across varied linguistic contexts.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning (minor), Working Memory (minor)
L3: ",L2
MMMU,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2311.16502,"MMMU evaluates multimodal understanding across many disciplines, requiring models to answer questions grounded in images (diagrams, charts, screenshots) plus text. It stresses integrating visual evidence with language to perform domain reasoning.","L1: Visual Perception, Language Comprehension
L2: Multisensory Integration, Scene Understanding & Visual Reasoning, Logical Reasoning (minor), Attention (minor)
L3: ",L2
LAB-Bench FigQA,https://huggingface.co/datasets/futurehouse/lab-bench,https://arxiv.org/abs/2407.10362,"LAB-Bench FigQA tests whether models can interpret and reason about complex scientific figures from biology papers (e.g., plots, microscopy panels, multi-part diagrams). It emphasizes extracting the right evidence from figures and applying domain knowledge to answer questions correctly.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Semantic Understanding & Context Recognition, Logical Reasoning, Attention (minor)
L3: ",L2
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning focuses on reasoning over scientific figures and chart-like visual artifacts associated with arXiv-style papers, where answers require interpreting quantitative/structural information from visuals. It targets chart/figure literacy combined with multi-step inference and explanation generation.","L1: Visual Perception, Language Production (minor)
L2: Scene Understanding & Visual Reasoning, Logical Reasoning, Semantic Understanding & Context Recognition (minor), Working Memory (minor)
L3: ",L2
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU evaluates multimodal reasoning over videos, requiring temporal integration of visual information to answer questions about events, actions, and evolving scenes. It stresses maintaining coherence across frames and using temporal cues to infer correct answers.","L1: Visual Perception, Language Comprehension (minor), Auditory Processing (minor)
L2: Scene Understanding & Visual Reasoning, Attention, Working Memory
L3: Cognitive Timing & Predictive Modeling",L3
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"The FACTS Benchmark Suite systematically evaluates factuality, including whether models produce statements that are supported by evidence and whether they avoid hallucinations across varied setups. It targets reliability mechanisms such as grounding, calibration, and resisting fabrication when information is missing.","L1: Language Comprehension (minor)
L2: Semantic Understanding & Context Recognition, Adaptive Error Correction (minor), Working Memory (minor)
L3: Inhibitory Control, Self-reflection (minor)",L3
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA evaluates physical commonsense reasoning across a broad set of languages and culturally diverse contexts, often using paired-choice questions about plausible actions or outcomes. It stresses robust commonsense inference that transfers across linguistic and contextual variation.","L1: Language Comprehension
L2: Logical Reasoning, Semantic Understanding & Context Recognition
L3: Cognitive Flexibility (minor)",L2
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) is a long-context evaluation where multiple similar “needle” requests are embedded in long “haystacks,” and the model must retrieve or reproduce the correct response for a specified needle. It probes robustness to interference, accurate reference tracking, and sustained context use over very long inputs.","L1: Language Comprehension
L2: Working Memory, Attention, Episodic Memory (minor)
L3: Inhibitory Control (minor)",L2
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval evaluates economically valuable, well-specified professional knowledge work across dozens of occupations, using expert human judges to score outputs such as presentations, spreadsheets, and plans. It emphasizes end-to-end execution quality, adherence to requirements, and producing usable artifacts under real-world constraints.","L1: Language Production
L2: Planning, Decision-making, Semantic Understanding & Context Recognition, Working Memory (minor)
L3: Social Reasoning & Theory of Mind (minor)",L2
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks tests reasoning over graph-structured data by asking models to follow edges, traverse paths, or compute properties of walks under specified rules. It emphasizes structured multi-step inference with careful bookkeeping over many hops.","L1: 
L2: Spatial Representation & Mapping, Logical Reasoning, Working Memory, Attention (minor), Planning (minor)
L3: ",L2
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon evaluates tool-using agents on suites of tasks that require selecting, sequencing, and correctly invoking tools to accomplish goals. It stresses robust orchestration, handling tool failures, and integrating tool results into a coherent final outcome.","L1: Language Comprehension (minor), Language Production (minor)
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory
L3: ",L2
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath evaluates advanced, research-level mathematics with problems designed to resist memorization and require substantial derivations and verification. It targets deep multi-step reasoning, careful error checking, and sustaining complex symbolic threads to a correct conclusion.","L1: 
L2: Logical Reasoning, Working Memory, Planning, Adaptive Error Correction (minor)
L3: ",L2
