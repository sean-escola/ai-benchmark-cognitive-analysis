Benchmark,Website,Paper,Description,Cognitive Functions
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates software engineering ability by asking a model (often in an agent scaffold) to produce patches that fix real GitHub issues, then running tests to verify correctness. The “Verified” subset focuses on tasks that have been human-validated as solvable with a clear evaluation signal, emphasizing end-to-end debugging and code change synthesis.","Planning, Adaptive Error Correction, Logical Reasoning, Working Memory, Language Comprehension (minor), Language Production (minor)"
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 measures an agent’s ability to complete real tasks in a command-line environment, such as inspecting files, running programs, and iteratively fixing errors. Success depends on choosing effective shell actions, interpreting tool outputs, and recovering from failures under realistic constraints.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Attention (minor), Language Comprehension (minor)"
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp evaluates web-browsing and research behavior: models must search, read, and synthesize information from multiple sources to answer questions that are difficult to solve without browsing. It stresses retrieval strategy, verification, and integrating evidence across documents.","Planning, Attention, Semantic Understanding & Context Recognition, Working Memory, Decision-making (minor), Language Production (minor)"
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench evaluates interactive agent performance in multi-turn customer support settings (e.g., retail, airline, telecom) with simulated users and programmatic APIs. Models must follow domain policies while resolving user requests, maintaining consistent state across turns, and using tools correctly.","Social Reasoning & Theory of Mind, Decision-making, Planning, Inhibitory Control, Working Memory, Language Comprehension (minor), Language Production (minor)"
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld measures multimodal “computer use” where an agent operates within an operating-system-like GUI to complete tasks over multiple steps. It requires interpreting screenshots, locating relevant UI elements, and executing sequences of actions robustly despite interface and state changes.","Visual Perception, Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Spatial Representation & Mapping, Planning, Sensorimotor Coordination (minor)"
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI tests fluid reasoning via small grid-based puzzles where a model must infer a hidden transformation rule from a few input–output examples and apply it to a new input. It emphasizes abstraction, compositional rule induction, and generalization beyond memorized patterns.","Logical Reasoning, Cognitive Flexibility, Working Memory, Spatial Representation & Mapping, Visual Perception (minor), Attention (minor)"
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon autonomous agent behavior by simulating the management of a vending-machine business over an extended period. The agent must make sequential business decisions (inventory, pricing, supplier interactions) to maximize outcomes, testing coherence over many steps.","Planning, Decision-making, Reward Mechanisms, Working Memory, Adaptive Error Correction (minor), Social Reasoning & Theory of Mind (minor)"
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas evaluates real-world tool use through the Model Context Protocol by requiring multi-step workflows across tool servers and APIs. Models must select appropriate tools, invoke them with correct arguments, handle errors, and synthesize results into a final answer.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Language Comprehension (minor), Language Production (minor)"
FinanceAgent,https://www.vals.ai/benchmarks/finance_agent,,"FinanceAgent assesses performance on tasks representative of entry-level financial analysis, such as interpreting financial documents, building analyses, and producing structured outputs. It emphasizes correctly applying domain rules, maintaining consistency across assumptions, and producing decision-relevant artifacts.","Logical Reasoning, Semantic Understanding & Context Recognition, Planning, Decision-making, Working Memory, Language Production (minor)"
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates agentic cybersecurity capability on large collections of tasks involving identifying known vulnerabilities and, in some settings, discovering new ones in real open-source projects. Success requires reading code, hypothesizing failure modes, testing ideas, and iterating toward valid exploits or fixes.","Logical Reasoning, Planning, Adaptive Error Correction, Decision-making, Working Memory, Inhibitory Control (minor)"
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"SpreadsheetBench measures an agent’s ability to understand and manipulate complex spreadsheets derived from real-world workflows. It tests structured data transformation, formula reasoning, and multi-step edits where correctness depends on both local cell changes and global sheet consistency.","Logical Reasoning, Working Memory, Planning, Attention, Adaptive Error Correction (minor), Decision-making (minor)"
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a difficult multimodal benchmark spanning frontier academic and professional questions, designed to probe advanced reasoning rather than routine recall. Depending on the setup, models may need to synthesize information across modalities and maintain high precision under ambiguity.","Language Comprehension, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Scene Understanding & Visual Reasoning (minor), Multisensory Integration (minor)"
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 evaluates competition-style mathematical problem solving using the 2025 AIME question set. It emphasizes multi-step derivations, careful constraint handling, and exact final answers (often under time-like pressure assumptions even if not enforced).","Logical Reasoning, Working Memory, Cognitive Flexibility, Adaptive Error Correction (minor), Attention (minor)"
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,GPQA Diamond is a high-difficulty subset of GPQA consisting of graduate-level science multiple-choice questions designed to be “Google-proof.” It probes deep conceptual understanding and reasoning under distractors rather than surface pattern matching.,"Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Decision-making (minor), Language Comprehension (minor)"
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,MMMLU extends broad academic knowledge testing across many subjects into multiple non-English languages. It evaluates whether a model can understand and reason about domain content when expressed in different languages and cultural/linguistic contexts.,"Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory (minor), Language Production (minor)"
MMMU,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2311.16502,"MMMU is a multimodal benchmark where questions require combining text with images (e.g., diagrams, charts, tables) across diverse disciplines. It stresses visual grounding of concepts and reasoning that integrates perceptual evidence with linguistic instructions.","Multisensory Integration, Scene Understanding & Visual Reasoning, Visual Perception, Language Comprehension, Logical Reasoning, Working Memory (minor)"
LAB-Bench FigQA,https://huggingface.co/datasets/futurehouse/lab-bench,https://arxiv.org/abs/2407.10362,"LAB-Bench FigQA tests whether models can interpret complex scientific figures from biology papers and answer questions that depend on visual evidence. It emphasizes careful figure reading, mapping labels to mechanisms, and drawing correct inferences from plots and schematics.","Scene Understanding & Visual Reasoning, Visual Perception, Visual Attention & Eye Movements, Logical Reasoning, Language Comprehension (minor), Working Memory (minor)"
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning evaluates reasoning over scientific charts and figures, where the correct answer depends on interpreting plotted data, axes, and visual encodings. It probes quantitative and relational inference grounded in visual inputs, sometimes benefiting from external computation tools.","Scene Understanding & Visual Reasoning, Visual Perception, Logical Reasoning, Attention, Working Memory (minor)"
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU extends multimodal understanding to video, requiring models to answer questions using temporally unfolding visual information and accompanying text. It stresses tracking events over time, linking actions to outcomes, and integrating cues across frames.","Visual Perception, Cognitive Timing & Predictive Modeling, Working Memory, Attention, Scene Understanding & Visual Reasoning, Multisensory Integration (minor)"
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"The FACTS Benchmark Suite systematically evaluates factuality-related behaviors, such as grounding to evidence, resisting unsupported claims, and maintaining consistency under adversarial or misleading contexts. It targets truthfulness and calibration beyond simple QA accuracy.","Semantic Understanding & Context Recognition, Inhibitory Control, Logical Reasoning (minor), Working Memory (minor), Self-reflection (minor)"
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA evaluates physical commonsense reasoning across languages and locales, focusing on understanding everyday interactions with objects and environments. It tests whether models can select plausible actions/outcomes using implicit physical constraints rather than memorized phrasing.","Semantic Understanding & Context Recognition, Logical Reasoning, Spatial Representation & Mapping, Cognitive Timing & Predictive Modeling (minor), Language Comprehension (minor)"
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) is a long-context evaluation where multiple similar “needle” requests and responses are embedded in long conversations, and the model must retrieve the correct referenced item. It probes robustness of long-range coreference, interference resistance, and precise recall under distractors.","Working Memory, Attention, Episodic Memory, Language Comprehension, Inhibitory Control (minor)"
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval measures well-specified professional knowledge-work performance across many occupations, judged by expert humans via pairwise comparisons of produced work products (e.g., spreadsheets, slides, plans). It emphasizes end-to-end execution quality, instruction following, and producing usable artifacts under realistic constraints.","Planning, Decision-making, Language Production, Semantic Understanding & Context Recognition, Working Memory (minor), Reward Mechanisms (minor)"
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks evaluates structured reasoning over graphs described in text (e.g., performing BFS-like traversals or identifying parent relationships) often in long contexts. It tests whether a model can maintain a consistent internal representation of a graph and execute algorithmic reasoning steps reliably.","Spatial Representation & Mapping, Logical Reasoning, Working Memory, Attention (minor), Planning (minor)"
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon evaluates agentic tool use across diverse tasks that require selecting, sequencing, and correctly calling tools to reach a goal. It emphasizes robustness to tool errors, parameter correctness, and integrating tool outputs into coherent final responses.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Language Comprehension (minor), Language Production (minor)"
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath is a benchmark of expert-level mathematics designed to be challenging and more resistant to memorization, emphasizing novel, multi-step proofs or computations. It probes deep mathematical reasoning, abstraction, and sustained multi-stage problem solving, often with optional computational assistance in some evaluations.","Logical Reasoning, Working Memory, Cognitive Flexibility, Adaptive Error Correction (minor), Attention (minor)"
