Benchmark,Website,Paper,Description,Cognitive Functions
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates models on real GitHub issues where the goal is to produce a patch that makes a repository’s tests pass, with tasks filtered/verified for solvability and clearer evaluation. It emphasizes end-to-end software engineering: understanding bug reports, modifying code, and validating fixes via automated test outcomes.","Logical Reasoning, Planning, Adaptive Error Correction, Working Memory, Language Comprehension, Language Production"
SWE-bench Multilingual,https://www.swebench.com/multilingual.html,https://arxiv.org/abs/2504.21798,"SWE-bench Multilingual extends SWE-bench-style evaluation beyond Python to multiple programming languages, testing whether a model can generalize software engineering behaviors across different ecosystems and toolchains. Success requires correctly interpreting issue descriptions and implementing patches that satisfy language-appropriate tests and conventions.","Logical Reasoning, Planning, Adaptive Error Correction, Working Memory, Language Comprehension, Language Production"
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro is a more challenging, contamination-resistant software engineering benchmark with harder, more diverse real-world tasks across multiple languages and repositories. It stresses robust debugging, correct patch synthesis, and tool-aware iteration under stricter evaluation and broader task variety than SWE-bench Verified.","Logical Reasoning, Planning, Adaptive Error Correction, Working Memory, Language Comprehension, Language Production, Inhibitory Control (minor)"
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 evaluates agentic performance in command-line environments on realistic tasks such as installing dependencies, editing files, running programs, and diagnosing failures. It measures how well a model can plan and execute multi-step terminal workflows while recovering from errors and environment constraints.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Language Comprehension, Language Production (minor)"
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp evaluates research-style web browsing: models must search, navigate sources, and synthesize answers from retrieved documents, often requiring multi-step information gathering. The benchmark emphasizes reliable tool use, source integration, and maintaining context across a browsing session.","Planning, Decision-making, Attention, Working Memory, Episodic Memory (minor), Language Comprehension, Language Production, Self-reflection (minor)"
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench measures whether an agent can resolve multi-turn customer-support tasks by interacting with simulated users and APIs while following domain policies (e.g., retail, airline, telecom). It probes policy adherence under conversational pressure, correct tool/API invocation, and coherent multi-step problem resolution.","Social Reasoning & Theory of Mind, Decision-making, Planning, Inhibitory Control, Working Memory, Language Comprehension, Language Production, Empathy (minor)"
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld evaluates multimodal computer-use agents that must complete tasks in a full operating-system environment using screenshots and interaction actions over many steps. It emphasizes UI understanding, long-horizon execution, and robust recovery from mistakes in realistic desktop workflows.","Visual Perception, Visual Attention & Eye Movements (minor), Scene Understanding & Visual Reasoning, Planning, Decision-making, Sensorimotor Coordination, Working Memory, Adaptive Error Correction"
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI tests few-shot “fluid” reasoning on novel grid-based transformation puzzles, where models must infer latent rules from a small set of examples and generalize to new inputs. It is designed to reduce reliance on memorized knowledge and instead emphasize abstraction and transferable reasoning patterns.","Cognitive Flexibility, Logical Reasoning, Working Memory, Spatial Representation & Mapping, Visual Perception, Planning (minor)"
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon autonomy by having an agent manage a simulated vending-machine business over an extended timeframe, optimizing inventory, pricing, procurement, and cashflow. It stresses sustained coherence, strategic decision-making, and adaptation to changing market dynamics across many steps.","Planning, Decision-making, Reward Mechanisms, Working Memory, Adaptive Error Correction (minor), Language Comprehension (minor), Language Production (minor), Motivational Drives (minor)"
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas measures real-world tool use via the Model Context Protocol (MCP), requiring models to discover relevant tools, call them correctly, handle errors/retries, and combine outputs into a correct final response. Tasks reflect production-like multi-step workflows across heterogeneous services and APIs.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Language Comprehension, Language Production (minor), Inhibitory Control (minor)"
FinanceAgent,https://www.vals.ai/benchmarks/finance_agent,,"FinanceAgent evaluates whether a model can perform tasks expected of an entry-level financial analyst, such as extracting facts from documents, performing calculations, building structured analyses, and justifying conclusions. It emphasizes domain reasoning, numerical accuracy, and producing decision-relevant artifacts under realistic constraints.","Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Planning (minor), Language Comprehension, Language Production, Decision-making (minor)"
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity agent capabilities on large-scale tasks such as locating known vulnerabilities from descriptions and discovering new vulnerabilities in real codebases. It stresses precise reasoning about program behavior, constructing exploits or proof-of-vulnerability artifacts, and iterating based on tool feedback.","Logical Reasoning, Planning, Adaptive Error Correction, Working Memory, Language Comprehension, Language Production (minor), Inhibitory Control (minor)"
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"SpreadsheetBench tests spreadsheet understanding and manipulation on realistic workbooks, requiring models to navigate sheets, interpret formulas/tables, and produce correct edits or derived outputs. It emphasizes structured reasoning over semi-formal artifacts and careful multi-step transformations with low tolerance for small errors.","Semantic Understanding & Context Recognition, Working Memory, Planning, Logical Reasoning, Adaptive Error Correction (minor), Attention (minor)"
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a difficult, broad benchmark intended to probe frontier-level knowledge and reasoning, including questions that may require multi-step inference and, in some setups, tool use (e.g., search or code). It aims to stress capabilities beyond standard academic QA, including robustness on complex, long-form problems and multimodal items.","Language Comprehension, Logical Reasoning, Working Memory, Semantic Understanding & Context Recognition, Planning (minor), Scene Understanding & Visual Reasoning (minor), Visual Perception (minor)"
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 evaluates competition-style mathematics problem solving, typically requiring creative multi-step reasoning rather than routine computation. It is often used to measure a model’s ability to maintain consistent intermediate constraints and avoid arithmetic/logical slips under time-test-like conditions.","Logical Reasoning, Working Memory, Planning (minor), Adaptive Error Correction (minor)"
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,GPQA Diamond is a high-quality subset of GPQA with difficult graduate-level science multiple-choice questions designed to be hard to answer via superficial pattern matching. It probes deep conceptual understanding and careful reasoning under distractor options.,"Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Language Comprehension, Decision-making (minor)"
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends MMLU-style evaluation across many subjects and multiple languages, testing whether models can answer knowledge-and-reasoning questions beyond English. It emphasizes cross-lingual generalization and stable performance across diverse linguistic contexts and domains.","Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning (minor), Working Memory (minor), Language Production (minor)"
MMMU,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2311.16502,"MMMU is a multimodal, multi-discipline benchmark where questions require jointly reasoning over images and text, often in expert domains (e.g., charts, diagrams, technical figures). It evaluates whether models can ground language in visual evidence and perform multi-step inference from complex visual inputs.","Visual Perception, Scene Understanding & Visual Reasoning, Multisensory Integration, Language Comprehension, Logical Reasoning, Working Memory (minor), Visual Attention & Eye Movements (minor)"
LAB-Bench FigQA,https://huggingface.co/datasets/futurehouse/lab-bench,https://arxiv.org/abs/2407.10362,"LAB-Bench FigQA focuses on interpreting scientific figures from biology papers, requiring models to extract evidence from plots/diagrams and answer targeted questions about experimental results. It stresses fine-grained figure understanding and the ability to connect visual signals to domain concepts and claims.","Visual Perception, Scene Understanding & Visual Reasoning, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory (minor), Attention (minor)"
WebArena,https://webarena.dev/,https://arxiv.org/abs/2307.13854,"WebArena evaluates autonomous web agents performing realistic multi-step tasks across several self-hosted web apps (e.g., shopping, CMS, forums, Git workflows), requiring navigation, form filling, and stateful interaction. It measures long-horizon planning and robust execution under dynamic UI and partial observability.","Planning, Decision-making, Visual Perception, Scene Understanding & Visual Reasoning, Sensorimotor Coordination, Working Memory, Adaptive Error Correction, Attention (minor), Inhibitory Control (minor)"
