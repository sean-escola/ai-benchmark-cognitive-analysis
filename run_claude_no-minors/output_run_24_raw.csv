Benchmark,Website,Paper,Description,Cognitive Functions
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates agentic software engineering by asking a model to generate patches that resolve real issues in open-source Python repositories, with correctness judged by unit/integration tests. The “Verified” split uses human-vetted tasks intended to reduce ambiguity and ensure each task is solvable as specified, emphasizing reliable end-to-end coding behavior rather than only code synthesis.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition, Language Comprehension (minor), Language Production (minor)"
SWE-bench Multilingual,https://www.swebench.com/multilingual.html,https://arxiv.org/abs/2504.21798,"SWE-bench Multilingual extends SWE-bench-style repository repair tasks beyond Python to multiple programming languages, requiring models to understand heterogeneous toolchains, test harnesses, and language idioms. It stresses cross-language transfer and robustness of the same agent loop (read context → plan → edit → test → iterate) across diverse ecosystems.","Planning, Cognitive Flexibility, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition, Language Comprehension (minor), Language Production (minor)"
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro is a larger, more difficult software engineering benchmark designed to be more industrially representative and more resistant to contamination, spanning multiple languages and more complex change requirements. It emphasizes sustained multi-step problem solving in real codebases—triaging the failure, localizing the bug, implementing a fix, and satisfying tests—under realistic constraints.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition, Logical Reasoning, Inhibitory Control (minor)"
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 evaluates agents operating in a command-line environment on practical tasks such as installing dependencies, inspecting files, running programs, debugging errors, and producing correct artifacts. Success requires reliable tool use, iterative troubleshooting, and maintaining state across many terminal interactions rather than answering a single prompt.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition, Inhibitory Control (minor)"
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp measures deep-research and browsing agents that must locate and synthesize information from a large document collection, emphasizing search strategy and evidence-backed answers. It probes long-context behaviors such as tracking leads, reconciling conflicting sources, and compiling a coherent final response after multiple retrieval steps.","Planning, Attention, Episodic Memory, Semantic Understanding & Context Recognition, Language Comprehension, Language Production, Self-reflection (minor)"
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench evaluates tool-using customer-support agents in simulated domains (e.g., retail, airline, telecom) where models must interact with APIs and a user while adhering to domain policies. It stresses multi-turn workflow execution, policy compliance under pressure, and producing user-facing resolutions that are both correct and procedurally appropriate.","Decision-making, Planning, Inhibitory Control, Social Reasoning & Theory of Mind, Semantic Understanding & Context Recognition, Language Production, Empathy (minor)"
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld evaluates multimodal computer-use agents that must complete tasks in a realistic operating-system environment using screenshots and actions like clicking, typing, and navigating applications. It probes grounded perception-to-action loops, where success depends on interpreting UI state, planning sequences of interactions, and recovering from errors over many steps.","Visual Perception, Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Sensorimotor Coordination, Planning, Decision-making, Adaptive Error Correction (minor)"
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI presents few-shot grid transformation puzzles intended to measure fluid reasoning: models infer latent rules from a small set of input–output examples and apply them to a new input. It targets systematic generalization, compositional pattern discovery, and robustness to novelty rather than memorized knowledge.","Logical Reasoning, Cognitive Flexibility, Spatial Representation & Mapping, Working Memory, Attention, Cognitive Timing & Predictive Modeling (minor)"
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon autonomy by placing an agent in a simulated year-long vending-machine business that requires sourcing inventory, setting prices, negotiating with suppliers, and adapting to market dynamics. The score reflects cumulative strategic choices across thousands of decisions, emphasizing coherence, long-term planning, and goal maintenance.","Planning, Decision-making, Reward Mechanisms, Working Memory, Episodic Memory, Motivational Drives (minor), Self-reflection (minor)"
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas evaluates real-world tool use via the Model Context Protocol, requiring models to discover tools, call them with correct schemas, manage errors/retries, and integrate results across multi-step workflows. It emphasizes reliable agentic orchestration across heterogeneous services rather than single-call function execution.","Planning, Decision-making, Working Memory, Adaptive Error Correction, Semantic Understanding & Context Recognition, Inhibitory Control (minor)"
FinanceAgent,https://www.vals.ai/benchmarks/finance_agent,,"FinanceAgent evaluates agents on tasks typical of an entry-level financial analyst, such as extracting information from documents, performing calculations, building analyses, and drafting investment or reporting outputs. It stresses domain-grounded reasoning and accurate synthesis across multiple sources and steps, often with structured outputs.","Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Planning, Decision-making, Language Production (minor)"
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity capabilities by testing whether an agent can identify known vulnerabilities from descriptions and also discover previously unknown vulnerabilities in real open-source projects. It probes systematic debugging and adversarial thinking: reading code, hypothesizing failure modes, crafting exploits or proofs, and iterating based on feedback.","Logical Reasoning, Planning, Adaptive Error Correction, Working Memory, Inhibitory Control (minor), Semantic Understanding & Context Recognition"
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"SpreadsheetBench evaluates the ability to understand, navigate, and modify complex spreadsheets using realistic tasks (e.g., formulas, formatting, tables, and data transformations). It stresses precise multi-step manipulation and consistency across a structured artifact, where small mistakes can cascade into incorrect final outputs.","Working Memory, Planning, Decision-making, Logical Reasoning, Adaptive Error Correction (minor), Semantic Understanding & Context Recognition"
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam (HLE) is a difficult, frontier-focused benchmark spanning advanced questions across many domains (often multimodal), intended to test broad expert-level reasoning and knowledge under realistic ambiguity. It emphasizes integrating evidence, performing multi-step inference, and producing justified answers when tasks exceed typical exam difficulty.","Logical Reasoning, Semantic Understanding & Context Recognition, Language Comprehension, Language Production, Multisensory Integration (minor), Visual Perception (minor), Working Memory"
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 is a set of competition-style mathematics problems that require nontrivial, multi-step derivations and careful symbolic manipulation. It primarily tests deliberate mathematical reasoning and error-free intermediate steps under a compact answer format.","Logical Reasoning, Working Memory, Planning (minor), Attention (minor), Language Comprehension (minor)"
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a subset of challenging graduate-level multiple-choice science questions curated to be “Google-proof” and to separate experts from non-experts. It probes precise scientific reasoning, disambiguation of closely related concepts, and resisting superficial pattern-matching.","Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Decision-making (minor), Language Comprehension (minor)"
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends MMLU to multiple languages, testing broad academic knowledge and reasoning across many subjects in non-English settings. It emphasizes multilingual comprehension and the ability to apply the same concepts under varied linguistic and cultural surface forms.","Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning, Cognitive Flexibility, Working Memory (minor), Language Production (minor)"
MMMU,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2311.16502,"MMMU is a multimodal benchmark covering many disciplines where questions require reasoning over images (e.g., diagrams, plots, photos) together with text. It probes whether models can extract relevant visual evidence and integrate it with domain knowledge to answer expert-style questions.","Visual Perception, Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Attention (minor), Semantic Understanding & Context Recognition (minor)"
LAB-Bench FigQA,https://huggingface.co/datasets/futurehouse/lab-bench,https://arxiv.org/abs/2407.10362,"LAB-Bench FigQA evaluates whether models can correctly interpret and reason about complex scientific figures from biology papers, including plots, panels, and annotated visual elements. It stresses extracting quantitative/relational information from figures and mapping it to precise answers grounded in the visual evidence.","Visual Perception, Scene Understanding & Visual Reasoning, Attention, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory (minor)"
WebArena,https://webarena.dev/,https://arxiv.org/abs/2307.13854,"WebArena evaluates autonomous web agents on realistic tasks across multiple web apps (e.g., shopping, CMS, forums, Git hosting, maps), requiring navigation, form filling, and multi-step completion under dynamic UIs. It probes end-to-end planning and grounded interaction, including recovering from mistakes and maintaining task state over long action sequences.","Planning, Decision-making, Visual Perception, Scene Understanding & Visual Reasoning, Sensorimotor Coordination, Working Memory, Inhibitory Control (minor)"
