Benchmark,Website,Paper,Description,Cognitive Functions
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates LLMs on real GitHub issues by asking them to generate code patches that make an existing test suite pass. The “Verified” subset contains tasks that have been manually checked to be solvable and to have reliable evaluation via tests, emphasizing robust end-to-end debugging and patch generation rather than short code snippets.","Planning, Logical Reasoning, Adaptive Error Correction, Working Memory, Language Comprehension, Language Production, Decision-making (minor)"
SWE-bench Multilingual,https://www.swebench.com/multilingual.html,https://arxiv.org/abs/2504.21798,"SWE-bench Multilingual extends SWE-bench-style repo-and-issue patching beyond Python to multiple programming languages, toolchains, and ecosystems. It stresses whether an agent can transfer debugging and repair skills across languages while still satisfying concrete test-based correctness criteria.","Planning, Logical Reasoning, Adaptive Error Correction, Cognitive Flexibility, Working Memory, Language Comprehension, Language Production"
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro is a larger, harder software engineering benchmark designed to be more realistic and more resistant to superficial solutions, with tasks spanning multiple languages and more complex repos. It measures whether agents can sustain longer debugging loops, coordinate edits across files, and converge on patches that satisfy stronger correctness checks.","Planning, Logical Reasoning, Adaptive Error Correction, Working Memory, Decision-making, Language Comprehension, Language Production"
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 evaluates agentic task execution in a command-line environment, where the model must run shell commands, inspect files, and iteratively fix issues to accomplish real tasks. Success depends on correctly choosing actions under partial feedback from the environment (stdout/stderr, exit codes) and recovering from mistakes over multi-step trajectories.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Logical Reasoning, Language Comprehension, Language Production (minor), Inhibitory Control (minor)"
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp measures web-browsing research ability: the agent must search, read, and synthesize information from multiple documents to answer questions that require evidence gathering. It emphasizes tool-mediated information retrieval and maintaining coherence across a multi-step research process rather than recall from parametric memory alone.","Planning, Decision-making, Attention, Working Memory, Episodic Memory (minor), Semantic Understanding & Context Recognition, Language Comprehension, Language Production"
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench evaluates interactive customer-support agents that must use APIs/tools while following domain policies in multi-turn conversations with simulated users. It stresses policy adherence, dialogue management, and tool-calling reliability under realistic conversational pressure, including handling edge cases and conflicting objectives.","Social Reasoning & Theory of Mind, Inhibitory Control, Decision-making, Planning, Working Memory, Language Comprehension, Language Production, Empathy (minor)"
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld evaluates multimodal “computer use” agents that operate a real or realistic desktop OS by perceiving screens and executing UI actions to complete tasks. The benchmark probes grounded perception-to-action loops (locating UI elements, sequencing interactions) and robustness to interface variability and errors across long horizons.","Visual Perception, Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Sensorimotor Coordination, Planning, Decision-making, Adaptive Error Correction, Working Memory"
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI tests few-shot abstract reasoning on grid-based puzzles where models must infer latent rules from a small number of examples and apply them to new inputs. It targets fluid reasoning and generalization under distribution shift, with minimal reliance on world knowledge and heavy reliance on structured pattern induction.","Logical Reasoning, Working Memory, Cognitive Flexibility, Spatial Representation & Mapping, Scene Understanding & Visual Reasoning, Attention (minor), Cognitive Timing & Predictive Modeling (minor)"
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon autonomous agency by having models run a simulated vending-machine business over many decisions (pricing, inventory, supplier negotiation) while adapting to market dynamics. It emphasizes sustained coherence, strategic planning, and iterative optimization under delayed outcomes rather than single-turn accuracy.","Planning, Decision-making, Reward Mechanisms, Working Memory, Adaptive Error Correction, Motivational Drives (minor), Self-reflection (minor), Language Production (minor)"
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas measures real-world tool use through the Model Context Protocol by requiring models to discover tools, invoke them correctly, handle errors, and compose multi-step workflows across heterogeneous services. It focuses on reliable action sequencing and integration of tool outputs into accurate final responses.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Language Comprehension, Language Production"
FinanceAgent,https://www.vals.ai/benchmarks/finance_agent,,"FinanceAgent evaluates whether a model can perform tasks typical of an entry-level financial analyst, often involving multi-step reasoning over documents, calculations, and structured outputs (e.g., summaries, models, recommendations). It probes applied domain reasoning and correctness under task constraints rather than open-ended conversation.","Logical Reasoning, Decision-making, Planning, Working Memory, Semantic Understanding & Context Recognition, Language Production (minor)"
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity agent capability on real-world vulnerability tasks, including identifying known vulnerabilities from descriptions and discovering new issues in codebases. It emphasizes precise technical reasoning, systematic exploration, and iterative debugging-like correction when hypotheses fail.","Logical Reasoning, Planning, Adaptive Error Correction, Working Memory, Attention (minor), Inhibitory Control (minor), Language Comprehension"
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"SpreadsheetBench evaluates an agent’s ability to understand, manipulate, and generate complex spreadsheets derived from real workflows. Tasks typically require multi-step operations (reading tables, applying formulas, restructuring sheets) and verification of outputs, stressing procedural accuracy and error recovery.","Planning, Decision-making, Working Memory, Logical Reasoning, Adaptive Error Correction, Semantic Understanding & Context Recognition (minor), Language Production (minor)"
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a large, frontier-level benchmark spanning many subjects and modalities, intended to test broad academic reasoning and knowledge under challenging questions. It typically emphasizes careful multi-step reasoning and synthesis, sometimes with optional tool use, over shallow pattern matching.","Language Comprehension, Logical Reasoning, Working Memory, Semantic Understanding & Context Recognition, Planning (minor), Scene Understanding & Visual Reasoning (minor)"
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 consists of competition-style mathematics problems requiring exact numeric answers, often solvable via nontrivial algebraic, combinatorial, or geometric reasoning. The benchmark probes stepwise symbolic reasoning and maintaining intermediate results without relying on external knowledge.","Logical Reasoning, Working Memory, Planning (minor), Attention (minor)"
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a hard multiple-choice science QA benchmark designed to be “Google-proof,” with questions that require deep reasoning rather than quick lookup. The Diamond subset is curated for quality, stressing careful reading, domain understanding, and elimination of distractors.","Language Comprehension, Logical Reasoning, Working Memory, Semantic Understanding & Context Recognition, Decision-making (minor)"
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends MMLU-style multiple-choice evaluation across many languages and academic subjects to assess multilingual knowledge and reasoning. It tests whether models can preserve performance under language shifts, including handling culturally and linguistically varied phrasing and concepts.","Language Comprehension, Semantic Understanding & Context Recognition, Cognitive Flexibility, Working Memory (minor), Logical Reasoning (minor)"
MMMU,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2311.16502,"MMMU is a multimodal benchmark spanning many disciplines where models must answer questions that integrate images (diagrams, charts, figures) with text. It emphasizes visual understanding plus cross-modal reasoning, often requiring multi-step interpretation rather than direct recognition.","Visual Perception, Scene Understanding & Visual Reasoning, Multisensory Integration, Language Comprehension, Logical Reasoning, Working Memory (minor), Visual Attention & Eye Movements (minor)"
LAB-Bench FigQA,https://huggingface.co/datasets/futurehouse/lab-bench,https://arxiv.org/abs/2407.10362,"LAB-Bench FigQA evaluates whether models can correctly interpret complex scientific figures from biology papers and answer targeted questions about them. It stresses fine-grained figure reading (axes, labels, multi-panel layouts) and linking visual evidence to scientific claims under uncertainty.","Visual Perception, Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Language Comprehension, Logical Reasoning, Working Memory (minor), Semantic Understanding & Context Recognition (minor)"
WebArena,https://webarena.dev/,https://arxiv.org/abs/2307.13854,"WebArena evaluates autonomous web agents performing realistic multi-step tasks across several web apps (e-commerce, forums, code hosting, CMS), requiring navigation, form filling, and state tracking. It probes end-to-end planning and error recovery under dynamic UIs and partial observability from webpages.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Visual Perception, Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning (minor), Language Comprehension (minor), Language Production (minor)"
