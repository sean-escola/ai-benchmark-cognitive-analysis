Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates whether a model can solve real GitHub software engineering issues by producing a code patch that makes the project’s tests pass. The “Verified” subset emphasizes tasks that have been validated as solvable and uses an execution-based harness to check correctness, reflecting realistic debugging and code-edit workflows.","L1: Language Comprehension (minor)
L2: Planning, Adaptive Error Correction, Logical Reasoning, Working Memory, Decision-making (minor)
L3: ",L2
SWE-bench Multilingual,https://www.swebench.com/multilingual.html,https://arxiv.org/abs/2504.21798,"SWE-bench Multilingual extends the SWE-bench paradigm beyond Python, testing whether models can repair real repositories across multiple programming languages and ecosystems. It probes whether agents can transfer debugging and patch-construction strategies across different syntax, tooling, and conventions.","L1: Language Comprehension (minor)
L2: Planning, Adaptive Error Correction, Logical Reasoning, Working Memory
L3: Cognitive Flexibility",L3
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro is a more challenging, more contamination-resistant software engineering benchmark designed to better approximate industrial difficulty, including more diverse tasks and multiple languages. It stresses end-to-end problem solving: understanding an issue, locating relevant code, implementing a fix, and validating it under a realistic evaluation harness.","L1: 
L2: Planning, Adaptive Error Correction, Logical Reasoning, Decision-making, Working Memory, Semantic Understanding & Context Recognition (minor)
L3: ",L2
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 evaluates agentic performance on real tasks inside a command-line environment, requiring models to choose commands, inspect files, run programs, and iteratively diagnose failures. Success depends on chaining tool interactions over multiple steps while maintaining state about what has been tried and what remains.","L1: Language Comprehension (minor)
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory
L3: Inhibitory Control (minor)",L2
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp measures how well a model can use browsing/search-like tool interactions to answer difficult information-seeking questions that require multi-step investigation. It emphasizes decomposing queries, gathering evidence across sources, and synthesizing a final answer grounded in retrieved material.","L1: Language Comprehension
L2: Planning, Attention, Episodic Memory, Semantic Understanding & Context Recognition, Logical Reasoning (minor)
L3: ",L2
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench evaluates interactive tool-using agents in multi-turn customer-service-style environments (e.g., retail, airline, telecom) with policies and APIs. It tests whether an agent can follow domain rules, ask clarifying questions, and reliably execute API/tool actions while maintaining coherent dialogue state.","L1: Language Production (minor)
L2: Decision-making, Planning, Working Memory
L3: Social Reasoning & Theory of Mind, Inhibitory Control, Empathy (minor)",L3
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld is a benchmark for multimodal computer-use agents operating a real (or realistic) desktop OS via screenshots and tool actions (mouse/keyboard-like operations). Tasks require navigating GUIs, locating information, and performing procedural operations across applications under step limits.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Planning, Decision-making, Sensorimotor Coordination (minor), Working Memory (minor)
L3: ",L2
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,ARC-AGI presents few-shot grid transformation problems intended to measure fluid reasoning: models infer the underlying rule from a small set of input-output examples and apply it to a new input. It is designed to minimize reliance on memorized knowledge and emphasize abstraction and systematic generalization.,"L1: Visual Perception (minor)
L2: Logical Reasoning, Working Memory, Spatial Representation & Mapping, Planning (minor)
L3: Cognitive Flexibility",L3
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon autonomous agency in a simulated business setting where the agent must manage a vending-machine operation over an extended period. High scores require sustained coherence, strategic planning, adaptation to changing conditions, and effective handling of multi-step operational decisions.","L1: 
L2: Planning, Decision-making, Reward Mechanisms, Episodic Memory, Working Memory
L3: Self-reflection (minor), Social Reasoning & Theory of Mind (minor)",L2
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas measures real-world tool use through the Model Context Protocol, requiring models to discover appropriate tools, call them with correct arguments, handle errors, and integrate returned data. It reflects production-like multi-tool workflows, emphasizing robustness and correct orchestration over multiple steps.","L1: Language Comprehension (minor)
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory
L3: ",L2
FinanceAgent,https://www.vals.ai/benchmarks/finance_agent,,"FinanceAgent assesses performance on tasks typical of an entry-level financial analyst, often involving multi-step reasoning with documents, calculations, and tool-assisted workflows. It emphasizes producing correct, auditable outputs (e.g., analyses, summaries, models) under realistic constraints and formatting expectations.","L1: Language Production (minor)
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Planning, Decision-making, Working Memory
L3: ",L2
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity agent capabilities on tasks such as identifying known vulnerabilities from descriptions and discovering new vulnerabilities in real open-source projects. It requires navigating codebases, reasoning about exploit conditions, and iteratively testing hypotheses under an execution/verification setup.","L1: 
L2: Logical Reasoning, Planning, Adaptive Error Correction, Working Memory, Attention (minor)
L3: Inhibitory Control (minor)",L2
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"SpreadsheetBench measures an agent’s ability to understand, edit, and compute with complex spreadsheets, often requiring multi-step transformations, formula reasoning, and careful manipulation of structured data. It stresses procedural accuracy and consistency under realistic spreadsheet operations and dependencies.","L1: 
L2: Working Memory, Logical Reasoning, Planning, Adaptive Error Correction, Semantic Understanding & Context Recognition (minor), Motor Coordination (minor)
L3: ",L2
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a large, multi-domain (often multimodal) benchmark intended to probe frontier-level academic reasoning and knowledge across many subjects. Questions are designed to be difficult and to test synthesis and problem solving rather than simple retrieval, and evaluations are commonly run with and without tools/search.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Scene Understanding & Visual Reasoning (minor), Planning (minor)
L3: ",L2
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 uses problems from the American Invitational Mathematics Examination to evaluate mathematical reasoning and symbolic problem solving under competition constraints. It emphasizes deriving exact answers through multi-step reasoning, often benefiting from careful intermediate bookkeeping.","L1: 
L2: Logical Reasoning, Working Memory, Planning (minor), Attention (minor)
L3: ",L2
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a subset of the GPQA benchmark consisting of high-quality, difficult graduate-level science multiple-choice questions where experts succeed and non-experts tend to fail. It probes deep scientific reasoning and disciplined selection among plausible distractors under a fixed-choice format.","L1: Language Comprehension
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Decision-making, Working Memory (minor)
L3: ",L2
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends MMLU-style academic evaluation to multiple languages, testing breadth of knowledge and reasoning across many subjects in multilingual settings. It emphasizes cross-lingual understanding and consistent reasoning despite changes in language, phrasing, and cultural/linguistic context.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory (minor)
L3: Cognitive Flexibility (minor)",L2
MMMU,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2311.16502,"MMMU is a multimodal benchmark spanning many disciplines where models must answer questions that combine text with images such as diagrams, charts, and figures. It probes integration of visual evidence with textual context to reach a correct conclusion, often requiring multi-step visual reasoning.","L1: Visual Perception, Language Comprehension
L2: Multisensory Integration, Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Logical Reasoning (minor)
L3: ",L2
LAB-Bench FigQA,https://huggingface.co/datasets/futurehouse/lab-bench,https://arxiv.org/abs/2407.10362,"LAB-Bench FigQA evaluates whether models can interpret and reason about complex scientific figures from biology research papers, including extracting relationships, trends, and experimental implications. It focuses on figure-grounded scientific reasoning where the key evidence is embedded in visual plots and annotations.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory (minor)
L3: ",L2
WebArena,https://webarena.dev/,https://arxiv.org/abs/2307.13854,"WebArena evaluates autonomous web agents performing realistic tasks across multiple web applications (e-commerce, CMS, forums, code hosting, maps) with dynamic pages and multi-step goals. It emphasizes robust navigation, state tracking across steps, and correct action selection under partial observability and noisy interfaces.","L1: Visual Perception, Language Comprehension (minor)
L2: Planning, Decision-making, Working Memory, Adaptive Error Correction, Visual Attention & Eye Movements
L3: Social Reasoning & Theory of Mind (minor)",L2
