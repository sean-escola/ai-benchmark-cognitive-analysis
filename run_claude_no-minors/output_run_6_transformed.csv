Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates software engineering agents by giving a real GitHub issue plus a repository snapshot and requiring the model to produce a correct patch that makes the test suite pass. The “Verified” subset contains problems that have been manually checked to be solvable and to have reliable evaluation via unit tests, enabling more dependable comparisons across models and scaffolds.","L1: Language Comprehension (minor), Language Production (minor)
L2: Planning, Logical Reasoning, Adaptive Error Correction, Working Memory
L3: ",L2
SWE-bench Multilingual,https://www.swebench.com/multilingual.html,https://arxiv.org/abs/2504.21798,"SWE-bench Multilingual extends SWE-bench-style patch generation to multiple programming languages, testing whether an agent can diagnose and fix real issues beyond Python. It stresses cross-language generalization while preserving the same core setup: repository context, problem statement, and test-driven verification.","L1: Language Comprehension (minor)
L2: Planning, Logical Reasoning, Adaptive Error Correction, Working Memory
L3: Cognitive Flexibility",L3
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro is a larger, more difficult software engineering benchmark designed to be more realistic and more contamination-resistant than earlier SWE-bench variants. It typically involves longer issue descriptions, trickier repos, and higher demands on end-to-end agent reliability under test-based grading.","L1: 
L2: Planning, Decision-making, Logical Reasoning, Adaptive Error Correction, Working Memory
L3: Cognitive Flexibility",L3
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 evaluates agents performing real tasks in a command-line environment (e.g., debugging, installing dependencies, manipulating files, running programs, and verifying outputs). Success requires the agent to choose and sequence shell commands, interpret errors, and iteratively converge on a working solution under resource and time constraints.","L1: 
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory, Logical Reasoning, Attention (minor)
L3: ",L2
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp measures deep web-research capability by requiring models to answer questions that demand multi-step browsing and evidence gathering rather than single-pass recall. The benchmark emphasizes verifying claims across sources, handling long-horizon search, and synthesizing a final grounded response.","L1: Language Production (minor)
L2: Planning, Decision-making, Attention, Working Memory, Semantic Understanding & Context Recognition, Episodic Memory (minor)
L3: ",L2
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench evaluates interactive customer-support agents in simulated domains (e.g., retail, airline, telecom) where the model must converse with a user and call APIs while following domain policies. It probes reliability under multi-turn constraints, including policy compliance, tool correctness, and coherent task completion despite noisy or adversarial user behavior.","L1: Language Comprehension (minor)
L2: Decision-making, Planning, Working Memory
L3: Social Reasoning & Theory of Mind, Inhibitory Control, Empathy (minor)",L3
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld is a multimodal computer-use benchmark where an agent operates a real or simulated desktop OS to complete tasks across applications using screenshots and interface interactions. It tests end-to-end perception–cognition–action loops: reading UI state, planning clicks/typing sequences, and recovering from mistakes or unexpected UI changes.","L1: Visual Perception
L2: Visual Attention & Eye Movements, Sensorimotor Coordination, Planning, Adaptive Error Correction, Working Memory, Decision-making
L3: ",L2
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI is a few-shot abstract reasoning benchmark based on grid transformation puzzles, where the model must infer latent rules from a handful of input–output examples and apply them to a new input. It is designed to emphasize novel pattern induction and compositional reasoning rather than memorized knowledge.","L1: 
L2: Logical Reasoning, Spatial Representation & Mapping, Working Memory, Attention (minor)
L3: Cognitive Flexibility",L3
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon autonomous agency in a simulated business environment where the agent runs a vending-machine company over an extended period. The model must manage inventory, pricing, supplier negotiation, and changing market conditions, with final profit/balance reflecting sustained strategic competence.","L1: 
L2: Planning, Decision-making, Reward Mechanisms, Working Memory, Adaptive Error Correction
L3: Motivational Drives (minor), Self-reflection (minor)",L2
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas benchmarks real-world tool use through the Model Context Protocol (MCP), requiring models to discover, invoke, and chain multiple tools across production-like server environments. It emphasizes correct parameterization, error handling, and multi-step workflow execution rather than single-call function use.","L1: Language Comprehension (minor), Language Production (minor)
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory
L3: ",L2
FinanceAgent,https://www.vals.ai/benchmarks/finance_agent,,"FinanceAgent measures agent performance on tasks typical of an entry-level financial analyst, such as extracting and reconciling figures, building analyses, and producing decision-support artifacts under realistic constraints. The benchmark targets applied reasoning with domain context, often involving multi-step workflows and tool-assisted computation.","L1: Language Production (minor)
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Planning, Working Memory, Decision-making
L3: ",L2
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity agents on tasks like locating known vulnerabilities from descriptions and, in some settings, discovering new vulnerabilities in real open-source projects. It stresses accurate program analysis, iterative debugging/exploitation attempts, and robust handling of partial progress signals under pass@1-style scoring.","L1: 
L2: Logical Reasoning, Planning, Adaptive Error Correction, Working Memory, Attention (minor)
L3: Inhibitory Control (minor)",L2
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"SpreadsheetBench tests whether models can navigate and manipulate complex spreadsheets to answer questions or produce correct transformed files, often mirroring business analytics workflows. It emphasizes structured operations (formulas, tables, references) and careful verification to avoid subtle arithmetic or referencing errors.","L1: Language Comprehension (minor)
L2: Working Memory, Planning, Logical Reasoning, Attention, Adaptive Error Correction
L3: ",L2
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a difficult multimodal benchmark spanning frontier academic and professional knowledge, with questions intended to be challenging even for strong generalist systems. It evaluates the ability to integrate domain knowledge with careful reasoning and, when enabled in some settings, to use tools to verify and compute answers.","L1: Language Comprehension, Visual Perception (minor)
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Scene Understanding & Visual Reasoning (minor)
L3: ",L2
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 is a competition-style mathematics evaluation consisting of short-answer problems that require multi-step symbolic reasoning and careful algebraic manipulation. It primarily measures mathematical problem-solving under tight correctness constraints, often exposing brittleness in intermediate-step reasoning.","L1: 
L2: Logical Reasoning, Working Memory, Planning, Attention (minor)
L3: ",L2
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a high-quality subset of GPQA composed of graduate-level, “Google-proof” multiple-choice science questions that are difficult for non-experts. It targets deep scientific reasoning and precise reading of technical language, aiming to reduce gains from shallow pattern matching or retrieval alone.","L1: Language Comprehension (minor)
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory
L3: ",L2
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU is a multilingual extension of MMLU, testing broad academic knowledge and reasoning across many subjects in multiple non-English languages. It probes whether a model can maintain comparable competence across languages, including understanding culturally and linguistically varied prompts while preserving reasoning quality.","L1: Language Comprehension, Language Production (minor)
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory (minor)
L3: ",L2
MMMU,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2311.16502,"MMMU (Massive Multi-discipline Multimodal Understanding and Reasoning) evaluates expert-level multimodal reasoning using text plus images across many disciplines. Tasks often require interpreting diagrams, plots, and visual layouts and combining them with textual context to select or generate correct answers.","L1: Visual Perception, Language Comprehension (minor)
L2: Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Working Memory (minor)
L3: ",L2
LAB-Bench FigQA,https://huggingface.co/datasets/futurehouse/lab-bench,https://arxiv.org/abs/2407.10362,"LAB-Bench FigQA focuses on reasoning over complex scientific figures from biology papers, requiring models to extract salient visual evidence and answer questions grounded in the figure content. It emphasizes accurate interpretation of plots, labels, and multi-panel layouts, often in conjunction with domain-specific scientific context.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Attention, Logical Reasoning, Semantic Understanding & Context Recognition, Multisensory Integration (minor)
L3: ",L2
WebArena,https://webarena.dev/,https://arxiv.org/abs/2307.13854,"WebArena benchmarks autonomous web agents performing realistic tasks across multiple web apps (e-commerce, CMS, collaboration, code hosting) within a controlled environment. Agents must navigate pages, fill forms, click elements, and maintain task state across many steps, with success graded by functional task completion.","L1: Visual Perception
L2: Planning, Decision-making, Working Memory, Visual Attention & Eye Movements, Sensorimotor Coordination, Adaptive Error Correction
L3: Inhibitory Control (minor)",L2
