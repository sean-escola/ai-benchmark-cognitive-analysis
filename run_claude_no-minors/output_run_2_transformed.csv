Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,SWE-bench Verified evaluates models on real-world GitHub issues where the model must produce code patches that make repository tests pass. The “Verified” split uses human-validated tasks intended to be solvable and to reduce evaluation noise from ambiguous problem statements.,"L1: Language Comprehension (minor)
L2: Logical Reasoning, Planning, Working Memory, Adaptive Error Correction, Decision-making (minor)
L3: ",L2
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 measures an agent’s ability to complete practical tasks inside a command-line environment (e.g., debugging, manipulating files, running tools, and executing workflows). It emphasizes multi-step interaction, iteration, and recovering from tool or environment errors under realistic constraints.","L1: Language Comprehension (minor)
L2: Planning, Adaptive Error Correction, Working Memory, Decision-making, Attention (minor)
L3: ",L2
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp evaluates “deep research” agents that browse and synthesize information from documents (often via search and retrieval) to answer difficult questions. It stresses navigating sources, keeping track of evidence, and producing grounded, coherent outputs across multiple steps.","L1: Language Comprehension, Language Production (minor)
L2: Planning, Attention, Working Memory, Semantic Understanding & Context Recognition, Decision-making (minor)
L3: ",L2
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench evaluates interactive agent performance in multi-turn customer-support scenarios (e.g., retail, airline, telecom) that require using tools/APIs while following policies. Success depends on accurate dialogue management, consistent constraint-following, and robust multi-step task completion.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Working Memory
L3: Social Reasoning & Theory of Mind, Inhibitory Control",L3
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld benchmarks multimodal “computer use” agents that operate an OS-like environment by interpreting screens and executing actions to accomplish tasks. It tests the full perception–cognition–action loop, including UI understanding, tool/action sequencing, and error recovery.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements (minor), Planning, Decision-making, Sensorimotor Coordination, Working Memory
L3: ",L2
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,ARC-AGI measures fluid pattern induction on small grid-based puzzles where models infer transformation rules from a few examples. It is designed to emphasize generalization and novel reasoning rather than memorized knowledge.,"L1: Visual Perception
L2: Logical Reasoning, Working Memory, Spatial Representation & Mapping
L3: Cognitive Flexibility",L3
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon autonomous agency in a simulated business environment where the agent manages a vending operation over many decisions. Models must plan, adapt to changing conditions, and optimize outcomes (e.g., inventory, pricing, supplier interactions) over extended time.","L1: 
L2: Planning, Decision-making, Working Memory, Reward Mechanisms, Adaptive Error Correction (minor)
L3: Cognitive Timing & Predictive Modeling (minor)",L2
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas evaluates real-world tool use via Model Context Protocol servers, requiring models to discover appropriate tools, call them correctly, and compose multi-step workflows. It emphasizes reliable execution across tool errors, parameterization, and cross-tool coordination.","L1: Language Comprehension (minor), Language Production (minor)
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory
L3: ",L2
FinanceAgent,https://www.vals.ai/benchmarks/finance_agent,,"FinanceAgent measures agent performance on tasks representative of an entry-level financial analyst, often involving structured reasoning, document/statement understanding, and tool-assisted analysis. It stresses producing correct, defensible outputs under domain constraints and quantitative checks.","L1: 
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Planning, Decision-making, Working Memory, Adaptive Error Correction (minor)
L3: ",L2
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity agent capabilities on real-world vulnerability tasks, including locating known vulnerabilities from descriptions and attempting to discover new ones. It tests systematic investigation, hypothesis-driven debugging, and precise interaction with code and tooling.","L1: 
L2: Logical Reasoning, Planning, Adaptive Error Correction, Working Memory, Attention (minor)
L3: Inhibitory Control (minor)",L2
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"SpreadsheetBench evaluates the ability to understand, edit, and compute with complex spreadsheets using realistic tasks. It stresses structured manipulation, multi-step transformations, formula/logic consistency, and careful checking of outputs.","L1: 
L2: Logical Reasoning, Working Memory, Planning, Adaptive Error Correction, Attention
L3: ",L2
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a large, frontier-difficulty benchmark spanning expert knowledge and reasoning, with many questions designed to be hard to answer by memorization alone. It includes tasks that can involve complex inference, synthesis, and (in multimodal settings) interpreting images or other artifacts.","L1: Language Comprehension, Visual Perception (minor)
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Scene Understanding & Visual Reasoning (minor)
L3: ",L2
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,AIME 2025 evaluates mathematical problem solving on competition-style questions that demand multi-step derivations and careful symbolic manipulation. Performance reflects robustness on novel math problems under tight correctness requirements.,"L1: 
L2: Logical Reasoning, Working Memory, Planning (minor)
L3: Cognitive Flexibility (minor)",L2
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,GPQA Diamond is a subset of extremely difficult graduate-level science multiple-choice questions designed to be “Google-proof.” It probes deep scientific understanding and disciplined reasoning under adversarial distractors.,"L1: 
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory (minor), Decision-making (minor)
L3: ",L2
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends broad academic knowledge evaluation into multiple languages, testing whether models maintain competence across linguistic contexts. It emphasizes multilingual understanding and consistent reasoning across diverse subject areas.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning (minor), Working Memory (minor)
L3: ",L2
MMMU,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2311.16502,"MMMU evaluates multidisciplinary understanding and reasoning using both text and images, requiring models to integrate visual evidence with language-based instructions. Tasks span diagrams, charts, and domain-specific visuals that demand careful interpretation and inference.","L1: Visual Perception, Language Comprehension
L2: Multisensory Integration, Scene Understanding & Visual Reasoning, Logical Reasoning, Working Memory (minor)
L3: ",L2
LAB-Bench FigQA,https://huggingface.co/datasets/futurehouse/lab-bench,https://arxiv.org/abs/2407.10362,"LAB-Bench FigQA tests whether models can correctly interpret and reason over scientific figures from biology papers, often requiring extracting relationships, trends, and experimental implications. It is intended to reflect the figure-centric reasoning common in real scientific workflows.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Logical Reasoning, Semantic Understanding & Context Recognition, Attention (minor), Working Memory (minor)
L3: ",L2
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning evaluates reasoning over scientific charts/figures drawn from research-paper contexts, requiring models to read visual encodings and answer structured questions. It emphasizes accurate extraction plus higher-level inference beyond simple OCR or caption copying.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Logical Reasoning, Semantic Understanding & Context Recognition, Attention (minor), Working Memory (minor)
L3: ",L2
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU extends multimodal understanding to video, requiring models to integrate information across frames and answer questions that depend on temporal context. It stresses maintaining coherent representations over time and reasoning about events and actions.","L1: Visual Perception
L2: Attention, Working Memory, Scene Understanding & Visual Reasoning (minor), Multisensory Integration (minor)
L3: Cognitive Timing & Predictive Modeling",L3
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"The FACTS Benchmark Suite systematically evaluates factuality by testing whether model outputs remain consistent with provided sources and reality constraints across varied settings. It is designed to surface hallucinations, unsupported claims, and failures of attribution/grounding.","L1: Language Production (minor)
L2: Semantic Understanding & Context Recognition, Adaptive Error Correction (minor), Working Memory (minor)
L3: Inhibitory Control",L3
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA evaluates physical commonsense reasoning across languages and cultural contexts using non-parallel, multilingual data. It targets whether models can select plausible actions/explanations grounded in everyday physics and affordances, rather than relying on English-centric cues.","L1: Language Comprehension
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Spatial Representation & Mapping (minor)
L3: ",L2
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) tests long-context multi-round coreference resolution by embedding multiple similar “needle” requests inside a large “haystack” of text and asking for the correct associated response. The 8-needle setting increases interference, stressing robust retrieval of the right referent and context span.","L1: Language Comprehension
L2: Working Memory, Attention, Semantic Understanding & Context Recognition
L3: ",L2
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval evaluates economically valuable, well-specified professional knowledge-work tasks across many occupations by comparing model-produced work products to human professionals via expert judging. Tasks often require creating structured artifacts (e.g., spreadsheets, plans, presentations) with correctness, clarity, and adherence to requirements.","L1: Language Production
L2: Planning, Decision-making, Semantic Understanding & Context Recognition, Working Memory (minor)
L3: Self-reflection (minor), Social Reasoning & Theory of Mind (minor)",L2
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks evaluates structured reasoning over graph-defined relationships, often requiring multi-step traversal, path following, or neighborhood queries under long-context setups. It stresses systematic, compositional inference rather than surface-level pattern matching.","L1: 
L2: Logical Reasoning, Working Memory, Spatial Representation & Mapping, Attention (minor)
L3: ",L2
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon evaluates an agent’s ability to solve tasks by selecting, sequencing, and correctly invoking tools across varied domains. It emphasizes robust tool planning, parameter correctness, and recovery from failures or partial results to reach end-to-end task completion.","L1: Language Comprehension (minor)
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory
L3: ",L2
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,FrontierMath measures expert-level mathematical reasoning on problems designed to be challenging for state-of-the-art models and to reduce contamination. It targets deep multi-step proofs/derivations and precise quantitative reasoning under strict correctness.,"L1: 
L2: Logical Reasoning, Working Memory, Planning
L3: Cognitive Flexibility (minor)",L2
