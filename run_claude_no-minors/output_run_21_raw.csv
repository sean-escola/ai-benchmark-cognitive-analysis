Benchmark,Website,Paper,Description,Cognitive Functions
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates software engineering agents on real GitHub issues by requiring them to generate patches that make a project’s tests pass. The Verified split emphasizes tasks that have been human-validated as solvable, aiming to reduce ambiguity and evaluation noise compared with raw SWE-bench.","Planning, Logical Reasoning, Adaptive Error Correction, Working Memory, Language Comprehension (minor), Language Production (minor)"
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 tests agentic performance on real command-line tasks inside sandboxed environments, where models must navigate files, run programs, and debug failures. Success depends on executing correct sequences of shell actions under resource and time constraints.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Attention (minor)"
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp evaluates deep-research style web agents on information-seeking questions that require multi-step browsing and synthesis rather than single-page lookup. It emphasizes searching, reading, cross-checking, and composing a final answer grounded in retrieved sources.","Planning, Attention, Semantic Understanding & Context Recognition, Decision-making, Episodic Memory (minor), Language Production (minor)"
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench measures an agent’s ability to solve multi-turn customer-support tasks while using tools/APIs and adhering to domain policies (e.g., retail, airline, telecom). It stresses consistent policy-following, reliable tool invocation, and helpful natural-language interaction under constraints.","Social Reasoning & Theory of Mind, Decision-making, Planning, Inhibitory Control, Language Comprehension, Language Production, Empathy (minor)"
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld evaluates multimodal computer-use agents performing tasks in operating-system-like GUI environments (apps, settings, web, files) with a step budget. Models must perceive screens, decide actions (click/type), recover from mistakes, and finish end-to-end workflows.","Visual Perception, Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Planning, Sensorimotor Coordination, Working Memory, Adaptive Error Correction"
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI is a “fluid intelligence” benchmark of abstract grid transformation tasks, where the model infers rules from a few input-output examples and applies them to a new grid. It targets generalization to novel patterns with minimal data and discourages memorization of fixed templates.","Logical Reasoning, Cognitive Flexibility, Working Memory, Spatial Representation & Mapping, Scene Understanding & Visual Reasoning (minor)"
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon autonomous agency by simulating the operation of a vending-machine business over an extended period with many sequential decisions. Strong performance requires coherent strategy, adapting to changing conditions, and balancing short-term costs vs long-term profit.","Planning, Decision-making, Reward Mechanisms, Working Memory, Episodic Memory (minor), Self-reflection (minor)"
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas benchmarks real-world tool use via the Model Context Protocol, requiring models to discover tools, call them correctly, handle errors/retries, and integrate outputs into answers. It emphasizes multi-step workflows across heterogeneous, production-like APIs rather than single-shot tool calls.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Language Comprehension (minor)"
FinanceAgent,https://www.vals.ai/benchmarks/finance_agent,,"FinanceAgent evaluates whether models can perform tasks typical of an entry-level financial analyst, such as analysis, research, calculations, and producing finance-oriented deliverables. It emphasizes correct domain reasoning and structured outputs with appropriate assumptions and citations when required.","Semantic Understanding & Context Recognition, Logical Reasoning, Decision-making, Planning (minor), Working Memory, Language Production (minor)"
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity agent capabilities on real-world vulnerability tasks, including identifying known vulnerabilities and discovering previously unknown ones in open-source projects. It stresses precise technical reasoning, iterative debugging, and safe, correct interaction with code and tooling.","Logical Reasoning, Planning, Adaptive Error Correction, Inhibitory Control (minor), Decision-making (minor), Working Memory"
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"SpreadsheetBench tests agents on complex spreadsheet tasks derived from real workflows (editing formulas, restructuring sheets, computing values, and producing correct artifacts). It rewards reliable, step-by-step manipulation and verification across many interdependent cells and operations.","Working Memory, Planning, Logical Reasoning, Adaptive Error Correction, Attention (minor)"
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam (HLE) is a difficult multimodal benchmark intended to probe frontier-level knowledge and reasoning across domains, including questions that may require synthesis across modalities. It evaluates both accuracy and the ability to handle complex, expert-style prompts under realistic conditions.","Language Comprehension, Logical Reasoning, Working Memory, Visual Perception (minor), Scene Understanding & Visual Reasoning (minor), Semantic Understanding & Context Recognition"
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 consists of competition mathematics problems that require multi-step derivations, careful algebraic manipulation, and exact final answers. It is commonly used to measure mathematical reasoning robustness without relying on domain-specific tools.","Logical Reasoning, Working Memory, Adaptive Error Correction (minor), Planning (minor)"
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a high-difficulty subset of graduate-level multiple-choice science questions designed to be resistant to simple web search. It targets deep conceptual understanding and multi-step reasoning in physics, chemistry, and biology.","Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Decision-making (minor)"
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends broad academic knowledge testing across many subjects to multiple languages, assessing both understanding and reasoning outside English. It probes whether models can transfer knowledge and follow task instructions robustly across linguistic contexts.","Language Comprehension, Language Production (minor), Semantic Understanding & Context Recognition, Logical Reasoning (minor), Working Memory (minor)"
MMMU,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2311.16502,"MMMU measures multimodal understanding and reasoning across many disciplines, combining text with images such as diagrams, charts, and figures. It emphasizes integrating visual evidence with textual context to answer expert-level questions.","Visual Perception, Scene Understanding & Visual Reasoning, Multisensory Integration, Language Comprehension, Logical Reasoning, Attention (minor)"
LAB-Bench FigQA,https://huggingface.co/datasets/futurehouse/lab-bench,https://arxiv.org/abs/2407.10362,"LAB-Bench FigQA evaluates whether models can interpret and reason over complex scientific figures from biology papers, such as plots and microscopy panels. It stresses extracting quantitative/relational information from visuals and connecting it to domain concepts.","Scene Understanding & Visual Reasoning, Visual Perception, Attention, Logical Reasoning, Semantic Understanding & Context Recognition (minor)"
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,CharXiv Reasoning tests scientific figure/document reasoning by asking questions that require interpreting content from arXiv-style papers (often involving charts/figures) and performing multi-step inference. It emphasizes grounded reasoning from technical materials rather than generic commonsense.,"Language Comprehension, Scene Understanding & Visual Reasoning, Logical Reasoning, Working Memory, Attention (minor), Semantic Understanding & Context Recognition (minor)"
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU extends multimodal understanding to videos, requiring models to answer questions that depend on temporal visual evidence and narrative context. It targets long-range integration across frames and robustness to distractors in dynamic scenes.","Visual Perception, Scene Understanding & Visual Reasoning, Attention, Working Memory, Cognitive Timing & Predictive Modeling (minor), Multisensory Integration (minor)"
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"The FACTS Benchmark Suite systematically evaluates factuality and grounding, measuring whether model outputs remain faithful to provided sources and avoid hallucinated claims. It is designed to capture both factual correctness and calibration-like behaviors (e.g., acknowledging uncertainty).","Semantic Understanding & Context Recognition, Inhibitory Control, Self-reflection (minor), Language Production (minor), Working Memory (minor)"
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA evaluates physical commonsense reasoning across many languages and cultural contexts using non-parallel prompts, testing whether models select plausible actions/solutions in everyday scenarios. It emphasizes robust understanding beyond English-centric phrasing and distributions.","Semantic Understanding & Context Recognition, Language Comprehension, Logical Reasoning (minor), Social Reasoning & Theory of Mind (minor)"
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) is a long-context, multi-round coreference/retrieval benchmark where multiple similar “needle” requests are embedded in long “haystacks,” and the model must reproduce the correct referenced response. It stresses maintaining and retrieving the right association across long, distracting contexts.","Working Memory, Attention, Episodic Memory, Semantic Understanding & Context Recognition"
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval measures well-specified professional knowledge-work performance across many occupations via human-judge comparisons, focusing on producing real work artifacts (e.g., spreadsheets, presentations, plans). It is intended to reflect economically meaningful end-to-end task completion rather than narrow QA accuracy.","Planning, Decision-making, Language Production, Semantic Understanding & Context Recognition, Self-reflection (minor), Reward Mechanisms (minor)"
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks evaluates reasoning over graph-structured problems represented in text, requiring models to perform multi-step traversals (e.g., BFS-like walks) and return correct nodes/paths. It targets systematic, compositional reasoning under long-context distraction and strict correctness requirements.","Spatial Representation & Mapping, Logical Reasoning, Working Memory, Attention, Planning (minor)"
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon evaluates tool-using agents on diverse, multi-step tasks requiring correct selection and sequencing of tool calls, argument construction, and integration of results into a final response. It emphasizes reliability under tool errors, partial information, and complex workflows.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Inhibitory Control (minor), Language Comprehension (minor)"
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath is an expert-level mathematics benchmark designed to be substantially harder and more contamination-resistant than standard contest sets, spanning problems that demand deeper proof-style and technical reasoning. It is used to track progress on advanced mathematical problem solving, often with optional tool support depending on the evaluation setup.","Logical Reasoning, Working Memory, Cognitive Flexibility, Planning (minor), Adaptive Error Correction (minor)"
