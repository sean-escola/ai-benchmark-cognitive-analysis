Benchmark,Website,Paper,Description,Cognitive Functions
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates code-fixing agents on real GitHub issues by requiring a patch that makes a project’s tests pass, with tasks curated/verified to be solvable and to have reliable evaluation. It emphasizes end-to-end software engineering behavior: reading issue context, locating relevant code, implementing changes, and validating via tests under a standardized harness.","Planning, Logical Reasoning, Adaptive Error Correction, Working Memory, Language Comprehension, Language Production (minor), Decision-making (minor)"
SWE-bench Multilingual,https://www.swebench.com/multilingual.html,https://arxiv.org/abs/2504.21798,"SWE-bench Multilingual extends SWE-bench-style repository patching beyond Python to multiple programming languages, assessing whether agents can generalize debugging and implementation skills across ecosystems. Success requires understanding different build/test toolchains and language-specific conventions while still producing a correct patch under automated evaluation.","Planning, Logical Reasoning, Cognitive Flexibility, Adaptive Error Correction, Working Memory, Language Comprehension (minor)"
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro is a larger, more difficult and more contamination-resistant software engineering benchmark spanning multiple languages and more complex industrial-style tasks. It stresses robust agentic coding under realistic repo constraints, often requiring deeper diagnosis, multi-file changes, and careful regression avoidance to pass tests.","Planning, Logical Reasoning, Adaptive Error Correction, Decision-making, Working Memory, Cognitive Flexibility (minor)"
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 evaluates autonomous agents operating in command-line environments to accomplish practical tasks (e.g., debugging, environment setup, data manipulation) using shell commands and tooling. It probes whether an agent can iteratively observe outputs/errors, adjust strategy, and complete goals within real execution constraints.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Attention (minor), Language Comprehension (minor)"
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp evaluates “deep research” style browsing agents that must search, read, and synthesize information from a controlled web document index to answer questions. It emphasizes evidence-based retrieval, cross-document integration, and maintaining task coherence while using search/browse tools under evaluation constraints.","Planning, Attention, Semantic Understanding & Context Recognition, Language Comprehension, Working Memory, Decision-making (minor)"
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench measures interactive tool-using agents in customer-support-style domains (e.g., retail, airline, telecom), requiring multi-turn dialogue with simulated users and programmatic API actions while following policies. It tests the agent’s ability to balance helpfulness with constraints, manage state over turns, and execute correct tool calls to resolve cases.","Social Reasoning & Theory of Mind, Inhibitory Control, Decision-making, Planning, Working Memory, Language Comprehension, Language Production, Empathy (minor)"
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld evaluates multimodal “computer use” agents that interact with operating-system desktops to complete tasks across applications via clicks, typing, and navigation. It stresses perception-action coupling: interpreting UI screenshots, choosing actions, and recovering from errors across long multi-step workflows.","Visual Perception, Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements (minor), Spatial Representation & Mapping, Sensorimotor Coordination, Planning, Decision-making, Adaptive Error Correction (minor)"
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI (Abstraction and Reasoning Corpus) tests fluid, few-shot pattern induction on grid-based puzzles, where models must infer latent rules from a handful of input-output examples. It is designed to emphasize generalization to novel tasks over memorization, requiring abstraction, compositional reasoning, and rapid hypothesis testing.","Logical Reasoning, Cognitive Flexibility, Working Memory, Spatial Representation & Mapping, Visual Perception (minor), Scene Understanding & Visual Reasoning (minor)"
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon agent coherence by simulating running a vending-machine business over an extended period, scoring final outcomes such as profitability. It probes sustained planning, adapting to a changing market, and integrating information across many sequential decisions and interactions (e.g., supplier negotiation, inventory control).","Planning, Decision-making, Reward Mechanisms, Episodic Memory, Working Memory (minor), Social Reasoning & Theory of Mind (minor), Self-reflection (minor)"
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas assesses real-world tool use via the Model Context Protocol by requiring multi-step workflows that involve discovering tools, invoking them correctly, handling failures, and synthesizing results. It targets reliable API interaction and orchestration behavior rather than purely linguistic competence.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Language Comprehension (minor), Semantic Understanding & Context Recognition (minor)"
FinanceAgent,https://www.vals.ai/benchmarks/finance_agent,,"FinanceAgent evaluates agent performance on tasks representative of an entry-level financial analyst, such as analyzing financial documents, building/validating calculations, and producing grounded recommendations. It emphasizes structured reasoning over quantitative and textual evidence while maintaining consistency and professional reporting standards.","Logical Reasoning, Semantic Understanding & Context Recognition, Planning, Decision-making, Working Memory, Language Production (minor)"
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity agents on vulnerability identification and discovery tasks grounded in real software projects, including finding known issues from descriptions and uncovering new vulnerabilities. It stresses careful code reasoning, hypothesis-driven exploration, and iterative debugging-like refinement under realistic constraints.","Logical Reasoning, Planning, Adaptive Error Correction, Attention (minor), Working Memory, Inhibitory Control (minor)"
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"SpreadsheetBench evaluates agents on complex spreadsheet tasks derived from real workflows, requiring navigation, formula manipulation, restructuring, and error-free outputs. It probes multi-step procedural competence, consistency across many dependent cells, and the ability to detect and correct subtle mistakes.","Planning, Working Memory, Logical Reasoning, Adaptive Error Correction, Attention, Decision-making (minor)"
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam (HLE) is a large, difficult benchmark spanning expert-level questions at the frontier of human knowledge, including multimodal items. It evaluates deep reasoning and synthesis across domains, often requiring integrating information from problem statements (and, in tool-enabled settings, external evidence and computation).","Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Scene Understanding & Visual Reasoning (minor), Multisensory Integration (minor), Planning (minor)"
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 consists of competition mathematics problems from the American Invitational Mathematics Examination, typically requiring multi-step symbolic reasoning and careful constraint handling. It measures structured problem solving rather than retrieval, with answers scored objectively.","Logical Reasoning, Planning, Working Memory, Attention (minor)"
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,GPQA Diamond is a high-quality subset of GPQA with difficult graduate-level science multiple-choice questions designed to be “Google-proof” and to distinguish expert reasoning from superficial pattern matching. It tests scientific understanding and reasoning under strong distractors in a standardized format.,"Semantic Understanding & Context Recognition, Logical Reasoning, Decision-making, Working Memory (minor), Language Comprehension (minor)"
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends MMLU-style academic knowledge and reasoning evaluation across many languages, measuring whether models retain competence beyond English. It probes multilingual understanding of subject matter and the ability to answer consistently across linguistic contexts.","Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning (minor), Working Memory (minor), Language Production (minor)"
MMMU,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2311.16502,"MMMU is a broad multimodal benchmark covering many disciplines where models must answer questions that combine text with images (e.g., diagrams, charts, scenes). It emphasizes integrating visual evidence with domain knowledge and reasoning to select or generate correct answers.","Multisensory Integration, Visual Perception, Scene Understanding & Visual Reasoning, Language Comprehension, Logical Reasoning, Working Memory (minor), Attention (minor)"
LAB-Bench FigQA,https://huggingface.co/datasets/futurehouse/lab-bench,https://arxiv.org/abs/2407.10362,LAB-Bench FigQA evaluates whether models can correctly interpret complex scientific figures from biology papers and answer questions grounded in those visuals. It stresses precise extraction of evidence from plots/diagrams and combining it with domain reasoning to avoid hallucinated conclusions.,"Scene Understanding & Visual Reasoning, Visual Perception, Multisensory Integration, Logical Reasoning, Attention, Semantic Understanding & Context Recognition (minor)"
WebArena,https://webarena.dev/,https://arxiv.org/abs/2307.13854,"WebArena evaluates autonomous web agents completing realistic tasks across multiple web apps (e-commerce, CMS, forums, code hosting), requiring navigation, form filling, and stateful interaction with dynamic pages. It emphasizes long-horizon planning, robust UI understanding, and recovery from intermediate failures under an automated grader.","Planning, Decision-making, Working Memory, Adaptive Error Correction, Visual Perception, Scene Understanding & Visual Reasoning, Sensorimotor Coordination, Attention (minor)"
