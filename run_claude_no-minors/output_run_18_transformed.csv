Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates models on real GitHub issues where the goal is to produce a patch that makes a repository’s tests pass, with tasks filtered/verified for solvability and clearer evaluation. It emphasizes end-to-end software engineering: understanding bug reports, modifying code, and validating fixes via automated test outcomes.","L1: Language Comprehension, Language Production
L2: Logical Reasoning, Planning, Adaptive Error Correction, Working Memory
L3: ",L2
SWE-bench Multilingual,https://www.swebench.com/multilingual.html,https://arxiv.org/abs/2504.21798,"SWE-bench Multilingual extends SWE-bench-style evaluation beyond Python to multiple programming languages, testing whether a model can generalize software engineering behaviors across different ecosystems and toolchains. Success requires correctly interpreting issue descriptions and implementing patches that satisfy language-appropriate tests and conventions.","L1: Language Comprehension, Language Production
L2: Logical Reasoning, Planning, Adaptive Error Correction, Working Memory
L3: ",L2
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro is a more challenging, contamination-resistant software engineering benchmark with harder, more diverse real-world tasks across multiple languages and repositories. It stresses robust debugging, correct patch synthesis, and tool-aware iteration under stricter evaluation and broader task variety than SWE-bench Verified.","L1: Language Comprehension, Language Production
L2: Logical Reasoning, Planning, Adaptive Error Correction, Working Memory
L3: Inhibitory Control (minor)",L2
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 evaluates agentic performance in command-line environments on realistic tasks such as installing dependencies, editing files, running programs, and diagnosing failures. It measures how well a model can plan and execute multi-step terminal workflows while recovering from errors and environment constraints.","L1: Language Comprehension, Language Production (minor)
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory
L3: ",L2
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp evaluates research-style web browsing: models must search, navigate sources, and synthesize answers from retrieved documents, often requiring multi-step information gathering. The benchmark emphasizes reliable tool use, source integration, and maintaining context across a browsing session.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Attention, Working Memory, Episodic Memory (minor)
L3: Self-reflection (minor)",L2
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench measures whether an agent can resolve multi-turn customer-support tasks by interacting with simulated users and APIs while following domain policies (e.g., retail, airline, telecom). It probes policy adherence under conversational pressure, correct tool/API invocation, and coherent multi-step problem resolution.","L1: Language Comprehension, Language Production
L2: Decision-making, Planning, Working Memory
L3: Social Reasoning & Theory of Mind, Inhibitory Control, Empathy (minor)",L3
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld evaluates multimodal computer-use agents that must complete tasks in a full operating-system environment using screenshots and interaction actions over many steps. It emphasizes UI understanding, long-horizon execution, and robust recovery from mistakes in realistic desktop workflows.","L1: Visual Perception
L2: Visual Attention & Eye Movements (minor), Scene Understanding & Visual Reasoning, Planning, Decision-making, Sensorimotor Coordination, Working Memory, Adaptive Error Correction
L3: ",L2
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI tests few-shot “fluid” reasoning on novel grid-based transformation puzzles, where models must infer latent rules from a small set of examples and generalize to new inputs. It is designed to reduce reliance on memorized knowledge and instead emphasize abstraction and transferable reasoning patterns.","L1: Visual Perception
L2: Logical Reasoning, Working Memory, Spatial Representation & Mapping, Planning (minor)
L3: Cognitive Flexibility",L3
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon autonomy by having an agent manage a simulated vending-machine business over an extended timeframe, optimizing inventory, pricing, procurement, and cashflow. It stresses sustained coherence, strategic decision-making, and adaptation to changing market dynamics across many steps.","L1: Language Comprehension (minor), Language Production (minor)
L2: Planning, Decision-making, Reward Mechanisms, Working Memory, Adaptive Error Correction (minor)
L3: Motivational Drives (minor)",L2
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas measures real-world tool use via the Model Context Protocol (MCP), requiring models to discover relevant tools, call them correctly, handle errors/retries, and combine outputs into a correct final response. Tasks reflect production-like multi-step workflows across heterogeneous services and APIs.","L1: Language Comprehension, Language Production (minor)
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory
L3: Inhibitory Control (minor)",L2
FinanceAgent,https://www.vals.ai/benchmarks/finance_agent,,"FinanceAgent evaluates whether a model can perform tasks expected of an entry-level financial analyst, such as extracting facts from documents, performing calculations, building structured analyses, and justifying conclusions. It emphasizes domain reasoning, numerical accuracy, and producing decision-relevant artifacts under realistic constraints.","L1: Language Comprehension, Language Production
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Planning (minor), Decision-making (minor)
L3: ",L2
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity agent capabilities on large-scale tasks such as locating known vulnerabilities from descriptions and discovering new vulnerabilities in real codebases. It stresses precise reasoning about program behavior, constructing exploits or proof-of-vulnerability artifacts, and iterating based on tool feedback.","L1: Language Comprehension, Language Production (minor)
L2: Logical Reasoning, Planning, Adaptive Error Correction, Working Memory
L3: Inhibitory Control (minor)",L2
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"SpreadsheetBench tests spreadsheet understanding and manipulation on realistic workbooks, requiring models to navigate sheets, interpret formulas/tables, and produce correct edits or derived outputs. It emphasizes structured reasoning over semi-formal artifacts and careful multi-step transformations with low tolerance for small errors.","L1: 
L2: Semantic Understanding & Context Recognition, Working Memory, Planning, Logical Reasoning, Adaptive Error Correction (minor), Attention (minor)
L3: ",L2
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a difficult, broad benchmark intended to probe frontier-level knowledge and reasoning, including questions that may require multi-step inference and, in some setups, tool use (e.g., search or code). It aims to stress capabilities beyond standard academic QA, including robustness on complex, long-form problems and multimodal items.","L1: Language Comprehension, Visual Perception (minor)
L2: Logical Reasoning, Working Memory, Semantic Understanding & Context Recognition, Planning (minor), Scene Understanding & Visual Reasoning (minor)
L3: ",L2
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 evaluates competition-style mathematics problem solving, typically requiring creative multi-step reasoning rather than routine computation. It is often used to measure a model’s ability to maintain consistent intermediate constraints and avoid arithmetic/logical slips under time-test-like conditions.","L1: 
L2: Logical Reasoning, Working Memory, Planning (minor), Adaptive Error Correction (minor)
L3: ",L2
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,GPQA Diamond is a high-quality subset of GPQA with difficult graduate-level science multiple-choice questions designed to be hard to answer via superficial pattern matching. It probes deep conceptual understanding and careful reasoning under distractor options.,"L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Decision-making (minor)
L3: ",L2
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends MMLU-style evaluation across many subjects and multiple languages, testing whether models can answer knowledge-and-reasoning questions beyond English. It emphasizes cross-lingual generalization and stable performance across diverse linguistic contexts and domains.","L1: Language Comprehension, Language Production (minor)
L2: Semantic Understanding & Context Recognition, Logical Reasoning (minor), Working Memory (minor)
L3: ",L2
MMMU,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2311.16502,"MMMU is a multimodal, multi-discipline benchmark where questions require jointly reasoning over images and text, often in expert domains (e.g., charts, diagrams, technical figures). It evaluates whether models can ground language in visual evidence and perform multi-step inference from complex visual inputs.","L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Working Memory (minor), Visual Attention & Eye Movements (minor)
L3: ",L2
LAB-Bench FigQA,https://huggingface.co/datasets/futurehouse/lab-bench,https://arxiv.org/abs/2407.10362,"LAB-Bench FigQA focuses on interpreting scientific figures from biology papers, requiring models to extract evidence from plots/diagrams and answer targeted questions about experimental results. It stresses fine-grained figure understanding and the ability to connect visual signals to domain concepts and claims.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory (minor), Attention (minor)
L3: ",L2
WebArena,https://webarena.dev/,https://arxiv.org/abs/2307.13854,"WebArena evaluates autonomous web agents performing realistic multi-step tasks across several self-hosted web apps (e.g., shopping, CMS, forums, Git workflows), requiring navigation, form filling, and stateful interaction. It measures long-horizon planning and robust execution under dynamic UI and partial observability.","L1: Visual Perception
L2: Planning, Decision-making, Scene Understanding & Visual Reasoning, Sensorimotor Coordination, Working Memory, Adaptive Error Correction, Attention (minor)
L3: Inhibitory Control (minor)",L2
