Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates software engineering ability by asking a model (often in an agent scaffold) to produce patches that fix real GitHub issues, then running tests to verify correctness. The “Verified” subset focuses on tasks that have been human-validated as solvable with a clear evaluation signal, emphasizing end-to-end debugging and code change synthesis.","L1: Language Comprehension (minor), Language Production (minor)
L2: Planning, Adaptive Error Correction, Logical Reasoning, Working Memory
L3: ",L2
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 measures an agent’s ability to complete real tasks in a command-line environment, such as inspecting files, running programs, and iteratively fixing errors. Success depends on choosing effective shell actions, interpreting tool outputs, and recovering from failures under realistic constraints.","L1: Language Comprehension (minor)
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory, Attention (minor)
L3: ",L2
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp evaluates web-browsing and research behavior: models must search, read, and synthesize information from multiple sources to answer questions that are difficult to solve without browsing. It stresses retrieval strategy, verification, and integrating evidence across documents.","L1: Language Production (minor)
L2: Planning, Attention, Semantic Understanding & Context Recognition, Working Memory, Decision-making (minor)
L3: ",L2
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench evaluates interactive agent performance in multi-turn customer support settings (e.g., retail, airline, telecom) with simulated users and programmatic APIs. Models must follow domain policies while resolving user requests, maintaining consistent state across turns, and using tools correctly.","L1: Language Comprehension (minor), Language Production (minor)
L2: Decision-making, Planning, Working Memory
L3: Social Reasoning & Theory of Mind, Inhibitory Control",L3
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld measures multimodal “computer use” where an agent operates within an operating-system-like GUI to complete tasks over multiple steps. It requires interpreting screenshots, locating relevant UI elements, and executing sequences of actions robustly despite interface and state changes.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Spatial Representation & Mapping, Planning, Sensorimotor Coordination (minor)
L3: ",L2
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI tests fluid reasoning via small grid-based puzzles where a model must infer a hidden transformation rule from a few input–output examples and apply it to a new input. It emphasizes abstraction, compositional rule induction, and generalization beyond memorized patterns.","L1: Visual Perception (minor)
L2: Logical Reasoning, Working Memory, Spatial Representation & Mapping, Attention (minor)
L3: Cognitive Flexibility",L3
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon autonomous agent behavior by simulating the management of a vending-machine business over an extended period. The agent must make sequential business decisions (inventory, pricing, supplier interactions) to maximize outcomes, testing coherence over many steps.","L1: 
L2: Planning, Decision-making, Reward Mechanisms, Working Memory, Adaptive Error Correction (minor)
L3: Social Reasoning & Theory of Mind (minor)",L2
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas evaluates real-world tool use through the Model Context Protocol by requiring multi-step workflows across tool servers and APIs. Models must select appropriate tools, invoke them with correct arguments, handle errors, and synthesize results into a final answer.","L1: Language Comprehension (minor), Language Production (minor)
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory
L3: ",L2
FinanceAgent,https://www.vals.ai/benchmarks/finance_agent,,"FinanceAgent assesses performance on tasks representative of entry-level financial analysis, such as interpreting financial documents, building analyses, and producing structured outputs. It emphasizes correctly applying domain rules, maintaining consistency across assumptions, and producing decision-relevant artifacts.","L1: Language Production (minor)
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Planning, Decision-making, Working Memory
L3: ",L2
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates agentic cybersecurity capability on large collections of tasks involving identifying known vulnerabilities and, in some settings, discovering new ones in real open-source projects. Success requires reading code, hypothesizing failure modes, testing ideas, and iterating toward valid exploits or fixes.","L1: 
L2: Logical Reasoning, Planning, Adaptive Error Correction, Decision-making, Working Memory
L3: Inhibitory Control (minor)",L2
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"SpreadsheetBench measures an agent’s ability to understand and manipulate complex spreadsheets derived from real-world workflows. It tests structured data transformation, formula reasoning, and multi-step edits where correctness depends on both local cell changes and global sheet consistency.","L1: 
L2: Logical Reasoning, Working Memory, Planning, Attention, Adaptive Error Correction (minor), Decision-making (minor)
L3: ",L2
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a difficult multimodal benchmark spanning frontier academic and professional questions, designed to probe advanced reasoning rather than routine recall. Depending on the setup, models may need to synthesize information across modalities and maintain high precision under ambiguity.","L1: Language Comprehension
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Scene Understanding & Visual Reasoning (minor), Multisensory Integration (minor)
L3: ",L2
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 evaluates competition-style mathematical problem solving using the 2025 AIME question set. It emphasizes multi-step derivations, careful constraint handling, and exact final answers (often under time-like pressure assumptions even if not enforced).","L1: 
L2: Logical Reasoning, Working Memory, Adaptive Error Correction (minor), Attention (minor)
L3: Cognitive Flexibility",L3
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,GPQA Diamond is a high-difficulty subset of GPQA consisting of graduate-level science multiple-choice questions designed to be “Google-proof.” It probes deep conceptual understanding and reasoning under distractors rather than surface pattern matching.,"L1: Language Comprehension (minor)
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Decision-making (minor)
L3: ",L2
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,MMMLU extends broad academic knowledge testing across many subjects into multiple non-English languages. It evaluates whether a model can understand and reason about domain content when expressed in different languages and cultural/linguistic contexts.,"L1: Language Comprehension, Language Production (minor)
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory (minor)
L3: ",L2
MMMU,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2311.16502,"MMMU is a multimodal benchmark where questions require combining text with images (e.g., diagrams, charts, tables) across diverse disciplines. It stresses visual grounding of concepts and reasoning that integrates perceptual evidence with linguistic instructions.","L1: Visual Perception, Language Comprehension
L2: Multisensory Integration, Scene Understanding & Visual Reasoning, Logical Reasoning, Working Memory (minor)
L3: ",L2
LAB-Bench FigQA,https://huggingface.co/datasets/futurehouse/lab-bench,https://arxiv.org/abs/2407.10362,"LAB-Bench FigQA tests whether models can interpret complex scientific figures from biology papers and answer questions that depend on visual evidence. It emphasizes careful figure reading, mapping labels to mechanisms, and drawing correct inferences from plots and schematics.","L1: Visual Perception, Language Comprehension (minor)
L2: Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Logical Reasoning, Working Memory (minor)
L3: ",L2
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning evaluates reasoning over scientific charts and figures, where the correct answer depends on interpreting plotted data, axes, and visual encodings. It probes quantitative and relational inference grounded in visual inputs, sometimes benefiting from external computation tools.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Logical Reasoning, Attention, Working Memory (minor)
L3: ",L2
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU extends multimodal understanding to video, requiring models to answer questions using temporally unfolding visual information and accompanying text. It stresses tracking events over time, linking actions to outcomes, and integrating cues across frames.","L1: Visual Perception
L2: Working Memory, Attention, Scene Understanding & Visual Reasoning, Multisensory Integration (minor)
L3: Cognitive Timing & Predictive Modeling",L3
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"The FACTS Benchmark Suite systematically evaluates factuality-related behaviors, such as grounding to evidence, resisting unsupported claims, and maintaining consistency under adversarial or misleading contexts. It targets truthfulness and calibration beyond simple QA accuracy.","L1: 
L2: Semantic Understanding & Context Recognition, Logical Reasoning (minor), Working Memory (minor)
L3: Inhibitory Control, Self-reflection (minor)",L3
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA evaluates physical commonsense reasoning across languages and locales, focusing on understanding everyday interactions with objects and environments. It tests whether models can select plausible actions/outcomes using implicit physical constraints rather than memorized phrasing.","L1: Language Comprehension (minor)
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Spatial Representation & Mapping
L3: Cognitive Timing & Predictive Modeling (minor)",L2
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) is a long-context evaluation where multiple similar “needle” requests and responses are embedded in long conversations, and the model must retrieve the correct referenced item. It probes robustness of long-range coreference, interference resistance, and precise recall under distractors.","L1: Language Comprehension
L2: Working Memory, Attention, Episodic Memory
L3: Inhibitory Control (minor)",L2
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval measures well-specified professional knowledge-work performance across many occupations, judged by expert humans via pairwise comparisons of produced work products (e.g., spreadsheets, slides, plans). It emphasizes end-to-end execution quality, instruction following, and producing usable artifacts under realistic constraints.","L1: Language Production
L2: Planning, Decision-making, Semantic Understanding & Context Recognition, Working Memory (minor), Reward Mechanisms (minor)
L3: ",L2
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks evaluates structured reasoning over graphs described in text (e.g., performing BFS-like traversals or identifying parent relationships) often in long contexts. It tests whether a model can maintain a consistent internal representation of a graph and execute algorithmic reasoning steps reliably.","L1: 
L2: Spatial Representation & Mapping, Logical Reasoning, Working Memory, Attention (minor), Planning (minor)
L3: ",L2
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon evaluates agentic tool use across diverse tasks that require selecting, sequencing, and correctly calling tools to reach a goal. It emphasizes robustness to tool errors, parameter correctness, and integrating tool outputs into coherent final responses.","L1: Language Comprehension (minor), Language Production (minor)
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory
L3: ",L2
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath is a benchmark of expert-level mathematics designed to be challenging and more resistant to memorization, emphasizing novel, multi-step proofs or computations. It probes deep mathematical reasoning, abstraction, and sustained multi-stage problem solving, often with optional computational assistance in some evaluations.","L1: 
L2: Logical Reasoning, Working Memory, Adaptive Error Correction (minor), Attention (minor)
L3: Cognitive Flexibility",L3
