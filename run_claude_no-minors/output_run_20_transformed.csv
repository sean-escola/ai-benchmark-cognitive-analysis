Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates software engineering agents on real GitHub issues by requiring them to generate a patch that makes a project’s tests pass. The “Verified” split consists of tasks confirmed by humans to be solvable and reliably graded, aiming to measure end-to-end bug fixing rather than isolated code writing.","L1: Language Comprehension (minor), Language Production (minor)
L2: Planning, Decision-making, Logical Reasoning, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition
L3: ",L2
SWE-bench Multilingual,https://www.swebench.com/multilingual.html,https://arxiv.org/abs/2504.21798,"SWE-bench Multilingual extends SWE-bench-style repository patching to multiple programming languages, testing whether agents can localize and fix bugs beyond Python ecosystems. It probes cross-language generalization in understanding codebases, toolchains, and tests under realistic repo constraints.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Planning, Decision-making, Adaptive Error Correction, Working Memory (minor)
L3: Cognitive Flexibility",L3
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro is a larger, more challenging software engineering benchmark emphasizing industrially relevant tasks and stronger contamination resistance, typically spanning multiple languages and more complex repos. It focuses on sustained, end-to-end engineering competence: diagnosing issues, implementing fixes, and satisfying automated checks.","L1: Language Production (minor)
L2: Planning, Decision-making, Logical Reasoning, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition
L3: Cognitive Flexibility (minor)",L2
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 measures agent performance on real command-line tasks inside sandboxed environments, such as debugging, building, and manipulating files and processes. Success depends on selecting correct shell commands, interpreting outputs, and iteratively recovering from errors under resource constraints.","L1: Language Comprehension (minor)
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory, Logical Reasoning, Attention (minor)
L3: ",L2
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp evaluates “browse-and-answer” capability on information-seeking questions that require searching, gathering evidence, and synthesizing an answer from multiple sources (often with a controlled corpus or reproducible setup). It stresses research-style decomposition, retrieval strategy, and attribution-grounded synthesis rather than pure recall.","L1: Language Production (minor)
L2: Planning, Attention, Semantic Understanding & Context Recognition, Working Memory, Logical Reasoning, Episodic Memory (minor)
L3: Self-reflection (minor)",L2
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench evaluates interactive customer-support agents that must follow domain policies while using APIs/tools across multi-turn dialogues (e.g., retail, airline, telecom). The benchmark emphasizes robust instruction/policy adherence, state tracking across turns, and choosing compliant action sequences under user pressure.","L1: Language Comprehension, Language Production
L2: Decision-making, Planning, Working Memory
L3: Inhibitory Control, Social Reasoning & Theory of Mind, Empathy (minor)",L3
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld is a multimodal computer-use benchmark where agents interact with a desktop OS to complete tasks (navigation, settings changes, file operations, web/app workflows) via screenshots and actions. It tests perception-to-action coupling, multi-step planning, and robustness to UI variability and intermediate failures.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Sensorimotor Coordination, Planning, Decision-making, Adaptive Error Correction (minor), Working Memory (minor)
L3: ",L2
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,ARC-AGI tests few-shot abstract reasoning over grid-based puzzles: models infer latent transformation rules from a handful of input–output examples and must generalize to a new input. It is designed to emphasize fluid reasoning and compositional generalization with minimal training signal from the task itself.,"L1: Visual Perception
L2: Logical Reasoning, Spatial Representation & Mapping, Working Memory, Planning (minor)
L3: Cognitive Flexibility, Cognitive Timing & Predictive Modeling (minor)",L3
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon agent coherence and strategy by simulating management of a vending-machine business over an extended period with many sequential decisions. High scores require sustained goal pursuit (inventory, pricing, supplier negotiation), adaptation to changing conditions, and error recovery without drifting off-task.","L1: Language Production (minor)
L2: Planning, Decision-making, Reward Mechanisms, Working Memory, Episodic Memory (minor), Adaptive Error Correction (minor)
L3: Motivational Drives (minor)",L2
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas measures real-world tool-use competence via the Model Context Protocol (MCP), requiring models to discover and correctly invoke tools across multi-step workflows and synthesize results. Tasks stress correct API selection/argumenting, handling tool errors, and coordinating information across calls in production-like settings.","L1: Language Comprehension (minor), Language Production (minor)
L2: Planning, Decision-making, Semantic Understanding & Context Recognition, Adaptive Error Correction, Working Memory
L3: ",L2
FinanceAgent,https://www.vals.ai/benchmarks/finance_agent,,"FinanceAgent evaluates performance on tasks representative of an entry-level financial analyst, such as extracting information from documents, performing calculations, and producing justified outputs. It emphasizes structured analysis, numerical reasoning, and assembling decision-ready summaries from heterogeneous financial inputs.","L1: Language Production (minor)
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Planning, Decision-making
L3: ",L2
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity agent capability on large-scale, real-software vulnerability tasks, including identifying known vulnerability instances and, in some settings, finding new ones. It stresses code comprehension, adversarial/defensive reasoning, and iterative hypothesis testing under realistic constraints.","L1: 
L2: Logical Reasoning, Planning, Decision-making, Adaptive Error Correction, Semantic Understanding & Context Recognition, Working Memory (minor)
L3: Inhibitory Control (minor)",L2
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"SpreadsheetBench measures an agent’s ability to navigate and manipulate complex spreadsheets to solve real-world inspired problems, often requiring edits, formulas, and multi-step transformations. It tests whether models can maintain state across many cell-level operations while keeping outputs consistent with constraints.","L1: Visual Perception (minor)
L2: Planning, Working Memory, Decision-making, Logical Reasoning, Adaptive Error Correction (minor), Sensorimotor Coordination (minor)
L3: ",L2
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a frontier, multimodal benchmark spanning difficult questions across many expert domains, designed to stress deep reasoning and knowledge integration. It often requires combining textual evidence, diagrams/figures, and careful multi-step inference rather than short factual recall.","L1: Language Comprehension, Language Production (minor)
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Multisensory Integration, Scene Understanding & Visual Reasoning (minor)
L3: ",L2
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 consists of competition mathematics problems that typically require multi-step derivations, careful constraint handling, and exact answers. It is used to assess structured mathematical reasoning and reliability under nontrivial problem-solving depth.","L1: 
L2: Logical Reasoning, Working Memory, Planning, Decision-making (minor)
L3: Cognitive Timing & Predictive Modeling (minor)",L2
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a challenging multiple-choice science benchmark curated to be “Google-proof,” with questions that require expert-level reasoning in biology, chemistry, and physics. The Diamond subset emphasizes high-quality items where experts succeed and non-experts often fail, targeting deep scientific understanding over surface cues.","L1: Language Comprehension
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory (minor), Decision-making (minor)
L3: ",L2
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends MMLU-style breadth testing across many subjects into multiple languages, probing both knowledge and reasoning under multilingual variation. It assesses whether models preserve competence across linguistic contexts and domain shifts without heavy task-specific prompting.","L1: Language Comprehension, Language Production (minor)
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory (minor)
L3: Cognitive Flexibility",L3
MMMU,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2311.16502,"MMMU is a multimodal, multi-discipline benchmark requiring models to answer expert-level questions grounded in images (charts, diagrams, tables, scenes) plus text. It targets cross-modal reasoning: extracting visual information, aligning it with textual context, and performing multi-step inference.","L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Visual Attention & Eye Movements (minor), Working Memory (minor)
L3: ",L2
LAB-Bench FigQA,https://huggingface.co/datasets/futurehouse/lab-bench,https://arxiv.org/abs/2407.10362,"LAB-Bench FigQA evaluates whether models can correctly interpret complex scientific figures from biology papers and answer questions that depend on precise visual reading and domain context. It is designed to reflect realistic scientific analysis where key evidence is embedded in plots, schematics, and multi-panel figures.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Multisensory Integration, Semantic Understanding & Context Recognition, Logical Reasoning, Visual Attention & Eye Movements (minor), Working Memory (minor)
L3: ",L2
WebArena,https://webarena.dev/,https://arxiv.org/abs/2307.13854,"WebArena evaluates autonomous web agents performing realistic tasks across multiple web apps (e-commerce, CMS, forums, code hosting, maps), requiring navigation and interaction with dynamic pages. It stresses long-horizon planning, state tracking, and robust execution despite UI noise, partial observability, and action failures.","L1: Visual Perception
L2: Planning, Decision-making, Working Memory, Attention, Sensorimotor Coordination, Adaptive Error Correction
L3: Inhibitory Control (minor)",L2
