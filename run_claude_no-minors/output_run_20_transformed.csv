Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates software engineering by giving a model a real GitHub issue and a repository, then scoring whether the model can produce a correct patch that passes the project’s tests. The “Verified” subset includes tasks validated by human engineers to be solvable, reducing noise from ambiguous or broken issues.","L1: Language Comprehension (minor), Language Production (minor)
L2: Planning, Logical Reasoning, Adaptive Error Correction, Working Memory, Decision-making (minor)
L3: ",L2
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 measures an agent’s ability to complete real-world command-line tasks in a sandboxed terminal environment. Success requires choosing effective shell commands, interpreting outputs and errors, and iterating until the task is solved under realistic constraints.","L1: Language Comprehension (minor)
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory, Sensorimotor Coordination (minor)
L3: ",L2
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp evaluates deep-research style question answering where models must search, read, and synthesize information from a controlled document collection to produce correct final answers. It targets long-horizon information seeking, evidence integration, and robustness to partial or misleading snippets.","L1: Language Comprehension (minor)
L2: Planning, Attention, Semantic Understanding & Context Recognition, Working Memory, Decision-making, Episodic Memory (minor)
L3: ",L2
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench measures agent performance in simulated customer-support domains (e.g., retail, airline, telecom) where the agent must follow policies while interacting with tools and a simulated user over multiple turns. It emphasizes policy adherence, conversational grounding, and correct API/tool usage under realistic dialogue dynamics.","L1: Language Comprehension
L2: Planning, Decision-making, Working Memory (minor)
L3: Social Reasoning & Theory of Mind, Inhibitory Control, Empathy (minor)",L3
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld evaluates autonomous computer-use agents on tasks performed inside a graphical operating-system environment (e.g., navigating apps, settings, files, and web UIs). Models must perceive screenshots/UI state, plan multi-step interactions, and execute actions reliably to reach task goals.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Planning, Decision-making, Sensorimotor Coordination, Working Memory (minor)
L3: ",L2
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,ARC-AGI tests fluid reasoning via grid-based puzzles where a model must infer the underlying transformation rule from a few input–output examples and apply it to a new input. The benchmark is designed to reward generalization and abstraction rather than memorized knowledge.,"L1: 
L2: Logical Reasoning, Working Memory, Spatial Representation & Mapping, Attention (minor), Planning (minor)
L3: Cognitive Flexibility",L3
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon autonomous agency by placing a model in a simulated year-long vending-machine business management scenario. The agent must make thousands of coherent decisions—procurement, pricing, inventory, negotiation, and adaptation—to maximize final balance.","L1: 
L2: Planning, Decision-making, Reward Mechanisms, Working Memory, Semantic Understanding & Context Recognition
L3: Self-reflection (minor), Motivational Drives (minor)",L2
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas measures real-world tool-use competence over multi-step workflows implemented via the Model Context Protocol (MCP). Tasks require discovering appropriate tools, forming correct tool calls, handling errors/retries, and synthesizing results across multiple tool interactions.","L1: Language Comprehension (minor), Language Production (minor)
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory
L3: ",L2
FinanceAgent,https://www.vals.ai/benchmarks/finance_agent,,"FinanceAgent evaluates whether models can perform tasks expected of an entry-level financial analyst, such as analyzing filings, building or checking analyses, and producing structured finance outputs. It emphasizes accuracy, domain reasoning, and the ability to translate financial context into correct decisions and artifacts.","L1: Language Comprehension (minor), Language Production (minor)
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Decision-making, Planning (minor), Working Memory (minor)
L3: ",L2
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity agent capabilities on large-scale, real-world vulnerability tasks, including finding known vulnerabilities from descriptions and discovering new ones. It tests systematic reasoning over codebases, adversarial thinking, and iterative debugging/exploitation workflows.","L1: Language Comprehension (minor)
L2: Logical Reasoning, Planning, Adaptive Error Correction, Working Memory
L3: Inhibitory Control (minor)",L2
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"SpreadsheetBench measures an agent’s ability to understand, edit, and compute with real spreadsheets using programmatic tools (e.g., reading cells, writing formulas, generating tables). It stresses structured manipulation, consistency across many dependent cells, and error recovery when outputs don’t validate.","L1: 
L2: Logical Reasoning, Planning, Working Memory, Adaptive Error Correction, Attention (minor), Spatial Representation & Mapping (minor)
L3: ",L2
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam (HLE) is a large multimodal benchmark intended to probe frontier-level academic and professional knowledge, reasoning, and synthesis across diverse domains. Depending on evaluation settings, models may need to use tools (e.g., search, code) while avoiding retrieval-based contamination and hallucinated claims.","L1: Language Comprehension, Visual Perception (minor)
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Scene Understanding & Visual Reasoning (minor), Planning (minor)
L3: ",L2
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,AIME 2025 uses problems from the American Invitational Mathematics Examination to test competition-style mathematical reasoning. Tasks are short-answer math problems that require multi-step derivations and careful symbolic manipulation under time-like constraints.,"L1: 
L2: Logical Reasoning, Working Memory, Attention (minor), Planning (minor)
L3: ",L2
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a subset of GPQA consisting of high-quality graduate-level science multiple-choice questions designed to be difficult to answer via superficial pattern matching or simple web lookup. It tests deep scientific understanding, elimination reasoning, and careful reading of distractors.","L1: Language Comprehension (minor)
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory (minor)
L3: ",L2
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends MMLU to multiple languages, testing broad academic knowledge and reasoning across many subjects and non-English settings. It is commonly used to evaluate multilingual generalization, cross-lingual robustness, and consistent subject-matter reasoning.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory (minor)
L3: Cognitive Flexibility (minor)",L2
MMMU,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2311.16502,"MMMU evaluates multimodal understanding and reasoning across many disciplines, requiring models to answer questions grounded in images (diagrams, charts, photos) paired with text. It emphasizes integrating visual evidence with domain knowledge and multi-step reasoning.","L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Attention (minor)
L3: ",L2
LAB-Bench FigQA,https://huggingface.co/datasets/futurehouse/lab-bench,https://arxiv.org/abs/2407.10362,"LAB-Bench FigQA tests whether models can accurately interpret scientific figures from biology papers and answer questions that require extracting and reasoning over visual evidence. It targets research-relevant figure literacy, including reading plots, schematic relationships, and multi-panel layouts.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Semantic Understanding & Context Recognition, Logical Reasoning, Attention
L3: ",L2
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning evaluates reasoning over figures and charts from scientific documents, often requiring quantitative and relational inference from visual encodings. It aims to measure whether models can go beyond OCR-like extraction and perform correct, grounded visual-analytical reasoning.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Logical Reasoning, Semantic Understanding & Context Recognition (minor), Working Memory (minor), Attention (minor)
L3: ",L2
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU extends multimodal reasoning benchmarks to the video domain, requiring models to answer questions based on temporally unfolding visual content (and sometimes accompanying text/audio context). It stresses temporal integration, event understanding, and multi-step reasoning grounded in video evidence.","L1: Visual Perception, Language Comprehension (minor)
L2: Scene Understanding & Visual Reasoning, Working Memory, Attention, Multisensory Integration (minor)
L3: Cognitive Timing & Predictive Modeling",L3
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"FACTS Benchmark Suite systematically evaluates factuality and faithfulness, probing whether model outputs remain grounded in provided sources and whether they avoid fabricating unsupported claims across diverse tasks. It is designed to capture both retrieval/grounding failures and hallucination-like behaviors in realistic settings.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory (minor)
L3: Inhibitory Control (minor)",L2
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA evaluates commonsense and physical interaction reasoning in a multilingual, culturally broader setting, using non-parallel data to reduce simple translation artifacts. It probes whether models can choose plausible actions or explanations grounded in everyday physical causality across languages.","L1: Language Comprehension
L2: Logical Reasoning, Semantic Understanding & Context Recognition
L3: Cognitive Flexibility (minor)",L2
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) evaluates long-context multi-round coreference resolution by embedding multiple similar “needle” interactions inside a long “haystack” conversation and asking the model to retrieve the correct response for a specified needle. It stresses sustained context tracking, interference resistance, and precise retrieval from very long inputs.","L1: Language Comprehension
L2: Working Memory, Attention, Episodic Memory, Adaptive Error Correction (minor)
L3: ",L2
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval evaluates economically valuable professional knowledge work across many occupations, using expert human judgments to compare model outputs against (or alongside) professional baselines. Tasks request real work products (e.g., spreadsheets, plans, presentations), testing end-to-end quality rather than short-form QA accuracy.","L1: Language Production
L2: Planning, Decision-making, Semantic Understanding & Context Recognition, Working Memory (minor)
L3: Self-reflection (minor), Social Reasoning & Theory of Mind (minor)",L2
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks measures a model’s ability to perform structured graph traversal and relational reasoning when graph descriptions and queries are presented in text, often at long context lengths. It targets algorithmic reasoning (e.g., BFS-like operations) and accurate maintenance of intermediate state across many entities.","L1: 
L2: Logical Reasoning, Spatial Representation & Mapping, Working Memory, Attention (minor)
L3: ",L2
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon evaluates how reliably models can solve tasks by selecting and composing tool calls across a broad tool ecosystem, often requiring multi-step workflows and error handling. It emphasizes orchestration quality: choosing the right tools, forming valid calls, recovering from failures, and integrating tool outputs into final answers.","L1: Language Comprehension (minor)
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory
L3: ",L2
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath is a difficult mathematics benchmark aimed at expert-level problem solving, with tiered difficulty that includes problems beyond standard competition math. It is designed to test genuine mathematical reasoning and derivation ability rather than memorized templates.","L1: 
L2: Logical Reasoning, Working Memory, Planning (minor)
L3: Cognitive Flexibility (minor)",L2
