Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates software engineering agents on real GitHub issues by requiring them to produce a code patch that makes a failing test suite pass. The “Verified” subset consists of tasks validated by humans to be solvable and to have reliable evaluation signals, aiming to reduce noise and reward genuine debugging and implementation skill.","L1: Language Comprehension (minor), Language Production (minor)
L2: Planning, Decision-making, Logical Reasoning, Working Memory, Adaptive Error Correction
L3: ",L2
SWE-bench Multilingual,https://www.swebench.com/multilingual.html,https://arxiv.org/abs/2504.21798,"SWE-bench Multilingual extends SWE-bench-style issue resolution to multiple programming languages, measuring whether agents can transfer software engineering competence beyond Python. Tasks still require end-to-end repository understanding, patch creation, and passing tests, while introducing additional complexity from language- and ecosystem-specific tooling.","L1: 
L2: Planning, Decision-making, Logical Reasoning, Working Memory, Semantic Understanding & Context Recognition, Adaptive Error Correction
L3: Cognitive Flexibility",L3
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro is a larger and harder software engineering benchmark designed to be more representative of professional, industrial code maintenance and feature work across multiple languages. It emphasizes robustness to contamination and requires stronger long-horizon reasoning over large codebases, dependency ecosystems, and evaluation harnesses.","L1: 
L2: Planning, Decision-making, Logical Reasoning, Working Memory, Semantic Understanding & Context Recognition, Adaptive Error Correction
L3: Cognitive Flexibility",L3
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 evaluates agentic performance in realistic command-line environments, where models must choose and execute sequences of shell commands to complete tasks. It measures operational competence such as troubleshooting, environment navigation, iterative debugging, and using tooling under resource and step constraints.","L1: Language Comprehension (minor)
L2: Planning, Decision-making, Working Memory, Adaptive Error Correction, Logical Reasoning, Attention (minor)
L3: ",L2
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp evaluates “deep research” style browsing agents by asking questions whose answers require searching, synthesizing, and verifying information across multiple sources. The benchmark stresses tool-mediated information foraging, evidence integration, and resisting plausible but unsupported conclusions.","L1: Language Production (minor)
L2: Planning, Decision-making, Semantic Understanding & Context Recognition, Episodic Memory, Working Memory, Attention
L3: ",L2
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench measures interactive agent performance in multi-turn customer-support-like settings where the agent must use tools/APIs while following domain policies (e.g., retail, airline, telecom). It tests whether an agent can maintain state across a dialogue, choose compliant actions, and reliably execute tool calls to resolve the user’s issue.","L1: Language Comprehension, Language Production
L2: Decision-making, Planning, Working Memory
L3: Social Reasoning & Theory of Mind, Inhibitory Control, Empathy (minor)",L3
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld evaluates multimodal computer-use agents on operating-system tasks (e.g., using apps, navigating UIs, editing content) under step limits. It stresses perception of screen state, grounding language instructions in UI elements, and planning action sequences with recovery from UI or tool errors.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Sensorimotor Coordination, Planning, Decision-making, Working Memory, Adaptive Error Correction
L3: ",L2
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI is a fluid-intelligence benchmark where systems infer latent transformation rules from a few input–output grid examples and generalize to new inputs. It aims to minimize reliance on memorized knowledge by emphasizing novel, abstract pattern induction and compositional reasoning.","L1: Visual Perception
L2: Logical Reasoning, Working Memory, Spatial Representation & Mapping, Scene Understanding & Visual Reasoning
L3: Cognitive Flexibility",L3
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon agent coherence by simulating a vending-machine business over an extended period, scoring final profit/balance after many decisions. Success requires sustained strategic planning, adaptive inventory and pricing choices, and robust handling of changing conditions and constraints.","L1: 
L2: Planning, Decision-making, Reward Mechanisms, Episodic Memory, Working Memory, Adaptive Error Correction (minor)
L3: Motivational Drives (minor)",L2
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas assesses tool-use via the Model Context Protocol by requiring models to discover, invoke, and compose real tools across multi-step workflows. It emphasizes correct API selection and parameterization, handling intermediate failures, and synthesizing results into accurate end outputs.","L1: Language Comprehension (minor), Language Production (minor)
L2: Planning, Decision-making, Working Memory, Semantic Understanding & Context Recognition, Adaptive Error Correction
L3: ",L2
FinanceAgent,https://www.vals.ai/benchmarks/finance_agent,,"FinanceAgent evaluates agents on tasks resembling entry-level financial analyst work, such as analyzing documents, extracting structured facts, performing calculations, and producing grounded write-ups. It stresses reliable multi-step quantitative reasoning, domain-specific interpretation, and consistent handling of assumptions and constraints.","L1: Language Production (minor)
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Planning, Decision-making
L3: ",L2
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity agents on vulnerability discovery and exploitation-oriented tasks grounded in real-world software, including identifying known weaknesses and uncovering new ones. It emphasizes precise technical reasoning, iterative hypothesis testing, and robust debugging of attack or detection workflows.","L1: 
L2: Logical Reasoning, Planning, Decision-making, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition
L3: ",L2
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"SpreadsheetBench evaluates the ability to understand, edit, and compute over complex spreadsheets drawn from real-world-like tasks. It stresses structured data manipulation, formula reasoning, multi-step transformations, and producing correct artifacts under workflow constraints.","L1: 
L2: Logical Reasoning, Planning, Working Memory, Semantic Understanding & Context Recognition, Decision-making, Adaptive Error Correction (minor)
L3: ",L2
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a difficult multimodal benchmark intended to probe broad expert-level knowledge and reasoning at the frontier, across many subjects and question styles. It stresses integrating information from text and, when present, images/figures, while maintaining precision on complex, multi-step questions.","L1: Language Comprehension, Visual Perception (minor)
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Multisensory Integration, Scene Understanding & Visual Reasoning (minor)
L3: ",L2
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 consists of competition-style mathematics problems that require multi-step derivations, careful algebraic manipulation, and exact numeric answers. It primarily measures mathematical reasoning and error-free symbolic thinking under tight constraints, rather than tool execution or world knowledge.","L1: 
L2: Logical Reasoning, Working Memory, Attention (minor), Planning (minor)
L3: ",L2
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a curated subset of graduate-level, “Google-proof” multiple-choice science questions designed to resist superficial retrieval and reward deep understanding. It probes precise reasoning in physics, chemistry, and biology with distractors that punish shallow pattern matching.","L1: Language Comprehension
L2: Logical Reasoning, Working Memory, Semantic Understanding & Context Recognition
L3: ",L2
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends MMLU to multiple languages, testing broad academic knowledge and reasoning across many subjects in a multilingual setting. It measures both knowledge and the ability to preserve meaning and select correct answers across linguistic variation.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Working Memory (minor), Logical Reasoning (minor)
L3: Cognitive Flexibility",L3
MMMU,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2311.16502,"MMMU is a multidisciplinary multimodal benchmark where models answer expert-level questions that require reasoning over images (e.g., diagrams, charts, scientific visuals) alongside text. It emphasizes grounded visual understanding, cross-modal integration, and multi-step problem solving.","L1: Visual Perception, Language Comprehension
L2: Multisensory Integration, Scene Understanding & Visual Reasoning, Logical Reasoning, Working Memory (minor)
L3: ",L2
LAB-Bench FigQA,https://huggingface.co/datasets/futurehouse/lab-bench,https://arxiv.org/abs/2407.10362,"LAB-Bench FigQA evaluates whether models can correctly interpret and reason about scientific figures from biology research papers, including extracting trends, comparing conditions, and mapping visual evidence to claims. It targets figure-grounded reasoning rather than general biology trivia, stressing careful reading of axes, legends, and experimental context.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Attention, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory (minor)
L3: ",L2
WebArena,https://webarena.dev/,https://arxiv.org/abs/2307.13854,"WebArena evaluates autonomous web agents on realistic tasks across multiple web apps (e-commerce, CMS, forums, code hosting), requiring navigation, form filling, and multi-step workflows. It stresses grounding instructions in dynamic UIs, maintaining progress toward goals across long trajectories, and recovering from mistakes during interaction.","L1: Visual Perception
L2: Planning, Decision-making, Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Sensorimotor Coordination, Working Memory, Adaptive Error Correction
L3: ",L2
