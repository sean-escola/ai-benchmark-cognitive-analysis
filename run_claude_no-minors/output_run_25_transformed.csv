Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates an agent’s ability to fix real issues in open-source Python repositories by generating code patches that make failing tests pass. The “Verified” variant filters tasks to those confirmed solvable and reliably testable, emphasizing end-to-end repair rather than standalone code generation.","L1: Language Comprehension (minor)
L2: Planning, Logical Reasoning, Adaptive Error Correction, Working Memory, Decision-making (minor)
L3: ",L2
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 measures how well an agent can complete realistic tasks in a command-line environment, typically requiring sequences of shell commands, file edits, debugging, and verification. It stresses robust interaction loops—observe outputs, decide next actions, and recover from errors—under tooling constraints.","L1: Language Comprehension (minor)
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory, Sensorimotor Coordination (minor)
L3: ",L2
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp evaluates web-browsing agents on information-seeking tasks that require searching, reading, and synthesizing evidence from multiple sources. It emphasizes tool-augmented reasoning under partial observability (web results), including managing distractions, source quality, and long context across browsing steps.","L1: Language Production (minor)
L2: Planning, Attention, Semantic Understanding & Context Recognition, Working Memory, Decision-making
L3: ",L2
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench tests interactive customer-support agents in simulated domains (e.g., retail, airline, telecom) that must follow policies while using tools/APIs over multi-turn dialogues. Success depends on maintaining consistent state, respecting constraints, and making reliable tool calls despite ambiguous or adversarial user behavior.","L1: Language Comprehension, Language Production
L2: Decision-making, Planning (minor)
L3: Social Reasoning & Theory of Mind, Inhibitory Control, Empathy (minor)",L3
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld evaluates multimodal “computer use” agents that operate graphical desktop environments to complete tasks such as navigating apps, filling forms, and managing files. It stresses perception-to-action grounding: understanding screens, tracking UI state, and executing correct multi-step interactions.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Spatial Representation & Mapping, Attention, Planning, Sensorimotor Coordination, Adaptive Error Correction (minor)
L3: ",L2
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,ARC-AGI measures “fluid” abstract reasoning by requiring models to infer latent rules from a few input–output grid examples and apply them to new inputs. It is designed to reduce reliance on memorized knowledge and instead test systematic generalization and compositional pattern discovery.,"L1: 
L2: Logical Reasoning, Working Memory, Spatial Representation & Mapping, Attention (minor)
L3: Cognitive Flexibility",L3
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon agent coherence and strategy in a simulated vending-machine business over many steps (e.g., procurement, pricing, inventory, negotiation). Performance is scored via sustained profitability, requiring consistent goal pursuit, adaptation, and error recovery across extended trajectories.","L1: 
L2: Planning, Decision-making, Reward Mechanisms, Working Memory
L3: Cognitive Timing & Predictive Modeling (minor), Self-reflection (minor)",L2
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas measures real-world tool use via the Model Context Protocol, focusing on selecting appropriate tools, calling them with correct arguments, and composing multi-step workflows across services. It emphasizes reliability under tool errors, retries, and changing intermediate states rather than purely verbal problem solving.","L1: Language Comprehension (minor), Language Production (minor)
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory
L3: Inhibitory Control (minor)",L2
FinanceAgent,https://www.vals.ai/benchmarks/finance_agent,,"FinanceAgent evaluates agent performance on tasks resembling entry-level financial analysis, such as interpreting financial documents, building analyses, and producing structured deliverables. It typically requires multi-step reasoning with domain constraints, careful numerical handling, and coherent reporting.","L1: Language Production
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Planning, Decision-making, Working Memory
L3: ",L2
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity agents on identifying known vulnerabilities and discovering new ones in real open-source codebases, often under time and tooling constraints. It probes vulnerability reasoning, exploit/patch logic, and iterative debugging of hypotheses against concrete program behavior.","L1: 
L2: Logical Reasoning, Planning, Adaptive Error Correction, Working Memory
L3: Inhibitory Control (minor)",L2
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"SpreadsheetBench measures the ability to understand and manipulate complex spreadsheets, including cleaning data, editing formulas, and producing correct structured outputs. It tests procedural competence in a common professional artifact format where small mistakes can cascade across dependent cells.","L1: Visual Perception (minor)
L2: Logical Reasoning, Planning, Working Memory, Adaptive Error Correction, Semantic Understanding & Context Recognition
L3: ",L2
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a frontier, multi-domain benchmark of difficult questions spanning expert knowledge and reasoning, with both text-only and multimodal items. It is intended to stress broad generalization, careful synthesis, and (when enabled) tool-augmented problem solving beyond memorized facts.","L1: Language Comprehension, Visual Perception (minor)
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Planning (minor)
L3: ",L2
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 evaluates competition-style mathematics problem solving under strict answer formats. Problems typically require multi-step derivations, symbolic manipulation, and careful case handling where partial progress does not guarantee correctness.","L1: 
L2: Logical Reasoning, Working Memory, Planning, Adaptive Error Correction (minor)
L3: ",L2
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a curated subset of very difficult, “Google-proof” graduate-level science multiple-choice questions. It probes deep conceptual understanding and multi-step scientific reasoning rather than shallow recall or easily searchable facts.","L1: 
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory (minor), Decision-making (minor)
L3: ",L2
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends broad academic knowledge testing into multiple non-English languages across many subjects. It evaluates cross-lingual comprehension and reasoning, including whether models preserve meaning and solve problems consistently across linguistic contexts.","L1: Language Comprehension, Language Production (minor)
L2: Semantic Understanding & Context Recognition, Logical Reasoning (minor), Working Memory (minor)
L3: ",L2
MMMU,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2311.16502,"MMMU evaluates multidisciplinary multimodal understanding by combining text with images (e.g., diagrams, charts, scientific figures) and asking expert-level questions. It stresses visual grounding, extracting relevant details, and integrating them with domain knowledge and reasoning.","L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Working Memory, Attention, Multisensory Integration (minor)
L3: ",L2
LAB-Bench FigQA,https://huggingface.co/datasets/futurehouse/lab-bench,https://arxiv.org/abs/2407.10362,"LAB-Bench FigQA focuses on answering questions about scientific figures from biology papers, requiring correct interpretation of plots, microscopy images, and schematic diagrams. It targets research-relevant visual reasoning where subtle cues, axes, and experimental context matter.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Semantic Understanding & Context Recognition, Logical Reasoning, Attention
L3: ",L2
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning tests reasoning over scientific charts and figures associated with arXiv-style papers, often requiring quantitative reading of plots and linking text context to visual evidence. It emphasizes structured interpretation (axes, legends, trends) and multi-step inference rather than generic image captioning.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Logical Reasoning, Working Memory, Attention, Planning (minor)
L3: ",L2
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU evaluates multimodal reasoning over short videos, requiring models to track events, objects, and temporal relationships to answer questions. It stresses maintaining coherent state across frames and integrating visual evidence with language understanding.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Working Memory, Attention, Multisensory Integration (minor)
L3: Cognitive Timing & Predictive Modeling (minor)",L2
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"The FACTS Benchmark Suite systematically evaluates factuality-related behaviors such as hallucination, faithfulness to provided sources, and consistency under paraphrase or adversarial conditions. It targets whether models can produce accurate statements and appropriately hedge or abstain when evidence is insufficient.","L1: Language Production
L2: Semantic Understanding & Context Recognition, Working Memory (minor)
L3: Inhibitory Control, Self-reflection (minor)",L3
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA evaluates physical commonsense reasoning across many languages using non-parallel (not direct translations) items, testing whether models can generalize intuitive physics and everyday affordances cross-lingually. It reduces shortcuts from memorized English-only patterns by expanding linguistic and cultural coverage.","L1: Language Comprehension
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Spatial Representation & Mapping (minor)
L3: ",L2
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,MRCR v2 (8-needle) is a long-context evaluation where multiple similar “needle” requests are embedded in long “haystack” dialogues and the model must retrieve the correct referenced response. It stresses precise long-range dependency handling and resistance to distractors across extended contexts.,"L1: 
L2: Working Memory, Attention, Semantic Understanding & Context Recognition, Episodic Memory (minor), Adaptive Error Correction (minor)
L3: ",L2
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval measures performance on well-specified professional knowledge-work tasks (e.g., creating presentations, spreadsheets, schedules) across many occupations, judged against expert human outputs. It targets real-world deliverable quality, including adherence to constraints, clarity, and usefulness rather than only question answering.","L1: Language Production
L2: Planning, Decision-making, Semantic Understanding & Context Recognition, Working Memory
L3: Self-reflection (minor), Social Reasoning & Theory of Mind (minor)",L2
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks evaluates long-context algorithmic reasoning by asking models to perform graph traversal and relational queries over graph descriptions embedded in text. It stresses systematic multi-step computation, maintaining intermediate states, and accurately following graph structure under distractors.","L1: 
L2: Logical Reasoning, Working Memory, Spatial Representation & Mapping, Planning (minor), Attention (minor)
L3: ",L2
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon evaluates generalized tool-calling competence across a diverse set of tools and tasks, including selecting the right tools, composing multi-step calls, and integrating outputs into a final solution. It emphasizes robustness to tool errors and the ability to coordinate heterogeneous actions into coherent workflows.","L1: Language Comprehension (minor)
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory
L3: Inhibitory Control (minor)",L2
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath evaluates advanced, research-adjacent mathematical problem solving, emphasizing problems that resist superficial pattern matching and often require novel multi-step reasoning. It targets depth of mathematical inference and reliability under complex derivations, sometimes with optional tool support (e.g., Python).","L1: 
L2: Logical Reasoning, Working Memory, Planning, Adaptive Error Correction (minor)
L3: Cognitive Flexibility",L3
