Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,SWE-bench Verified evaluates software engineering agents by giving real GitHub issues and requiring a correct code patch that passes the project’s tests. The “Verified” subset uses tasks validated by human experts as solvable and focuses on end-to-end debugging and implementation in realistic repos.,"L1: Language Comprehension (minor)
L2: Logical Reasoning, Planning, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition, Attention (minor)
L3: ",L2
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 measures an agent’s ability to complete practical tasks in a command-line environment (e.g., installing tools, manipulating files, running programs, and diagnosing failures). Success typically requires iterative execution, reading outputs, and adjusting actions under resource and time constraints.","L1: Language Comprehension (minor)
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition, Attention (minor)
L3: ",L2
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp evaluates deep web research: agents must navigate documents and web-like corpora to answer complex questions requiring multi-step evidence gathering. The benchmark emphasizes sustained information seeking, source integration, and robust answering rather than single-shot recall.","L1: Language Comprehension (minor), Language Production (minor)
L2: Planning, Decision-making, Semantic Understanding & Context Recognition, Working Memory, Attention, Logical Reasoning (minor)
L3: ",L2
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench evaluates interactive agent performance in multi-turn customer-support-style environments (e.g., retail, airline, telecom) with policy constraints and tool/API actions. Agents must manage dialogue, follow rules, and reliably execute workflows despite ambiguity and user pressure.","L1: Language Comprehension, Language Production
L2: Decision-making, Planning, Working Memory, Reward Mechanisms (minor)
L3: Social Reasoning & Theory of Mind, Inhibitory Control, Empathy (minor)",L3
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld benchmarks multimodal computer-use agents on realistic desktop tasks that require interacting with GUIs (opening apps, navigating settings, filling forms, etc.). It stresses perception-to-action grounding: interpreting screenshots, selecting UI targets, and executing sequences of operations under step limits.","L1: Visual Perception, Language Comprehension (minor)
L2: Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Sensorimotor Coordination, Planning, Decision-making, Working Memory (minor)
L3: ",L2
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI measures “fluid” abstract reasoning on novel grid-based tasks where models infer a transformation rule from a few input–output examples. It is designed to reduce reliance on memorized knowledge and instead probe generalization, compositional pattern discovery, and systematic reasoning.","L1: 
L2: Logical Reasoning, Working Memory, Spatial Representation & Mapping, Scene Understanding & Visual Reasoning, Attention (minor), Planning (minor)
L3: Cognitive Flexibility",L3
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon agent coherence by simulating operation of a vending-machine business over many decisions (inventory, suppliers, pricing, negotiation). High scores require consistent strategy, adaptation to changing conditions, and avoiding compounding mistakes across extended trajectories.","L1: Language Production (minor)
L2: Planning, Decision-making, Reward Mechanisms, Working Memory, Adaptive Error Correction (minor)
L3: Cognitive Timing & Predictive Modeling (minor), Self-reflection (minor), Social Reasoning & Theory of Mind (minor)",L2
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas evaluates real-world tool use through the Model Context Protocol (MCP), requiring models to discover tools, call them correctly, handle errors/retries, and synthesize results. Tasks are typically multi-step workflows that mirror production integrations rather than toy tool demonstrations.","L1: Language Comprehension (minor), Language Production (minor)
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition
L3: Inhibitory Control (minor)",L2
FinanceAgent,https://www.vals.ai/benchmarks/finance_agent,,"FinanceAgent targets tasks expected of an entry-level financial analyst, such as document-based research, spreadsheet-like reasoning, and summarizing financial insights with constraints. It emphasizes following a workflow, applying domain knowledge, and producing decision-relevant outputs.","L1: Language Production
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Planning, Decision-making, Working Memory, Attention (minor)
L3: ",L2
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity agent capabilities on real-world vulnerability tasks at scale, including locating known vulnerabilities from descriptions and discovering new ones. It stresses code comprehension, exploit/bug reasoning, and iterative debugging-like workflows under pass@1 conditions.","L1: 
L2: Logical Reasoning, Planning, Adaptive Error Correction, Working Memory, Attention (minor), Semantic Understanding & Context Recognition (minor)
L3: Inhibitory Control (minor)",L2
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"SpreadsheetBench evaluates an agent’s ability to work with complex spreadsheets derived from realistic scenarios, including editing, formula logic, and structured data manipulation. Success requires tracking constraints across sheets, applying consistent transformations, and producing verifiable outputs.","L1: 
L2: Logical Reasoning, Planning, Working Memory, Adaptive Error Correction, Attention (minor), Semantic Understanding & Context Recognition (minor), Decision-making (minor)
L3: ",L2
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,Humanity’s Last Exam is a difficult multi-modal benchmark intended to probe frontier knowledge and reasoning across many expert domains. Questions often require integrating specialized text with images/figures and producing a single best answer under strict grading.,"L1: Language Comprehension, Language Production (minor), Visual Perception (minor)
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Scene Understanding & Visual Reasoning (minor)
L3: ",L2
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 is a competition-math evaluation drawn from the American Invitational Mathematics Examination, featuring nontrivial algebra, geometry, number theory, and combinatorics problems. It tests precise symbolic reasoning and robustness against small logical slips across multi-step solutions.","L1: 
L2: Logical Reasoning, Working Memory, Adaptive Error Correction (minor), Planning (minor)
L3: Cognitive Flexibility (minor)",L2
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a high-quality subset of GPQA consisting of extremely challenging graduate-level science multiple-choice questions. It is designed to be “Google-proof,” emphasizing deep understanding and multi-step scientific reasoning rather than superficial recall.","L1: Language Comprehension (minor)
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory (minor), Decision-making (minor)
L3: ",L2
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends the MMLU-style academic knowledge and reasoning test across many languages, aiming to measure cross-lingual competence rather than English-only performance. It probes whether models can transfer concepts and reasoning patterns across linguistic contexts.","L1: Language Comprehension, Language Production (minor)
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory (minor)
L3: ",L2
MMMU,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2311.16502,"MMMU is a multidisciplinary multimodal benchmark where models answer questions that require interpreting images (charts, diagrams, screenshots) alongside text. It emphasizes expert-level multimodal reasoning, including visual grounding and cross-domain knowledge application.","L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Attention (minor), Working Memory (minor)
L3: ",L2
LAB-Bench FigQA,https://huggingface.co/datasets/futurehouse/lab-bench,https://arxiv.org/abs/2407.10362,"LAB-Bench FigQA evaluates whether models can correctly interpret scientific figures from biology papers and answer questions that require reading plots, labels, and visual encodings. It targets research-relevant visual analysis, where small perceptual or reasoning errors can flip conclusions.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Logical Reasoning, Semantic Understanding & Context Recognition (minor), Working Memory (minor)
L3: ",L2
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning tests visual reasoning over charts and figures (often from scientific documents), requiring models to extract quantitative and relational information and answer structured questions. Many settings allow tool use (e.g., Python) to support measurement-like computation and verification.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Logical Reasoning, Working Memory (minor), Planning (minor), Adaptive Error Correction (minor), Attention (minor)
L3: ",L2
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU extends multimodal understanding to the temporal domain, requiring reasoning over sequences of video frames plus text prompts. Tasks stress tracking events, aligning visual evidence across time, and answering questions that depend on dynamic context.","L1: Visual Perception, Language Comprehension (minor)
L2: Working Memory, Attention, Scene Understanding & Visual Reasoning, Multisensory Integration (minor)
L3: Cognitive Timing & Predictive Modeling",L3
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"The FACTS Benchmark Suite systematically evaluates factuality by testing whether model outputs are supported by provided sources and whether they avoid fabricating details. It focuses on verifiable grounding and error patterns like hallucination, conflation, and unsupported elaboration.","L1: Language Comprehension (minor), Language Production (minor)
L2: Semantic Understanding & Context Recognition, Attention (minor), Adaptive Error Correction (minor)
L3: Inhibitory Control, Self-reflection (minor)",L3
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA evaluates physical commonsense reasoning across languages and cultural contexts, typically asking which action or explanation best fits a practical situation. It targets generalizable “how the world works” reasoning rather than domain-specialized textbook knowledge.","L1: Language Comprehension (minor)
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Decision-making (minor), Spatial Representation & Mapping (minor)
L3: ",L2
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) tests long-context, multi-round co-reference resolution by embedding multiple near-duplicate “needle” interactions inside long “haystack” conversations and asking the model to reproduce the response corresponding to a specific instance. It stresses precise retrieval under distraction and interference across very long contexts.","L1: 
L2: Working Memory, Attention, Episodic Memory, Semantic Understanding & Context Recognition
L3: Inhibitory Control (minor), Cognitive Timing & Predictive Modeling (minor)",L2
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval measures performance on well-specified professional knowledge work across many occupations by comparing model-produced work products (e.g., spreadsheets, presentations, plans) against industry professionals. Scoring typically relies on expert human judgment of correctness, usefulness, and deliverable quality under realistic constraints.","L1: Language Production
L2: Planning, Decision-making, Semantic Understanding & Context Recognition, Working Memory (minor), Reward Mechanisms (minor)
L3: Self-reflection (minor)",L2
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks evaluates structured reasoning over graph-structured inputs placed in long contexts, such as following edges, answering reachability queries, or identifying parents along traversals. It emphasizes systematic step-by-step state tracking and resistance to distraction over lengthy inputs.","L1: 
L2: Working Memory, Logical Reasoning, Spatial Representation & Mapping, Attention (minor), Planning (minor)
L3: ",L2
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon evaluates how reliably models can select and invoke tools across diverse tasks, including multi-step workflows with intermediate tool outputs. It stresses correct tool choice, parameterization, recovery from tool errors, and final answer synthesis.","L1: Language Comprehension (minor)
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition (minor)
L3: Inhibitory Control (minor)",L2
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath evaluates expert-level mathematics with problems designed to be difficult for current models and more resistant to memorization. It emphasizes deep multi-step derivations, careful case analysis, and high precision where minor errors often invalidate solutions.","L1: 
L2: Logical Reasoning, Working Memory, Adaptive Error Correction (minor), Planning (minor)
L3: Cognitive Flexibility",L3
