Benchmark,Website,Paper,Description,Cognitive Functions
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,SWE-bench Verified evaluates software engineering agents on real GitHub issues where the model must generate a code patch that passes the repository’s tests. The “Verified” subset focuses on tasks that have been confirmed solvable and are graded by whether the produced patch resolves the issue under the benchmark harness.,"Planning, Logical Reasoning, Working Memory, Adaptive Error Correction, Language Comprehension, Language Production (minor)"
SWE-bench Multilingual,https://www.swebench.com/multilingual.html,https://arxiv.org/abs/2504.21798,"SWE-bench Multilingual extends SWE-bench-style issue-to-patch evaluation beyond Python, covering multiple programming languages and ecosystems. It tests whether models can understand issues, navigate unfamiliar codebases, and implement fixes across diverse toolchains and language conventions.","Cognitive Flexibility, Planning, Logical Reasoning, Working Memory, Adaptive Error Correction, Language Comprehension, Semantic Understanding & Context Recognition"
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro is a harder, more contamination-resistant software engineering benchmark with a larger and more industrially realistic task set. Models must produce correct multi-file patches in real repositories, often requiring deeper repo understanding, dependency reasoning, and more robust debugging than standard SWE-bench tasks.","Planning, Decision-making, Logical Reasoning, Working Memory, Adaptive Error Correction, Semantic Understanding & Context Recognition, Language Comprehension (minor)"
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 evaluates agentic performance on real tasks in a command-line environment, such as debugging, building, configuring software, and manipulating files via shell commands. Success depends on choosing effective command sequences, interpreting tool outputs, and iterating when errors occur under resource and time constraints.","Planning, Decision-making, Working Memory, Adaptive Error Correction, Logical Reasoning, Attention (minor), Language Comprehension (minor)"
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp evaluates deep-research style agents that must browse and synthesize information from a controlled document index to answer complex questions reproducibly. It emphasizes search strategy, evidence aggregation, and faithful synthesis from retrieved sources rather than open-web variability.","Planning, Decision-making, Working Memory, Semantic Understanding & Context Recognition, Language Comprehension, Language Production, Self-reflection (minor)"
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench measures how well agents complete multi-turn customer-support tasks by combining natural language interaction with API/tool use while following domain policies (e.g., retail, airline, telecom). It stresses consistent policy adherence, state tracking across turns, and pragmatic tool decisions under conversational pressure.","Social Reasoning & Theory of Mind, Decision-making, Planning, Working Memory, Inhibitory Control, Language Comprehension, Language Production, Empathy (minor)"
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld benchmarks multimodal “computer use” agents operating in realistic desktop environments, requiring navigation across GUIs to complete tasks in applications and browsers. It tests whether an agent can perceive screen state, decide next interactions, and recover from mistakes over long action sequences.","Visual Perception, Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Sensorimotor Coordination, Planning, Decision-making, Working Memory, Adaptive Error Correction"
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI is a fluid-intelligence benchmark based on abstract grid transformation puzzles with only a few demonstrations per task. Models must infer latent rules and generalize them to new inputs, emphasizing novel pattern discovery rather than memorized domain knowledge.","Scene Understanding & Visual Reasoning, Spatial Representation & Mapping, Logical Reasoning, Working Memory, Cognitive Flexibility, Attention, Visual Perception (minor)"
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon autonomy by having an agent run a simulated vending-machine business over many steps, making pricing, purchasing, and negotiation decisions. Performance is measured by final business outcomes, requiring sustained coherence, strategic planning, and adaptation to changing conditions.","Planning, Decision-making, Reward Mechanisms, Working Memory, Episodic Memory (minor), Motivational Drives (minor), Social Reasoning & Theory of Mind (minor), Language Production (minor)"
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas evaluates real-world tool use via the Model Context Protocol (MCP), where agents must discover tools, call them correctly, handle errors, and compose multi-step workflows across services. It emphasizes reliable orchestration across multiple tool calls and robust recovery when tool outputs are imperfect.","Planning, Decision-making, Working Memory, Adaptive Error Correction, Language Comprehension, Language Production (minor), Inhibitory Control (minor)"
FinanceAgent,https://www.vals.ai/benchmarks/finance_agent,,"FinanceAgent measures performance on tasks representative of an entry-level financial analyst, such as analysis, modeling, and report-style reasoning over financial documents and constraints. It stresses numerate reasoning, correct application of finance concepts, and producing actionable outputs under professional standards.","Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Decision-making, Planning (minor), Language Production (minor)"
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity agents on identifying known vulnerabilities and discovering new ones in real open-source software given task descriptions and codebases. It probes systematic investigation, exploit-relevant reasoning, and iterative debugging under realistic constraints.","Logical Reasoning, Planning, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition, Inhibitory Control (minor)"
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"SpreadsheetBench tests an agent’s ability to understand, edit, and compute within complex spreadsheets derived from real-world scenarios. Tasks often require locating relevant cells, applying correct formulas or transformations, and maintaining consistency across multiple sheets and constraints.","Working Memory, Logical Reasoning, Planning, Adaptive Error Correction, Semantic Understanding & Context Recognition, Visual Perception (minor), Scene Understanding & Visual Reasoning (minor)"
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam (HLE) is a large, frontier-level benchmark spanning many domains (and often multiple modalities) intended to probe advanced reasoning and breadth of knowledge. Questions are designed to be difficult for models without careful multi-step reasoning and, in tool-enabled settings, effective use of search and computation.","Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Planning (minor), Visual Perception (minor), Multisensory Integration (minor)"
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,AIME 2025 consists of competition-style math problems that require precise symbolic manipulation and multi-step derivations under time-like constraints. It primarily measures mathematical reasoning and the ability to maintain correctness across long chains of intermediate steps.,"Logical Reasoning, Working Memory, Planning, Attention (minor), Adaptive Error Correction (minor)"
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,GPQA Diamond is a high-quality subset of GPQA containing extremely challenging graduate-level science multiple-choice questions intended to be “Google-proof.” It emphasizes deep conceptual understanding and careful reasoning rather than shallow recall or pattern matching.,"Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Language Comprehension, Decision-making (minor)"
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends broad academic knowledge evaluation to many languages, testing whether models can answer subject-matter questions across disciplines in multilingual settings. It probes cross-lingual robustness, instruction following in non-English languages, and domain knowledge expressed through varied linguistic forms.","Language Comprehension, Semantic Understanding & Context Recognition, Cognitive Flexibility, Logical Reasoning (minor), Working Memory (minor)"
MMMU,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2311.16502,"MMMU is a multimodal benchmark requiring reasoning over images and text across many professional and academic topics. Tasks often involve interpreting diagrams, charts, tables, and screenshots and combining visual evidence with language instructions to select or generate correct answers.","Visual Perception, Scene Understanding & Visual Reasoning, Multisensory Integration, Language Comprehension, Logical Reasoning, Working Memory (minor), Attention (minor)"
LAB-Bench FigQA,https://huggingface.co/datasets/futurehouse/lab-bench,https://arxiv.org/abs/2407.10362,"LAB-Bench FigQA evaluates whether models can correctly interpret complex scientific figures from biology papers, including reading axes, legends, and experimental relationships. It stresses figure-grounded reasoning and extracting precise claims from visual evidence rather than relying on general biology knowledge alone.","Visual Perception, Scene Understanding & Visual Reasoning, Attention, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory (minor), Multisensory Integration (minor)"
WebArena,https://webarena.dev/,https://arxiv.org/abs/2307.13854,"WebArena benchmarks autonomous web agents that must complete realistic tasks across multiple web apps (e-commerce, CMS, forums, collaboration tools) through multi-step navigation and interaction. Agents need to interpret changing web states, plan action sequences, and recover from partial failures while satisfying task objectives.","Planning, Decision-making, Working Memory, Adaptive Error Correction, Visual Perception, Visual Attention & Eye Movements, Sensorimotor Coordination, Language Comprehension (minor), Language Production (minor)"
