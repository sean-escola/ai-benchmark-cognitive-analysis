Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,SWE-bench Verified evaluates autonomous software-engineering agents on real GitHub issues by requiring them to produce a patch that makes the repository’s tests pass. The Verified split uses human-validated tasks intended to reduce noise from ambiguous or unsolvable issues and focuses on end-to-end debugging and code changes under repository constraints.,"L1: Language Comprehension (minor)
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory, Logical Reasoning
L3: ",L2
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 measures agentic performance on real command-line tasks (e.g., navigating files, running programs, installing dependencies, using CLI utilities, and completing multi-step objectives) inside isolated environments. It emphasizes robust tool use and iterative debugging under resource and time constraints rather than single-turn Q&A.","L1: Language Comprehension (minor)
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory
L3: Inhibitory Control (minor)",L2
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp evaluates “deep research” agents that must answer questions by using web browsing/search tools and synthesizing evidence from multiple sources. It tests whether an agent can plan an information-gathering strategy, manage context while reading, and produce an answer grounded in retrieved content.","L1: Language Comprehension, Language Production (minor)
L2: Planning, Decision-making, Semantic Understanding & Context Recognition, Episodic Memory, Attention
L3: ",L2
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench evaluates tool-using, policy-following conversational agents in simulated customer-support domains (e.g., retail, airline, telecom) with multi-turn interactions and API actions. Agents must resolve the user’s request while adhering to domain policies, handling exceptions, and maintaining consistency over long dialogues.","L1: Language Comprehension, Language Production (minor)
L2: Decision-making, Planning, Working Memory
L3: Social Reasoning & Theory of Mind, Inhibitory Control",L3
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld evaluates multimodal “computer use” agents that operate a real or realistic desktop environment by interpreting screenshots and executing UI actions to complete tasks. It stresses long-horizon, error-prone interaction loops including navigation, form filling, file operations, and troubleshooting when the UI state changes unexpectedly.","L1: Visual Perception
L2: Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Sensorimotor Coordination, Planning, Adaptive Error Correction, Spatial Representation & Mapping (minor)
L3: ",L2
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI evaluates fluid, few-shot reasoning via grid-based puzzles where models infer a hidden transformation rule from a small number of input–output examples. The benchmark is designed to emphasize generalization and compositional reasoning over memorization, with puzzles intended to be novel and diverse.","L1: 
L2: Logical Reasoning, Working Memory, Spatial Representation & Mapping, Attention (minor)
L3: Cognitive Flexibility",L3
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon autonomy in a simulated business setting where an agent runs a vending-machine operation over many steps (e.g., sourcing inventory, pricing, handling communication, and adapting to market dynamics). Success is measured by sustained coherence and strategic decision-making that maximizes long-term returns.","L1: 
L2: Planning, Decision-making, Reward Mechanisms, Working Memory, Adaptive Error Correction
L3: Motivational Drives (minor)",L2
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas evaluates real-world tool use through the Model Context Protocol (MCP), requiring models to discover tools, call them with correct arguments, handle errors/retries, and integrate outputs across multi-step workflows. It targets practical agent reliability when interacting with production-like APIs and heterogeneous tool ecosystems.","L1: Language Comprehension (minor)
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory
L3: ",L2
FinanceAgent,https://www.vals.ai/benchmarks/finance_agent,,"FinanceAgent evaluates the ability of an agent to perform tasks resembling entry-level financial analyst work, such as extracting information, building analyses, and producing finance-oriented deliverables with appropriate assumptions and reasoning. It emphasizes multi-step problem solving, numerical/financial reasoning, and structured reporting.","L1: Language Production (minor)
L2: Logical Reasoning, Planning, Decision-making, Semantic Understanding & Context Recognition, Working Memory (minor)
L3: ",L2
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity agent capabilities on tasks spanning identification of known vulnerabilities in real open-source projects and discovery of new vulnerabilities. The benchmark emphasizes systematic exploration, hypothesis-driven debugging, and correct remediation/verification behavior under realistic code and tooling constraints.","L1: 
L2: Logical Reasoning, Planning, Decision-making, Adaptive Error Correction, Attention (minor)
L3: Inhibitory Control (minor)",L2
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"SpreadsheetBench evaluates agents on complex spreadsheet tasks derived from real-world use cases, requiring reading, editing, formula manipulation, and producing correct computed outputs. It stresses structured reasoning over tables, error checking, and maintaining consistency across many interdependent cells and sheets.","L1: 
L2: Working Memory, Logical Reasoning, Planning, Adaptive Error Correction, Attention (minor), Semantic Understanding & Context Recognition (minor)
L3: ",L2
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam (HLE) is a difficult multimodal benchmark positioned at the frontier of expert knowledge and reasoning, combining questions that may require deep domain understanding and careful multi-step inference. It is often evaluated with and without tools (e.g., search, code) to measure both raw reasoning and tool-augmented problem solving.","L1: Language Comprehension, Visual Perception (minor)
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory (minor), Planning (minor)
L3: ",L2
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 evaluates competition-style mathematics problem solving drawn from the American Invitational Mathematics Examination. Problems typically require multi-step derivations, careful algebra/number theory/combinatorics reasoning, and precise final answers under tight constraints.","L1: 
L2: Logical Reasoning, Working Memory, Attention (minor), Adaptive Error Correction (minor)
L3: ",L2
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a high-quality subset of GPQA consisting of challenging graduate-level multiple-choice questions in biology, chemistry, and physics designed to be difficult to answer by simple web search. It primarily measures deep scientific knowledge combined with careful reasoning under adversarially selected distractors.","L1: 
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory (minor)
L3: Inhibitory Control (minor)",L2
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends broad academic knowledge testing into multiple languages, evaluating models across many subjects and linguistic contexts. It probes whether knowledge and reasoning abilities transfer across languages while maintaining robustness to translation and cultural/terminological variation.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning (minor), Working Memory (minor)
L3: ",L2
MMMU,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2311.16502,"MMMU is a multimodal benchmark that tests expert-level knowledge and reasoning using both images and text across many disciplines. Questions often require interpreting diagrams, charts, or screenshots and combining them with domain knowledge to select correct answers.","L1: Visual Perception, Language Comprehension (minor)
L2: Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Attention (minor)
L3: ",L2
LAB-Bench FigQA,https://huggingface.co/datasets/futurehouse/lab-bench,https://arxiv.org/abs/2407.10362,"LAB-Bench FigQA evaluates whether models can correctly interpret complex scientific figures from biology papers and answer questions that require extracting trends, comparisons, and experimental implications. It targets practical scientific figure literacy rather than generic image captioning.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Attention, Logical Reasoning, Semantic Understanding & Context Recognition (minor)
L3: ",L2
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning evaluates multimodal reasoning over scientific documents, emphasizing questions that require understanding figures, captions, and surrounding text from arXiv-style papers. It measures the ability to connect visual evidence with technical language and perform multi-step inference (often aided by tools like Python in some setups).","L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Working Memory (minor)
L3: ",L2
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU evaluates multimodal understanding and reasoning over videos, including temporal events, actions, and causal relationships, typically paired with text questions. It stresses integrating information across time and maintaining coherent interpretations when evidence is distributed across many frames.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Working Memory, Attention (minor), Multisensory Integration (minor)
L3: Cognitive Timing & Predictive Modeling",L3
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"FACTS Benchmark Suite evaluates factuality and grounding behavior, testing whether models produce statements that are supported by provided evidence and whether they avoid confabulation in ambiguous settings. It aggregates multiple factuality-focused tasks to better characterize error modes beyond a single QA metric.","L1: 
L2: Semantic Understanding & Context Recognition, Logical Reasoning (minor), Attention (minor)
L3: Inhibitory Control, Self-reflection (minor)",L3
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA extends physical commonsense reasoning about everyday interactions (e.g., which action best accomplishes a goal with objects) across many languages in a non-parallel way. It probes whether models retain grounded intuitive physics/affordance priors and can apply them consistently across linguistic and cultural contexts.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Spatial Representation & Mapping (minor), Multisensory Integration (minor)
L3: ",L2
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,MRCR v2 (8-needle) tests long-context multi-round coreference resolution by embedding multiple similar “needle” mentions inside a long “haystack” conversation or document and asking the model to retrieve the correct referenced content. It is designed to measure robustness of attention and retrieval when many distractors look nearly identical.,"L1: Language Comprehension
L2: Working Memory, Attention, Episodic Memory
L3: Inhibitory Control (minor)",L2
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval evaluates economically meaningful professional knowledge work by having models produce real work products (e.g., presentations, spreadsheets, plans) across many occupations, then comparing outputs against industry professionals via expert judging. It emphasizes end-to-end task execution quality, including meeting specifications, structuring deliverables, and making sound decisions under constraints.","L1: Language Production
L2: Planning, Decision-making, Semantic Understanding & Context Recognition, Working Memory (minor)
L3: Self-reflection (minor)",L2
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks evaluates long-context structured reasoning by requiring models to traverse graphs described in text (e.g., follow edges, perform BFS-like operations, or track parent relationships) and output correct nodes or paths. It targets systematic symbolic-style computation under large context windows and heavy distractor pressure.","L1: 
L2: Logical Reasoning, Working Memory, Spatial Representation & Mapping, Attention (minor)
L3: ",L2
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon evaluates general tool-using agents across diverse tasks that require selecting tools, composing multi-step tool sequences, and validating outputs. It stresses reliability under tool errors, argument formatting, and the ability to coordinate across heterogeneous external functions rather than solving everything in-model.","L1: 
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory
L3: Inhibitory Control (minor)",L2
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath evaluates advanced mathematics at or beyond typical competition levels, including problems intended to be difficult for both humans and models and often requiring deep multi-step reasoning. It is used to track progress on rigorous mathematical inference, especially when paired with tool use (e.g., Python) for verification in some settings.","L1: 
L2: Logical Reasoning, Working Memory, Attention (minor), Adaptive Error Correction (minor), Planning (minor)
L3: ",L2
