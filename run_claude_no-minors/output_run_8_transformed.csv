Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates an agent’s ability to resolve real-world software engineering issues by generating patches in actual GitHub repositories and passing the associated tests. The “Verified” subset focuses on tasks that have been validated as solvable and uses an execution-based grading setup, emphasizing end-to-end debugging and code modification rather than only answering questions about code.","L1: Language Comprehension (minor)
L2: Planning, Logical Reasoning, Working Memory, Adaptive Error Correction, Semantic Understanding & Context Recognition, Decision-making (minor)
L3: ",L2
SWE-bench Multilingual,https://www.swebench.com/multilingual.html,https://arxiv.org/abs/2504.21798,"SWE-bench Multilingual extends the SWE-bench paradigm beyond Python to multiple programming languages, testing whether agents can localize, modify, and validate fixes across diverse ecosystems. It emphasizes robustness to different build systems, language idioms, and tooling conventions while maintaining the same repository-level, test-driven notion of correctness.","L1: 
L2: Planning, Logical Reasoning, Adaptive Error Correction, Working Memory (minor), Semantic Understanding & Context Recognition (minor)
L3: Cognitive Flexibility",L3
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro is a more challenging, broader software engineering benchmark intended to be more contamination-resistant and industrially relevant, with a larger and harder task set spanning multiple languages. It stresses long-horizon repository understanding, correct integration with project constraints, and reliable patch generation under realistic evaluation harnesses.","L1: 
L2: Planning, Decision-making, Adaptive Error Correction, Logical Reasoning, Working Memory
L3: Cognitive Flexibility",L3
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 evaluates agents operating in a command-line environment on realistic tasks such as installing dependencies, running programs, diagnosing failures, and manipulating files and processes. Success depends on choosing effective shell actions, interpreting system feedback, and iterating toward a working state under resource and time constraints.","L1: Language Comprehension (minor)
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory, Sensorimotor Coordination (minor)
L3: ",L2
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp measures a model’s ability to perform deep, multi-step information-seeking using browsing or search tools, typically requiring synthesis across multiple documents. It targets agentic research behaviors like query refinement, evidence tracking, and producing grounded answers rather than relying on parametric recall alone.","L1: Language Production (minor)
L2: Planning, Attention, Semantic Understanding & Context Recognition, Working Memory, Episodic Memory (minor), Decision-making (minor)
L3: ",L2
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench (tau2-bench) evaluates interactive customer-support agents that must follow domain policies while using tools/APIs in multi-turn conversations (e.g., retail, airline, telecom). It emphasizes consistent policy adherence, correct tool invocation over long dialogs, and user-aligned resolution strategies under constraints and exceptions.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Working Memory
L3: Social Reasoning & Theory of Mind, Inhibitory Control, Empathy (minor)",L3
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld evaluates multimodal computer-use agents that complete tasks in a full operating-system environment by perceiving screens and executing actions (mouse/keyboard-like operations via tools). It tests the ability to navigate GUIs, locate relevant interface elements, and execute correct multi-step workflows in dynamic, partially observable settings.","L1: Visual Perception
L2: Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Planning, Decision-making, Working Memory, Sensorimotor Coordination
L3: ",L2
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,ARC-AGI is a “fluid intelligence” benchmark based on abstract grid transformation tasks where the model must infer latent rules from only a few examples and apply them to novel inputs. It is designed to stress generalization to unfamiliar pattern-composition problems rather than learned domain knowledge.,"L1: 
L2: Logical Reasoning, Spatial Representation & Mapping, Scene Understanding & Visual Reasoning, Working Memory, Attention (minor)
L3: Cognitive Flexibility",L3
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon agent coherence and decision-making by simulating management of a vending-machine business over an extended time period. Agents must plan procurement and pricing, negotiate or communicate with suppliers (often via email-like interactions), manage inventory, and adapt strategies as conditions change.","L1: 
L2: Planning, Decision-making, Reward Mechanisms, Working Memory, Episodic Memory (minor)
L3: Self-reflection (minor), Motivational Drives (minor)",L2
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas measures real-world tool-use performance via the Model Context Protocol, focusing on selecting appropriate tools, calling them with correct arguments, handling errors, and composing multi-step workflows. It evaluates whether agents can reliably integrate tool outputs into final answers under production-like API interactions.","L1: Language Comprehension (minor)
L2: Planning, Decision-making, Working Memory, Adaptive Error Correction, Semantic Understanding & Context Recognition (minor)
L3: ",L2
FinanceAgent,https://www.vals.ai/benchmarks/finance_agent,,"FinanceAgent evaluates performance on tasks typical of an entry-level financial analyst, such as interpreting financial documents, performing structured analyses, and producing professional artifacts (e.g., narratives, tables, or models) with appropriate justification. It emphasizes correctness under domain constraints, multi-step reasoning, and practical decision support.","L1: Language Production (minor)
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Planning (minor), Decision-making (minor)
L3: ",L2
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity-capable agents on tasks including identifying known vulnerabilities in real open-source projects from descriptions and, in some settings, discovering previously unknown vulnerabilities. It emphasizes precise code reasoning, hypothesis testing through execution, and iterative debugging under realistic constraints.","L1: 
L2: Logical Reasoning, Planning, Adaptive Error Correction, Working Memory, Decision-making (minor)
L3: Inhibitory Control (minor)",L2
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"SpreadsheetBench evaluates agents on real-world spreadsheet tasks requiring navigation, editing, formula manipulation, and structured reasoning over tabular layouts. It stresses multi-step operations with tool feedback (e.g., opening/editing files), correctness of transformations, and attention to spreadsheet structure and constraints.","L1: 
L2: Planning, Working Memory, Logical Reasoning, Attention, Adaptive Error Correction (minor), Semantic Understanding & Context Recognition (minor)
L3: ",L2
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a high-difficulty benchmark spanning frontier academic and professional questions, often including multimodal items, designed to stress broad knowledge and deep reasoning. It is frequently evaluated in both tool-free and tool-enabled settings (e.g., search/code), emphasizing synthesis, rigor, and robustness against hallucination on hard queries.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory (minor), Multisensory Integration (minor), Scene Understanding & Visual Reasoning (minor)
L3: ",L2
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 evaluates mathematical problem-solving on competition-style questions that require multi-step derivations, careful case analysis, and exact final answers. It targets compositional reasoning under tight correctness criteria, often distinguishing between superficial pattern-matching and faithful symbolic reasoning.","L1: 
L2: Logical Reasoning, Working Memory, Planning (minor)
L3: Cognitive Flexibility (minor)",L2
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a difficult subset of graduate-level, “Google-proof” multiple-choice science questions curated to reduce easy lookup and reward genuine understanding. It evaluates scientific reasoning and domain comprehension across physics, chemistry, and biology, under adversarially hard distractors.","L1: 
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory (minor), Decision-making (minor)
L3: ",L2
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU is the multilingual extension of MMLU, testing knowledge and reasoning across many subjects in multiple non-English languages. It evaluates whether models can maintain competence under language shifts, including understanding subtle question phrasing and selecting correct answers under culturally and linguistically varied contexts.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory (minor)
L3: Cognitive Flexibility",L3
MMMU,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2311.16502,"MMMU evaluates multimodal understanding and reasoning across diverse academic domains by requiring models to combine textual information with images (e.g., diagrams, plots, figures). It emphasizes grounded visual reasoning, cross-modal integration, and domain-aware interpretation rather than isolated captioning or OCR.","L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Attention (minor), Working Memory (minor)
L3: ",L2
LAB-Bench FigQA,https://huggingface.co/datasets/futurehouse/lab-bench,https://arxiv.org/abs/2407.10362,"LAB-Bench FigQA tests whether models can answer questions about complex scientific figures from biology papers, requiring extraction of quantitative/qualitative relationships from plots and diagrams. It stresses accurate figure interpretation, scientific-context grounding, and multi-step reasoning over visual evidence rather than general biology recall alone.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Multisensory Integration, Semantic Understanding & Context Recognition, Logical Reasoning, Visual Attention & Eye Movements (minor)
L3: ",L2
WebArena,https://webarena.dev/,https://arxiv.org/abs/2307.13854,"WebArena evaluates autonomous web agents performing realistic tasks across multiple web apps (e-commerce, CMS, forums, code hosting, maps) using browser interactions. It tests long-horizon navigation, form filling, information extraction, and goal completion under dynamic pages and tool/action constraints.","L1: Visual Perception
L2: Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Planning, Decision-making, Working Memory, Sensorimotor Coordination (minor)
L3: Inhibitory Control (minor)",L2
