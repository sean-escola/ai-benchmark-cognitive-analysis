Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates software engineering agents on real GitHub issues by requiring them to generate a patch that makes a project’s tests pass. The “Verified” subset contains tasks confirmed by human engineers to be solvable, emphasizing end-to-end debugging and codebase modification under realistic constraints.","L1: Language Comprehension (minor), Language Production (minor)
L2: Planning, Adaptive Error Correction, Logical Reasoning, Working Memory
L3: ",L2
SWE-bench Multilingual,https://www.swebench.com/multilingual.html,https://arxiv.org/abs/2504.21798,"SWE-bench Multilingual extends SWE-bench-style issue-to-patch evaluation beyond Python to multiple programming languages, measuring whether agents can apply software engineering skill across diverse ecosystems. It emphasizes adapting debugging and patching strategies to different tooling, idioms, and build/test setups.","L1: Language Comprehension (minor), Language Production (minor)
L2: Planning, Adaptive Error Correction, Logical Reasoning, Working Memory
L3: Cognitive Flexibility",L3
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro is a more difficult and more contamination-resistant software engineering benchmark where models must produce correct patches for realistic issues across multiple languages and repositories. It targets industrially relevant engineering behaviors like complex debugging, refactoring, and respecting project conventions under stricter evaluation conditions.","L1: 
L2: Planning, Adaptive Error Correction, Logical Reasoning, Working Memory
L3: Cognitive Flexibility, Inhibitory Control (minor)",L3
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 evaluates agents on real tasks performed in a command-line environment, such as installing dependencies, manipulating files, running programs, and diagnosing failures. It measures reliable tool use and iterative problem-solving when feedback arrives through logs, exit codes, and partial outputs.","L1: 
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory, Logical Reasoning
L3: Inhibitory Control (minor)",L2
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp evaluates “deep research” agents that must search, read, and synthesize information across many documents to answer difficult questions, often requiring multi-step evidence gathering. It emphasizes selecting good queries, tracking provenance, and integrating disparate facts into a final response.","L1: Language Production (minor)
L2: Planning, Attention, Decision-making, Semantic Understanding & Context Recognition, Working Memory, Episodic Memory (minor)
L3: ",L2
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench measures interactive agent performance in simulated customer-support domains (e.g., retail, airline, telecom) where the agent must use tools/APIs while following domain policies over multiple turns. It stresses policy adherence, robust conversation control, and correct tool invocation under changing user goals and constraints.","L1: Language Comprehension, Language Production
L2: Decision-making, Planning
L3: Inhibitory Control, Social Reasoning & Theory of Mind, Empathy (minor)",L3
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld evaluates multimodal computer-use agents that must complete tasks inside an operating-system desktop environment (apps, windows, settings) within a step budget. It tests whether an agent can perceive UI state, choose actions (click/type/scroll), recover from errors, and finish long tool chains.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Sensorimotor Coordination, Planning, Decision-making, Adaptive Error Correction, Working Memory (minor)
L3: ",L2
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI tests fluid reasoning on abstract grid transformation puzzles where only a few input-output examples are provided, and models must infer the hidden rule. It is designed to reduce reliance on memorized knowledge and to emphasize generalization, compositional pattern discovery, and rule induction.","L1: 
L2: Logical Reasoning, Working Memory, Spatial Representation & Mapping, Scene Understanding & Visual Reasoning, Planning (minor)
L3: Cognitive Flexibility",L3
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon agent coherence by simulating management of a vending-machine business over an extended period, with many sequential decisions affecting final profit. It probes sustained strategy, adaptation to market dynamics, and consistent execution of multi-step operational workflows.","L1: 
L2: Planning, Decision-making, Reward Mechanisms, Adaptive Error Correction, Working Memory
L3: Motivational Drives (minor), Self-reflection (minor)",L2
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas measures real-world tool use through the Model Context Protocol, requiring models to discover relevant tools, call them with correct arguments, handle errors, and synthesize results across multi-step workflows. The benchmark targets robust “tool-using agent” behavior in production-like API environments.","L1: Language Comprehension (minor), Language Production (minor)
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory
L3: ",L2
FinanceAgent,https://www.vals.ai/benchmarks/finance_agent,,"FinanceAgent evaluates tasks typical of an entry-level financial analyst, such as interpreting documents, performing calculations, and producing structured outputs with finance-specific constraints. It emphasizes applied reasoning, accuracy, and multi-step analysis that mirrors professional workflows.","L1: Language Production (minor)
L2: Logical Reasoning, Decision-making, Planning, Working Memory, Semantic Understanding & Context Recognition
L3: ",L2
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity agents on finding known vulnerabilities from high-level descriptions and, in some settings, discovering new weaknesses in real open-source codebases. It stresses systematic investigation, hypothesis testing against code behavior, and careful validation under realistic constraints.","L1: 
L2: Planning, Logical Reasoning, Adaptive Error Correction, Working Memory, Decision-making (minor)
L3: Inhibitory Control",L3
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"SpreadsheetBench measures an agent’s ability to work with complex spreadsheets (navigation, editing formulas, formatting, and transforming data) to solve real-world-style tasks. It emphasizes procedural correctness and maintaining consistency across interdependent cells, sheets, and constraints.","L1: 
L2: Planning, Decision-making, Working Memory, Adaptive Error Correction, Logical Reasoning (minor), Semantic Understanding & Context Recognition (minor)
L3: ",L2
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam (HLE) is a difficult multimodal benchmark intended to probe frontier knowledge and reasoning across many domains, often requiring integrating information from text and images. It is typically used to gauge broad expert-level problem solving, including questions where shallow pattern matching is insufficient.","L1: Language Comprehension, Visual Perception (minor)
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Scene Understanding & Visual Reasoning (minor)
L3: ",L2
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 evaluates competition-style mathematics problem solving under constraints similar to timed exams, requiring exact numeric answers. It emphasizes multi-step derivations, algebraic manipulation, and careful error avoidance rather than knowledge lookup.","L1: 
L2: Logical Reasoning, Working Memory, Planning (minor), Adaptive Error Correction (minor)
L3: ",L2
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a curated subset of very difficult graduate-level science multiple-choice questions designed to be “Google-proof” and to discriminate expert reasoning from superficial pattern matching. It targets deep conceptual understanding and multi-hop scientific reasoning across biology, chemistry, and physics.","L1: Language Comprehension
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory (minor), Decision-making (minor)
L3: ",L2
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends MMLU-style academic knowledge testing to multiple languages, assessing whether models can answer subject-area questions across diverse linguistic contexts. It probes multilingual generalization, robust understanding of prompts, and consistent reasoning across languages.","L1: Language Comprehension, Language Production (minor)
L2: Semantic Understanding & Context Recognition, Logical Reasoning (minor), Working Memory (minor)
L3: Cognitive Flexibility",L3
MMMU,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2311.16502,"MMMU is a multimodal benchmark spanning many disciplines where models must answer questions that require interpreting images (diagrams, charts, figures) together with text. It emphasizes integrated visual-language reasoning and domain knowledge applied to realistic multimodal problems.","L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Multisensory Integration, Visual Attention & Eye Movements (minor), Logical Reasoning (minor)
L3: ",L2
LAB-Bench FigQA,https://huggingface.co/datasets/futurehouse/lab-bench,https://arxiv.org/abs/2407.10362,"LAB-Bench FigQA tests whether models can correctly interpret complex scientific figures from biology papers and answer questions that depend on accurate visual extraction and scientific reasoning. It is designed to reflect practical research workflows where critical information is embedded in plots, schematics, and multi-panel figures.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Multisensory Integration, Attention, Semantic Understanding & Context Recognition, Logical Reasoning (minor)
L3: ",L2
WebArena,https://webarena.dev/,https://arxiv.org/abs/2307.13854,"WebArena evaluates autonomous web agents that must complete realistic tasks across multiple websites (e.g., e-commerce, CMS, forums, code hosting) using a browser interface. It measures long-horizon navigation, form filling, state tracking, and robustness to dynamic pages and multi-step goals.","L1: Visual Perception
L2: Planning, Decision-making, Scene Understanding & Visual Reasoning, Sensorimotor Coordination, Adaptive Error Correction, Working Memory, Attention (minor)
L3: ",L2
