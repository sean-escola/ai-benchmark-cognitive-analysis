Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates agentic software engineering by asking a model to generate patches that fix real issues in open-source Python repositories, with solutions validated by tests. The “Verified” subset filters tasks to those confirmed solvable and reduces evaluation noise, emphasizing end-to-end debugging and code modification fidelity.","L1: Language Comprehension (minor), Language Production (minor)
L2: Planning, Logical Reasoning, Adaptive Error Correction, Working Memory
L3: ",L2
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 measures how well an agent can accomplish real command-line tasks in a sandboxed terminal environment (e.g., installing tools, editing files, running commands, diagnosing failures). It stresses iterative interaction, error recovery, and tool-mediated execution rather than purely “answering” questions.","L1: Language Comprehension (minor)
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory, Attention (minor)
L3: ",L2
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp evaluates deep web research by requiring models/agents to search, read, and synthesize information to answer questions that cannot be solved from parametric knowledge alone. It emphasizes retrieval strategy, evidence integration across sources, and robustness to misleading or incomplete pages.","L1: 
L2: Planning, Decision-making, Semantic Understanding & Context Recognition, Episodic Memory, Working Memory, Attention (minor)
L3: ",L2
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench evaluates interactive customer-support-style agent behavior in simulated domains (e.g., retail, airline, telecom), where the model must call APIs, follow policies, and resolve multi-turn user issues. It probes reliability over long dialogues, policy adherence under pressure, and coordinated tool use with user-facing communication.","L1: Language Comprehension, Language Production
L2: Decision-making, Planning
L3: Inhibitory Control, Social Reasoning & Theory of Mind (minor)",L3
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld evaluates multimodal computer-use agents on tasks performed in a real operating system environment, requiring UI navigation, text entry, and application workflows. Success depends on grounding instructions in screenshots/GUI state and executing long action sequences without losing the task goal.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Planning, Decision-making, Sensorimotor Coordination (minor)
L3: ",L2
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI measures “fluid” pattern induction: given a few input–output grid examples, the model must infer a latent rule and produce the correct output grid for a new input. It is designed to reduce benefits from memorization and instead reward compositional abstraction and generalization from scarce demonstrations.","L1: 
L2: Logical Reasoning, Working Memory, Spatial Representation & Mapping, Attention (minor)
L3: Cognitive Flexibility",L3
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon autonomous agency in a simulated business environment where the model manages a vending machine operation over many decisions and time steps. It requires sustained coherence, strategic planning, and adaptation to changing demand, inventory, and negotiation outcomes.","L1: 
L2: Planning, Decision-making, Reward Mechanisms, Working Memory, Episodic Memory (minor), Semantic Understanding & Context Recognition (minor)
L3: ",L2
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas evaluates real-world tool use through the Model Context Protocol by testing whether models can discover, invoke, and compose tools across multi-step workflows. Tasks often involve handling tool errors, retries, and integrating tool outputs into a correct final response.","L1: Language Comprehension (minor), Language Production (minor)
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory
L3: ",L2
FinanceAgent,https://www.vals.ai/benchmarks/finance_agent,,"FinanceAgent evaluates whether a model can perform tasks resembling entry-level financial analysis, such as interpreting financial documents, computing metrics, and producing structured deliverables. It stresses domain-grounded reasoning, numerical consistency, and multi-step workflow execution under realistic constraints.","L1: Language Production (minor)
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Planning, Decision-making, Working Memory
L3: ",L2
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity agent capabilities on large-scale tasks involving finding known vulnerabilities and discovering new ones in real open-source projects. It emphasizes code comprehension, hypothesis-driven debugging, and careful iterative testing to validate security-relevant behaviors.","L1: 
L2: Logical Reasoning, Planning, Adaptive Error Correction, Semantic Understanding & Context Recognition, Working Memory
L3: ",L2
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"SpreadsheetBench measures an agent’s ability to understand, edit, and compute within complex spreadsheets derived from realistic scenarios. It tests multi-step transformations, formula reasoning, and maintaining correctness across interconnected cells, tables, and formatting constraints.","L1: 
L2: Working Memory, Logical Reasoning, Planning, Adaptive Error Correction, Attention (minor)
L3: ",L2
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a multimodal benchmark intended to probe frontier-level academic and professional problem solving across many domains and question styles. It stresses cross-domain reasoning, careful reading of prompts and figures, and (in tool-enabled settings) effective use of external search/code for verification.","L1: Language Comprehension, Visual Perception (minor)
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Planning (minor)
L3: ",L2
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 evaluates competition-style mathematics problem solving using questions from the American Invitational Mathematics Examination. It emphasizes multi-step symbolic reasoning, maintaining constraints across derivations, and avoiding arithmetic/algebraic slips under time-like pressure.","L1: 
L2: Logical Reasoning, Working Memory, Adaptive Error Correction (minor), Planning (minor)
L3: ",L2
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a hard multiple-choice science QA subset designed to be resistant to superficial web search and to require expert-level reasoning across physics, chemistry, and biology. It probes deep conceptual understanding, careful discrimination between plausible distractors, and multi-hop inference.","L1: 
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory (minor), Decision-making (minor)
L3: ",L2
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends MMLU to multiple languages, evaluating broad academic knowledge and reasoning across many subjects and non-English prompts. It mainly tests multilingual comprehension and retrieval of learned concepts, with some demand for structured reasoning in technical topics.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning (minor), Working Memory (minor)
L3: ",L2
MMMU,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2311.16502,"MMMU evaluates multimodal expert-level understanding across many disciplines by combining images (e.g., diagrams, charts, screenshots) with text questions and multi-choice answers. It probes integrated visual–text reasoning, spatial/diagram interpretation, and domain-specific knowledge application.","L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Visual Attention & Eye Movements (minor)
L3: ",L2
LAB-Bench FigQA,https://huggingface.co/datasets/futurehouse/lab-bench,https://arxiv.org/abs/2407.10362,"LAB-Bench FigQA measures scientific figure understanding in biology papers by asking questions that require interpreting plots, schematics, and experimental result figures. It stresses evidence extraction from dense visuals and aligning that evidence with domain concepts and textual cues.","L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Logical Reasoning, Attention (minor)
L3: ",L2
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning evaluates reasoning over figures (notably charts) drawn from scientific documents, requiring models to read axes/legends, compare trends, and answer structured questions grounded in the visual. It targets robust chart literacy rather than generic image captioning.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Logical Reasoning, Semantic Understanding & Context Recognition (minor), Attention (minor)
L3: ",L2
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU extends multimodal understanding to video by requiring temporal integration across frames to answer questions about actions, events, and instructional/visual content. It stresses maintaining context over time and reasoning about dynamic scenes rather than static snapshots.","L1: Visual Perception, Auditory Processing (minor)
L2: Scene Understanding & Visual Reasoning, Working Memory, Attention
L3: Cognitive Timing & Predictive Modeling (minor)",L2
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"FACTS Benchmark Suite evaluates factuality by testing whether model outputs are supported by provided evidence and whether they avoid hallucinated or contradicted claims. It emphasizes precise grounding, careful attribution, and resisting plausible-sounding fabrication in generation.","L1: Language Comprehension, Language Production
L2: Semantic Understanding & Context Recognition, Adaptive Error Correction (minor)
L3: Inhibitory Control, Self-reflection (minor)",L3
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA evaluates physical commonsense and procedural reasoning across many languages using non-parallel (not directly translated) items, aiming to reduce translation artifacts. It probes whether models can select plausible actions/outcomes in everyday physical situations under diverse linguistic framing.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Spatial Representation & Mapping (minor), Sensorimotor Coordination (minor)
L3: ",L2
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) is a long-context evaluation where multiple similar “needle” interactions are embedded in a long “haystack,” and the model must retrieve and reproduce the correct target response. It stresses robust coreference/recall under heavy distraction and long-range dependencies.","L1: Language Comprehension
L2: Working Memory, Attention, Episodic Memory, Adaptive Error Correction (minor)
L3: ",L2
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval evaluates well-specified professional knowledge work across many occupations by judging the quality of produced work artifacts (e.g., spreadsheets, plans, written deliverables) against expert baselines. It probes end-to-end task execution: interpreting requirements, structuring outputs, and making pragmatic tradeoffs consistent with real workplace constraints.","L1: Language Production
L2: Planning, Decision-making, Semantic Understanding & Context Recognition, Working Memory
L3: Social Reasoning & Theory of Mind (minor)",L2
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks evaluates structured reasoning over graphs by asking models to follow walks/traversals and answer questions that require tracking nodes, edges, and paths. It emphasizes compositional, stepwise computation and resistance to shortcutting when distractor structure is present.","L1: 
L2: Spatial Representation & Mapping, Working Memory, Logical Reasoning, Attention (minor)
L3: ",L2
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon evaluates general tool-use competence across diverse APIs and tasks, focusing on whether models can select appropriate tools, call them correctly, and integrate results into accurate final outputs. It stresses robustness to tool errors, schema mismatches, and multi-tool workflows.","L1: Language Comprehension (minor)
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory
L3: Inhibitory Control (minor)",L2
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath evaluates expert-level mathematics with problems designed to be challenging for frontier models and more resistant to memorization, often requiring deep multi-step derivations. It probes sustained formal reasoning, careful bookkeeping of assumptions, and the ability to navigate complex solution spaces without collapsing into heuristic guesswork.","L1: 
L2: Logical Reasoning, Working Memory, Planning, Adaptive Error Correction (minor)
L3: ",L2
