Benchmark,Website,Paper,Description,Cognitive Functions
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,SWE-bench Verified evaluates software engineering agents on real GitHub issues by requiring them to produce patches that make a failing test suite pass. The Verified variant uses a curated subset of tasks that are human-validated as solvable and aims to reduce noise from ambiguous or incorrect problem instances.,"Planning, Decision-making, Adaptive Error Correction, Working Memory, Logical Reasoning, Language Comprehension, Semantic Understanding & Context Recognition (minor)"
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 evaluates agentic problem-solving in command-line environments, where models must accomplish real tasks by issuing shell commands and inspecting outputs. It emphasizes iterative debugging, tool use, and robustness to environment feedback under realistic resource and time constraints.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Language Comprehension (minor), Inhibitory Control (minor)"
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp measures an agent’s ability to perform web-based information-seeking and synthesis, typically requiring multiple navigation steps, evidence gathering, and final answer generation. It stresses reliability under partial information and the need to reconcile conflicting sources.","Planning, Decision-making, Attention, Working Memory, Episodic Memory (minor), Language Comprehension, Semantic Understanding & Context Recognition, Language Production (minor)"
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench evaluates interactive customer-support agents that must follow domain policies while using tools/APIs in multi-turn dialogues with a simulated user. It probes whether the agent can maintain state, handle exceptions, and adhere to constraints despite user pressure or ambiguous situations.","Social Reasoning & Theory of Mind, Inhibitory Control, Planning, Decision-making, Working Memory, Language Comprehension, Language Production, Empathy (minor)"
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld evaluates multimodal computer-use agents on real desktop-like operating system tasks (e.g., navigating UIs, managing files, and using applications). Success requires grounding actions in visual observations and executing long, error-prone action chains with recovery from misclicks or misunderstandings.","Visual Perception, Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Sensorimotor Coordination, Planning, Decision-making, Adaptive Error Correction, Working Memory, Spatial Representation & Mapping (minor)"
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,ARC-AGI is a few-shot abstract reasoning benchmark where systems infer hidden rules from a handful of input–output grid examples and apply them to a new grid. It is designed to emphasize fluid reasoning and compositional generalization rather than memorization of task patterns.,"Logical Reasoning, Cognitive Flexibility, Working Memory, Spatial Representation & Mapping, Scene Understanding & Visual Reasoning (minor), Attention (minor)"
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon autonomy by having an agent manage a simulated vending-machine business over an extended period (e.g., a year) with many sequential decisions. It stresses sustained coherence, planning under uncertainty, and adapting strategy based on market and operational feedback.","Planning, Decision-making, Reward Mechanisms, Cognitive Timing & Predictive Modeling, Working Memory, Episodic Memory (minor), Adaptive Error Correction (minor)"
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas measures real-world tool-use via the Model Context Protocol (MCP), requiring models to discover relevant tools, invoke them correctly, and compose multi-step workflows. It emphasizes correct API usage, error handling, and integrating tool outputs into final responses.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Language Comprehension, Language Production (minor)"
FinanceAgent,https://www.vals.ai/benchmarks/finance_agent,,"FinanceAgent evaluates performance on tasks typical of an entry-level financial analyst, such as analysis, reasoning over financial documents/data, and producing decision-support outputs. It stresses quantitative and domain reasoning, structured communication, and workflow execution across subtasks.","Logical Reasoning, Planning, Decision-making, Working Memory, Semantic Understanding & Context Recognition, Language Production"
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity agent capabilities on large-scale, real-software vulnerability tasks, including identifying known weaknesses and discovering new ones. It stresses investigative reasoning, hypothesis testing, and iterative correction based on tool outputs and code evidence.","Logical Reasoning, Planning, Decision-making, Adaptive Error Correction, Working Memory, Language Comprehension (minor)"
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"SpreadsheetBench evaluates an agent’s ability to understand, edit, and compute with complex spreadsheets using realistic tasks drawn from practical scenarios. It probes structured manipulation, multi-step transformations, and verification of computed results under constraints.","Planning, Logical Reasoning, Decision-making, Working Memory, Adaptive Error Correction (minor), Semantic Understanding & Context Recognition (minor)"
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a frontier, multi-domain benchmark aimed at measuring advanced academic and professional reasoning across challenging questions, including multimodal items. It emphasizes synthesis, careful reading, and robust problem-solving rather than shallow pattern matching.","Language Comprehension, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Planning (minor), Visual Perception (minor)"
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 uses problems from the American Invitational Mathematics Examination, a competition-style math test requiring multi-step derivations and precise final answers. It primarily measures mathematical reasoning and error-free symbolic/quantitative manipulation.","Logical Reasoning, Working Memory, Adaptive Error Correction (minor), Planning (minor)"
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a difficult multiple-choice science benchmark designed to be resistant to simple web lookup, focusing on graduate-level questions in physics, chemistry, and biology. The Diamond subset targets higher-quality items where experts succeed and non-experts tend to fail.","Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory (minor)"
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,MMMLU extends broad knowledge and reasoning evaluation to multiple languages across many academic subjects. It probes whether models can transfer competencies beyond English while maintaining consistent task understanding under multilingual prompts.,"Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning (minor), Language Production (minor)"
MMMU,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2311.16502,MMMU evaluates multimodal understanding across diverse disciplines by combining images with text questions that require expert-style reasoning. Tasks often demand interpreting diagrams/figures and integrating them with domain knowledge to select or generate correct answers.,"Visual Perception, Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Working Memory (minor), Attention (minor)"
LAB-Bench FigQA,https://huggingface.co/datasets/futurehouse/lab-bench,https://arxiv.org/abs/2407.10362,LAB-Bench FigQA evaluates whether models can correctly interpret scientific figures from biology papers and answer questions requiring careful visual analysis. It stresses extracting quantitative/structural information from plots and diagrams and connecting it to scientific context.,"Visual Perception, Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Attention, Semantic Understanding & Context Recognition (minor)"
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning evaluates reasoning over scientific-paper visuals (e.g., charts/figures) and accompanying context, emphasizing faithful extraction and multi-step inference from the visual evidence. It is often used to test whether models can ground conclusions in diagrammatic data rather than relying on priors.","Visual Perception, Scene Understanding & Visual Reasoning, Attention, Logical Reasoning, Multisensory Integration (minor), Working Memory (minor)"
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU extends multimodal reasoning to the temporal domain, requiring models to answer questions based on video content that may include actions, events, and scene changes. It probes temporal integration and the ability to maintain and update hypotheses as new frames provide evidence.","Visual Perception, Cognitive Timing & Predictive Modeling, Working Memory, Attention, Scene Understanding & Visual Reasoning (minor), Multisensory Integration (minor)"
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"The FACTS Benchmark Suite is a collection of factuality-focused evaluations intended to systematically measure whether model outputs are correct, well-grounded, and robust to common sources of hallucination. It emphasizes verifiable claims, faithful use of provided context, and calibration about uncertainty.","Semantic Understanding & Context Recognition, Language Comprehension, Language Production, Inhibitory Control, Self-reflection (minor), Working Memory (minor)"
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,Global PIQA evaluates physical commonsense and practical reasoning across many languages and culturally diverse settings using non-parallel items. It stresses whether models can infer plausible actions/outcomes in everyday physical situations beyond English-centric distributions.,"Semantic Understanding & Context Recognition, Logical Reasoning, Language Comprehension, Social Reasoning & Theory of Mind (minor)"
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) tests long-context multi-round coreference resolution by placing multiple similar “needle” interactions within a long “haystack” of dialogue and asking the model to retrieve the correct referenced content. The 8-needle variant increases distractor density, stressing robust retrieval and interference resistance at long context lengths.","Working Memory, Attention, Episodic Memory, Language Comprehension, Semantic Understanding & Context Recognition (minor), Inhibitory Control (minor)"
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval evaluates economically valuable, well-specified knowledge-work tasks across many occupations by judging the quality of produced work products (e.g., spreadsheets, presentations, plans). It emphasizes end-to-end task execution: understanding requirements, organizing deliverables, and iteratively refining outputs to professional standards.","Planning, Decision-making, Language Production, Semantic Understanding & Context Recognition, Working Memory, Adaptive Error Correction (minor), Social Reasoning & Theory of Mind (minor)"
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks evaluates graph-based reasoning, where models must follow edges/relations through structured graphs to answer traversal and reachability-style questions. It stresses maintaining intermediate states and applying consistent stepwise inference over symbolic structure.","Spatial Representation & Mapping, Working Memory, Logical Reasoning, Attention (minor)"
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon evaluates tool-using agents on diverse tasks that require selecting, calling, and composing tools correctly across multi-step workflows. It emphasizes robustness to tool errors, correct argument formatting, and coherent integration of tool outputs into a final solution.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Language Comprehension (minor), Language Production (minor)"
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath evaluates advanced mathematical reasoning on research-level and expert-level problems, often requiring long derivations, novel combinations of techniques, and high precision. It is intended to better separate frontier models on genuinely difficult math rather than competition items with common templates.","Logical Reasoning, Working Memory, Planning (minor), Adaptive Error Correction (minor)"
