Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates software-engineering agents on real GitHub issues by requiring them to produce a code patch that makes a repository’s tests pass. The “Verified” subset contains tasks validated by humans to be solvable and aims to reduce noisy or ambiguous problem instances, emphasizing reliable end-to-end bug fixing.","L1: Language Comprehension (minor), Language Production (minor)
L2: Planning, Logical Reasoning, Adaptive Error Correction, Working Memory
L3: ",L2
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 measures how well an agent can complete practical tasks in a command-line environment (e.g., inspecting files, running programs, configuring tools) under realistic constraints. It emphasizes multi-step execution, debugging, and robustness to tool/OS feedback rather than single-shot question answering.","L1: 
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory, Logical Reasoning (minor), Attention (minor)
L3: ",L2
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp evaluates “deep research” behavior: the model must search, read, and synthesize information from a constrained web/document collection to answer difficult questions. Performance depends on selecting relevant sources, integrating evidence, and maintaining consistency across many retrieved snippets.","L1: Language Production (minor)
L2: Planning, Decision-making, Semantic Understanding & Context Recognition, Episodic Memory (minor), Working Memory
L3: ",L2
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench tests interactive agent behavior in simulated customer-support domains (e.g., retail, airline, telecom) where the agent must follow policies while using programmatic tools/APIs in multi-turn dialogs. It probes reliability under policy constraints, correct tool invocation, and staying aligned with user intent without exploiting loopholes.","L1: Language Comprehension, Language Production (minor)
L2: Decision-making, Planning, Reward Mechanisms (minor)
L3: Social Reasoning & Theory of Mind, Inhibitory Control",L3
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld evaluates multimodal computer-use agents that must operate a desktop-like environment to complete tasks (navigating GUIs, clicking, typing, and interpreting visual feedback). It stresses grounded perception-action loops, long-horizon task execution, and recovery from mistakes in real UI workflows.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements (minor), Spatial Representation & Mapping, Sensorimotor Coordination, Planning, Adaptive Error Correction
L3: ",L2
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI measures abstract, few-shot pattern induction: given a small number of input–output grid examples, the model must infer the hidden transformation and produce the correct output for a new input. It is designed to emphasize fluid reasoning and generalization to novel tasks rather than memorized knowledge.","L1: Visual Perception (minor)
L2: Logical Reasoning, Working Memory, Spatial Representation & Mapping
L3: Cognitive Flexibility",L3
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon autonomous agency by simulating management of a vending-machine business over many decisions (pricing, inventory, supplier negotiation, adapting to demand). Scores typically reflect sustained coherence, strategic planning, and the ability to learn from outcomes across an extended trajectory.","L1: 
L2: Planning, Decision-making, Reward Mechanisms, Episodic Memory, Working Memory (minor)
L3: Self-reflection (minor)",L2
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas measures real-world tool use through the Model Context Protocol (MCP): models must discover relevant tools, call them with correct schemas, and combine outputs across multi-step workflows. It targets reliability of API orchestration, error handling, and composing results into correct final answers.","L1: Language Comprehension (minor), Language Production (minor)
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory
L3: ",L2
FinanceAgent,https://www.vals.ai/benchmarks/finance_agent,,"FinanceAgent evaluates whether a model can complete tasks representative of an entry-level financial analyst (e.g., interpreting filings, building analyses, producing well-justified recommendations). It emphasizes quantitative/structured reasoning, constraint tracking, and producing professional, verifiable outputs.","L1: Language Production (minor)
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Planning (minor), Decision-making, Working Memory
L3: ",L2
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity capability at scale by asking agents to find known vulnerabilities from descriptions and to discover previously unknown vulnerabilities in real open-source projects. It stresses systematic code understanding, adversarial thinking, and iterative debugging/exploitation attempts under realistic constraints.","L1: 
L2: Logical Reasoning, Planning, Adaptive Error Correction, Working Memory (minor)
L3: Inhibitory Control (minor)",L2
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"SpreadsheetBench measures how well models can understand, edit, and compute with complex spreadsheets drawn from realistic scenarios. It tests structured manipulation (formulas, tables, formatting), multi-step transformations, and verification of results under tool-based workflows.","L1: Language Comprehension (minor)
L2: Working Memory, Logical Reasoning, Planning, Adaptive Error Correction, Attention (minor)
L3: ",L2
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a large, frontier-style benchmark spanning difficult questions across many disciplines, including multimodal items, intended to test broad expert-level reasoning and knowledge integration. It emphasizes solving novel, high-difficulty problems rather than narrow task proficiency.","L1: Language Comprehension, Visual Perception (minor)
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Multisensory Integration (minor)
L3: ",L2
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 evaluates competition-style mathematics problem solving using the 2025 AIME questions, typically requiring multi-step derivations and careful algebra/number theory reasoning. It primarily probes deliberate reasoning under minimal external context, with strong sensitivity to small logical mistakes.","L1: 
L2: Logical Reasoning, Working Memory, Planning (minor), Adaptive Error Correction (minor)
L3: Cognitive Flexibility (minor)",L2
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a subset of extremely difficult graduate-level science multiple-choice questions designed to be “Google-proof,” requiring real domain understanding and reasoning. The Diamond split focuses on high-quality items where experts succeed and non-experts frequently fail.","L1: Language Comprehension (minor)
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory (minor)
L3: ",L2
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends MMLU to multiple languages, assessing breadth of academic knowledge and reasoning across many subjects in multilingual settings. It probes whether models preserve competence under translation and culturally varied linguistic contexts.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning (minor), Working Memory (minor)
L3: ",L2
MMMU,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2311.16502,"MMMU evaluates multimodal understanding and reasoning across many disciplines by requiring models to answer questions grounded in images (diagrams, charts, screenshots) plus text. It stresses integrating visual evidence with domain knowledge and multi-step reasoning.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Working Memory (minor), Visual Attention & Eye Movements (minor)
L3: ",L2
LAB-Bench FigQA,https://huggingface.co/datasets/futurehouse/lab-bench,https://arxiv.org/abs/2407.10362,"LAB-Bench FigQA tests whether models can correctly read and reason over complex scientific figures from biology papers (e.g., plots, microscopy panels, multi-part figure layouts). It targets practical figure-grounded inference: extracting the right visual signal, mapping it to scientific concepts, and answering precisely.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Attention, Semantic Understanding & Context Recognition, Logical Reasoning (minor)
L3: ",L2
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning evaluates reasoning over scientific charts/figures from arXiv-style papers, often requiring quantitative reading (axes, legends), cross-referencing captions, and multi-step inference. It stresses robust chart interpretation and translating visual evidence into correct, well-specified answers.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Logical Reasoning, Semantic Understanding & Context Recognition (minor), Working Memory (minor)
L3: ",L2
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU measures multimodal reasoning over videos, requiring understanding of events, temporal relationships, and visual details across frames paired with language questions. It probes whether models can maintain coherent interpretations across time and integrate them into grounded answers.","L1: Visual Perception
L2: Attention, Working Memory, Scene Understanding & Visual Reasoning, Multisensory Integration (minor)
L3: Cognitive Timing & Predictive Modeling",L3
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"The FACTS Benchmark Suite systematically evaluates factuality-related behaviors, such as correctness, grounding to evidence, and resistance to producing unsupported claims across diverse settings. It targets reliable truthfulness and calibration rather than pure capability on knowledge-heavy questions.","L1: Language Production (minor)
L2: Semantic Understanding & Context Recognition, Episodic Memory (minor)
L3: Inhibitory Control, Self-reflection (minor)",L3
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA evaluates physical commonsense reasoning across many languages and cultural contexts using non-parallel multilingual data, aiming to test whether models generalize beyond English-centric priors. It probes robust everyday reasoning about physical interactions and plausible outcomes expressed in diverse languages.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning
L3: Social Reasoning & Theory of Mind (minor)",L2
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 is a long-context, multi-round coreference/recall evaluation where multiple similar “needle” requests are embedded into a long “haystack,” and the model must retrieve the correct referenced content (e.g., the response to the nth needle). The 8-needle setting stresses sustained context tracking, disambiguation among near-duplicates, and robustness as context length grows.","L1: 
L2: Working Memory, Attention, Episodic Memory, Semantic Understanding & Context Recognition
L3: Inhibitory Control (minor)",L2
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval evaluates economically valuable “knowledge work” by having models produce real work artifacts (e.g., spreadsheets, presentations, schedules) across many occupations, judged by experts in side-by-side comparisons. It emphasizes end-to-end task completion quality, adherence to requirements, and professional decision-making under constraints.","L1: Language Production
L2: Planning, Decision-making, Semantic Understanding & Context Recognition, Working Memory (minor)
L3: Self-reflection (minor)",L2
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks tests reasoning over graph-structured data by requiring models to perform traversals, track paths, and answer queries tied to graph topology (e.g., BFS-like exploration or parent-pointer reasoning). It is designed to probe structured relational reasoning and robustness to combinatorial complexity.","L1: 
L2: Spatial Representation & Mapping, Logical Reasoning, Working Memory, Attention (minor)
L3: ",L2
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon evaluates general-purpose tool-using agents across a variety of tasks that require selecting among tools, composing multi-step calls, and integrating tool outputs into final responses. It stresses robustness to tool errors, schema adherence, and strategic decomposition of problems into executable steps.","L1: Language Comprehension (minor), Language Production (minor)
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory
L3: ",L2
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath evaluates very challenging mathematics problems (including multiple difficulty tiers), designed to stress advanced reasoning rather than routine computation. It probes sustained multi-step derivations, careful verification, and the ability to navigate novel mathematical structures with minimal guidance.","L1: 
L2: Logical Reasoning, Working Memory, Planning (minor), Adaptive Error Correction (minor)
L3: Cognitive Flexibility (minor)",L2
