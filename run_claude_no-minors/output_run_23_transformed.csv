Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates end-to-end software engineering by giving a model a real GitHub repository, an issue description, and a test suite; the model must produce a patch that makes the tests pass. The “Verified” subset consists of tasks that were validated (e.g., by human review and/or stronger filtering) to be solvable and to have reliable evaluation, reducing noise from ambiguous issues and flaky tests.","L1: Language Comprehension
L2: Planning, Logical Reasoning, Working Memory, Adaptive Error Correction, Semantic Understanding & Context Recognition, Decision-making (minor)
L3: ",L2
SWE-bench Multilingual,https://www.swebench.com/multilingual.html,https://arxiv.org/abs/2504.21798,"SWE-bench Multilingual extends SWE-bench-style repository-level bug fixing beyond Python to multiple programming languages, requiring models to interpret issues, navigate unfamiliar codebases, and apply correct language-specific edits. It emphasizes cross-language generalization while keeping the same core requirement: produce a patch that satisfies the project’s tests or checks.","L1: Language Comprehension, Language Production (minor)
L2: Planning, Logical Reasoning, Working Memory, Adaptive Error Correction, Semantic Understanding & Context Recognition
L3: Cognitive Flexibility",L3
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro is a harder, larger-scale software engineering benchmark designed to be more challenging and more representative of professional development work than earlier SWE-bench variants. Tasks typically require deeper debugging and integration across files and components, with stronger emphasis on robustness and reduced susceptibility to shortcut solving.","L1: Language Comprehension
L2: Planning, Logical Reasoning, Working Memory, Adaptive Error Correction, Decision-making, Semantic Understanding & Context Recognition
L3: Cognitive Flexibility (minor)",L2
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 evaluates agentic performance in real command-line environments, where the model must accomplish practical tasks by issuing shell commands, inspecting outputs, and iteratively correcting mistakes. It measures reliability under tool use and long-horizon execution, with success judged by task-specific checkers rather than subjective grading.","L1: Language Comprehension (minor)
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition, Logical Reasoning (minor)
L3: ",L2
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp evaluates deep-research style web browsing where models must search, read, and synthesize information across multiple documents to answer difficult questions. It is designed to stress retrieval strategy, source integration, and avoiding unsupported claims when evidence is dispersed across pages.","L1: Language Comprehension, Language Production (minor)
L2: Planning, Attention, Semantic Understanding & Context Recognition, Episodic Memory, Working Memory, Logical Reasoning
L3: ",L2
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench evaluates interactive, multi-turn customer-support agents that must use tools/APIs while following domain policies (e.g., retail, airline, telecom). The benchmark stresses consistent policy adherence, correct state tracking over dialogue, and robust tool use under realistic conversational pressure.","L1: Language Comprehension, Language Production
L2: Decision-making, Planning, Working Memory
L3: Social Reasoning & Theory of Mind, Inhibitory Control, Empathy (minor)",L3
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld is a multimodal computer-use benchmark where agents operate a desktop-like environment to complete tasks by interpreting the screen and taking UI actions. It measures perception-to-action competence on realistic workflows, including navigation, form filling, file operations, and error recovery.","L1: Visual Perception
L2: Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Sensorimotor Coordination, Planning, Decision-making, Working Memory, Adaptive Error Correction (minor)
L3: ",L2
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,ARC-AGI is a few-shot abstraction and reasoning benchmark where models infer a latent rule from a small number of input–output grid examples and apply it to a new grid. It targets generalization to novel transformations and compositional pattern induction rather than memorized task formats.,"L1: 
L2: Logical Reasoning, Working Memory, Spatial Representation & Mapping, Attention, Scene Understanding & Visual Reasoning
L3: Cognitive Flexibility, Cognitive Timing & Predictive Modeling (minor)",L3
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon autonomous decision-making in a simulated vending-machine business over many steps (e.g., procurement, pricing, inventory, and negotiation-like interactions). The score is typically tied to final financial outcomes, requiring sustained coherence, adaptation to changing conditions, and strategic planning.","L1: 
L2: Planning, Decision-making, Reward Mechanisms, Episodic Memory, Working Memory, Adaptive Error Correction (minor)
L3: Motivational Drives (minor), Social Reasoning & Theory of Mind (minor)",L2
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas measures real-world tool-use via the Model Context Protocol (MCP), where models must discover appropriate tools, call them with correct arguments, handle failures, and synthesize results. It emphasizes multi-step workflow execution across heterogeneous APIs and environments.","L1: Language Comprehension (minor)
L2: Planning, Decision-making, Working Memory, Semantic Understanding & Context Recognition, Adaptive Error Correction
L3: Inhibitory Control (minor)",L2
FinanceAgent,https://www.vals.ai/benchmarks/finance_agent,,"FinanceAgent evaluates whether models can perform tasks typical of an entry-level financial analyst, such as extracting relevant information, performing calculations, building analyses, and producing structured outputs. The tasks are designed to capture applied reasoning and process reliability rather than isolated trivia.","L1: Language Comprehension, Language Production (minor)
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Decision-making, Planning, Working Memory
L3: ",L2
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates agentic cybersecurity capability at scale, including identifying known vulnerabilities from descriptions and attempting to discover new issues in real open-source projects. It stresses practical reasoning about code, exploitation constraints, and iterative debugging under tool-driven workflows.","L1: 
L2: Logical Reasoning, Planning, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition, Decision-making (minor)
L3: Inhibitory Control (minor)",L2
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"SpreadsheetBench evaluates spreadsheet-centric problem solving on complex workbooks, requiring models to navigate, edit, and compute using spreadsheet operations and/or programmatic tooling. It targets structured manipulation, formula reasoning, and producing correct outputs under realistic tabular constraints.","L1: 
L2: Logical Reasoning, Working Memory, Planning, Adaptive Error Correction, Semantic Understanding & Context Recognition, Decision-making (minor)
L3: ",L2
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a difficult, broad benchmark intended to probe frontier knowledge and reasoning across many domains, often including multimodal questions. It stresses synthesis and correctness under challenging prompts, and can be evaluated with or without external tools (e.g., search/code) depending on the setup.","L1: Language Comprehension, Language Production (minor)
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Multisensory Integration, Scene Understanding & Visual Reasoning (minor)
L3: ",L2
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 comprises competition-style math problems requiring multi-step derivations and precise answers, typically without relying on external factual recall. It emphasizes symbolic manipulation, careful constraint tracking, and error-free reasoning to reach a single numeric result.","L1: Language Comprehension (minor)
L2: Logical Reasoning, Working Memory, Planning (minor), Adaptive Error Correction (minor)
L3: ",L2
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a high-difficulty multiple-choice science benchmark curated so that subject-matter experts answer correctly while non-experts often fail, reducing the value of superficial heuristics. It targets deep scientific reasoning and careful discrimination among plausible distractors.","L1: Language Comprehension (minor)
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Decision-making (minor)
L3: ",L2
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends broad academic knowledge testing across many subjects and multiple languages, measuring how well models generalize knowledge and reasoning beyond English. It is commonly used to evaluate multilingual understanding, cross-lingual robustness, and consistency across topics.","L1: Language Comprehension, Language Production (minor)
L2: Semantic Understanding & Context Recognition, Logical Reasoning (minor), Decision-making (minor)
L3: Cognitive Flexibility",L3
MMMU,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2311.16502,"MMMU is a multimodal benchmark spanning many disciplines where models must answer questions grounded in images (e.g., diagrams, charts, photos) alongside text. It emphasizes integrating visual evidence with domain knowledge and reasoning to select or generate correct answers.","L1: Visual Perception, Language Comprehension (minor)
L2: Scene Understanding & Visual Reasoning, Multisensory Integration, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory (minor)
L3: ",L2
LAB-Bench FigQA,https://huggingface.co/datasets/futurehouse/lab-bench,https://arxiv.org/abs/2407.10362,"LAB-Bench FigQA evaluates whether models can correctly interpret and reason over complex scientific figures from biology papers (e.g., plots, microscopy panels, multi-part diagrams). It targets evidence-based answers from figures rather than generic biology recall, often requiring attention to fine visual details and captions.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory (minor)
L3: ",L2
WebArena,https://webarena.dev/,https://arxiv.org/abs/2307.13854,"WebArena evaluates autonomous web agents performing multi-step tasks across realistic web apps (e.g., shopping, CMS, code hosting, forums, maps), requiring navigation, form interaction, and stateful progress tracking. Success is judged by task completion, stressing robustness to dynamic pages and long action sequences.","L1: Visual Perception
L2: Planning, Decision-making, Visual Attention & Eye Movements, Sensorimotor Coordination, Working Memory, Adaptive Error Correction, Semantic Understanding & Context Recognition (minor)
L3: ",L2
