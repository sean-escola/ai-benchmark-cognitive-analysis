Benchmark,Website,Paper,Description,Cognitive Functions
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates software engineering agents on real GitHub issues where the model must produce a code patch that makes the repository’s tests pass. The “Verified” subset consists of tasks that have been human-validated as solvable, emphasizing reliability of automated evaluation via unit tests.","Language Comprehension, Planning, Logical Reasoning, Adaptive Error Correction, Working Memory (minor), Decision-making (minor)"
SWE-bench Multilingual,https://www.swebench.com/multilingual.html,https://arxiv.org/abs/2504.21798,"SWE-bench Multilingual extends SWE-bench to multiple programming languages, requiring models to resolve real-world issues across diverse language ecosystems and tooling conventions. It emphasizes cross-language generalization, code understanding, and patch generation under varying build/test workflows.","Language Comprehension, Cognitive Flexibility, Planning, Logical Reasoning, Adaptive Error Correction, Semantic Understanding & Context Recognition (minor)"
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro is a larger, more difficult software engineering benchmark designed to better reflect industrial complexity and to be more resistant to contamination and shortcutting. It typically requires deeper repo understanding, longer dependency chains, and more robust debugging than SWE-bench Verified.","Language Comprehension, Planning, Logical Reasoning, Adaptive Error Correction, Working Memory (minor), Inhibitory Control (minor)"
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 measures how well an agent can complete realistic tasks in a command-line environment, such as installing dependencies, manipulating files, running programs, and debugging failures. It stresses iterative interaction with a tool (the shell) under resource and state constraints typical of real systems.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Language Comprehension (minor), Attention (minor)"
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp evaluates deep research and browsing agents on questions that require gathering and synthesizing evidence from many documents rather than relying on memorized facts. It is designed to test search strategy, source integration, and faithful citation-like grounding over multi-step information-seeking episodes.","Planning, Semantic Understanding & Context Recognition, Working Memory, Decision-making, Episodic Memory (minor), Language Production (minor)"
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench measures agentic customer-support performance in simulated domains (e.g., retail, airline, telecom) where models must converse with a user and call APIs while adhering to domain policies. It emphasizes multi-turn procedural compliance, tool-use reliability, and resolving user goals under constraints.","Social Reasoning & Theory of Mind, Decision-making, Planning, Language Comprehension, Language Production, Inhibitory Control (minor), Empathy (minor)"
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld evaluates multimodal “computer use” agents that must operate a desktop-like environment to complete tasks via GUI interactions, often using screenshots and action tools (click, type, scroll). It probes end-to-end perception-action loops, including interpreting interfaces and executing multi-step procedures.","Visual Perception, Visual Attention & Eye Movements, Sensorimotor Coordination, Planning, Decision-making, Adaptive Error Correction (minor)"
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI tests fluid reasoning by asking models to infer latent rules from a small number of input-output grid examples and then apply the inferred transformation to a new grid. It aims to minimize reliance on prior knowledge, focusing on novel pattern discovery and systematic generalization.","Logical Reasoning, Cognitive Flexibility, Working Memory, Spatial Representation & Mapping, Scene Understanding & Visual Reasoning (minor)"
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon autonomous agency by simulating operation of a vending machine business over extended time, scoring outcomes like final balance. Success requires sustained coherence, strategic adaptation to market dynamics, and consistent execution across many sequential decisions.","Planning, Decision-making, Reward Mechanisms, Working Memory, Episodic Memory (minor), Self-reflection (minor)"
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas assesses real-world tool use via the Model Context Protocol, requiring models to discover tools, execute multi-step workflows across MCP servers, and integrate results into correct final answers. It stresses correct API invocation, error recovery, and coordination across heterogeneous tools and data sources.","Planning, Decision-making, Adaptive Error Correction, Working Memory (minor), Language Comprehension (minor)"
FinanceAgent,https://www.vals.ai/benchmarks/finance_agent,,"FinanceAgent evaluates agent performance on tasks resembling entry-level financial analyst work, such as financial modeling, document-driven analysis, and producing justified recommendations. It emphasizes domain-specific reasoning, structured output quality, and accurate integration of numbers and assumptions.","Logical Reasoning, Semantic Understanding & Context Recognition, Decision-making, Planning (minor), Working Memory (minor), Language Production (minor)"
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity agent capability on large-scale vulnerability tasks, including identifying known vulnerabilities from descriptions and discovering new issues in real software projects. It emphasizes rigorous technical reasoning, careful reading of codebases, and iterative debugging/testing under realistic constraints.","Logical Reasoning, Planning, Adaptive Error Correction, Attention (minor), Working Memory (minor), Inhibitory Control (minor)"
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"SpreadsheetBench measures how well agents can navigate, manipulate, and compute within complex spreadsheets derived from realistic scenarios. Tasks often require multi-step transformations, formula reasoning, and consistent handling of structured tabular data under tool-based execution.","Planning, Working Memory, Logical Reasoning, Adaptive Error Correction, Attention (minor), Decision-making (minor)"
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a challenging multimodal benchmark intended to probe frontier knowledge and reasoning across many disciplines, with questions designed to be difficult for both models and non-expert humans. It stresses robust reasoning, careful interpretation of prompts (and sometimes images), and avoiding overconfident hallucinated answers.","Language Comprehension, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory (minor), Scene Understanding & Visual Reasoning (minor), Visual Perception (minor)"
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 consists of competition-level mathematics problems that require multi-step symbolic reasoning and careful constraint handling. Performance reflects an agent’s ability to plan solution paths, maintain intermediate states, and execute error-free derivations under time-pressured-style problem formats.","Logical Reasoning, Working Memory, Planning (minor), Adaptive Error Correction (minor)"
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a high-difficulty, graduate-level multiple-choice science benchmark designed to be resistant to simple web search and shallow pattern matching. The “Diamond” subset focuses on high-quality questions where experts succeed and non-experts frequently fail, emphasizing deep conceptual understanding and careful reasoning.","Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory (minor), Decision-making (minor)"
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU is a multilingual extension of broad academic knowledge testing across many subjects, evaluating both knowledge and reasoning in multiple languages. It measures cross-lingual generalization and robustness to linguistic variation in questions and answer options.","Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning, Cognitive Flexibility (minor), Language Production (minor)"
MMMU,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2311.16502,"MMMU is a large-scale multimodal benchmark spanning many disciplines where models must answer questions grounded in images (diagrams, charts, screenshots) plus text. It targets integrated vision-language reasoning and the ability to extract and manipulate information from diverse visual formats.","Visual Perception, Scene Understanding & Visual Reasoning, Multisensory Integration, Language Comprehension, Working Memory (minor), Attention (minor)"
LAB-Bench FigQA,https://huggingface.co/datasets/futurehouse/lab-bench,https://arxiv.org/abs/2407.10362,"LAB-Bench FigQA evaluates whether models can correctly interpret and reason about complex scientific figures from biology papers, such as plots, schematic diagrams, and multi-panel figures. It emphasizes extracting evidence from visuals and connecting it to scientific claims and experimental context.","Visual Perception, Scene Understanding & Visual Reasoning, Semantic Understanding & Context Recognition, Logical Reasoning, Attention (minor), Multisensory Integration (minor)"
WebArena,https://webarena.dev/,https://arxiv.org/abs/2307.13854,"WebArena evaluates autonomous web agents in a realistic, self-hosted set of web applications (e-commerce, CMS, forums, Git workflows, maps), requiring navigation and interaction to complete tasks. It stresses multi-step planning, resilient recovery from UI/interaction errors, and maintaining goals over long trajectories.","Planning, Decision-making, Visual Perception, Visual Attention & Eye Movements, Sensorimotor Coordination, Working Memory, Adaptive Error Correction (minor)"
