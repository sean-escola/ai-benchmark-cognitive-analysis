Benchmark,Website,Paper,Description,Cognitive Functions
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates software engineering agents on real GitHub issues by requiring them to produce code patches that make a project’s tests pass. The “Verified” subset consists of tasks confirmed by human reviewers to be solvable and correctly specified, emphasizing reliable end-to-end debugging and implementation rather than guesswork.","Planning, Adaptive Error Correction, Logical Reasoning, Working Memory, Semantic Understanding & Context Recognition, Language Comprehension (minor)"
SWE-bench Multilingual,https://www.swebench.com/multilingual.html,https://arxiv.org/abs/2504.21798,"SWE-bench Multilingual extends SWE-bench-style repository issue fixing to multiple programming languages, testing whether agents can generalize beyond Python-centric workflows. Tasks still require producing correct patches under test suites, but with added demands from differing language ecosystems, tooling, and conventions.","Cognitive Flexibility, Planning, Adaptive Error Correction, Semantic Understanding & Context Recognition, Working Memory, Language Comprehension (minor)"
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro is a larger, harder software engineering benchmark designed to be more industrially relevant and more robust to contamination than earlier SWE-bench variants. It tests multi-language code changes and often requires deeper repo understanding, multi-step fixes, and careful verification against tests.","Planning, Adaptive Error Correction, Logical Reasoning, Working Memory, Cognitive Flexibility, Semantic Understanding & Context Recognition"
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 evaluates agentic performance in command-line environments, where models must navigate filesystems, run commands, interpret outputs, and complete realistic tasks (e.g., debugging, building, data work). Success depends on iterating based on tool feedback under resource and time constraints, resembling practical “computer use” without a GUI.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Attention (minor), Semantic Understanding & Context Recognition (minor)"
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp measures an agent’s ability to answer difficult questions by using web-style search and reading, typically requiring multi-step information gathering and synthesis. It emphasizes research behavior—query formulation, source selection, cross-checking, and summarization—rather than memorized recall.","Planning, Decision-making, Semantic Understanding & Context Recognition, Language Comprehension, Working Memory, Self-reflection (minor)"
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench evaluates interactive tool-using agents in simulated customer-support domains (e.g., retail, airline, telecom) where the agent must call APIs, follow policies, and resolve multi-turn user requests. It probes consistency, constraint-following, and pragmatic communication under realistic dialogue dynamics.","Social Reasoning & Theory of Mind, Inhibitory Control, Decision-making, Planning, Language Comprehension, Language Production, Empathy (minor)"
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld benchmarks multimodal “computer use” by asking agents to complete tasks on a real operating system via screenshots and UI actions, such as navigating apps and changing settings. It stresses perception-to-action integration, robust interaction with dynamic interfaces, and recovery from mistakes over many steps.","Visual Perception, Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Planning, Decision-making, Sensorimotor Coordination, Adaptive Error Correction (minor)"
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI measures “fluid” abstract reasoning using grid transformation puzzles, where models infer the hidden rule from a few examples and generalize to a new input. It is designed to reduce reliance on language priors and reward pattern induction, compositional reasoning, and systematic generalization.","Logical Reasoning, Cognitive Flexibility, Working Memory, Spatial Representation & Mapping, Visual Perception (minor), Scene Understanding & Visual Reasoning (minor)"
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon autonomy in a simulated business setting, where an agent runs a vending machine operation over an extended time period to maximize final balance. It requires sustained planning, dealing with changing conditions, and using communications and tools to negotiate, stock inventory, and adapt strategy.","Planning, Decision-making, Reward Mechanisms, Cognitive Timing & Predictive Modeling, Working Memory, Episodic Memory (minor)"
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas evaluates real-world tool use through the Model Context Protocol by requiring models to discover, invoke, and chain multiple tools across production-like servers. Tasks typically involve multi-step workflows with error handling, retries, and synthesis of returned data into a final answer.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition, Language Comprehension (minor)"
FinanceAgent,https://www.vals.ai/benchmarks/finance_agent,,"FinanceAgent assesses agent performance on tasks typical of an entry-level financial analyst, such as extracting facts from documents, performing calculations, building analyses, and producing justified recommendations. It emphasizes structured reasoning with domain constraints, careful use of numbers, and clear, decision-oriented reporting.","Logical Reasoning, Semantic Understanding & Context Recognition, Planning, Decision-making, Working Memory, Language Production (minor)"
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity agent capability on large suites of realistic vulnerability tasks, including identifying known weaknesses from descriptions and, in some settings, discovering new vulnerabilities. It stresses code understanding, adversarial thinking, iterative testing, and correct remediation or exploitation reasoning under constraints.","Logical Reasoning, Planning, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition, Inhibitory Control (minor)"
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"SpreadsheetBench evaluates whether models can navigate and manipulate complex spreadsheets to solve realistic tasks (e.g., transformations, formulas, auditing, and reporting). It requires maintaining state across many cells and sheets and executing correct procedural steps while checking intermediate results for consistency.","Planning, Decision-making, Working Memory, Adaptive Error Correction, Attention (minor), Semantic Understanding & Context Recognition (minor)"
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam (HLE) is a challenging, expert-level benchmark spanning many subjects and often involving multimodal inputs, intended to probe frontier knowledge and reasoning near the limits of current models. Questions tend to require synthesis, careful inference, and avoidance of shallow pattern matching.","Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Scene Understanding & Visual Reasoning (minor), Multisensory Integration (minor)"
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 is a competition-math benchmark drawn from the American Invitational Mathematics Examination, featuring problems that require multi-step derivations and precise symbolic manipulation. It is often used to test mathematical reasoning robustness and error-free execution under time-like constraints.","Logical Reasoning, Working Memory, Planning, Cognitive Flexibility (minor), Adaptive Error Correction (minor)"
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a subset of GPQA containing the highest-quality graduate-level science multiple-choice questions, designed to be difficult to answer via simple web search. It emphasizes deep domain understanding and careful elimination among plausible distractors, rewarding coherent scientific reasoning.","Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Decision-making (minor), Inhibitory Control (minor)"
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends MMLU-style academic questions across many non-English languages, measuring multilingual knowledge and reasoning. It stresses consistent understanding of subject matter across linguistic contexts, including translation-robust semantics and culturally varied phrasing.","Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning, Cognitive Flexibility, Working Memory (minor)"
MMMU,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2311.16502,"MMMU is a multidisciplinary multimodal benchmark requiring models to answer expert-level questions grounded in images (e.g., diagrams, charts, figures) plus text. It probes visual reasoning and the integration of visual evidence with domain knowledge across many fields.","Multisensory Integration, Scene Understanding & Visual Reasoning, Visual Perception, Language Comprehension, Logical Reasoning, Working Memory (minor)"
LAB-Bench FigQA,https://huggingface.co/datasets/futurehouse/lab-bench,https://arxiv.org/abs/2407.10362,"LAB-Bench FigQA tests whether models can correctly interpret complex scientific figures from biology papers and answer questions that require reading plots, annotations, and experimental schematics. It emphasizes faithful extraction of visual evidence and reasoning about it in a scientific context rather than generic captioning.","Scene Understanding & Visual Reasoning, Visual Perception, Attention, Semantic Understanding & Context Recognition, Logical Reasoning, Multisensory Integration (minor)"
WebArena,https://webarena.dev/,https://arxiv.org/abs/2307.13854,"WebArena evaluates autonomous web agents on realistic multi-step tasks across several self-hosted web apps (e-commerce, CMS, forums, code hosting, maps), requiring navigation and form-filling. It stresses long-horizon planning, robust UI interaction, and recovering from mistakes while satisfying task constraints.","Planning, Decision-making, Working Memory, Visual Perception, Scene Understanding & Visual Reasoning, Sensorimotor Coordination, Adaptive Error Correction (minor), Inhibitory Control (minor)"
