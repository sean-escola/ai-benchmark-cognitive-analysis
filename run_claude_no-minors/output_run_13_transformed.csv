Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,SWE-bench Verified evaluates models on real GitHub issues by asking them to produce code patches that make failing tests pass. The “Verified” subset consists of tasks validated by humans as solvable and is scored by running the repository’s tests after applying the model’s patch.,"L1: Language Comprehension (minor), Language Production (minor)
L2: Planning, Logical Reasoning, Adaptive Error Correction, Working Memory, Decision-making (minor)
L3: ",L2
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 measures an agent’s ability to complete real tasks inside a command-line environment, typically using shell commands, file edits, and scripting. Success depends on executing multi-step workflows, interpreting tool outputs, and iteratively debugging under realistic constraints.","L1: Language Comprehension (minor)
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory, Attention (minor), Sensorimotor Coordination (minor)
L3: ",L2
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp evaluates “browse-the-web” style research: models must search, read, and synthesize information from online sources to answer questions, often requiring evidence aggregation. It stresses long-horizon information seeking, source selection, and coherent synthesis under time/context limits.","L1: Language Production (minor)
L2: Planning, Attention, Semantic Understanding & Context Recognition, Working Memory, Decision-making
L3: ",L2
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench evaluates interactive customer-support agents in simulated domains (e.g., retail, airline, telecom) that must follow policies while using tools/APIs across multi-turn dialogues. Performance depends on correctly interpreting user intent, applying domain rules, and maintaining consistent state across the interaction.","L1: Language Comprehension, Language Production (minor)
L2: Decision-making, Planning, Working Memory
L3: Social Reasoning & Theory of Mind, Inhibitory Control, Empathy (minor)",L3
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld benchmarks multimodal computer-use agents that operate desktop applications via screenshots and action primitives, completing tasks across websites and OS apps. It emphasizes end-to-end perception–decision–action loops, including navigation, UI understanding, and error recovery over long sequences.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Planning, Decision-making, Working Memory, Adaptive Error Correction (minor), Sensorimotor Coordination (minor)
L3: ",L2
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,ARC-AGI tests few-shot fluid reasoning using grid-based input–output examples where the model must infer a hidden rule and apply it to a new input. It is designed to emphasize novel pattern induction and generalization rather than memorization of known tasks.,"L1: 
L2: Logical Reasoning, Working Memory, Spatial Representation & Mapping, Attention (minor)
L3: Cognitive Flexibility",L3
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon autonomy by having an agent run a simulated vending-machine business over extended time, managing inventory, pricing, supplier negotiation, and cash flow. The score reflects sustained coherence and strategic decision-making across many sequential actions.","L1: 
L2: Planning, Decision-making, Reward Mechanisms, Working Memory, Adaptive Error Correction (minor)
L3: Motivational Drives (minor), Social Reasoning & Theory of Mind (minor)",L2
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas measures real-world tool use via the Model Context Protocol, requiring models to discover tools, call them correctly, handle errors, and synthesize results across multi-step workflows. It targets robust orchestration of external capabilities in production-like tool ecosystems.","L1: Language Comprehension (minor)
L2: Planning, Decision-making, Working Memory, Adaptive Error Correction, Attention (minor)
L3: ",L2
FinanceAgent,https://www.vals.ai/benchmarks/finance_agent,,"FinanceAgent evaluates tasks typical of an entry-level financial analyst, such as analyzing documents/data, building or checking calculations, and producing finance-focused deliverables. It stresses accurate quantitative reasoning, structured reporting, and multi-step workflows that resemble real analyst work.","L1: Language Production (minor)
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Planning, Decision-making, Working Memory, Adaptive Error Correction (minor)
L3: ",L2
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity agents on tasks such as locating known vulnerabilities in real open-source projects and attempting discovery of new issues. It stresses code understanding, hypothesis-driven debugging, and iterative testing against realistic software artifacts.","L1: 
L2: Logical Reasoning, Planning, Adaptive Error Correction, Working Memory, Attention (minor), Decision-making (minor)
L3: ",L2
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"SpreadsheetBench tests whether models can correctly navigate, manipulate, and compute within complex spreadsheets based on realistic problem statements. It emphasizes structured reasoning over tabular data, formula logic, and careful multi-step editing under constraints.","L1: Language Comprehension (minor)
L2: Logical Reasoning, Working Memory, Planning, Adaptive Error Correction, Attention, Spatial Representation & Mapping (minor)
L3: ",L2
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam (HLE) is a difficult, frontier-knowledge benchmark spanning many subjects and often requiring multi-step reasoning; some settings include multimodal questions and/or tool use. It aims to probe breadth and depth of knowledge, reasoning under uncertainty, and synthesis across domains.","L1: Language Comprehension, Language Production (minor)
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Scene Understanding & Visual Reasoning (minor), Planning (minor)
L3: ",L2
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 consists of competition-style math problems that require multi-step derivations rather than rote calculation. It primarily evaluates algebraic and combinatorial reasoning, with performance sensitive to symbolic manipulation accuracy and maintaining intermediate constraints.","L1: 
L2: Logical Reasoning, Working Memory, Planning (minor), Adaptive Error Correction (minor)
L3: Cognitive Flexibility (minor)",L2
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a high-quality subset of graduate-level science multiple-choice questions designed to be “Google-proof” and resistant to superficial pattern matching. It emphasizes deep scientific understanding, careful elimination among close distractors, and multi-step reasoning.","L1: Language Comprehension (minor)
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Decision-making
L3: ",L2
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends the MMLU-style academic knowledge test to many non-English languages, covering broad subjects via multiple-choice questions. It probes multilingual comprehension and whether reasoning/knowledge transfers robustly across languages and scripts.","L1: Language Comprehension, Language Production (minor)
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory (minor)
L3: Cognitive Flexibility (minor)",L2
MMMU,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2311.16502,"MMMU evaluates multimodal understanding by combining images (e.g., charts, diagrams, scenes) with questions across many disciplines. It stresses integrating visual evidence with textual reasoning to select or produce correct answers.","L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Working Memory (minor), Attention (minor)
L3: ",L2
LAB-Bench FigQA,https://huggingface.co/datasets/futurehouse/lab-bench,https://arxiv.org/abs/2407.10362,LAB-Bench FigQA focuses on interpreting complex scientific figures from biology papers and answering questions that require extracting quantitative/structural information. It targets figure literacy: mapping visual encodings to scientific meaning and reasoning over experiment-style plots and diagrams.,"L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Semantic Understanding & Context Recognition, Logical Reasoning, Attention, Working Memory (minor)
L3: ",L2
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning evaluates reasoning over figures, equations, and technical content derived from arXiv-style scientific documents, often requiring multi-step interpretation rather than surface reading. It emphasizes robust extraction of visual/structured cues and translating them into correct scientific answers.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory (minor), Attention (minor)
L3: ",L2
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU extends multimodal reasoning to the video setting, where models must answer questions requiring understanding events, objects, and temporal changes across frames. It stresses temporal integration, tracking, and aligning language questions with dynamic visual evidence.","L1: Visual Perception
L2: Working Memory, Attention, Scene Understanding & Visual Reasoning, Multisensory Integration (minor)
L3: Cognitive Timing & Predictive Modeling",L3
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"FACTS Benchmark Suite systematically evaluates factuality, including hallucination tendencies, grounding/attribution, and consistency across conditions. It targets whether models can reliably distinguish supported vs unsupported claims and manage uncertainty in responses.","L1: Language Production (minor)
L2: Semantic Understanding & Context Recognition, Working Memory (minor)
L3: Inhibitory Control, Self-reflection (minor)",L3
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA evaluates physical commonsense reasoning across many languages, asking models to choose actions or explanations that are physically plausible in everyday situations. It probes whether embodied/physical intuitions and commonsense constraints transfer across linguistic settings.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Spatial Representation & Mapping (minor)
L3: Cognitive Flexibility (minor)",L2
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) measures long-context multi-round coreference resolution by inserting multiple similar “needle” requests into long “haystacks” and asking the model to reproduce the correct referenced response. It stresses robust retrieval under distractors, interference, and very long contexts.","L1: 
L2: Working Memory, Attention, Episodic Memory, Semantic Understanding & Context Recognition
L3: Inhibitory Control (minor)",L2
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval evaluates economically valuable, well-specified professional knowledge-work tasks across many occupations using expert human judging of produced artifacts (e.g., spreadsheets, presentations, plans). It emphasizes end-to-end task execution quality, including structure, correctness, and usefulness under real workplace constraints.","L1: Language Production
L2: Planning, Decision-making, Semantic Understanding & Context Recognition, Adaptive Error Correction (minor)
L3: Social Reasoning & Theory of Mind (minor), Self-reflection (minor)",L2
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks tests a model’s ability to follow and reason over graph-structured information presented in text, answering queries about connectivity, parents/ancestors, or traversal steps. It stresses faithful multi-step traversal, resistance to distractors, and accurate intermediate-state tracking.","L1: 
L2: Logical Reasoning, Working Memory, Spatial Representation & Mapping, Attention, Planning (minor)
L3: ",L2
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon evaluates general-purpose tool-use competence across diverse APIs and multi-step tasks, including selecting tools, issuing correct calls, and integrating results into final answers. It emphasizes robust orchestration, error handling, and maintaining task state across tool interactions.","L1: Language Comprehension (minor)
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory, Attention (minor)
L3: ",L2
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath evaluates models on expert-level mathematics problems designed to be difficult and relatively resistant to memorization, often requiring long chains of reasoning and precise symbolic/quantitative steps. It targets deep problem-solving ability rather than routine computation.","L1: 
L2: Logical Reasoning, Working Memory, Planning (minor), Adaptive Error Correction (minor)
L3: Cognitive Flexibility",L3
