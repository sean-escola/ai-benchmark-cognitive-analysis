Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates software engineering by asking a model/agent to generate patches that resolve real GitHub issues in Python repositories, with correctness checked by running the project’s tests. The “Verified” subset uses tasks validated to be solvable and aims to reduce noisy labels and ambiguous problems compared with earlier SWE-bench variants.","L1: Language Comprehension (minor), Language Production (minor)
L2: Planning, Logical Reasoning, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition
L3: ",L2
SWE-bench Multilingual,https://www.swebench.com/multilingual.html,https://arxiv.org/abs/2504.21798,"SWE-bench Multilingual extends SWE-bench-style issue-to-patch evaluation beyond Python, covering multiple programming languages and ecosystems. It tests whether agents can understand repository context, implement fixes, and satisfy test suites across varied language/tooling conventions.","L1: Language Comprehension (minor), Language Production (minor)
L2: Planning, Logical Reasoning, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition
L3: Cognitive Flexibility",L3
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro is a harder, more industrially oriented software engineering benchmark where models must produce correct code changes for a larger and more diverse set of real-world tasks across multiple languages. It is designed to be more challenging and more resistant to superficial shortcuts, emphasizing robust end-to-end problem solving in repositories.","L1: 
L2: Planning, Logical Reasoning, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition, Decision-making (minor)
L3: Cognitive Flexibility (minor)",L2
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 evaluates autonomous performance on real tasks in a command-line environment (e.g., diagnosing issues, running programs, manipulating files, and producing correct artifacts). It emphasizes iterative action, tool invocation, and recovering from errors under realistic execution constraints.","L1: Language Comprehension (minor), Language Production (minor)
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory
L3: Inhibitory Control (minor)",L2
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp measures deep-research and browsing capability by requiring models/agents to find, integrate, and justify answers using information from the web (or a controlled corpus, depending on the setup). It stresses multi-step information seeking, synthesis across sources, and maintaining coherence over long interactions.","L1: Language Comprehension (minor), Language Production (minor)
L2: Planning, Attention, Episodic Memory, Semantic Understanding & Context Recognition, Logical Reasoning
L3: ",L2
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench evaluates agentic tool use in multi-turn customer-support-like environments where the model must interact with simulated users and APIs while adhering to domain policies. It tests reliable tool calling, policy-following behavior, and resolving tasks over extended dialogues with stateful constraints.","L1: Language Comprehension, Language Production
L2: Decision-making, Planning, Working Memory
L3: Inhibitory Control, Social Reasoning & Theory of Mind, Empathy (minor)",L3
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld evaluates multimodal “computer use” agents that must complete tasks on an operating system by perceiving screens and interacting via actions such as clicking, typing, and navigation. Success depends on interpreting UI state, executing long-horizon action sequences, and correcting mistakes under step limits.","L1: Visual Perception, Language Comprehension (minor)
L2: Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Spatial Representation & Mapping, Planning, Decision-making, Adaptive Error Correction, Working Memory, Sensorimotor Coordination (minor)
L3: ",L2
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI is a few-shot abstract reasoning benchmark where systems infer hidden rules from small sets of grid-based input–output examples and must apply them to new inputs. It targets “fluid” generalization to novel patterns rather than knowledge recall, with strong penalties for brittle heuristics.","L1: Visual Perception
L2: Logical Reasoning, Working Memory, Spatial Representation & Mapping, Scene Understanding & Visual Reasoning, Planning (minor), Attention (minor)
L3: Cognitive Flexibility",L3
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon agent coherence and strategy in a simulated vending-machine business over an extended period, with performance measured by final financial outcomes. Agents must manage inventory, negotiate, adapt to market dynamics, and sustain consistent goals across many steps.","L1: 
L2: Planning, Decision-making, Reward Mechanisms, Working Memory, Episodic Memory (minor), Adaptive Error Correction (minor)
L3: Motivational Drives (minor)",L2
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas measures real-world tool use via the Model Context Protocol (MCP), requiring models to discover appropriate tools, invoke them correctly, and compose multi-step workflows across external services. It emphasizes reliable API/tool interaction, error handling, and synthesizing results into correct outputs.","L1: Language Comprehension (minor), Language Production (minor)
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory
L3: Inhibitory Control (minor)",L2
FinanceAgent,https://www.vals.ai/benchmarks/finance_agent,,"FinanceAgent evaluates agent performance on tasks typical of an entry-level financial analyst, such as retrieving relevant information, performing calculations, and producing structured, decision-relevant outputs. The benchmark stresses applied quantitative reasoning, domain constraints, and multi-step analysis rather than simple fact recall.","L1: Language Production (minor)
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Planning, Decision-making (minor)
L3: ",L2
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity agents on real vulnerability tasks, including identifying known vulnerabilities from descriptions and discovering new issues in open-source codebases. It tests iterative technical investigation, hypothesis-driven debugging, and precise patch or exploit-relevant reasoning under realistic constraints.","L1: 
L2: Logical Reasoning, Planning, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition, Decision-making (minor)
L3: Inhibitory Control (minor)",L2
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"SpreadsheetBench evaluates the ability to understand, edit, and compute over complex spreadsheets derived from realistic tasks, often requiring multi-step transformations and correct formula or data operations. It stresses structured manipulation, consistency across dependencies, and producing verifiably correct final files.","L1: Language Comprehension (minor)
L2: Planning, Decision-making, Working Memory, Adaptive Error Correction, Logical Reasoning (minor), Scene Understanding & Visual Reasoning (minor)
L3: ",L2
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a difficult multimodal benchmark intended to probe frontier-level knowledge and reasoning across many domains using challenging questions (often requiring deep inference rather than memorization). Depending on evaluation setup, models may be tested with or without tools like web search or code execution, highlighting both core reasoning and agentic augmentation.","L1: Language Comprehension, Language Production (minor), Visual Perception (minor)
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Scene Understanding & Visual Reasoning (minor), Planning (minor)
L3: ",L2
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 consists of competition-style math problems that require multi-step symbolic and quantitative reasoning, typically without reliance on external tools. It is widely used to measure mathematical problem solving, compositional reasoning, and robustness under exact-answer scoring.","L1: Language Comprehension (minor)
L2: Logical Reasoning, Working Memory, Planning (minor), Adaptive Error Correction (minor)
L3: ",L2
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a high-quality subset of GPQA composed of very challenging, “Google-proof” graduate-level science multiple-choice questions. It aims to measure deep scientific reasoning and understanding, with questions selected so that non-experts typically fail while experts succeed.","L1: Language Comprehension (minor)
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Decision-making (minor)
L3: ",L2
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends MMLU-style evaluation to multiple languages, testing broad academic knowledge and reasoning across many subjects with standardized multiple-choice questions. It probes cross-lingual generalization and whether capabilities transfer beyond English without task-specific fine-tuning.","L1: Language Comprehension, Language Production (minor)
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory (minor)
L3: Cognitive Flexibility",L3
MMMU,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2311.16502,"MMMU is a multidisciplinary multimodal benchmark where models answer questions requiring joint understanding of text and images, including diagrams, charts, and figures across many domains. It emphasizes integrating visual evidence with textual context to support correct reasoning.","L1: Visual Perception, Language Comprehension
L2: Multisensory Integration, Scene Understanding & Visual Reasoning, Logical Reasoning, Attention (minor), Working Memory (minor)
L3: ",L2
LAB-Bench FigQA,https://huggingface.co/datasets/futurehouse/lab-bench,https://arxiv.org/abs/2407.10362,"LAB-Bench FigQA evaluates whether models can correctly interpret complex scientific figures from biology papers and answer figure-grounded questions. It stresses precise extraction of information from plots/diagrams, linking visual elements to domain concepts, and avoiding hallucinated figure content.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Multisensory Integration, Semantic Understanding & Context Recognition, Logical Reasoning, Attention (minor), Working Memory (minor)
L3: ",L2
WebArena,https://webarena.dev/,https://arxiv.org/abs/2307.13854,"WebArena evaluates autonomous web agents on realistic multi-step tasks across several web apps (e-commerce, CMS, forums, collaboration), requiring navigation, form filling, and state tracking. It probes end-to-end agent competence: perceiving UI state, planning action sequences, and recovering from mistakes while meeting task objectives.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Planning, Decision-making, Adaptive Error Correction, Working Memory, Sensorimotor Coordination (minor)
L3: Inhibitory Control (minor)",L2
