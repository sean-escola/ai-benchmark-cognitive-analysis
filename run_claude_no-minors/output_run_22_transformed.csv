Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates software engineering agents on real GitHub issues where a model must produce a patch that makes repository tests pass. The “Verified” subset uses problems curated to be solvable and reliably graded, emphasizing end-to-end debugging, code modification, and regression avoidance.","L1: Language Comprehension (minor)
L2: Planning, Decision-making, Logical Reasoning, Adaptive Error Correction, Working Memory
L3: ",L2
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 measures an agent’s ability to complete real tasks in a command-line environment (e.g., navigating files, running programs, installing dependencies, and debugging failures). Success typically requires iterative experimentation, interpreting tool output, and choosing corrective actions under constraints.","L1: Language Comprehension (minor)
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory
L3: ",L2
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp evaluates “deep research” agents that must answer questions by searching and reading documents rather than relying only on parametric memory. It stresses decomposition of an information need into search steps, evidence gathering, and synthesizing a justified final answer.","L1: Language Comprehension (minor)
L2: Planning, Semantic Understanding & Context Recognition, Episodic Memory, Working Memory, Decision-making
L3: ",L2
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench evaluates interactive agent behavior in customer-support-style domains (e.g., retail, airline, telecom) where the agent must follow policies while using tools/APIs over multiple turns. It emphasizes consistency, policy adherence, and robust resolution of user needs in a simulated environment.","L1: Language Comprehension, Language Production
L2: Decision-making, Planning (minor)
L3: Social Reasoning & Theory of Mind, Inhibitory Control",L3
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld benchmarks multimodal computer-use agents that operate a desktop-like environment to complete tasks across applications and websites. It requires interpreting screenshots/UI state, selecting actions (clicks/typing), and recovering from mistakes over long action sequences.","L1: Visual Perception
L2: Visual Attention & Eye Movements, Sensorimotor Coordination, Planning, Decision-making, Adaptive Error Correction (minor)
L3: ",L2
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,ARC-AGI is a “fluid intelligence” benchmark based on abstract grid puzzles where models infer hidden transformation rules from a few examples. It is designed to reward generalization to novel patterns and compositional reasoning rather than memorization of domain facts.,"L1: 
L2: Logical Reasoning, Working Memory, Spatial Representation & Mapping, Attention (minor)
L3: Cognitive Flexibility",L3
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon agent coherence by placing the model in a simulated vending-machine business it must run over an extended period. It must make many sequential decisions (inventory, pricing, negotiation, budgeting) where early choices affect later outcomes.","L1: 
L2: Planning, Decision-making, Reward Mechanisms, Working Memory, Episodic Memory (minor), Adaptive Error Correction (minor)
L3: ",L2
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas assesses real-world tool use through the Model Context Protocol, requiring models to discover, call, and chain tools across multi-step workflows. It emphasizes correct API invocation, error handling, and synthesis of tool outputs into task-complete responses.","L1: Language Comprehension (minor)
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory
L3: Inhibitory Control (minor)",L2
FinanceAgent,https://www.vals.ai/benchmarks/finance_agent,,"FinanceAgent evaluates agent performance on tasks resembling entry-level financial analyst work, often involving multi-step analysis, quantitative reasoning, and producing structured deliverables. It probes whether models can apply domain concepts reliably under realistic task framing.","L1: Language Production (minor)
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Planning, Decision-making, Working Memory
L3: ",L2
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity agents on identifying and exploiting (or locating) vulnerabilities in real open-source codebases, including both known and previously unknown issues. It stresses code understanding, hypothesis-driven investigation, and careful, iterative debugging-like workflows.","L1: 
L2: Logical Reasoning, Planning, Adaptive Error Correction, Working Memory
L3: Inhibitory Control (minor)",L2
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"SpreadsheetBench measures an agent’s ability to read, edit, and compute with spreadsheets to solve realistic tasks (e.g., transformations, formulas, analysis, formatting). It captures structured problem solving where small cell-level errors can cascade into incorrect outputs.","L1: 
L2: Planning, Decision-making, Working Memory, Adaptive Error Correction, Semantic Understanding & Context Recognition (minor)
L3: ",L2
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a difficult multimodal benchmark aimed at frontier-level academic and professional questions, spanning text and image inputs. It targets broad knowledge plus rigorous reasoning, often requiring careful interpretation of problem statements and evidence.","L1: Language Comprehension, Visual Perception (minor)
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory (minor)
L3: ",L2
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 uses problems from the American Invitational Mathematics Examination, a competition-style math set with concise numeric answers. It stresses multi-step symbolic reasoning and error-free manipulation under tight problem specifications.","L1: 
L2: Logical Reasoning, Working Memory, Planning (minor)
L3: Cognitive Flexibility (minor)",L2
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a curated subset of very challenging, “Google-proof” graduate-level science multiple-choice questions. It emphasizes deep understanding and reasoning over physics/chemistry/biology concepts rather than surface pattern matching.","L1: Language Comprehension (minor)
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory (minor)
L3: ",L2
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends broad subject testing to multiple languages, evaluating knowledge and reasoning across many domains in non-English settings. It probes whether capability generalizes across linguistic contexts rather than being confined to English-only priors.","L1: Language Comprehension, Language Production (minor)
L2: Semantic Understanding & Context Recognition, Working Memory (minor)
L3: ",L2
MMMU,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2311.16502,"MMMU is a multidisciplinary multimodal benchmark requiring models to answer questions that depend on both images (diagrams, charts, figures) and text. It tests integrated visual understanding, cross-modal grounding, and domain reasoning across many expert subjects.","L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Attention (minor)
L3: ",L2
LAB-Bench FigQA,https://huggingface.co/datasets/futurehouse/lab-bench,https://arxiv.org/abs/2407.10362,LAB-Bench FigQA evaluates whether models can correctly interpret complex scientific figures from biology papers and answer questions about them. It focuses on extracting quantitative/structural information from visuals and applying domain reasoning to draw correct conclusions.,"L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Attention, Logical Reasoning, Semantic Understanding & Context Recognition (minor)
L3: ",L2
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning evaluates reasoning over scientific figures/charts associated with research-paper contexts, often benefiting from quantitative interpretation and stepwise analysis. It targets robust chart reading, mapping visual elements to claims, and avoiding superficial caption-based guessing.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Logical Reasoning, Attention (minor), Semantic Understanding & Context Recognition (minor)
L3: ",L2
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU extends multimodal understanding to the temporal domain by asking questions that require comprehension of video content. It stresses maintaining and integrating information across frames, tracking events, and aligning narrative/visual cues.","L1: Visual Perception
L2: Working Memory, Attention, Scene Understanding & Visual Reasoning, Multisensory Integration (minor)
L3: Cognitive Timing & Predictive Modeling (minor)",L2
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"FACTS Benchmark Suite systematically evaluates LLM factuality, including whether generated statements are supported by sources or remain consistent with known facts under varied prompting. It targets hallucination resistance and faithful grounding rather than task completion alone.","L1: Language Production
L2: Semantic Understanding & Context Recognition, Working Memory (minor)
L3: Inhibitory Control, Self-reflection (minor)",L3
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA evaluates physical commonsense reasoning across many languages, emphasizing whether models can pick the most plausible action/solution in everyday scenarios. It probes transfer of grounded commonsense beyond English and beyond purely encyclopedic knowledge.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Spatial Representation & Mapping (minor)
L3: Social Reasoning & Theory of Mind (minor)",L2
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) is a long-context, multi-round coreference/retrieval benchmark where multiple similar “needle” interactions are embedded in a long “haystack,” and the model must reproduce the correct referenced response. It tests whether models can maintain and resolve entities and instructions across long documents and conversational turns.","L1: Language Comprehension
L2: Working Memory, Attention, Episodic Memory, Semantic Understanding & Context Recognition (minor)
L3: ",L2
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval evaluates economically valuable, well-specified professional knowledge work across many occupations using human judging (often via pairwise comparisons). Tasks commonly require producing real artifacts (e.g., presentations/spreadsheets/plans) with correct structure, constraints, and rationale.","L1: Language Production
L2: Planning, Decision-making, Semantic Understanding & Context Recognition, Adaptive Error Correction (minor)
L3: Social Reasoning & Theory of Mind (minor)",L2
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks tests reasoning over graph-structured data by asking models to follow paths, retrieve nodes, or answer queries that require consistent multi-step traversal. It stresses systematic state tracking and resisting distractors that look locally plausible but are globally inconsistent.","L1: 
L2: Logical Reasoning, Working Memory, Spatial Representation & Mapping, Attention (minor)
L3: Cognitive Flexibility (minor)",L2
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon evaluates multi-tool competence across diverse APIs and tasks, measuring whether models can select tools, sequence calls, and integrate results into a correct final output. It emphasizes robustness to tool errors, schema mismatches, and multi-step workflows that require bookkeeping.","L1: Language Comprehension (minor)
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory
L3: Inhibitory Control (minor)",L2
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath is an expert-level mathematics benchmark designed to be difficult for current systems and more resistant to memorization, emphasizing novel derivations and proof-like reasoning. It targets deep multi-step problem solving where small logical gaps typically lead to failure.","L1: 
L2: Logical Reasoning, Working Memory, Planning (minor)
L3: Cognitive Flexibility (minor)",L2
