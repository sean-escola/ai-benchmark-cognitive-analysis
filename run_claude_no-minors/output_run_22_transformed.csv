Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates software engineering agents on real GitHub issues by requiring them to produce code patches that make a project’s tests pass. The “Verified” subset consists of tasks confirmed by human reviewers to be solvable and correctly specified, emphasizing reliable end-to-end debugging and implementation rather than guesswork.","L1: Language Comprehension (minor)
L2: Planning, Adaptive Error Correction, Logical Reasoning, Working Memory, Semantic Understanding & Context Recognition
L3: ",L2
SWE-bench Multilingual,https://www.swebench.com/multilingual.html,https://arxiv.org/abs/2504.21798,"SWE-bench Multilingual extends SWE-bench-style repository issue fixing to multiple programming languages, testing whether agents can generalize beyond Python-centric workflows. Tasks still require producing correct patches under test suites, but with added demands from differing language ecosystems, tooling, and conventions.","L1: Language Comprehension (minor)
L2: Planning, Adaptive Error Correction, Semantic Understanding & Context Recognition, Working Memory
L3: Cognitive Flexibility",L3
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro is a larger, harder software engineering benchmark designed to be more industrially relevant and more robust to contamination than earlier SWE-bench variants. It tests multi-language code changes and often requires deeper repo understanding, multi-step fixes, and careful verification against tests.","L1: 
L2: Planning, Adaptive Error Correction, Logical Reasoning, Working Memory, Semantic Understanding & Context Recognition
L3: Cognitive Flexibility",L3
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 evaluates agentic performance in command-line environments, where models must navigate filesystems, run commands, interpret outputs, and complete realistic tasks (e.g., debugging, building, data work). Success depends on iterating based on tool feedback under resource and time constraints, resembling practical “computer use” without a GUI.","L1: 
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory, Attention (minor), Semantic Understanding & Context Recognition (minor)
L3: ",L2
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp measures an agent’s ability to answer difficult questions by using web-style search and reading, typically requiring multi-step information gathering and synthesis. It emphasizes research behavior—query formulation, source selection, cross-checking, and summarization—rather than memorized recall.","L1: Language Comprehension
L2: Planning, Decision-making, Semantic Understanding & Context Recognition, Working Memory
L3: Self-reflection (minor)",L2
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench evaluates interactive tool-using agents in simulated customer-support domains (e.g., retail, airline, telecom) where the agent must call APIs, follow policies, and resolve multi-turn user requests. It probes consistency, constraint-following, and pragmatic communication under realistic dialogue dynamics.","L1: Language Comprehension, Language Production
L2: Decision-making, Planning
L3: Social Reasoning & Theory of Mind, Inhibitory Control, Empathy (minor)",L3
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld benchmarks multimodal “computer use” by asking agents to complete tasks on a real operating system via screenshots and UI actions, such as navigating apps and changing settings. It stresses perception-to-action integration, robust interaction with dynamic interfaces, and recovery from mistakes over many steps.","L1: Visual Perception
L2: Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Planning, Decision-making, Sensorimotor Coordination, Adaptive Error Correction (minor)
L3: ",L2
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI measures “fluid” abstract reasoning using grid transformation puzzles, where models infer the hidden rule from a few examples and generalize to a new input. It is designed to reduce reliance on language priors and reward pattern induction, compositional reasoning, and systematic generalization.","L1: Visual Perception (minor)
L2: Logical Reasoning, Working Memory, Spatial Representation & Mapping, Scene Understanding & Visual Reasoning (minor)
L3: Cognitive Flexibility",L3
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon autonomy in a simulated business setting, where an agent runs a vending machine operation over an extended time period to maximize final balance. It requires sustained planning, dealing with changing conditions, and using communications and tools to negotiate, stock inventory, and adapt strategy.","L1: 
L2: Planning, Decision-making, Reward Mechanisms, Working Memory, Episodic Memory (minor)
L3: Cognitive Timing & Predictive Modeling",L3
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas evaluates real-world tool use through the Model Context Protocol by requiring models to discover, invoke, and chain multiple tools across production-like servers. Tasks typically involve multi-step workflows with error handling, retries, and synthesis of returned data into a final answer.","L1: Language Comprehension (minor)
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition
L3: ",L2
FinanceAgent,https://www.vals.ai/benchmarks/finance_agent,,"FinanceAgent assesses agent performance on tasks typical of an entry-level financial analyst, such as extracting facts from documents, performing calculations, building analyses, and producing justified recommendations. It emphasizes structured reasoning with domain constraints, careful use of numbers, and clear, decision-oriented reporting.","L1: Language Production (minor)
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Planning, Decision-making, Working Memory
L3: ",L2
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity agent capability on large suites of realistic vulnerability tasks, including identifying known weaknesses from descriptions and, in some settings, discovering new vulnerabilities. It stresses code understanding, adversarial thinking, iterative testing, and correct remediation or exploitation reasoning under constraints.","L1: 
L2: Logical Reasoning, Planning, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition
L3: Inhibitory Control (minor)",L2
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"SpreadsheetBench evaluates whether models can navigate and manipulate complex spreadsheets to solve realistic tasks (e.g., transformations, formulas, auditing, and reporting). It requires maintaining state across many cells and sheets and executing correct procedural steps while checking intermediate results for consistency.","L1: 
L2: Planning, Decision-making, Working Memory, Adaptive Error Correction, Attention (minor), Semantic Understanding & Context Recognition (minor)
L3: ",L2
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam (HLE) is a challenging, expert-level benchmark spanning many subjects and often involving multimodal inputs, intended to probe frontier knowledge and reasoning near the limits of current models. Questions tend to require synthesis, careful inference, and avoidance of shallow pattern matching.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Scene Understanding & Visual Reasoning (minor), Multisensory Integration (minor)
L3: ",L2
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 is a competition-math benchmark drawn from the American Invitational Mathematics Examination, featuring problems that require multi-step derivations and precise symbolic manipulation. It is often used to test mathematical reasoning robustness and error-free execution under time-like constraints.","L1: 
L2: Logical Reasoning, Working Memory, Planning, Adaptive Error Correction (minor)
L3: Cognitive Flexibility (minor)",L2
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a subset of GPQA containing the highest-quality graduate-level science multiple-choice questions, designed to be difficult to answer via simple web search. It emphasizes deep domain understanding and careful elimination among plausible distractors, rewarding coherent scientific reasoning.","L1: 
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Decision-making (minor)
L3: Inhibitory Control (minor)",L2
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends MMLU-style academic questions across many non-English languages, measuring multilingual knowledge and reasoning. It stresses consistent understanding of subject matter across linguistic contexts, including translation-robust semantics and culturally varied phrasing.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory (minor)
L3: Cognitive Flexibility",L3
MMMU,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2311.16502,"MMMU is a multidisciplinary multimodal benchmark requiring models to answer expert-level questions grounded in images (e.g., diagrams, charts, figures) plus text. It probes visual reasoning and the integration of visual evidence with domain knowledge across many fields.","L1: Visual Perception, Language Comprehension
L2: Multisensory Integration, Scene Understanding & Visual Reasoning, Logical Reasoning, Working Memory (minor)
L3: ",L2
LAB-Bench FigQA,https://huggingface.co/datasets/futurehouse/lab-bench,https://arxiv.org/abs/2407.10362,"LAB-Bench FigQA tests whether models can correctly interpret complex scientific figures from biology papers and answer questions that require reading plots, annotations, and experimental schematics. It emphasizes faithful extraction of visual evidence and reasoning about it in a scientific context rather than generic captioning.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Attention, Semantic Understanding & Context Recognition, Logical Reasoning, Multisensory Integration (minor)
L3: ",L2
WebArena,https://webarena.dev/,https://arxiv.org/abs/2307.13854,"WebArena evaluates autonomous web agents on realistic multi-step tasks across several self-hosted web apps (e-commerce, CMS, forums, code hosting, maps), requiring navigation and form-filling. It stresses long-horizon planning, robust UI interaction, and recovering from mistakes while satisfying task constraints.","L1: Visual Perception
L2: Planning, Decision-making, Working Memory, Scene Understanding & Visual Reasoning, Sensorimotor Coordination, Adaptive Error Correction (minor)
L3: Inhibitory Control (minor)",L2
