Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates software engineering agents on real GitHub issues by asking them to generate patches that make a project’s tests pass. The Verified subset focuses on tasks that have been curated/validated to be solvable under the benchmark’s setup, emphasizing end-to-end debugging and code changes rather than isolated coding puzzles.","L1: Language Comprehension (minor), Language Production (minor)
L2: Planning, Logical Reasoning, Working Memory, Adaptive Error Correction
L3: ",L2
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 measures an agent’s ability to complete real command-line tasks in a sandboxed environment (e.g., inspecting files, running programs, configuring tools, debugging failures). Success depends on executing correct sequences of terminal actions under resource and time constraints and recovering from errors during execution.","L1: Language Comprehension (minor)
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory
L3: ",L2
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp evaluates deep research and browsing agents by requiring them to find, verify, and synthesize answers using external documents rather than relying only on parametric memory. The tasks stress robust information-seeking behavior, evidence integration across sources, and producing grounded final responses.","L1: Language Production (minor)
L2: Planning, Attention, Semantic Understanding & Context Recognition, Working Memory, Decision-making (minor)
L3: ",L2
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench evaluates tool-using customer-support-style agents interacting over multiple turns with simulated users and APIs in domains like retail, airline, and telecom. It emphasizes policy adherence, consistent multi-step tool use, and maintaining coherent dialogue state across long interactions.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Working Memory
L3: Social Reasoning & Theory of Mind, Inhibitory Control, Empathy (minor)",L3
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld benchmarks multimodal computer-use agents that must operate a desktop-like environment to accomplish tasks across applications (e.g., web, documents, settings). It tests whether models can perceive UI state from screenshots, choose correct actions, and execute long-horizon workflows reliably.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Sensorimotor Coordination, Planning, Decision-making, Working Memory (minor)
L3: ",L2
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,ARC-AGI is a few-shot abstract reasoning benchmark based on grid transformation tasks where a model must infer hidden rules from a handful of examples. It is intended to emphasize novel pattern induction and generalization rather than memorized domain knowledge.,"L1: 
L2: Logical Reasoning, Working Memory, Spatial Representation & Mapping, Attention (minor)
L3: Cognitive Flexibility",L3
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon agent coherence by having an agent run a simulated vending-machine business over extended time (e.g., months of operations). The agent must manage inventory, pricing, supplier interactions, and budgeting to maximize final profit under changing conditions.","L1: 
L2: Planning, Decision-making, Working Memory, Reward Mechanisms, Adaptive Error Correction (minor)
L3: Motivational Drives (minor)",L2
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas evaluates real-world tool use via the Model Context Protocol by measuring whether models can discover tools, call them correctly, and compose multi-step workflows across heterogeneous servers. It stresses robustness to tool errors, argument formatting, and correct synthesis of tool outputs into final answers.","L1: Language Production (minor)
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory
L3: ",L2
FinanceAgent,https://www.vals.ai/benchmarks/finance_agent,,"FinanceAgent measures performance on tasks representative of an entry-level financial analyst, such as interpreting financial documents, performing calculations, and producing structured deliverables. It emphasizes accurate quantitative reasoning, domain-specific reading comprehension, and workflow-style problem decomposition.","L1: Language Production (minor)
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Planning, Decision-making, Working Memory
L3: ",L2
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity agent capabilities on realistic vulnerability-finding and exploitation-related tasks at scale, including identifying known weaknesses and discovering new ones. It emphasizes systematic investigation, careful interpretation of code and logs, and iterative debugging of hypotheses and exploits.","L1: 
L2: Logical Reasoning, Planning, Adaptive Error Correction, Working Memory
L3: Inhibitory Control (minor)",L2
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"SpreadsheetBench evaluates an agent’s ability to inspect, edit, and compute over complex spreadsheets resembling real workplace artifacts. Tasks often require multi-step transformations, formula reasoning, and producing correct structured outputs while preserving spreadsheet integrity.","L1: 
L2: Logical Reasoning, Working Memory, Planning, Adaptive Error Correction, Semantic Understanding & Context Recognition (minor)
L3: ",L2
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a challenging multimodal benchmark spanning frontier academic knowledge and reasoning across many domains, with questions designed to be difficult for models without strong reasoning and/or tool use. It evaluates whether systems can integrate provided context (and optionally external tools, depending on setup) to produce correct, well-grounded answers.","L1: Language Comprehension, Visual Perception (minor)
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Scene Understanding & Visual Reasoning (minor)
L3: ",L2
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 consists of competition-style math problems that require multi-step derivations, algebraic manipulation, and careful logical inference under tight answer formats. It is often used to stress-test mathematical reasoning beyond routine arithmetic or short word problems.","L1: 
L2: Logical Reasoning, Working Memory, Attention (minor), Planning (minor)
L3: ",L2
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a high-quality subset of GPQA comprising difficult graduate-level multiple-choice questions in physics, chemistry, and biology that are designed to be hard to “look up” and to require genuine reasoning. It evaluates deep scientific knowledge, careful reading, and elimination-style reasoning.","L1: Language Comprehension (minor)
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory (minor)
L3: ",L2
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends MMLU-style subject testing to multiple languages, probing broad academic knowledge and reasoning across many disciplines and languages. It is commonly used to assess multilingual generalization and whether knowledge and reasoning transfer beyond English.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning (minor), Working Memory (minor)
L3: ",L2
MMMU,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2311.16502,"MMMU is a multimodal benchmark spanning many subjects where questions combine text with images (e.g., diagrams, charts, scenes) and require expert-style reasoning. It tests whether models can fuse visual evidence with textual context to answer domain questions accurately.","L1: Visual Perception, Language Comprehension
L2: Multisensory Integration, Scene Understanding & Visual Reasoning, Logical Reasoning (minor)
L3: ",L2
LAB-Bench FigQA,https://huggingface.co/datasets/futurehouse/lab-bench,https://arxiv.org/abs/2407.10362,"LAB-Bench FigQA evaluates whether models can interpret complex scientific figures (commonly from biology papers) and answer questions that depend on visual details and scientific context. It stresses extracting relevant visual evidence (plots, labels, panels) and using it for correct scientific inference.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Semantic Understanding & Context Recognition, Logical Reasoning, Attention (minor)
L3: ",L2
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning tests reasoning over figures and content from scientific papers, often requiring chart/diagram understanding and quantitative or logical inference grounded in the visual. It emphasizes robust interpretation of scientific visuals and producing answers consistent with plotted data and captions.","L1: Visual Perception, Language Comprehension (minor)
L2: Scene Understanding & Visual Reasoning, Logical Reasoning, Working Memory (minor)
L3: ",L2
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU extends multimodal understanding to the video setting, requiring models to integrate information across multiple frames and align it with text questions. It evaluates temporal integration, event understanding, and maintaining coherence over visually evolving contexts.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Multisensory Integration, Working Memory, Attention (minor)
L3: ",L2
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"FACTS Benchmark Suite systematically evaluates factuality-related behaviors, such as whether a model’s responses remain consistent with evidence and avoid unsupported claims under varied prompting conditions. It is designed to probe reliability dimensions including hallucination-like errors and faithfulness to sources or stated constraints.","L1: Language Production (minor)
L2: Semantic Understanding & Context Recognition, Working Memory (minor)
L3: Inhibitory Control, Self-reflection (minor)",L3
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA is a multilingual/non-parallel variant of physical commonsense question answering, where models must choose plausible actions or explanations grounded in everyday physics. It is used to test whether physical intuition and commonsense reasoning hold across languages and cultural contexts.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning
L3: Social Reasoning & Theory of Mind (minor)",L2
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,MRCR v2 evaluates long-context multi-round co-reference resolution by inserting multiple similar “needle” requests into long “haystacks” and asking the model to retrieve the correct referenced response. The 8-needle setting stresses robustness under many distractors and long-range dependency tracking.,"L1: Language Comprehension (minor)
L2: Working Memory, Attention, Episodic Memory
L3: ",L2
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval measures performance on well-specified professional knowledge-work tasks spanning many occupations, where models must produce realistic work products (e.g., presentations, spreadsheets, schedules) that are judged against human professional outputs. It emphasizes end-to-end execution quality, instruction following, and producing usable artifacts under practical constraints.","L1: Language Production
L2: Planning, Decision-making, Semantic Understanding & Context Recognition, Adaptive Error Correction (minor)
L3: Social Reasoning & Theory of Mind (minor)",L2
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks evaluates reasoning over graph-structured problems presented in text, where a model must traverse or infer relationships (e.g., reachability, parent pointers, BFS-like reasoning) across many nodes. It targets systematic multi-step state tracking rather than surface-level pattern matching.","L1: 
L2: Spatial Representation & Mapping, Working Memory, Logical Reasoning, Attention (minor)
L3: ",L2
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon evaluates general tool-use competence across diverse APIs and task formats, emphasizing correct tool selection, argument construction, and multi-step composition. It is intended to test whether models can reliably act as agents that execute workflows, handle tool errors, and synthesize outputs into final answers.","L1: Language Production (minor)
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory
L3: ",L2
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath is an advanced mathematics benchmark targeting problems that require expert-level reasoning, novel derivations, and careful multi-step proofs or computations. It is designed to reduce gains from memorization and to stress deep, compositional mathematical problem solving.","L1: 
L2: Logical Reasoning, Working Memory, Planning (minor), Attention (minor)
L3: ",L2
