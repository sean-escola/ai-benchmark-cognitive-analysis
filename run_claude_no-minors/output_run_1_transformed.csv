Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates an agent’s ability to solve real-world GitHub issues by generating code patches that make failing tests pass. The “Verified” subset contains tasks that have been manually checked to be solvable and to have reliable evaluation via unit tests, emphasizing end-to-end bug fixing in realistic repositories.","L1: Language Comprehension (minor), Language Production (minor)
L2: Planning, Logical Reasoning, Adaptive Error Correction, Working Memory
L3: ",L2
SWE-bench Multilingual,https://www.swebench.com/multilingual.html,https://arxiv.org/abs/2504.21798,"SWE-bench Multilingual extends the SWE-bench paradigm to multiple programming languages, requiring models to understand diverse codebases, toolchains, and ecosystem conventions. It measures whether software-engineering competence transfers beyond Python while maintaining realistic repo-level constraints and test-based verification.","L1: 
L2: Planning, Logical Reasoning, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition (minor)
L3: Cognitive Flexibility",L3
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro is a larger, harder software engineering benchmark designed to be more industrially representative and more resistant to shortcutting via memorization. It evaluates end-to-end repo modification across multiple languages and more complex issue settings, stressing robustness under realistic constraints.","L1: 
L2: Planning, Decision-making, Logical Reasoning, Adaptive Error Correction, Working Memory
L3: Cognitive Flexibility (minor)",L2
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 measures how well agentic systems accomplish practical tasks in a command-line environment (e.g., debugging, environment setup, data processing) using shell commands and program outputs. It emphasizes iterative trial-and-error, tool execution, and recovery from mistakes under real system constraints.","L1: 
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition
L3: Inhibitory Control (minor)",L2
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp evaluates “deep research” agents that must search and read web content to answer challenging questions, typically requiring multi-step retrieval, source comparison, and synthesis. It probes whether an agent can manage context over long browsing trajectories and avoid brittle or hallucinated conclusions when evidence is distributed across documents.","L1: 
L2: Planning, Decision-making, Semantic Understanding & Context Recognition, Episodic Memory (minor), Working Memory
L3: Self-reflection (minor)",L2
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench evaluates interactive customer-support agents in simulated domains (e.g., retail, airline, telecom) that must use APIs/tools while following policies and maintaining a coherent multi-turn dialogue. It tests reliable tool use, policy adherence, and pragmatic communication under user pressure and ambiguous constraints.","L1: Language Comprehension
L2: Decision-making, Planning
L3: Social Reasoning & Theory of Mind, Inhibitory Control, Empathy (minor)",L3
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld evaluates multimodal computer-use agents that operate within a real or realistic operating-system desktop, perceiving screenshots and executing mouse/keyboard actions to complete tasks. It stresses long-horizon GUI navigation, error recovery, and grounding actions in visual state changes across many applications.","L1: Visual Perception
L2: Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Planning, Decision-making, Sensorimotor Coordination, Working Memory (minor), Spatial Representation & Mapping (minor)
L3: ",L2
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,ARC-AGI tests fluid reasoning on abstract grid-based puzzles where a system must infer transformation rules from a few input-output examples and apply them to a new input. The benchmark is designed to emphasize novel pattern induction and compositional generalization rather than rote knowledge.,"L1: Visual Perception
L2: Logical Reasoning, Spatial Representation & Mapping, Working Memory (minor)
L3: Cognitive Flexibility",L3
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 measures long-term coherence and strategy in a simulated “run a vending machine business” setting over many decision steps. Agents must plan purchases, pricing, inventory, and communications while adapting to changing conditions to maximize end-of-horizon profit.","L1: 
L2: Planning, Decision-making, Reward Mechanisms, Working Memory, Episodic Memory (minor)
L3: Self-reflection (minor)",L2
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas evaluates real-world tool use via the Model Context Protocol (MCP), requiring models to discover tools, call them correctly across multiple steps, handle failures, and integrate results into accurate final outputs. It focuses on reliability of structured actions in production-like API environments rather than pure text generation.","L1: Language Comprehension (minor)
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition
L3: ",L2
FinanceAgent,https://www.vals.ai/benchmarks/finance_agent,,"FinanceAgent assesses performance on tasks typical of an entry-level financial analyst, such as interpreting financial documents, performing calculations, constructing analyses, and producing decision-ready summaries. It emphasizes grounded quantitative reasoning and multi-step workflows that mirror professional finance deliverables.","L1: Language Production (minor)
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Planning, Decision-making, Working Memory
L3: ",L2
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity agent capabilities on real-world vulnerability tasks, including identifying known weaknesses from descriptions and discovering new vulnerabilities in open-source projects. It stresses systematic exploration, hypothesis testing, and iterative debugging under realistic code and system constraints.","L1: 
L2: Logical Reasoning, Planning, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition
L3: Inhibitory Control (minor)",L2
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"SpreadsheetBench tests whether models can understand, edit, and compute within complex spreadsheets derived from real-world scenarios. Tasks often require multi-step manipulation (formulas, formatting, table operations) and consistency checks, emphasizing reliable execution over brittle pattern matching.","L1: Visual Perception (minor)
L2: Planning, Decision-making, Working Memory, Adaptive Error Correction, Semantic Understanding & Context Recognition
L3: ",L2
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a difficult, multimodal academic benchmark spanning many domains and question styles, intended to probe frontier reasoning and knowledge at the edge of current systems. It rewards integrating evidence, performing multi-step reasoning, and (when tools are allowed) using external resources without introducing unsupported claims.","L1: Language Comprehension, Visual Perception (minor)
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory (minor)
L3: ",L2
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 consists of competition-style math problems that require precise symbolic reasoning, careful case analysis, and nontrivial algebra/geometry/number theory insights. The benchmark is typically scored by exact final answers, making it sensitive to small reasoning errors.","L1: 
L2: Logical Reasoning, Working Memory, Planning (minor)
L3: Cognitive Flexibility (minor)",L2
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a subset of graduate-level, “Google-proof” science multiple-choice questions curated to be difficult for non-experts and resistant to superficial retrieval. It evaluates deep scientific understanding and reasoning under distractors that penalize shallow pattern matching.","L1: Language Comprehension (minor)
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory (minor)
L3: ",L2
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends MMLU-style academic questions into multiple languages, measuring whether a model’s knowledge and reasoning transfer across linguistic settings. It probes multilingual comprehension under domain-specific terminology and varied cultural/linguistic phrasing.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory (minor)
L3: Cognitive Flexibility",L3
MMMU,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2311.16502,"MMMU is a multidisciplinary multimodal benchmark where questions combine text with images (charts, diagrams, scenes) across many expert domains. It evaluates whether models can ground reasoning in visual evidence and integrate it with domain knowledge to answer complex questions.","L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Working Memory (minor)
L3: ",L2
LAB-Bench FigQA,https://huggingface.co/datasets/futurehouse/lab-bench,https://arxiv.org/abs/2407.10362,"LAB-Bench FigQA focuses on question answering over complex scientific figures from biology papers, requiring careful extraction of values, trends, and experimental relationships from visualizations. It emphasizes accurate figure interpretation and evidence-based scientific reasoning rather than generic vision recognition.","L1: Visual Perception
L2: Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Multisensory Integration, Semantic Understanding & Context Recognition, Logical Reasoning
L3: ",L2
WebArena,https://webarena.dev/,https://arxiv.org/abs/2307.13854,"WebArena evaluates autonomous web agents performing realistic multi-step tasks across several web apps (e-commerce, CMS, forums, code hosting, maps) using a browser interface. Success requires planning and executing sequences of grounded interactions while tracking task state, forms, navigation, and dynamic page feedback.","L1: Visual Perception
L2: Planning, Decision-making, Scene Understanding & Visual Reasoning (minor), Sensorimotor Coordination, Working Memory, Semantic Understanding & Context Recognition (minor)
L3: Inhibitory Control (minor)",L2
