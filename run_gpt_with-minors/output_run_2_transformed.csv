Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 evaluates autonomous coding and sysadmin-style problem solving inside real command-line environments. Models must interpret a task, execute shell commands, edit files, and iterate until tests or verifiers pass, emphasizing reliable tool-driven execution under environment constraints.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory
L3: ",L2
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp measures “deep research” performance where an agent must search, read, and synthesize information from a controlled document collection or web-like index to answer questions. It stresses multi-step retrieval, evidence aggregation, and minimizing citation or reasoning errors across long tool-augmented traces.","L1: Language Comprehension
L2: Planning, Attention, Semantic Understanding & Context Recognition, Working Memory, Decision-making
L3: ",L2
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld benchmarks multimodal computer-use agents on realistic desktop tasks (e.g., navigating apps, configuring settings, filling forms) with a step limit. Models must perceive the screen, choose actions (mouse/keyboard-like), recover from mistakes, and complete goals in dynamic GUI environments.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Planning, Decision-making, Sensorimotor Coordination, Adaptive Error Correction
L3: ",L2
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI is a few-shot “fluid intelligence” benchmark using abstract grid-based transformation puzzles: given a handful of input–output examples, the model must infer the hidden rule and produce the correct output for a new input. It targets generalization to novel patterns rather than domain memorization.","L1: Visual Perception
L2: Logical Reasoning, Working Memory, Spatial Representation & Mapping, Scene Understanding & Visual Reasoning
L3: Cognitive Flexibility",L3
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon autonomous agency by running a simulated vending machine business over an extended period. Agents must plan inventory, pricing, procurement, and negotiation-like interactions while maintaining coherence and adapting strategy to changing conditions.","L1: 
L2: Planning, Decision-making, Reward Mechanisms, Working Memory, Adaptive Error Correction
L3: Cognitive Timing & Predictive Modeling, Self-reflection",L3
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym tests cybersecurity agent capability on tasks such as locating known vulnerabilities in real open-source projects and discovering previously unknown issues. It emphasizes structured investigation, codebase navigation, hypothesis testing, and producing correct exploit-relevant findings or patches under time/interaction constraints.","L1: 
L2: Logical Reasoning, Planning, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition, Decision-making
L3: ",L2
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"SpreadsheetBench evaluates the ability to understand, edit, and compute over complex spreadsheets resembling real workplace artifacts. Tasks often require reading tables, applying correct formulas/transformations, and producing verifiable outputs while managing multi-step dependencies across cells and sheets.","L1: Language Comprehension
L2: Logical Reasoning, Planning, Working Memory, Adaptive Error Correction, Decision-making
L3: ",L2
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a frontier, multimodal exam-style benchmark spanning advanced academic and professional topics. It requires integrating text, figures, and sometimes tool-like reasoning to answer challenging questions that are designed to stress general reasoning and broad knowledge under low contamination tolerance.","L1: Language Comprehension, Visual Perception
L2: Logical Reasoning, Scene Understanding & Visual Reasoning, Working Memory, Semantic Understanding & Context Recognition
L3: ",L2
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,GPQA Diamond is a high-quality subset of extremely difficult graduate-level science multiple-choice questions designed to be “Google-proof.” It targets deep scientific reasoning and careful elimination among plausible distractors rather than simple recall.,"L1: Language Comprehension
L2: Logical Reasoning, Working Memory, Semantic Understanding & Context Recognition
L3: ",L2
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro is a harder, professional-grade variant of MMMU that tests multidisciplinary multimodal understanding via questions grounded in images (charts, diagrams, screenshots) plus text. It stresses visual grounding, cross-domain knowledge, and robust reasoning in settings closer to real expert work.","L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Visual Attention & Eye Movements, Working Memory
L3: ",L2
MathArena Apex,https://matharena.ai/?view=problem&comp=apex--apex_2025,https://arxiv.org/abs/2505.23281,"MathArena Apex evaluates advanced mathematical problem solving with competition-style questions and rigorous scoring. It emphasizes multi-step derivations, selecting the right technique, and maintaining correctness under complex symbolic constraints.","L1: 
L2: Logical Reasoning, Working Memory, Planning, Adaptive Error Correction
L3: ",L2
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning evaluates reasoning over scientific figures and related context from arXiv-style papers, often involving charts and plotted results. Models must extract quantitative/qualitative information from visuals, align it with text, and answer higher-level inference questions; tool use (e.g., Python) can be enabled in some settings.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Semantic Understanding & Context Recognition, Logical Reasoning, Multisensory Integration, Working Memory
L3: ",L2
OmniDocBench 1.5,https://github.com/opendatalab/OmniDocBench,https://arxiv.org/abs/2412.07626,"OmniDocBench 1.5 evaluates document understanding and reconstruction across heterogeneous document types, including text, formulas, tables, and reading order. It emphasizes accurate visual-to-structured extraction and layout-aware interpretation rather than only plain OCR.","L1: Visual Perception, Language Comprehension
L2: Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Working Memory
L3: ",L2
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU benchmarks video understanding and multimodal reasoning across diverse domains, requiring models to integrate information across time. Tasks often depend on tracking events, recognizing salient frames, and answering questions that require temporal and causal inference.","L1: Visual Perception
L2: Multisensory Integration, Scene Understanding & Visual Reasoning, Working Memory, Attention
L3: Cognitive Timing & Predictive Modeling",L3
LiveCodeBench Pro,https://livecodebenchpro.com/projects/livecodebench-pro/leaderboard,https://arxiv.org/abs/2506.11928,"LiveCodeBench Pro evaluates code generation and debugging on up-to-date programming tasks under standardized judging, often summarized as an ELO-style rating. It emphasizes producing correct, runnable solutions under time pressure and reducing failure modes like off-by-one errors or incorrect API usage.","L1: Language Comprehension, Language Production
L2: Logical Reasoning, Planning, Adaptive Error Correction, Working Memory
L3: ",L2
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"FACTS Benchmark Suite evaluates LLM factuality across a collection of tasks designed to measure correctness, grounding, and resistance to hallucination. It emphasizes truthfulness under uncertainty, careful attribution to sources when available, and avoiding confident fabrication.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Working Memory
L3: Inhibitory Control, Self-reflection",L3
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA extends physical commonsense reasoning evaluation to many languages with non-parallel data, probing whether models can generalize intuitive physics and everyday affordances beyond English. It tests whether models choose the more plausible physical interaction outcome given a situation description.","L1: Language Comprehension
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Spatial Representation & Mapping
L3: Cognitive Flexibility",L3
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) is a long-context multi-round coreference/retrieval evaluation where multiple similar “needle” requests are embedded within a large “haystack” dialogue/document. The model must reproduce the correct response associated with a specified needle, testing robust long-range dependency handling and interference resistance.","L1: Language Comprehension
L2: Working Memory, Attention, Episodic Memory, Semantic Understanding & Context Recognition
L3: ",L2
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval evaluates economically valuable, well-specified knowledge work across many occupations, with outputs judged against professional baselines (often via pairwise comparisons). Tasks include producing realistic work artifacts (e.g., spreadsheets, plans, presentations), stressing end-to-end project execution quality rather than short QA.","L1: Language Production, Language Comprehension
L2: Planning, Decision-making, Working Memory
L3: Self-reflection, Social Reasoning & Theory of Mind",L3
SWE-Lancer,https://openai.com/index/swe-lancer/,https://arxiv.org/abs/2502.12115,"SWE-Lancer evaluates agentic software engineering on realistic repositories and tasks that resemble professional bug-fixing and feature work, emphasizing robustness and practical impact. It stresses translating natural-language requirements into correct patches and handling tooling, tests, and multi-file changes reliably.","L1: Language Comprehension
L2: Logical Reasoning, Planning, Adaptive Error Correction, Working Memory, Decision-making
L3: ",L2
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks tests long-context reasoning over graph-structured data encoded as sequences, such as following edges, tracking parents, or performing BFS-like traversals. It probes whether models can maintain consistent state over many steps and avoid drift in structured multi-hop reasoning.","L1: 
L2: Working Memory, Spatial Representation & Mapping, Logical Reasoning, Attention, Planning
L3: ",L2
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon measures general tool-use competence across varied APIs and multi-step workflows, focusing on selecting the right tools, calling them correctly, and recovering from tool errors. It emphasizes orchestration—keeping goals, intermediate results, and constraints aligned across a long action trace.","L1: Language Comprehension
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory
L3: ",L2
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath evaluates expert-level mathematics beyond standard contest sets, often requiring deeper multi-step reasoning and proof-like derivations; some tiers permit computational assistance. It is designed to probe the limits of mathematical reasoning and error-free symbolic manipulation under hard, novel problems.","L1: 
L2: Logical Reasoning, Working Memory, Planning, Adaptive Error Correction
L3: ",L2
