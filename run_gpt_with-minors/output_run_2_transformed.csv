Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates end-to-end software engineering by asking a model to produce a patch for real GitHub issues and validating solutions by running repository test suites. The “Verified” set emphasizes tasks that have been confirmed solvable and reliably testable, reducing false negatives from flaky tests and underspecified bug reports.","L1: 
L2: Planning, Logical Reasoning, Working Memory, Adaptive Error Correction, Semantic Understanding & Context Recognition, Decision-making
L3: ",L2
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro is a larger, harder successor benchmark that targets more realistic and contamination-resistant software engineering tasks, including greater diversity in repos and problem types. It emphasizes robustness on complex codebases where success depends on correctly diagnosing failures, making coherent multi-file edits, and satisfying automated checks.","L1: 
L2: Planning, Logical Reasoning, Working Memory, Adaptive Error Correction, Decision-making, Semantic Understanding & Context Recognition
L3: ",L2
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp measures deep-research and browsing-style question answering where models must locate, integrate, and cite information from a large document collection (or web-like corpus) under realistic retrieval constraints. Strong performance requires decomposing an information need, searching iteratively, and synthesizing evidence into a grounded final response.","L1: Language Comprehension, Language Production
L2: Planning, Attention, Working Memory, Episodic Memory, Semantic Understanding & Context Recognition, Decision-making
L3: ",L2
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench evaluates interactive agent behavior in multi-turn customer-support environments, where the agent must use tools/APIs and follow domain policies while helping a simulated user. The benchmark stresses reliability across long dialogues, correct tool invocation, and adherence to policy constraints rather than only producing a plausible answer.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Working Memory, Reward Mechanisms, Adaptive Error Correction
L3: Inhibitory Control, Social Reasoning & Theory of Mind",L3
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI tests fluid, few-shot abstract reasoning on grid-based transformation puzzles, where the model must infer the underlying rule from a handful of examples and apply it to a new input. It is designed to reduce reliance on memorized knowledge and instead probe novel pattern induction and compositional generalization.","L1: 
L2: Logical Reasoning, Working Memory, Attention, Spatial Representation & Mapping, Scene Understanding & Visual Reasoning
L3: Cognitive Flexibility",L3
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas evaluates real-world tool use via the Model Context Protocol, focusing on whether models can discover tools, call them with correct arguments, handle errors, and compose multi-step workflows across services. Tasks resemble production integrations where success depends on reliable action selection and correct synthesis of tool outputs.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Working Memory, Adaptive Error Correction, Semantic Understanding & Context Recognition
L3: ",L2
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a difficult multimodal benchmark intended to sit near the frontier of human knowledge, spanning many domains and problem formats. It probes whether models can reason through unfamiliar, technical questions, sometimes benefiting from tool use (e.g., search or code) while maintaining grounded, coherent answers.","L1: Language Comprehension, Language Production, Visual Perception
L2: Logical Reasoning, Working Memory, Semantic Understanding & Context Recognition, Scene Understanding & Visual Reasoning, Planning, Decision-making
L3: ",L2
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 is a competition-math evaluation based on the American Invitational Mathematics Examination, where each problem requires multi-step reasoning and a final integer answer. The tasks stress symbolic manipulation, careful case analysis, and error checking under tight problem statements.","L1: 
L2: Logical Reasoning, Working Memory, Planning, Adaptive Error Correction, Attention
L3: ",L2
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a curated subset of graduate-level, “Google-proof” multiple-choice science questions designed to be difficult for non-experts. It emphasizes disciplined reasoning over physics/chemistry/biology content where shallow pattern matching tends to fail.","L1: 
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Attention, Decision-making
L3: ",L2
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends broad academic knowledge testing into many non-English languages, covering dozens of subjects in a multiple-choice format. It measures whether models can maintain reasoning quality and factual reliability across languages and culturally varied formulations of similar concepts.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Attention
L3: ",L2
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro is a challenging multimodal benchmark that requires models to answer expert-style questions by combining textual context with images such as diagrams, charts, figures, and problem setups across disciplines. It stresses visual grounding, cross-modal integration, and multi-step reasoning beyond simple captioning.","L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Multisensory Integration, Spatial Representation & Mapping, Logical Reasoning, Working Memory, Visual Attention & Eye Movements
L3: ",L2
ScreenShot-Pro,https://gui-agent.github.io/grounding-leaderboard/,https://arxiv.org/abs/2504.07981,"ScreenShot-Pro evaluates models on understanding high-resolution screenshots from real software interfaces, requiring precise grounding in layout and UI semantics. Tasks often involve identifying relevant regions or controls and answering questions that depend on spatial relationships and interface conventions.","L1: Visual Perception, Language Comprehension
L2: Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Spatial Representation & Mapping, Multisensory Integration, Working Memory
L3: ",L2
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning tests scientific figure and chart understanding drawn from research-paper contexts, requiring models to read visual encodings, legends, and annotations and then reason about trends or claims. It targets faithful extraction from dense scientific visuals and correct quantitative/structural interpretation rather than generic image description.","L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Multisensory Integration, Logical Reasoning, Working Memory, Semantic Understanding & Context Recognition
L3: ",L2
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU extends multimodal understanding to the temporal domain by asking questions that depend on events across video frames, audio/visual cues, and narrative context. It stresses integrating information over time, tracking entities/actions, and answering with coherent reasoning about dynamic scenes.","L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Multisensory Integration, Working Memory, Attention, Logical Reasoning, Spatial Representation & Mapping
L3: Cognitive Timing & Predictive Modeling",L3
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) evaluates long-context retrieval and multi-round co-reference by embedding multiple similar “needle” interactions inside long “haystacks” and asking the model to reproduce the correct response for a specified needle. The 8-needle variant increases interference and distractor similarity, stressing robustness to confusion over long spans.","L1: 
L2: Working Memory, Episodic Memory, Attention, Semantic Understanding & Context Recognition, Logical Reasoning
L3: Inhibitory Control",L3
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval measures performance on well-specified professional knowledge-work tasks across many occupations, judged by expert humans via head-to-head comparisons against human-produced artifacts. It emphasizes producing usable work products (e.g., spreadsheets, plans, analyses) with correct reasoning, appropriate structure, and practical completeness.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Adaptive Error Correction
L3: Self-reflection, Social Reasoning & Theory of Mind",L3
SWE-Lancer,https://openai.com/index/swe-lancer/,https://arxiv.org/abs/2502.12115,"SWE-Lancer evaluates advanced, agentic software engineering on realistic tasks where models must understand a codebase, implement changes, and satisfy project-specific constraints and tests. It targets higher-level engineering competence such as coordinating multi-step edits, handling ambiguous requirements, and delivering patches that integrate cleanly.","L1: 
L2: Planning, Decision-making, Logical Reasoning, Working Memory, Adaptive Error Correction, Semantic Understanding & Context Recognition
L3: ",L2
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks tests structured reasoning over graph descriptions, typically requiring multi-hop traversal (e.g., BFS-style walks) and correct tracking of visited nodes/paths under distracting context. It emphasizes precise state tracking and systematic exploration rather than world knowledge.","L1: 
L2: Logical Reasoning, Working Memory, Attention, Spatial Representation & Mapping, Planning
L3: Inhibitory Control",L3
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon evaluates tool-using agents on tasks that require selecting appropriate tools, composing multi-step calls, and recovering from failures to reach a correct end result. It stresses reliable orchestration across heterogeneous tools and consistent integration of intermediate outputs into final answers.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Working Memory, Adaptive Error Correction, Semantic Understanding & Context Recognition
L3: ",L2
HMMT,https://matharena.ai/?view=problem&comp=hmmt--hmmt_feb_2025,https://arxiv.org/abs/2505.23281,"HMMT problems (e.g., Feb 2025 set) reflect high-difficulty competition mathematics from the Harvard-MIT Math Tournament, often requiring creative construction and non-routine proof or computation strategies. Success depends on sustained multi-step reasoning and careful verification of corner cases.","L1: 
L2: Logical Reasoning, Working Memory, Planning, Adaptive Error Correction, Attention
L3: Cognitive Flexibility",L3
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath is an expert-level mathematics benchmark designed to be difficult and relatively contamination-resistant, with tiered problem difficulty reaching beyond standard contest math. It probes whether models can perform long-horizon mathematical reasoning, maintain correctness across many steps, and avoid subtle logical or algebraic errors.","L1: 
L2: Logical Reasoning, Working Memory, Planning, Adaptive Error Correction, Attention
L3: Cognitive Flexibility",L3
