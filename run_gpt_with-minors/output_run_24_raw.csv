Benchmark,Website,Paper,Description,Cognitive Functions
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 evaluates agentic software execution in real command-line environments, where a model must solve tasks by issuing shell commands, editing files, and running programs. It measures end-to-end task completion under realistic constraints, including iterative debugging, tool use, and stateful interaction.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Language Comprehension, Language Production"
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp evaluates “deep research” question answering that requires searching, reading, and synthesizing information across many documents rather than relying on parametric memory alone. It tests whether models can plan a research strategy, track evidence, and produce a final grounded answer under long-horizon information seeking.","Planning, Attention, Episodic Memory, Working Memory, Semantic Understanding & Context Recognition, Decision-making"
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld benchmarks multimodal “computer use” by having agents operate a full desktop environment to complete tasks across real applications and workflows. Models must perceive screenshots/GUI state, choose actions (mouse/keyboard), and recover from mistakes over multi-step trajectories.","Visual Perception, Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Sensorimotor Coordination, Planning, Adaptive Error Correction, Decision-making"
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,ARC-AGI consists of novel grid-based pattern transformation puzzles where only a few input-output examples are provided and the model must infer the underlying rule. It is designed to emphasize fluid abstraction and generalization over memorization and domain-specific knowledge.,"Logical Reasoning, Cognitive Flexibility, Working Memory, Spatial Representation & Mapping, Scene Understanding & Visual Reasoning"
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon autonomy by placing an agent in a simulated vending machine business that must be operated over an extended period. Success depends on strategic decisions such as sourcing, pricing, inventory management, and adapting to changing market conditions.","Planning, Decision-making, Reward Mechanisms, Working Memory, Semantic Understanding & Context Recognition"
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity agents on tasks such as locating known vulnerabilities and discovering new ones in real open-source codebases. It stresses realistic security analysis workflows, including reasoning about program behavior, navigating repositories, and iterating when initial hypotheses fail.","Logical Reasoning, Planning, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition"
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"SpreadsheetBench measures an agent’s ability to interpret and manipulate complex spreadsheets, including reading structured tables, editing cells, applying formulas, and producing correct computed outputs. It emphasizes structured data reasoning and multi-step interactions that resemble real office analytics work.","Logical Reasoning, Working Memory, Planning, Adaptive Error Correction, Visual Perception, Decision-making"
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a large, frontier-difficulty benchmark spanning many domains and modalities, intended to probe deep reasoning and expert knowledge rather than surface pattern matching. It is commonly evaluated both without tools and with tools (e.g., search/code) to assess end-to-end problem solving and synthesis.","Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Scene Understanding & Visual Reasoning, Multisensory Integration"
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,GPQA Diamond is a curated subset of GPQA multiple-choice science questions designed to be challenging even for strong models and to reduce easy lookup. It targets graduate-level scientific understanding and reasoning under constrained answer formats.,"Language Comprehension, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Decision-making"
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro is a harder, more rigorous variant of multimodal academic evaluation where models answer questions that require jointly reasoning over images (e.g., diagrams, charts, figures) and text. It probes expert-level multimodal understanding and the ability to integrate visual evidence with linguistic instructions.","Visual Perception, Scene Understanding & Visual Reasoning, Multisensory Integration, Language Comprehension, Logical Reasoning, Visual Attention & Eye Movements"
MathArena Apex,https://matharena.ai/?view=problem&comp=apex--apex_2025,https://arxiv.org/abs/2505.23281,MathArena Apex is a competitive evaluation of advanced mathematics where models must solve challenging problems that often require multi-step derivations. It is designed to compare frontier mathematical reasoning performance under standardized problem sets and scoring.,"Logical Reasoning, Working Memory, Planning, Adaptive Error Correction"
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning evaluates whether models can answer questions grounded in scientific figures (often from arXiv-style papers), requiring careful reading of plots, axes, annotations, and relationships. Many items demand quantitative or causal inference from the visual evidence rather than memorized facts.","Visual Perception, Scene Understanding & Visual Reasoning, Language Comprehension, Logical Reasoning, Multisensory Integration, Working Memory"
OmniDocBench 1.5,https://github.com/opendatalab/OmniDocBench,https://arxiv.org/abs/2412.07626,"OmniDocBench 1.5 evaluates document understanding and OCR across heterogeneous PDF content such as text blocks, formulas, tables, and reading order. It emphasizes layout-aware extraction and faithful reconstruction of structured document content.","Visual Perception, Language Comprehension, Semantic Understanding & Context Recognition, Attention, Working Memory"
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU evaluates multimodal reasoning over short videos, where answering requires integrating information across frames and time rather than single-image perception. It stresses temporal comprehension and multi-step reasoning grounded in dynamic visual content.","Visual Perception, Scene Understanding & Visual Reasoning, Cognitive Timing & Predictive Modeling, Multisensory Integration, Working Memory, Attention"
LiveCodeBench Pro,https://livecodebenchpro.com/projects/livecodebench-pro/leaderboard,https://arxiv.org/abs/2506.11928,"LiveCodeBench Pro evaluates coding ability on continuously updated programming tasks, typically graded by executing tests to reduce ambiguity. It is designed to be more resistant to contamination and to reflect practical programming and debugging competence.","Logical Reasoning, Planning, Adaptive Error Correction, Working Memory, Language Comprehension, Language Production"
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"FACTS Benchmark Suite evaluates factuality-related behavior, including whether models produce accurate statements and appropriately ground or attribute claims when required. It targets hallucination failure modes and the ability to maintain consistency with available evidence and instructions.","Semantic Understanding & Context Recognition, Inhibitory Control, Self-reflection, Language Comprehension, Working Memory"
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA extends physical commonsense/procedural reasoning evaluation across many languages, testing whether models can choose plausible actions and outcomes in everyday scenarios. It probes whether reasoning generalizes beyond English while maintaining physical and intent coherence.","Language Comprehension, Logical Reasoning, Semantic Understanding & Context Recognition, Cognitive Flexibility, Spatial Representation & Mapping"
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) tests long-context multi-round coreference-like retrieval by embedding repeated, similar “needle” interactions inside long “haystack” conversations and asking the model to reproduce the correct target response. It stresses robust retrieval of specific details under extreme context length and distractors.","Working Memory, Episodic Memory, Attention, Language Comprehension, Semantic Understanding & Context Recognition"
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval evaluates economically meaningful professional work by having human judges compare model-generated artifacts (e.g., spreadsheets, presentations, plans) against outputs from skilled professionals across many occupations. It measures end-to-end task execution quality, including following specifications, producing usable deliverables, and prioritizing what matters for the user’s objective.","Planning, Decision-making, Language Production, Semantic Understanding & Context Recognition, Social Reasoning & Theory of Mind, Self-reflection"
SWE-Lancer,https://openai.com/index/swe-lancer/,https://arxiv.org/abs/2502.12115,"SWE-Lancer evaluates realistic software engineering work in code repositories, requiring models to understand a codebase, implement changes, and produce correct patches. It emphasizes reliability over multi-step development loops, including diagnosing failures and iterating toward a working solution.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Logical Reasoning, Language Comprehension"
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks evaluates structured reasoning over graphs described in text, asking models to execute procedures such as walks, BFS-style reachability, or parent tracking across many entities. It is designed to probe algorithmic reasoning and memory for relational structure under long contexts.","Spatial Representation & Mapping, Logical Reasoning, Working Memory, Planning, Attention"
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon evaluates general tool-using agents on tasks that require selecting the right tools, formatting correct calls, chaining outputs across steps, and handling tool failures. It is meant to reflect practical agent behavior in realistic multi-tool workflows rather than pure language-only answering.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Language Comprehension"
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath is a curated set of expert-level mathematics problems aimed at tracking progress toward research-grade mathematical reasoning. Its tiered difficulty stresses long, precise chains of inference and verification, often in settings where tool assistance can be compared to no-tool performance.","Logical Reasoning, Working Memory, Planning, Adaptive Error Correction"
