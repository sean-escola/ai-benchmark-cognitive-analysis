Benchmark,Website,Paper,Description,Cognitive Functions
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates software-engineering agents by asking them to produce patches that resolve real issues in GitHub repositories, then running the project’s tests to verify correctness. The “Verified” subset uses tasks that have been filtered/validated to be solvable and to have reliable evaluation signals, making it a common headline metric for agentic coding.","Planning, Logical Reasoning, Working Memory, Adaptive Error Correction, Decision-making, Language Comprehension"
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro is a harder, larger software-engineering benchmark intended to be more industrially representative and more resistant to shortcutting/contamination than earlier variants. It typically spans multiple programming languages and demands more robust debugging, codebase navigation, and patch synthesis under realistic constraints.","Planning, Logical Reasoning, Working Memory, Adaptive Error Correction, Decision-making, Cognitive Flexibility, Language Comprehension"
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp evaluates “deep research” behavior: models must browse or retrieve information from documents/web sources to answer questions that require multi-step investigation and synthesis. It emphasizes citation-quality grounding, cross-document integration, and avoiding hallucinations when the answer requires lookup rather than recall.","Planning, Decision-making, Attention, Working Memory, Semantic Understanding & Context Recognition, Language Comprehension, Language Production"
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench evaluates interactive, policy-constrained agent behavior in multi-turn customer-support style environments (e.g., retail, airline, telecom) with simulated users and tool/API calls. Success requires correctly following domain policies while still resolving the user’s goal through appropriate tool use across several steps.","Social Reasoning & Theory of Mind, Planning, Decision-making, Inhibitory Control, Attention, Working Memory, Language Comprehension, Language Production"
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,ARC-AGI (Abstraction and Reasoning Corpus) tests fluid reasoning by giving a few input–output grid examples and requiring inference of the latent transformation rule for a new grid. It is designed to measure generalization to novel pattern-manipulation tasks with minimal training signal per task.,"Logical Reasoning, Cognitive Flexibility, Working Memory, Attention, Spatial Representation & Mapping, Scene Understanding & Visual Reasoning"
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas measures real-world tool use via the Model Context Protocol (MCP), where models must discover, call, and chain tools hosted on MCP servers to complete tasks. It stresses reliable API invocation, multi-step workflow execution, and robust handling of tool errors and intermediate results.","Planning, Decision-making, Working Memory, Adaptive Error Correction, Attention, Language Comprehension, Language Production"
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a large, frontier-difficulty benchmark spanning many academic/professional domains (often including multimodal questions) intended to probe the limits of current generalist models. It is commonly reported both with and without tool access (e.g., web search, code) to separate pure reasoning/knowledge from tool-augmented performance.","Logical Reasoning, Semantic Understanding & Context Recognition, Language Comprehension, Language Production, Planning, Decision-making, Scene Understanding & Visual Reasoning, Working Memory"
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 uses problems from the American Invitational Mathematics Examination, requiring exact numerical answers (typically 0–999) and careful symbolic/quantitative reasoning. It is widely used to assess mathematical problem solving under competition-style constraints, sometimes also reported with tool support (e.g., Python) for verification.","Logical Reasoning, Working Memory, Attention, Cognitive Flexibility"
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,GPQA Diamond is the highest-quality subset of GPQA: challenging graduate-level multiple-choice science questions designed to be “Google-proof” and to require real reasoning rather than memorized facts. Performance depends on integrating scientific knowledge with multi-step inference under distractor options.,"Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Attention, Decision-making"
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends MMLU-style broad academic evaluation into multiple languages, testing knowledge and reasoning across many subjects with multilingual prompts. It is used to measure cross-lingual generalization and whether capabilities transfer beyond English.","Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Attention"
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro evaluates multimodal, multi-discipline understanding and reasoning using images (e.g., diagrams, charts, figures) paired with text questions, often at expert level. Compared to simpler vision QA, it emphasizes cross-modal grounding and higher-level reasoning over visual evidence.","Visual Perception, Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Attention, Working Memory, Language Comprehension"
ScreenShot-Pro,https://gui-agent.github.io/grounding-leaderboard/,https://arxiv.org/abs/2504.07981,"ScreenShot-Pro tests GUI understanding from high-resolution screenshots, requiring models to interpret interface layout, identify relevant elements, and answer questions or take actions grounded in the screen. It is used as a proxy for computer-use competence and precise visual grounding in professional software contexts.","Visual Perception, Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Spatial Representation & Mapping, Decision-making, Attention"
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning evaluates reasoning over scientific figures (often charts/plots) from arXiv-style papers, requiring extracting quantitative/relational information from visuals and integrating it with the question. It is commonly used to measure chart/figure comprehension and multi-step inference from scientific visual evidence.","Visual Perception, Scene Understanding & Visual Reasoning, Logical Reasoning, Attention, Working Memory, Multisensory Integration, Semantic Understanding & Context Recognition"
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU extends multimodal understanding to video, requiring models to integrate information across frames and time to answer questions about events, procedures, and visual evidence. It stresses temporal integration, robust perception, and reasoning grounded in dynamic scenes.","Visual Perception, Scene Understanding & Visual Reasoning, Cognitive Timing & Predictive Modeling, Attention, Working Memory, Multisensory Integration, Logical Reasoning"
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) is a long-context evaluation where multiple similar “needle” turns are embedded in a large “haystack” of dialogue, and the model must retrieve the correct referenced response/attribute for a specified needle. It probes whether models can maintain and use precise referential information across very long contexts under strong distractor interference.","Working Memory, Attention, Semantic Understanding & Context Recognition, Inhibitory Control, Language Comprehension"
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval evaluates economically valuable knowledge-work tasks across many occupations, where models produce real work artifacts (e.g., analyses, plans, documents, spreadsheets/slides) judged against professional baselines. It emphasizes end-to-end task completion quality, instruction following, and practical decision-making rather than only short-form Q&A.","Planning, Decision-making, Language Comprehension, Language Production, Self-reflection, Working Memory, Semantic Understanding & Context Recognition"
SWE-Lancer,https://openai.com/index/swe-lancer/,https://arxiv.org/abs/2502.12115,"SWE-Lancer is an agentic software-engineering benchmark focused on completing realistic engineering work beyond isolated bug fixes, often involving larger codebases, multi-step changes, and evaluation via tests or task-specific criteria. It targets reliability in end-to-end implementation and debugging under constraints closer to real development workflows.","Planning, Decision-making, Logical Reasoning, Working Memory, Adaptive Error Correction, Cognitive Flexibility, Language Comprehension"
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks evaluates long-context reasoning by placing graph descriptions/edges in context and asking traversal queries (e.g., reachability, BFS-like steps, parent/neighbor relationships) that require systematic navigation rather than surface pattern matching. It stresses consistent multi-step computation over structured data embedded in text.","Spatial Representation & Mapping, Logical Reasoning, Working Memory, Attention, Planning"
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon evaluates general tool-use competence across diverse APIs/tools, requiring models to select appropriate tools, sequence calls, and synthesize outputs into correct final answers. It emphasizes workflow reliability, error recovery, and the ability to coordinate heterogeneous tools under time/step constraints.","Planning, Decision-making, Working Memory, Adaptive Error Correction, Attention, Language Comprehension, Language Production"
HMMT,https://matharena.ai/?view=problem&comp=hmmt--hmmt_feb_2025,https://arxiv.org/abs/2505.23281,"HMMT uses problems from the Harvard-MIT Mathematics Tournament, typically harder and more proof/insight-oriented than many standard school exams. It tests deep mathematical reasoning, careful constraint handling, and sustained multi-step derivations under competition conditions.","Logical Reasoning, Working Memory, Attention, Cognitive Flexibility"
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath is a frontier-level mathematics benchmark designed to probe expert and research-adjacent problem solving, often with hidden or carefully controlled sets and tiered difficulty. It emphasizes long-horizon reasoning, precise symbolic manipulation, and robustness against shallow pattern matching.","Logical Reasoning, Working Memory, Attention, Cognitive Flexibility, Planning"
