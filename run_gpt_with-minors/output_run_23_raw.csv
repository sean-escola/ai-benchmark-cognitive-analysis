Benchmark,Website,Paper,Description,Cognitive Functions
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates whether a model can fix real GitHub issues by producing a patch that makes a repository’s tests pass. The “Verified” subset filters to problems that have been human-validated as solvable and well-specified, emphasizing correctness under realistic repo structure and dependencies.","Planning, Logical Reasoning, Working Memory, Adaptive Error Correction, Semantic Understanding & Context Recognition, Decision-making"
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro is a larger, harder software-engineering benchmark designed to be more industrially representative and more resistant to contamination than earlier SWE-bench variants. It tests end-to-end debugging and feature implementation in realistic codebases, often requiring longer-horizon reasoning across multiple files and languages.","Planning, Logical Reasoning, Working Memory, Adaptive Error Correction, Cognitive Flexibility, Semantic Understanding & Context Recognition, Decision-making"
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp evaluates “deep research” style question answering where the model must search a document collection (or the web, depending on the setting), gather evidence, and synthesize a final answer. Performance depends on iterative query formulation, evidence selection, and maintaining consistency while navigating noisy or incomplete sources.","Planning, Decision-making, Attention, Working Memory, Semantic Understanding & Context Recognition, Language Comprehension, Language Production"
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench measures interactive agent performance in multi-turn customer-support environments (e.g., retail, airline, telecom) with tools/APIs and policy constraints. The agent must follow domain rules while helping a simulated user, handling edge cases, and keeping conversation state coherent across turns.","Social Reasoning & Theory of Mind, Inhibitory Control, Decision-making, Planning, Working Memory, Language Comprehension, Language Production, Adaptive Error Correction"
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI tests few-shot, novel pattern induction on grid-based tasks where the system infers a transformation rule from a handful of examples. It is intended to probe fluid reasoning and generalization to unseen “concepts,” rather than recall of domain knowledge.","Logical Reasoning, Working Memory, Cognitive Flexibility, Spatial Representation & Mapping, Visual Perception, Scene Understanding & Visual Reasoning, Attention"
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas evaluates real-world tool use via the Model Context Protocol by requiring models to discover tools, call them correctly, and compose multi-step workflows across services. It emphasizes API understanding, error handling, and robust execution in production-like tool ecosystems.","Planning, Decision-making, Working Memory, Adaptive Error Correction, Semantic Understanding & Context Recognition, Language Comprehension"
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a large, frontier-difficulty benchmark spanning expert-level questions across many domains, often requiring multi-step reasoning and careful synthesis. It is frequently used to assess broad academic/technical competence and, in tool-enabled variants, research-and-verify behavior.","Language Comprehension, Logical Reasoning, Working Memory, Semantic Understanding & Context Recognition, Planning, Decision-making, Visual Perception"
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,AIME 2025 consists of competition-style mathematics problems that require precise symbolic reasoning and careful multi-step derivations. It is commonly used to evaluate mathematical problem solving both with and without computational tools.,"Logical Reasoning, Working Memory, Planning, Adaptive Error Correction"
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,GPQA Diamond is a subset of challenging graduate-level multiple-choice science questions curated to be difficult for non-experts and resistant to superficial pattern matching. It probes deep scientific reasoning and the ability to disambiguate close distractors under time-limited formats.,"Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Decision-making"
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends broad academic knowledge testing to multiple languages, measuring whether models can answer subject-area questions beyond English. It emphasizes cross-lingual generalization of factual knowledge and reasoning across many disciplines.","Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory"
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro evaluates expert-level multimodal understanding across many subjects where questions depend on both images and text (e.g., diagrams, charts, documents). It tests whether models can integrate visual evidence with domain knowledge and reasoning to select correct answers.","Visual Perception, Scene Understanding & Visual Reasoning, Multisensory Integration, Language Comprehension, Logical Reasoning, Attention, Working Memory"
ScreenShot-Pro,https://gui-agent.github.io/grounding-leaderboard/,https://arxiv.org/abs/2504.07981,"ScreenShot-Pro measures UI/screenshot understanding for agentic interaction with software interfaces, often requiring identifying on-screen elements and reasoning about layout. It is used to assess whether models can ground instructions in high-resolution visual interfaces similar to real computer-use tasks.","Visual Perception, Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Spatial Representation & Mapping, Decision-making, Attention"
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning focuses on interpreting and reasoning over scientific figures (plots, tables, and composite visuals) from research papers. The benchmark targets extraction of quantitative/structural information from visuals and combining it with textual context to answer reasoning questions.","Visual Perception, Scene Understanding & Visual Reasoning, Logical Reasoning, Working Memory, Semantic Understanding & Context Recognition, Language Comprehension, Attention"
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU extends multimodal understanding to videos, requiring temporal integration of visual events with text questions and options. It probes whether models can track entities and actions across frames and use that information for higher-level reasoning.","Visual Perception, Multisensory Integration, Working Memory, Attention, Scene Understanding & Visual Reasoning, Cognitive Timing & Predictive Modeling, Language Comprehension"
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,MRCR v2 (8-needle) tests long-context multi-round coreference and retrieval by placing repeated “needle” requests inside long “haystack” conversations and asking for the correct referenced response. It stresses maintaining and accessing the right contextual binding across many similar spans.,"Working Memory, Attention, Semantic Understanding & Context Recognition, Logical Reasoning, Cognitive Flexibility"
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval evaluates well-specified professional “knowledge work” tasks across many occupations, judged against human expert outputs (often via head-to-head comparisons). It emphasizes producing usable artifacts (e.g., plans, analyses, spreadsheets/presentations in tool-enabled settings) that meet constraints and stakeholder intent.","Planning, Decision-making, Language Production, Language Comprehension, Self-reflection, Semantic Understanding & Context Recognition, Social Reasoning & Theory of Mind"
SWE-Lancer,https://openai.com/index/swe-lancer/,https://arxiv.org/abs/2502.12115,"SWE-Lancer evaluates software engineering performance on real-world tasks with an emphasis on longer-horizon, agentic coding behavior and reliability under realistic constraints. It is used to measure end-to-end capability from understanding the problem and repository to delivering correct, test-passing changes.","Planning, Logical Reasoning, Working Memory, Adaptive Error Correction, Decision-making, Semantic Understanding & Context Recognition"
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks evaluates structured reasoning over graph-structured data, such as following paths, identifying reachable nodes, or tracking parent/ancestor relations under long contexts. It targets algorithmic, stepwise traversal behavior rather than open-ended language generation alone.","Spatial Representation & Mapping, Working Memory, Logical Reasoning, Attention, Planning"
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon benchmarks multi-tool problem solving where models must choose among tools, sequence calls, and integrate outputs to complete tasks. It emphasizes robust orchestration, tool selection under uncertainty, and recovery from tool errors or partial results.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition, Attention"
HMMT,https://matharena.ai/?view=problem&comp=hmmt--hmmt_feb_2025,https://arxiv.org/abs/2505.23281,"HMMT comprises high-difficulty competition math problems that often require creative insights, careful case analysis, and long multi-step proofs or computations. It is used to test mathematical depth beyond standard contest sets, especially under strict correctness requirements.","Logical Reasoning, Working Memory, Planning, Cognitive Flexibility, Adaptive Error Correction"
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath is designed to measure expert-level mathematics performance across tiers of difficulty, including problems intended to be challenging even for strong solvers. It evaluates sustained multi-step reasoning, precise formal manipulation, and, in tool-enabled modes, effective use of computation to support proofs and calculations.","Logical Reasoning, Working Memory, Planning, Cognitive Flexibility, Adaptive Error Correction"
