Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates a model’s ability to solve real-world software engineering issues by generating patches that make a repository’s tests pass, using a curated set of tasks validated as solvable. It emphasizes end-to-end debugging and codebase navigation under a standardized, single-attempt evaluation setting.","L1: Language Comprehension, Language Production
L2: Logical Reasoning, Planning, Working Memory, Adaptive Error Correction, Decision-making, Semantic Understanding & Context Recognition
L3: ",L2
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro is a larger and more challenging successor-style benchmark for software engineering, with harder tasks and broader coverage intended to be more contamination-resistant and industrially relevant. It stresses multi-file reasoning, robust patch generation, and reliably resolving complex dependency and test-failure chains.","L1: Language Comprehension, Language Production
L2: Logical Reasoning, Planning, Working Memory, Adaptive Error Correction, Decision-making, Semantic Understanding & Context Recognition
L3: Cognitive Flexibility",L3
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp evaluates deep research agents on questions that require multi-step web information gathering, synthesis, and verification rather than single-page lookup. It measures whether an agent can search, read, cross-check sources, and produce a correct final answer with appropriate evidence handling.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Working Memory, Episodic Memory, Attention, Logical Reasoning, Semantic Understanding & Context Recognition
L3: Self-reflection",L3
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench evaluates interactive tool-using agents in customer-service style domains (e.g., retail, airline, telecom) where the agent must follow policies while using APIs over multi-turn dialogs. Success depends on correct tool invocation, consistent policy adherence, and recovering from user or system complications across a conversation.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Working Memory, Adaptive Error Correction, Reward Mechanisms, Semantic Understanding & Context Recognition, Attention
L3: Social Reasoning & Theory of Mind, Inhibitory Control",L3
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,ARC-AGI measures generalization and fluid reasoning by requiring models to infer latent transformation rules from a few input–output grid examples and apply them to new grids. It is designed to reward abstract pattern induction and compositional reasoning rather than memorization of domain knowledge.,"L1: 
L2: Logical Reasoning, Working Memory, Spatial Representation & Mapping, Scene Understanding & Visual Reasoning, Attention, Decision-making, Planning
L3: Cognitive Flexibility",L3
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas benchmarks real-world tool use via the Model Context Protocol, requiring models to discover relevant tools, correctly call them, and integrate results across multi-step workflows. It focuses on practical API orchestration, error handling, and producing correct end outputs under realistic tool ecosystems.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Working Memory, Adaptive Error Correction, Semantic Understanding & Context Recognition, Attention
L3: Inhibitory Control",L3
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a frontier, multimodal benchmark spanning many expert domains, intended to test advanced reasoning and knowledge at the edge of current model capability. Questions often require combining domain understanding with careful, multi-step inference (and sometimes tool use, depending on the evaluation setup).","L1: Language Comprehension, Language Production, Visual Perception
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Attention, Decision-making, Planning, Multisensory Integration, Scene Understanding & Visual Reasoning
L3: ",L2
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 evaluates competition-style mathematics problem solving from the American Invitational Mathematics Examination. Problems require symbolic manipulation and multi-step reasoning with exact answers, rewarding correctness and disciplined constraint handling.","L1: 
L2: Logical Reasoning, Working Memory, Planning, Decision-making, Attention, Adaptive Error Correction
L3: ",L2
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a highly curated subset of challenging graduate-level science multiple-choice questions designed to be “Google-proof,” emphasizing reasoning over retrieval. The Diamond split focuses on high-quality items where experts succeed and non-experts tend to fail, highlighting deep scientific understanding and careful elimination.","L1: Language Comprehension
L2: Logical Reasoning, Working Memory, Decision-making, Semantic Understanding & Context Recognition, Attention
L3: ",L2
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends MMLU into multiple languages, testing broad academic knowledge and reasoning across many subjects in non-English settings. It probes whether models can maintain competence and instruction-following across languages and cultural contexts while answering exam-style questions.","L1: Language Comprehension
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Attention, Decision-making
L3: Cognitive Flexibility",L3
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro is a harder, more reliability-focused variant of MMMU that evaluates multimodal understanding across many disciplines, combining text with images such as diagrams, charts, and scientific figures. It stresses grounded visual reasoning and cross-modal integration needed for expert-level multimodal QA.","L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Multisensory Integration, Visual Attention & Eye Movements, Logical Reasoning, Working Memory, Attention, Decision-making
L3: ",L2
ScreenShot-Pro,https://gui-agent.github.io/grounding-leaderboard/,https://arxiv.org/abs/2504.07981,"ScreenShot-Pro evaluates a model’s ability to understand and act on information in high-resolution screenshots of graphical user interfaces, often requiring precise identification of UI elements and layout-dependent reasoning. It targets screenshot understanding for computer-use and professional software settings where small visual details matter.","L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Spatial Representation & Mapping, Logical Reasoning, Working Memory, Attention, Decision-making
L3: ",L2
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning evaluates reasoning over scientific figures and plots drawn from research papers, often requiring extraction of quantitative or structural information from charts. It emphasizes faithful visual-to-text reasoning and, in some settings, benefits from external computation tools for numerical accuracy.","L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Logical Reasoning, Working Memory, Attention, Decision-making
L3: ",L2
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU extends multimodal understanding to the temporal domain by asking questions that require integrating information across video frames and narration/text. It assesses whether models can track events, causal relations, and visual details over time rather than relying on single-frame cues.","L1: Visual Perception, Language Comprehension
L2: Multisensory Integration, Scene Understanding & Visual Reasoning, Working Memory, Attention, Decision-making, Logical Reasoning
L3: Cognitive Timing & Predictive Modeling",L3
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,MRCR v2 (8-needle) tests long-context retrieval and multi-round coreference by inserting multiple similar “needle” requests into long “haystacks” and asking the model to reproduce the response corresponding to a specific needle. It measures robust context integration under interference and distractors across very long inputs.,"L1: 
L2: Working Memory, Attention, Episodic Memory, Semantic Understanding & Context Recognition, Logical Reasoning, Decision-making
L3: Inhibitory Control",L3
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval evaluates well-specified professional knowledge work across many occupations by judging the quality of produced artifacts (e.g., plans, spreadsheets, presentations) against human professional baselines. It targets real-world task completion quality, including following constraints, organizing deliverables, and making sound practical decisions.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Working Memory, Semantic Understanding & Context Recognition, Logical Reasoning, Attention
L3: Self-reflection, Social Reasoning & Theory of Mind",L3
SWE-Lancer,https://openai.com/index/swe-lancer/,https://arxiv.org/abs/2502.12115,"SWE-Lancer evaluates higher-level software engineering work that is closer to contractor-style tasks, emphasizing broader task ownership and practical engineering judgment beyond narrow bug fixes. It stresses selecting appropriate changes, coordinating across project context, and delivering patches that satisfy real requirements.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Logical Reasoning, Working Memory, Adaptive Error Correction, Semantic Understanding & Context Recognition
L3: Cognitive Flexibility, Self-reflection",L3
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks evaluates structured reasoning over graph-encoded data, requiring models to follow paths, perform traversals (e.g., BFS-like operations), and answer queries that depend on multi-hop structure. It targets reliable stepwise computation and resistance to distraction when reasoning over large, structured contexts.","L1: 
L2: Logical Reasoning, Working Memory, Attention, Planning, Spatial Representation & Mapping, Decision-making, Semantic Understanding & Context Recognition
L3: ",L2
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon measures tool-use competence across diverse APIs and multi-step tasks, emphasizing correct tool selection, argument construction, and robust chaining of calls to reach a final objective. It highlights practical agent behavior such as recovering from tool errors and maintaining state across extended workflows.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Working Memory, Adaptive Error Correction, Attention, Semantic Understanding & Context Recognition
L3: Inhibitory Control",L3
HMMT,https://matharena.ai/?view=problem&comp=hmmt--hmmt_feb_2025,https://arxiv.org/abs/2505.23281,"HMMT evaluates competition mathematics performance using problems from the Harvard-MIT Mathematics Tournament, typically demanding deeper insight and longer solutions than many standard exam questions. It rewards sustained multi-step reasoning, careful case analysis, and precise computation under complex constraints.","L1: 
L2: Logical Reasoning, Working Memory, Planning, Decision-making, Attention, Adaptive Error Correction
L3: Cognitive Flexibility",L3
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath is an expert-level mathematics benchmark designed to probe the frontier of model mathematical reasoning on challenging, novel problems. It emphasizes rigorous multi-step derivations and, in tool-enabled settings, integrates external computation while still requiring correct problem decomposition and proof-like reasoning.","L1: 
L2: Logical Reasoning, Working Memory, Planning, Decision-making, Attention, Adaptive Error Correction
L3: Cognitive Flexibility",L3
