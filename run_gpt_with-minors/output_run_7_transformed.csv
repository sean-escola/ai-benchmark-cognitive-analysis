Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,SWE-bench Verified evaluates an LLM’s ability to solve real GitHub issues by producing a patch that passes a project’s tests. The “Verified” subset emphasizes tasks that have been validated as solvable and uses rigorous checking to reduce noise in problem statements and expected fixes.,"L1: Language Comprehension, Language Production
L2: Planning, Working Memory, Adaptive Error Correction, Decision-making, Logical Reasoning, Semantic Understanding & Context Recognition
L3: ",L2
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro is a harder, more contamination-resistant software engineering benchmark that expands task diversity and difficulty beyond the original SWE-bench setting. Models must generate correct patches across multiple languages and real repositories under stricter evaluation and broader task coverage.","L1: Language Comprehension, Language Production
L2: Planning, Working Memory, Adaptive Error Correction, Decision-making, Logical Reasoning, Semantic Understanding & Context Recognition
L3: ",L2
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp measures how well models answer questions that require multi-step web research, typically involving searching, reading, and synthesizing evidence from multiple documents. It is designed to evaluate browsing-style information foraging and the ability to produce supported answers rather than relying on parametric memory alone.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Attention, Working Memory, Episodic Memory, Semantic Understanding & Context Recognition
L3: Self-reflection",L3
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench evaluates interactive agents in simulated customer-support domains (e.g., retail, airline, telecom) where the agent must call APIs and converse over multiple turns. The benchmark emphasizes policy compliance, robust tool use, and resolving user goals under constraints and ambiguous conversational context.","L1: Language Comprehension, Language Production
L2: Decision-making, Planning, Working Memory, Adaptive Error Correction, Reward Mechanisms
L3: Social Reasoning & Theory of Mind, Empathy, Inhibitory Control",L3
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI tests “fluid” reasoning on novel grid-transformation puzzles where only a few input-output examples are provided per task. Success requires inferring the underlying rule and generalizing to a new input, emphasizing abstraction and systematic generalization rather than memorized knowledge.","L1: 
L2: Logical Reasoning, Working Memory, Attention, Spatial Representation & Mapping, Scene Understanding & Visual Reasoning, Planning
L3: Cognitive Flexibility",L3
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas assesses real-world tool use via the Model Context Protocol, requiring models to discover, invoke, and chain tools to complete tasks. It stresses correct API selection/parameterization, multi-step workflow execution, and recovery from tool errors or partial failures.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Working Memory, Adaptive Error Correction, Semantic Understanding & Context Recognition
L3: Inhibitory Control",L3
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a frontier, multi-domain benchmark intended to probe advanced reasoning and knowledge at the edge of expert difficulty, often with multimodal questions. It targets robust synthesis and problem solving under uncertainty, where shallow pattern matching is less likely to succeed.","L1: Language Comprehension, Language Production, Visual Perception
L2: Logical Reasoning, Working Memory, Semantic Understanding & Context Recognition, Scene Understanding & Visual Reasoning, Planning
L3: Self-reflection",L3
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 consists of competition-style mathematics problems that require multi-step derivations and careful symbolic manipulation. It is commonly used to measure mathematical reasoning accuracy, consistency, and the ability to carry intermediate results without arithmetic or logical slips.","L1: 
L2: Logical Reasoning, Working Memory, Planning, Attention, Adaptive Error Correction
L3: ",L2
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a high-quality subset of graduate-level, “Google-proof” multiple-choice science questions designed to be difficult for non-experts. It emphasizes deep scientific reasoning and precise reading of technical detail rather than retrieval of simple facts.","L1: Language Comprehension
L2: Logical Reasoning, Working Memory, Semantic Understanding & Context Recognition, Decision-making, Attention
L3: ",L2
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends MMLU-style breadth evaluation to multiple languages, testing knowledge and reasoning across many academic subjects in non-English settings. It probes whether models maintain consistent understanding and reasoning when the same kinds of questions are posed across diverse linguistic contexts.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Attention
L3: ",L2
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro is a multimodal benchmark requiring expert-level reasoning over text and images across many disciplines, with an emphasis on robust evaluation settings. Questions often require interpreting diagrams, charts, or figures and integrating them with domain knowledge to select or produce correct answers.","L1: Visual Perception, Language Comprehension, Language Production
L2: Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Working Memory, Attention
L3: ",L2
ScreenShot-Pro,https://gui-agent.github.io/grounding-leaderboard/,https://arxiv.org/abs/2504.07981,"ScreenShot-Pro evaluates GUI understanding and grounding from screenshots, where a model must reason about interface elements and their spatial relationships to identify targets or actions. It measures practical visual UI comprehension under realistic, high-resolution screens and varied application layouts.","L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Spatial Representation & Mapping, Sensorimotor Coordination, Decision-making, Working Memory
L3: ",L2
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning tests whether models can answer questions about scientific papers using the figures (and often accompanying captions/text), focusing on chart/plot interpretation and scientific visual reasoning. It emphasizes extracting quantitative/relational information from figures and connecting it to the question context.","L1: Visual Perception, Language Comprehension, Language Production
L2: Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Working Memory, Attention
L3: ",L2
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU evaluates multimodal understanding over videos, requiring models to integrate information across time (multiple frames/scenes) with textual prompts. Tasks often demand tracking events, recognizing changes, and using temporal context to answer complex questions.","L1: Visual Perception, Language Comprehension, Language Production
L2: Scene Understanding & Visual Reasoning, Multisensory Integration, Working Memory, Attention, Logical Reasoning
L3: Cognitive Timing & Predictive Modeling",L3
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 is a long-context, multi-round coreference-style evaluation where multiple similar “needle” interactions are embedded in a long “haystack” dialogue/log, and the model must retrieve the correct referenced response. The 8-needle variant stresses robust attention control and interference resistance across many competing, near-duplicate targets.","L1: Language Comprehension, Language Production
L2: Working Memory, Attention, Episodic Memory, Semantic Understanding & Context Recognition
L3: Inhibitory Control",L3
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval evaluates economically meaningful, well-specified professional tasks across many occupations, judged by expert humans comparing model outputs to professional baselines. It focuses on producing usable work artifacts (e.g., plans, analyses, structured documents) under real-world constraints and quality standards.","L1: Language Production, Language Comprehension
L2: Planning, Decision-making, Semantic Understanding & Context Recognition, Working Memory, Adaptive Error Correction
L3: Self-reflection, Social Reasoning & Theory of Mind",L3
SWE-Lancer,https://openai.com/index/swe-lancer/,https://arxiv.org/abs/2502.12115,"SWE-Lancer measures autonomous software engineering ability on tasks resembling contract/freelance work, often requiring understanding requirements, making multi-file changes, and delivering a working solution. It emphasizes end-to-end task completion and practical engineering judgment beyond isolated bug fixes.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Working Memory, Adaptive Error Correction, Semantic Understanding & Context Recognition
L3: Self-reflection",L3
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks tests long-context reasoning by requiring models to traverse graph structures described in text and answer queries about reachability, parent pointers, or paths. It stresses consistent stepwise tracking of states over many hops and resistance to distraction in large inputs.","L1: Language Comprehension
L2: Spatial Representation & Mapping, Working Memory, Attention, Logical Reasoning, Planning
L3: Inhibitory Control",L3
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon evaluates how reliably models can solve tasks by selecting and composing tools across multi-step workflows, often under time/interaction constraints and with noisy tool outputs. It targets practical orchestration: choosing the right tool, calling it correctly, and integrating results into a coherent final response.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Working Memory, Adaptive Error Correction, Semantic Understanding & Context Recognition
L3: Inhibitory Control",L3
HMMT,https://matharena.ai/?view=problem&comp=hmmt--hmmt_feb_2025,https://arxiv.org/abs/2505.23281,"HMMT problems are high-difficulty contest mathematics questions that typically require creative insights, multi-step proofs/derivations, and careful case analysis. As an evaluation, it targets advanced mathematical reasoning and the ability to maintain correctness over long solution chains.","L1: 
L2: Logical Reasoning, Working Memory, Planning, Attention, Adaptive Error Correction
L3: Cognitive Flexibility",L3
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath is an expert-level mathematics benchmark designed to measure progress on difficult, research-adjacent math problems across tiers of difficulty. It emphasizes deep multi-step reasoning, rigorous symbolic/quantitative accuracy, and sustained coherence on problems that resist short heuristic solutions.","L1: 
L2: Logical Reasoning, Working Memory, Planning, Attention, Adaptive Error Correction
L3: Cognitive Flexibility",L3
