Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 evaluates agentic software engineering in real command-line environments, where models must navigate a filesystem, run programs, inspect outputs, and iteratively fix issues. Success requires chaining tool actions (shell commands) with reasoning to reach a correct end state rather than producing a single answer.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory
L3: ",L2
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp evaluates long-horizon information seeking and “deep research” behavior over a constrained document collection, emphasizing reproducible retrieval and synthesis. Models must plan search steps, gather evidence across documents, and produce grounded final answers from what they found.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Attention, Working Memory, Episodic Memory, Semantic Understanding & Context Recognition
L3: ",L2
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld measures autonomous computer use in realistic desktop operating-system tasks (e.g., navigating apps and settings) under step limits. Models must interpret screenshots/GUI state, decide actions (click/type/scroll), and recover from mistakes as the environment changes.","L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Sensorimotor Coordination, Planning, Decision-making, Adaptive Error Correction, Working Memory
L3: ",L2
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI is a fluid-reasoning benchmark of abstract grid transformation puzzles where only a few examples are provided per task. Models must infer hidden rules, generalize to a new input, and avoid overfitting to superficial patterns.","L1: Visual Perception
L2: Logical Reasoning, Working Memory, Scene Understanding & Visual Reasoning, Spatial Representation & Mapping
L3: Cognitive Flexibility",L3
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon autonomous decision-making by having an agent run a simulated vending-machine business over an extended period. High scores require coherent multi-step strategy, adapting to market dynamics, and managing budgets and inventory over many turns.","L1: 
L2: Planning, Decision-making, Reward Mechanisms, Working Memory, Semantic Understanding & Context Recognition, Adaptive Error Correction
L3: ",L2
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity agents on tasks such as identifying known vulnerabilities from descriptions and discovering new vulnerabilities in real-world codebases. Models must reason about programs and exploit conditions, use tools to inspect code, and iteratively validate findings.","L1: Language Comprehension, Language Production
L2: Logical Reasoning, Planning, Decision-making, Adaptive Error Correction, Working Memory
L3: ",L2
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"SpreadsheetBench tests an agent’s ability to navigate, edit, and compute over real spreadsheet files using programmatic or application-like tooling. Tasks include extracting values, applying formulas, restructuring tables, and producing correct outputs under realistic constraints.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Working Memory, Adaptive Error Correction
L3: ",L2
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a challenging multimodal benchmark intended to probe frontier academic knowledge and reasoning across diverse subjects. Questions often require integrating domain knowledge with multi-step inference, and in some settings may involve tool use such as search or code to support analysis.","L1: Language Comprehension, Language Production, Visual Perception
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Multisensory Integration
L3: ",L2
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,GPQA Diamond is a high-quality subset of extremely difficult graduate-level science multiple-choice questions designed to be resistant to shallow pattern matching. It probes deep conceptual understanding and careful reasoning under ambiguity and distractors.,"L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Decision-making
L3: ",L2
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro evaluates multimodal understanding and reasoning over expert-level questions that require interpreting images (e.g., diagrams, charts, figures) alongside text. It emphasizes cross-domain reasoning where visual evidence must be combined with written context to select or produce the correct answer.","L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Working Memory, Decision-making
L3: ",L2
MathArena Apex,https://matharena.ai/?view=problem&comp=apex--apex_2025,https://arxiv.org/abs/2505.23281,MathArena Apex aggregates difficult competition-style mathematics problems and evaluates models’ ability to solve them reliably under standardized conditions. It emphasizes multi-step symbolic reasoning and maintaining long derivations without losing intermediate constraints.,"L1: 
L2: Logical Reasoning, Working Memory, Planning, Adaptive Error Correction
L3: Cognitive Flexibility",L3
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning evaluates reasoning over scientific paper figures and related context, often requiring quantitative interpretation and structured analysis. Models must extract relevant visual evidence from complex plots and connect it to the question to derive correct conclusions.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Multisensory Integration, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Attention
L3: ",L2
OmniDocBench 1.5,https://github.com/opendatalab/OmniDocBench,https://arxiv.org/abs/2412.07626,"OmniDocBench 1.5 evaluates document understanding and OCR across heterogeneous document elements such as text blocks, formulas, tables, and reading order. It targets faithful extraction/structuring of information from visually formatted pages rather than free-form generation.","L1: Visual Perception, Language Comprehension
L2: Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Working Memory
L3: ",L2
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU extends multimodal understanding to video, where questions depend on events unfolding across time. Models must track temporal changes, integrate cues across frames, and answer questions that require summarizing or reasoning about sequences.","L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Working Memory, Attention, Multisensory Integration
L3: Cognitive Timing & Predictive Modeling",L3
LiveCodeBench Pro,https://livecodebenchpro.com/projects/livecodebench-pro/leaderboard,https://arxiv.org/abs/2506.11928,"LiveCodeBench Pro evaluates coding ability in settings closer to real development, including multi-step problem solving and iterative refinement under time/interaction constraints. It emphasizes producing correct, executable code and debugging failures when tests or executions reveal issues.","L1: Language Production
L2: Planning, Logical Reasoning, Adaptive Error Correction, Working Memory, Decision-making
L3: ",L2
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"The FACTS Benchmark Suite systematically evaluates factuality and grounding, measuring whether model outputs remain consistent with provided evidence and avoid unsupported claims. It targets hallucination-related failure modes across multiple task formats and domains.","L1: Language Comprehension, Language Production
L2: Semantic Understanding & Context Recognition, Working Memory
L3: Inhibitory Control, Self-reflection",L3
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA evaluates practical physical commonsense reasoning across many languages and locales, focusing on understanding everyday situations and plausible actions or outcomes. It probes whether models can generalize “how the world works” beyond English-centric data and phrasing.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Spatial Representation & Mapping, Decision-making
L3: Cognitive Flexibility",L3
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) evaluates long-context retrieval and multi-round coreference resolution by embedding multiple similar “needle” interactions inside a long “haystack” conversation or document. The model must attend to the correct instance and reproduce the corresponding answer, testing robustness to interference.","L1: Language Comprehension
L2: Working Memory, Attention, Episodic Memory, Adaptive Error Correction
L3: ",L2
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval measures performance on well-specified professional knowledge-work tasks across many occupations, judged by expert human raters via pairwise comparisons. Tasks emphasize producing usable work products (e.g., plans, spreadsheets, presentations) under real-world constraints and quality standards.","L1: Language Production
L2: Planning, Decision-making, Semantic Understanding & Context Recognition, Working Memory
L3: Social Reasoning & Theory of Mind, Self-reflection",L3
SWE-Lancer,https://openai.com/index/swe-lancer/,https://arxiv.org/abs/2502.12115,"SWE-Lancer evaluates agentic software engineering on realistic repository-level tasks that require understanding existing code, making correct edits, and producing patches that satisfy requirements. It emphasizes end-to-end problem solving: triage, implementation, and verification against expected behavior.","L1: Language Comprehension, Language Production
L2: Planning, Logical Reasoning, Adaptive Error Correction, Working Memory, Decision-making
L3: ",L2
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks evaluates structured reasoning over graph traversal problems described in text, where models must simulate multi-step walks, parent/neighbor relations, or BFS-like procedures. It stresses precise state tracking across long sequences and resistance to distractor structure.","L1: 
L2: Spatial Representation & Mapping, Working Memory, Logical Reasoning, Attention
L3: Cognitive Flexibility",L3
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon evaluates tool-using agents on tasks requiring selecting appropriate tools, forming correct calls, handling tool errors, and synthesizing results into final answers. It focuses on reliability of multi-tool workflows rather than single-turn question answering.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory
L3: ",L2
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath evaluates advanced mathematics at the frontier of current model capability, emphasizing expert-level problem solving with high novelty and difficulty. It probes sustained multi-step reasoning, formal manipulation, and error checking, often benefiting from scratch-work or tool-assisted computation.","L1: 
L2: Logical Reasoning, Working Memory, Planning, Adaptive Error Correction
L3: Cognitive Flexibility",L3
