Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates models on real GitHub issues in Python repositories, where the system must generate a code patch that makes the test suite pass. The “Verified” subset uses human-validated tasks to reduce ambiguity and ensure solvability, emphasizing realistic debugging and implementation under repository constraints.","L1: 
L2: Logical Reasoning, Planning, Working Memory, Adaptive Error Correction, Decision-making, Semantic Understanding & Context Recognition
L3: ",L2
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro is a harder, more contamination-resistant software engineering benchmark spanning multiple programming languages and more complex issue types than SWE-bench Verified. Systems must understand large codebases, localize faults or missing features, implement patches, and validate results via tests in an agent-like loop.","L1: 
L2: Logical Reasoning, Planning, Working Memory, Adaptive Error Correction, Decision-making, Semantic Understanding & Context Recognition
L3: ",L2
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp measures deep-research agent capability on information-seeking questions that require searching, reading, and synthesizing evidence from many sources. It stresses query formulation, evidence tracking, and producing a grounded final answer rather than a single-step recall response.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Working Memory, Attention, Semantic Understanding & Context Recognition
L3: ",L2
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench evaluates interactive customer-support agents in simulated domains (e.g., retail, airline, telecom) that must follow policies while using tools/APIs across multi-turn dialogues. It probes robustness to user behavior, long-horizon task completion, and adherence to constraints under conversational pressure.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Working Memory
L3: Inhibitory Control, Social Reasoning & Theory of Mind",L3
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI (Abstraction and Reasoning Corpus) tests “fluid” pattern reasoning on novel grid-based tasks with only a few examples per task. Success requires inferring hidden rules, generalizing to new inputs, and avoiding overfitting to superficial cues.","L1: 
L2: Logical Reasoning, Working Memory, Attention, Spatial Representation & Mapping
L3: Cognitive Flexibility",L3
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas evaluates real-world tool use via the Model Context Protocol, requiring models to discover tools, call them correctly, handle errors, and compose multi-step workflows over authentic APIs. It emphasizes reliability in structured tool invocation and synthesis of tool outputs into correct final responses.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Working Memory, Adaptive Error Correction
L3: Inhibitory Control",L3
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a broad, frontier-level benchmark of difficult questions spanning advanced academic and real-world topics, often including multimodal inputs. It targets integrated reasoning, knowledge synthesis, and producing coherent, justifiable answers under uncertainty.","L1: Language Comprehension, Language Production, Visual Perception
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Scene Understanding & Visual Reasoning
L3: ",L2
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 evaluates competition-style mathematics problem solving drawn from the American Invitational Mathematics Examination. Problems typically require multi-step derivations, careful case analysis, and precise final numeric answers rather than proof writing.","L1: 
L2: Logical Reasoning, Working Memory, Planning, Attention
L3: ",L2
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a subset of the GPQA benchmark consisting of particularly difficult, graduate-level multiple-choice questions in physics, chemistry, and biology designed to be “Google-proof.” It emphasizes deep domain understanding and reasoning over memorized trivia.","L1: Language Comprehension
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory
L3: ",L2
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends MMLU to multiple languages, testing breadth of academic knowledge and reasoning across many subjects using standardized questions. It probes whether competence transfers across languages and scripts rather than being limited to English-only instruction tuning.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning
L3: ",L2
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro is a challenging multimodal benchmark spanning many disciplines where models must answer questions grounded in images (e.g., diagrams, charts, photos) plus text. It focuses on expert-level multimodal understanding and reasoning with reduced shortcut opportunities compared to earlier MMMU settings.","L1: Visual Perception, Language Comprehension
L2: Multisensory Integration, Scene Understanding & Visual Reasoning, Logical Reasoning, Working Memory
L3: ",L2
ScreenShot-Pro,https://gui-agent.github.io/grounding-leaderboard/,https://arxiv.org/abs/2504.07981,"ScreenShot-Pro evaluates screenshot understanding for professional and software-UI contexts, where models must interpret high-resolution interfaces and answer questions that depend on precise visual grounding. It stresses layout-sensitive perception, recognizing UI affordances, and mapping spatial relationships to correct answers.","L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Spatial Representation & Mapping, Working Memory
L3: ",L2
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning tests reasoning over scientific figures and visual elements from research papers, often requiring quantitative interpretation of plots and diagrammatic relationships. Many setups allow tool use (e.g., Python) to support measurement or calculation grounded in the figure content.","L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Logical Reasoning, Working Memory, Semantic Understanding & Context Recognition
L3: ",L2
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU evaluates multimodal understanding and reasoning over video, requiring integration of information across frames and time. Tasks emphasize tracking events, temporal relations, and higher-level interpretation of dynamic scenes.","L1: Visual Perception
L2: Working Memory, Scene Understanding & Visual Reasoning, Attention
L3: Cognitive Timing & Predictive Modeling",L3
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) is a long-context evaluation where multiple near-duplicate “needle” interactions are embedded in a large “haystack,” and the model must retrieve the correct referenced response. It stresses robust cross-reference resolution and resistance to distractors over very long contexts.","L1: 
L2: Working Memory, Attention, Semantic Understanding & Context Recognition, Episodic Memory, Logical Reasoning
L3: ",L2
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval evaluates well-specified professional knowledge-work tasks across many occupations, with outputs judged against expert professional performance (often via pairwise comparisons). Tasks commonly require producing real work artifacts (e.g., slides, spreadsheets, plans) with correct structure and practical decision quality.","L1: Language Production, Language Comprehension
L2: Planning, Decision-making, Semantic Understanding & Context Recognition, Working Memory
L3: ",L2
SWE-Lancer,https://openai.com/index/swe-lancer/,https://arxiv.org/abs/2502.12115,"SWE-Lancer evaluates software engineering on more open-ended, agentic work patterns such as implementing substantial changes, coordinating fixes across files, and meeting broader product requirements. It emphasizes end-to-end engineering competence beyond isolated bug fixes, including iterative refinement.","L1: 
L2: Planning, Logical Reasoning, Working Memory, Adaptive Error Correction, Decision-making, Semantic Understanding & Context Recognition
L3: ",L2
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks tests long-context relational reasoning by requiring models to traverse and query graph-structured information encoded in text (e.g., follow edges, find parents, compute BFS-like results). It stresses systematic multi-step traversal without losing track of intermediate nodes and constraints.","L1: 
L2: Spatial Representation & Mapping, Working Memory, Logical Reasoning, Attention, Planning
L3: ",L2
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon evaluates agentic tool use across diverse tools and multi-step tasks, emphasizing correct tool selection, parameterization, and composition under time and context constraints. It targets robust execution behavior, including recovering from tool errors and maintaining task state across steps.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Working Memory, Adaptive Error Correction
L3: Inhibitory Control",L3
HMMT,https://matharena.ai/?view=problem&comp=hmmt--hmmt_feb_2025,https://arxiv.org/abs/2505.23281,"HMMT evaluates advanced competition mathematics (from the Harvard-MIT Mathematics Tournament), typically featuring harder, multi-stage reasoning than standard high-school contests. Problems often require creative decomposition, careful algebraic/combintorial structure tracking, and precise outputs.","L1: 
L2: Logical Reasoning, Working Memory, Planning, Attention
L3: Cognitive Flexibility",L3
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath is a set of expert-level mathematics problems designed to measure progress toward research-grade mathematical reasoning, often with tiers reflecting increasing difficulty. It stresses deep multi-step derivations, selecting appropriate strategies under uncertainty, and maintaining correctness across long solution chains.","L1: 
L2: Logical Reasoning, Working Memory, Planning, Attention
L3: Cognitive Flexibility",L3
