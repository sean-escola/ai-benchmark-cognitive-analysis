Benchmark,Website,Paper,Description,Cognitive Functions
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 evaluates agentic software engineering in real command-line environments, where models must navigate a filesystem, run programs, inspect outputs, and iteratively fix issues. Success requires chaining tool actions (shell commands) with reasoning to reach a correct end state rather than producing a single answer.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Language Comprehension, Language Production"
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp evaluates long-horizon information seeking and “deep research” behavior over a constrained document collection, emphasizing reproducible retrieval and synthesis. Models must plan search steps, gather evidence across documents, and produce grounded final answers from what they found.","Planning, Decision-making, Attention, Working Memory, Episodic Memory, Semantic Understanding & Context Recognition, Language Comprehension, Language Production"
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld measures autonomous computer use in realistic desktop operating-system tasks (e.g., navigating apps and settings) under step limits. Models must interpret screenshots/GUI state, decide actions (click/type/scroll), and recover from mistakes as the environment changes.","Visual Perception, Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Sensorimotor Coordination, Planning, Decision-making, Adaptive Error Correction, Working Memory, Language Comprehension"
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI is a fluid-reasoning benchmark of abstract grid transformation puzzles where only a few examples are provided per task. Models must infer hidden rules, generalize to a new input, and avoid overfitting to superficial patterns.","Logical Reasoning, Cognitive Flexibility, Working Memory, Visual Perception, Scene Understanding & Visual Reasoning, Spatial Representation & Mapping"
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon autonomous decision-making by having an agent run a simulated vending-machine business over an extended period. High scores require coherent multi-step strategy, adapting to market dynamics, and managing budgets and inventory over many turns.","Planning, Decision-making, Reward Mechanisms, Working Memory, Semantic Understanding & Context Recognition, Adaptive Error Correction"
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity agents on tasks such as identifying known vulnerabilities from descriptions and discovering new vulnerabilities in real-world codebases. Models must reason about programs and exploit conditions, use tools to inspect code, and iteratively validate findings.","Logical Reasoning, Planning, Decision-making, Adaptive Error Correction, Working Memory, Language Comprehension, Language Production"
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"SpreadsheetBench tests an agent’s ability to navigate, edit, and compute over real spreadsheet files using programmatic or application-like tooling. Tasks include extracting values, applying formulas, restructuring tables, and producing correct outputs under realistic constraints.","Planning, Decision-making, Working Memory, Adaptive Error Correction, Language Comprehension, Language Production"
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a challenging multimodal benchmark intended to probe frontier academic knowledge and reasoning across diverse subjects. Questions often require integrating domain knowledge with multi-step inference, and in some settings may involve tool use such as search or code to support analysis.","Language Comprehension, Language Production, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Multisensory Integration, Visual Perception"
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,GPQA Diamond is a high-quality subset of extremely difficult graduate-level science multiple-choice questions designed to be resistant to shallow pattern matching. It probes deep conceptual understanding and careful reasoning under ambiguity and distractors.,"Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Language Comprehension, Decision-making"
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro evaluates multimodal understanding and reasoning over expert-level questions that require interpreting images (e.g., diagrams, charts, figures) alongside text. It emphasizes cross-domain reasoning where visual evidence must be combined with written context to select or produce the correct answer.","Visual Perception, Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Working Memory, Language Comprehension, Decision-making"
MathArena Apex,https://matharena.ai/?view=problem&comp=apex--apex_2025,https://arxiv.org/abs/2505.23281,MathArena Apex aggregates difficult competition-style mathematics problems and evaluates models’ ability to solve them reliably under standardized conditions. It emphasizes multi-step symbolic reasoning and maintaining long derivations without losing intermediate constraints.,"Logical Reasoning, Working Memory, Planning, Cognitive Flexibility, Adaptive Error Correction"
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning evaluates reasoning over scientific paper figures and related context, often requiring quantitative interpretation and structured analysis. Models must extract relevant visual evidence from complex plots and connect it to the question to derive correct conclusions.","Visual Perception, Scene Understanding & Visual Reasoning, Multisensory Integration, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Attention"
OmniDocBench 1.5,https://github.com/opendatalab/OmniDocBench,https://arxiv.org/abs/2412.07626,"OmniDocBench 1.5 evaluates document understanding and OCR across heterogeneous document elements such as text blocks, formulas, tables, and reading order. It targets faithful extraction/structuring of information from visually formatted pages rather than free-form generation.","Visual Perception, Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Language Comprehension, Working Memory"
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU extends multimodal understanding to video, where questions depend on events unfolding across time. Models must track temporal changes, integrate cues across frames, and answer questions that require summarizing or reasoning about sequences.","Visual Perception, Scene Understanding & Visual Reasoning, Cognitive Timing & Predictive Modeling, Working Memory, Attention, Multisensory Integration, Language Comprehension"
LiveCodeBench Pro,https://livecodebenchpro.com/projects/livecodebench-pro/leaderboard,https://arxiv.org/abs/2506.11928,"LiveCodeBench Pro evaluates coding ability in settings closer to real development, including multi-step problem solving and iterative refinement under time/interaction constraints. It emphasizes producing correct, executable code and debugging failures when tests or executions reveal issues.","Planning, Logical Reasoning, Adaptive Error Correction, Working Memory, Language Production, Decision-making"
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"The FACTS Benchmark Suite systematically evaluates factuality and grounding, measuring whether model outputs remain consistent with provided evidence and avoid unsupported claims. It targets hallucination-related failure modes across multiple task formats and domains.","Semantic Understanding & Context Recognition, Inhibitory Control, Self-reflection, Working Memory, Language Comprehension, Language Production"
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA evaluates practical physical commonsense reasoning across many languages and locales, focusing on understanding everyday situations and plausible actions or outcomes. It probes whether models can generalize “how the world works” beyond English-centric data and phrasing.","Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning, Cognitive Flexibility, Spatial Representation & Mapping, Decision-making"
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) evaluates long-context retrieval and multi-round coreference resolution by embedding multiple similar “needle” interactions inside a long “haystack” conversation or document. The model must attend to the correct instance and reproduce the corresponding answer, testing robustness to interference.","Working Memory, Attention, Episodic Memory, Language Comprehension, Adaptive Error Correction"
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval measures performance on well-specified professional knowledge-work tasks across many occupations, judged by expert human raters via pairwise comparisons. Tasks emphasize producing usable work products (e.g., plans, spreadsheets, presentations) under real-world constraints and quality standards.","Planning, Decision-making, Language Production, Semantic Understanding & Context Recognition, Working Memory, Social Reasoning & Theory of Mind, Self-reflection"
SWE-Lancer,https://openai.com/index/swe-lancer/,https://arxiv.org/abs/2502.12115,"SWE-Lancer evaluates agentic software engineering on realistic repository-level tasks that require understanding existing code, making correct edits, and producing patches that satisfy requirements. It emphasizes end-to-end problem solving: triage, implementation, and verification against expected behavior.","Planning, Logical Reasoning, Adaptive Error Correction, Working Memory, Decision-making, Language Comprehension, Language Production"
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks evaluates structured reasoning over graph traversal problems described in text, where models must simulate multi-step walks, parent/neighbor relations, or BFS-like procedures. It stresses precise state tracking across long sequences and resistance to distractor structure.","Spatial Representation & Mapping, Working Memory, Logical Reasoning, Attention, Cognitive Flexibility"
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon evaluates tool-using agents on tasks requiring selecting appropriate tools, forming correct calls, handling tool errors, and synthesizing results into final answers. It focuses on reliability of multi-tool workflows rather than single-turn question answering.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Language Comprehension, Language Production"
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath evaluates advanced mathematics at the frontier of current model capability, emphasizing expert-level problem solving with high novelty and difficulty. It probes sustained multi-step reasoning, formal manipulation, and error checking, often benefiting from scratch-work or tool-assisted computation.","Logical Reasoning, Working Memory, Planning, Cognitive Flexibility, Adaptive Error Correction"
