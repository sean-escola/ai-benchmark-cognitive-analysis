Benchmark,Website,Paper,Description,Cognitive Functions
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 evaluates agentic coding and systems tasks that must be completed inside a real command-line environment (e.g., using shell tools, editing files, running programs, and debugging). Success typically requires multi-step execution, interpreting tool outputs/errors, and iteratively refining commands to reach a verifiable end state.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Language Comprehension, Language Production"
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp evaluates “deep research” style question answering where a model must search, read, and synthesize information from a controlled web/document corpus. It emphasizes evidence gathering across multiple sources and producing a final answer that aligns with retrieved support under limited attempts.","Planning, Attention, Working Memory, Semantic Understanding & Context Recognition, Language Comprehension, Language Production, Decision-making"
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld benchmarks computer-use agents operating in interactive operating-system environments, requiring them to navigate GUIs, understand on-screen state, and execute sequences of actions to complete tasks. It tests robustness to UI variability, long-horizon task execution, and recovery from mistakes within a multimodal loop.","Visual Perception, Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Planning, Decision-making, Sensorimotor Coordination, Adaptive Error Correction, Working Memory"
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,ARC-AGI measures fluid reasoning on novel grid-based tasks where the model must infer a latent rule from a few demonstrations and apply it to new inputs. The benchmark is designed to reduce reliance on memorized knowledge and emphasize generalization to unfamiliar transformations.,"Logical Reasoning, Working Memory, Cognitive Flexibility, Planning, Attention"
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon autonomous agent performance in a simulated vending-machine business over many decision steps, scored by final economic outcomes. It requires maintaining coherent goals, adapting to changing conditions, and making strategic choices across extended time horizons.","Planning, Decision-making, Reward Mechanisms, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition"
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity agents on tasks such as finding known vulnerabilities in real open-source projects and discovering previously unknown weaknesses. Solving tasks typically requires reasoning over codebases, forming hypotheses about exploitability, and iteratively testing or refining approaches.","Logical Reasoning, Planning, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition, Attention"
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"SpreadsheetBench tests an agent’s ability to read, modify, and compute over complex spreadsheets using realistic artifacts and operations (e.g., formulas, tables, formatting, multi-sheet dependencies). It emphasizes structured manipulation, error detection, and producing correct final spreadsheet outputs.","Logical Reasoning, Working Memory, Planning, Adaptive Error Correction, Attention, Semantic Understanding & Context Recognition"
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam (HLE) is a difficult, frontier-style benchmark spanning advanced knowledge and reasoning, including multimodal questions, intended to stress-test models near the edge of current capabilities. It emphasizes integrating domain knowledge with multi-step reasoning under constrained evaluation settings.","Language Comprehension, Language Production, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Visual Perception, Scene Understanding & Visual Reasoning"
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a subset of exceptionally challenging graduate-level multiple-choice science questions designed to be “Google-proof” and to separate expert-level reasoning from shallow pattern matching. It stresses careful reading, scientific domain knowledge, and multi-step inference.","Language Comprehension, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Decision-making"
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro is a high-difficulty multimodal benchmark covering many academic subjects, where models must answer questions grounded in images (e.g., diagrams, charts, figures) plus text. It emphasizes expert-level visual understanding, cross-referencing visual evidence with domain concepts, and selecting correct answers.","Visual Perception, Scene Understanding & Visual Reasoning, Multisensory Integration, Visual Attention & Eye Movements, Language Comprehension, Logical Reasoning, Working Memory"
MathArena Apex,https://matharena.ai/?view=problem&comp=apex--apex_2025,https://arxiv.org/abs/2505.23281,"MathArena Apex evaluates advanced mathematical problem solving under competitive-style conditions, typically emphasizing correctness on hard questions rather than verbosity. It probes multi-step derivations, strategy selection, and reliability across diverse topics.","Logical Reasoning, Planning, Working Memory, Cognitive Flexibility, Adaptive Error Correction"
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning evaluates reasoning over scientific figures from research papers, requiring models to interpret plots/diagrams and answer questions that depend on fine-grained visual evidence. It stresses combining technical language with visual inference and quantitative/relational reasoning.","Visual Perception, Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Language Comprehension, Logical Reasoning, Working Memory, Semantic Understanding & Context Recognition"
OmniDocBench 1.5,https://github.com/opendatalab/OmniDocBench,https://arxiv.org/abs/2412.07626,"OmniDocBench 1.5 evaluates document understanding and OCR-style extraction across diverse page elements such as text, formulas, tables, and reading order. It measures whether models can accurately parse layout and recover structured content from complex documents.","Visual Perception, Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Language Comprehension, Semantic Understanding & Context Recognition, Working Memory"
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU benchmarks multimodal understanding and reasoning over video, requiring models to integrate information across frames and align it with textual questions. It emphasizes temporal comprehension, event reasoning, and maintaining relevant context over extended clips.","Visual Perception, Scene Understanding & Visual Reasoning, Cognitive Timing & Predictive Modeling, Working Memory, Attention, Language Comprehension, Multisensory Integration"
LiveCodeBench Pro,https://livecodebenchpro.com/projects/livecodebench-pro/leaderboard,https://arxiv.org/abs/2506.11928,"LiveCodeBench Pro evaluates coding capability on fresh or continuously updated programming tasks, often designed to reduce contamination and measure real-time generalization. It typically requires writing correct programs under constraints and debugging based on failures or edge cases.","Logical Reasoning, Planning, Working Memory, Adaptive Error Correction, Language Comprehension, Language Production"
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"FACTS Benchmark Suite systematically evaluates factuality-related behaviors, including whether a model’s outputs remain accurate, grounded, and resistant to hallucination across different settings. It emphasizes truthfulness under uncertainty and the ability to avoid fabricating unsupported details.","Semantic Understanding & Context Recognition, Self-reflection, Inhibitory Control, Working Memory, Language Comprehension, Language Production"
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA evaluates practical reasoning and question answering across many languages and locales, focusing on whether models can handle globally diverse contexts rather than only English-centric distributions. It probes robustness of commonsense/pragmatic inference and cross-lingual generalization.","Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning, Social Reasoning & Theory of Mind, Working Memory"
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) is a long-context evaluation that hides multiple similar “needle” references across lengthy multi-turn text and asks the model to retrieve the correct corresponding content. It stresses precise coreference tracking, interference resistance, and accurate recall under extreme context length.","Working Memory, Episodic Memory, Attention, Language Comprehension, Inhibitory Control"
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval evaluates well-specified professional knowledge-work tasks across many occupations, judged by expert humans against real deliverable quality (e.g., spreadsheets, presentations, plans). It emphasizes end-to-end task execution: interpreting requirements, producing polished artifacts, and making sound decisions under constraints.","Planning, Decision-making, Language Production, Semantic Understanding & Context Recognition, Working Memory, Adaptive Error Correction, Social Reasoning & Theory of Mind"
SWE-Lancer,https://openai.com/index/swe-lancer/,https://arxiv.org/abs/2502.12115,"SWE-Lancer evaluates software engineering performance on realistic tasks that require understanding codebases, implementing fixes or features, and producing patches that satisfy tests or specifications. It targets reliability in longer workflows, including diagnosing failures and iterating toward correct solutions.","Logical Reasoning, Planning, Adaptive Error Correction, Working Memory, Language Comprehension, Language Production"
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks evaluates algorithmic and structured reasoning by embedding graph problems in text and asking models to perform traversals or identify relationships (e.g., BFS-style reachability or parent links). It stresses consistent multi-step state tracking as the model “walks” through a graph representation.","Spatial Representation & Mapping, Working Memory, Logical Reasoning, Attention, Planning"
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon evaluates tool-using agents on tasks that require selecting appropriate tools, invoking them correctly, handling tool errors, and integrating results into final answers. It emphasizes orchestration across multiple tools and maintaining coherent progress toward a goal.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Attention, Language Comprehension, Language Production"
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath evaluates expert-level mathematics, including problems intended to be beyond routine competition math and closer to research-grade difficulty. It stresses deep multi-step reasoning, creativity in proof/derivation strategy, and robustness against subtle errors.","Logical Reasoning, Planning, Working Memory, Cognitive Flexibility, Adaptive Error Correction"
