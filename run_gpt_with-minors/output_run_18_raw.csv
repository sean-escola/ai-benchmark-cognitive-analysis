Benchmark,Website,Paper,Description,Cognitive Functions
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates an LLM’s ability to resolve real GitHub issues by generating patches that make a project’s tests pass. The “Verified” variant focuses on tasks that have been human-validated as solvable, emphasizing end-to-end debugging, code editing, and adherence to repository context under a fixed harness.","Planning, Logical Reasoning, Working Memory, Adaptive Error Correction, Language Comprehension, Language Production"
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro is a larger, harder software engineering benchmark designed to be more industrially representative and contamination-resistant, spanning multiple languages and more complex repositories. It tests whether models can reliably diagnose failures, implement correct fixes, and maintain robustness across diverse codebases and issue types.","Planning, Logical Reasoning, Working Memory, Adaptive Error Correction, Cognitive Flexibility, Language Comprehension, Language Production"
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp evaluates “deep research” agents on hard information-seeking questions where answers require multi-step web browsing rather than recalling facts. It measures an agent’s ability to plan a search strategy, extract and reconcile evidence across sources, and produce grounded final answers under tool-use constraints.","Planning, Decision-making, Attention, Working Memory, Episodic Memory, Semantic Understanding & Context Recognition, Language Comprehension, Language Production"
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench measures agentic performance in multi-turn customer-support-style environments where the agent must interact with simulated users and programmatic APIs while following domain policies. It emphasizes consistent policy adherence, correct tool invocation, and recovery from conversational or procedural deviations over long interaction horizons.","Social Reasoning & Theory of Mind, Decision-making, Planning, Inhibitory Control, Reward Mechanisms, Working Memory, Language Comprehension, Language Production, Adaptive Error Correction"
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI tests fluid reasoning by asking models to infer the hidden transformation rule from a few input-output grid examples and apply it to a new grid. Problems are intentionally novel and low-data, rewarding abstraction, compositional pattern discovery, and generalization beyond memorized templates.","Logical Reasoning, Cognitive Flexibility, Working Memory, Attention, Spatial Representation & Mapping, Scene Understanding & Visual Reasoning"
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas evaluates real-world tool use through Model Context Protocol (MCP) servers, requiring models to discover tools, call them correctly, handle errors/retries, and integrate results. It targets multi-step workflow execution with authentic API interactions and realistic failure modes.","Planning, Decision-making, Working Memory, Adaptive Error Correction, Inhibitory Control, Language Comprehension, Language Production"
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a broad, frontier-level benchmark spanning difficult questions (often multimodal) across many domains, intended to stress advanced reasoning and knowledge integration. It is commonly run both with and without tools (e.g., search/code) to separate intrinsic reasoning from tool-augmented problem solving.","Language Comprehension, Language Production, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Multisensory Integration, Planning"
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 consists of competition mathematics problems that demand multi-step derivations, careful case analysis, and precise final answers. It is typically used to measure structured mathematical reasoning and reliability under time-tested contest problem styles.","Logical Reasoning, Working Memory, Planning, Adaptive Error Correction"
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a curated subset of very challenging graduate-level science multiple-choice questions designed to be difficult to answer via superficial pattern matching. It emphasizes deep conceptual understanding and multi-step scientific reasoning across physics, chemistry, and biology.","Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Language Comprehension"
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,MMMLU is a multilingual extension of broad academic knowledge and reasoning tests across many subjects and languages. It evaluates whether models can maintain consistent understanding and inference across linguistic variation and culturally diverse phrasing.,"Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Cognitive Flexibility"
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro is a high-difficulty multimodal benchmark that requires models to answer expert-level questions grounded in images and accompanying text. It stresses cross-modal reasoning over figures, diagrams, tables, and real-world visuals in addition to domain knowledge.","Visual Perception, Scene Understanding & Visual Reasoning, Multisensory Integration, Working Memory, Attention, Language Comprehension, Logical Reasoning"
ScreenShot-Pro,https://gui-agent.github.io/grounding-leaderboard/,https://arxiv.org/abs/2504.07981,"ScreenShot-Pro evaluates screenshot understanding for real software interfaces, asking models to reason about UI state and answer questions that depend on precise visual grounding (often at high resolution). It is frequently used as a proxy for how well models can support computer-use agents that must interpret and act on GUIs.","Visual Perception, Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Spatial Representation & Mapping, Decision-making, Planning, Sensorimotor Coordination, Working Memory"
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning focuses on reasoning over scientific paper figures (e.g., plots, diagrams, and annotated visuals) and associated context to answer questions that require interpretation rather than OCR alone. It targets robust visual-scientific inference, including extracting trends, comparing conditions, and linking figure evidence to claims.","Visual Perception, Scene Understanding & Visual Reasoning, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Attention, Language Comprehension"
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU extends multimodal understanding to video, requiring temporal integration of events and visual cues to answer questions about dynamic scenes. It measures the ability to attend over time, maintain relevant context, and perform causal/temporal reasoning from sequences.","Visual Perception, Attention, Working Memory, Cognitive Timing & Predictive Modeling, Scene Understanding & Visual Reasoning, Multisensory Integration, Language Comprehension"
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) tests long-context multi-round coreference/recall by inserting multiple similar “needle” interactions into long “haystacks” and asking the model to reproduce the correct referenced response. The 8-needle setting is especially demanding on tracking, disambiguation, and maintaining fidelity over long contexts.","Working Memory, Attention, Episodic Memory, Semantic Understanding & Context Recognition, Adaptive Error Correction"
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval evaluates economically valuable, well-specified professional work across many occupations, where models must produce realistic work artifacts (e.g., plans, analyses, presentations/spreadsheets) judged against human professionals. It emphasizes following instructions, producing structured deliverables, and making correct tradeoffs under real workplace constraints.","Planning, Decision-making, Language Production, Language Comprehension, Semantic Understanding & Context Recognition, Working Memory, Self-reflection, Social Reasoning & Theory of Mind"
SWE-Lancer,https://openai.com/index/swe-lancer/,https://arxiv.org/abs/2502.12115,"SWE-Lancer evaluates software engineering in a “freelance task” style, where models must interpret requirements, modify codebases, and deliver solutions that satisfy tests or acceptance criteria. It emphasizes end-to-end execution quality, iterative debugging, and aligning implementation details with user intent under realistic constraints.","Planning, Decision-making, Logical Reasoning, Working Memory, Adaptive Error Correction, Cognitive Flexibility, Language Comprehension, Language Production"
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks evaluates long-context reasoning over graph-structured data by requiring models to follow traversal rules and compute properties such as reachability or parent relationships. It targets systematic step-by-step tracking over many nodes/edges, often stressing error accumulation and consistency.","Logical Reasoning, Working Memory, Attention, Spatial Representation & Mapping, Cognitive Flexibility, Adaptive Error Correction"
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon measures how well models can solve tasks by selecting and using tools across multi-step workflows, often with realistic API signatures and intermediate state. It stresses correct tool choice, argument construction, orchestration across calls, and robust recovery from tool or planning errors.","Planning, Decision-making, Working Memory, Adaptive Error Correction, Inhibitory Control, Language Comprehension, Language Production"
HMMT,https://matharena.ai/?view=problem&comp=hmmt--hmmt_feb_2025,https://arxiv.org/abs/2505.23281,"HMMT problems are high-school/early-undergraduate contest math tasks that demand careful multi-step reasoning, clever transformations, and precise answers. The benchmark is used to gauge mathematical problem-solving depth and reliability under challenging but well-defined constraints.","Logical Reasoning, Working Memory, Planning, Adaptive Error Correction"
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath is an expert-level mathematics benchmark intended to measure frontier reasoning on problems closer to research-style difficulty, often benefiting from tool-assisted computation or proof exploration. It emphasizes sustained rigor, deep abstraction, and minimizing subtle logical errors across long derivations.","Logical Reasoning, Working Memory, Planning, Cognitive Flexibility, Adaptive Error Correction"
