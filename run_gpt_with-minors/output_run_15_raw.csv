Benchmark,Website,Paper,Description,Cognitive Functions
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates a model’s ability to solve real GitHub issues by producing code patches that make the repository’s test suite pass. The “Verified” subset consists of problems that have been human-validated as solvable with the provided context and tests, emphasizing end-to-end debugging and implementation under realistic constraints.","Planning, Logical Reasoning, Working Memory, Adaptive Error Correction, Decision-making, Semantic Understanding & Context Recognition"
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro is a larger, more difficult software engineering benchmark intended to be more challenging and more contamination-resistant than SWE-bench Verified. It expands task diversity (including multiple programming languages) and emphasizes robust repository understanding, correct patch construction, and handling complex dependency and test failures.","Planning, Logical Reasoning, Working Memory, Adaptive Error Correction, Decision-making, Semantic Understanding & Context Recognition, Cognitive Flexibility"
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp measures an agent’s ability to answer difficult questions by searching and synthesizing information from the web (or a controlled web-like corpus) rather than relying purely on parametric memory. It targets deep research behaviors such as query formulation, source triangulation, and faithful citation-based answering.","Planning, Decision-making, Attention, Working Memory, Semantic Understanding & Context Recognition, Language Comprehension, Language Production, Self-reflection"
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench evaluates interactive customer-support agents operating in simulated domains (e.g., retail, airline, telecom) that require multi-turn dialogue plus tool/API use while complying with domain policies. It stresses consistent policy adherence, state tracking across turns, and reliable tool invocation to resolve user requests end-to-end.","Social Reasoning & Theory of Mind, Language Comprehension, Language Production, Planning, Decision-making, Working Memory, Inhibitory Control, Attention"
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI tests “fluid” abstract reasoning by asking models to infer transformations between input-output grid examples and apply the inferred rule to new inputs. Tasks are deliberately novel and sparse in supervision (few examples), emphasizing rapid rule induction rather than memorized patterns.","Logical Reasoning, Cognitive Flexibility, Working Memory, Attention, Spatial Representation & Mapping, Scene Understanding & Visual Reasoning"
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas assesses real-world tool use via the Model Context Protocol (MCP), where models must discover tools, call them correctly, handle errors, and compose multi-step workflows across services. It focuses on practical API orchestration and robustness in production-like tool environments.","Planning, Decision-making, Working Memory, Adaptive Error Correction, Semantic Understanding & Context Recognition, Inhibitory Control, Attention"
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam (HLE) is a frontier, multi-domain (often multimodal) benchmark intended to probe advanced reasoning and expert-level knowledge under challenging, long-form questions. It is designed to be difficult to answer from memorized snippets alone and often benefits from careful multi-step analysis and tool-assisted verification.","Logical Reasoning, Language Comprehension, Language Production, Working Memory, Semantic Understanding & Context Recognition, Planning, Decision-making, Scene Understanding & Visual Reasoning"
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 evaluates mathematical problem solving on competition-style questions that require multi-step derivations, careful case handling, and exact final answers. Performance reflects not just calculation skill but the ability to choose effective solution strategies under time/step constraints.","Logical Reasoning, Working Memory, Planning, Attention, Adaptive Error Correction"
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a hard multiple-choice science QA benchmark curated to be “Google-proof,” targeting questions that require deep conceptual understanding rather than shallow retrieval. The Diamond subset emphasizes high-quality, expert-validated items in physics, chemistry, and biology.","Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Attention, Language Comprehension, Decision-making"
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends the MMLU paradigm to multiple languages, testing broad academic knowledge and reasoning across many subjects in multilingual settings. It emphasizes cross-lingual comprehension, consistent reasoning, and reduced reliance on English-only priors.","Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Attention"
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro is a multimodal, multi-discipline benchmark where models answer expert-level questions grounded in images (e.g., diagrams, charts, scientific figures) plus text. The “Pro” variant is designed to be more challenging and to better test multimodal reasoning robustness.","Visual Perception, Scene Understanding & Visual Reasoning, Multisensory Integration, Language Comprehension, Logical Reasoning, Working Memory, Visual Attention & Eye Movements"
ScreenShot-Pro,https://gui-agent.github.io/grounding-leaderboard/,https://arxiv.org/abs/2504.07981,"ScreenShot-Pro evaluates whether models can understand high-resolution screenshots of software interfaces and answer questions or take grounded actions based on GUI content. It stresses fine-grained visual grounding (icons, menus, layout), spatial localization, and instruction-following in interface-centric tasks.","Visual Perception, Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Spatial Representation & Mapping, Language Comprehension, Planning, Decision-making"
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning tests reasoning over figures and content drawn from scientific papers (arXiv), often requiring interpreting plots, diagrams, and experimental descriptions. It measures the ability to connect textual scientific context with visual evidence and perform multi-step inference.","Scene Understanding & Visual Reasoning, Visual Perception, Language Comprehension, Logical Reasoning, Working Memory, Multisensory Integration, Attention"
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU extends multimodal understanding to the temporal domain, requiring models to answer questions about videos that involve events unfolding over time. It probes temporal integration, tracking entities/actions across frames, and combining visual and textual cues for reasoning.","Visual Perception, Scene Understanding & Visual Reasoning, Multisensory Integration, Working Memory, Attention, Cognitive Timing & Predictive Modeling, Language Comprehension"
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) is a long-context evaluation where multiple similar “needle” interactions are embedded in a large “haystack,” and the model must retrieve the correct referenced response. The 8-needle setting stresses robust context integration, interference resistance, and precise retrieval under long contexts.","Working Memory, Attention, Episodic Memory, Semantic Understanding & Context Recognition, Inhibitory Control"
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval evaluates well-specified professional knowledge-work tasks spanning many occupations, judged by expert humans via pairwise comparisons (e.g., creating spreadsheets, presentations, schedules, analyses). It emphasizes producing usable artifacts and making correct professional decisions under explicit constraints and objectives.","Planning, Decision-making, Language Production, Language Comprehension, Semantic Understanding & Context Recognition, Working Memory, Self-reflection, Adaptive Error Correction"
SWE-Lancer,https://openai.com/index/swe-lancer/,https://arxiv.org/abs/2502.12115,"SWE-Lancer evaluates software engineering agents on tasks resembling real contracting or freelance workflows, emphasizing end-to-end delivery of correct code changes in realistic repositories. It targets robustness to ambiguous requirements, iterative debugging, and producing patches that satisfy functional tests and constraints.","Planning, Decision-making, Logical Reasoning, Working Memory, Adaptive Error Correction, Semantic Understanding & Context Recognition, Cognitive Flexibility"
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks is a synthetic long-context reasoning benchmark where the model must follow paths or parent/neighbor relationships in large graphs described in text, often requiring multi-hop traversal. It targets structured retrieval and consistent multi-step reasoning under heavy distractors and long inputs.","Spatial Representation & Mapping, Working Memory, Attention, Logical Reasoning, Semantic Understanding & Context Recognition"
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon benchmarks tool-using agents on complex tasks requiring selecting, sequencing, and validating calls across multiple tools, often with partial failures and the need for retries. It emphasizes orchestration reliability, error recovery, and producing final answers grounded in tool outputs.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Attention, Inhibitory Control"
HMMT,https://matharena.ai/?view=problem&comp=hmmt--hmmt_feb_2025,https://arxiv.org/abs/2505.23281,"HMMT evaluates high-difficulty competition mathematics problems that often require creative insights, careful proof-like reasoning, and managing complex intermediate states. Compared to many exam-style sets, it tends to stress deeper strategy selection and multi-branch reasoning.","Logical Reasoning, Working Memory, Planning, Attention, Cognitive Flexibility"
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath is an advanced mathematics benchmark targeting expert-level problem solving, including problems intended to be beyond routine competition templates. It emphasizes sustained multi-step reasoning, rigorous manipulation of abstractions, and correctness under high difficulty (often with tool assistance in some settings).","Logical Reasoning, Working Memory, Planning, Attention, Cognitive Flexibility, Adaptive Error Correction"
