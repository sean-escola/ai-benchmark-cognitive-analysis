Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates software-engineering agents on a curated subset of real GitHub issues where humans verified that the tasks are solvable and the unit tests reliably check correctness. Models must understand a repository, implement a patch, and pass the project’s tests under a constrained, single-attempt setting in many evaluations.","L1: Language Comprehension, Language Production
L2: Planning, Logical Reasoning, Working Memory, Adaptive Error Correction, Decision-making
L3: ",L2
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro is a larger and harder software-engineering benchmark intended to be more industrially realistic and more resistant to shortcuts, including broader language coverage and more complex change requests. Models must navigate codebases, reason about requirements, and deliver patches that satisfy automated tests and constraints.","L1: Language Comprehension, Language Production
L2: Planning, Logical Reasoning, Working Memory, Adaptive Error Correction, Decision-making
L3: ",L2
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp measures an agent’s ability to answer difficult questions by searching and synthesizing information from the web (or a controlled web-like corpus in some settings). Success requires decomposing the question, issuing effective queries, validating sources, and integrating evidence into a final response.","L1: Language Comprehension
L2: Planning, Decision-making, Working Memory, Semantic Understanding & Context Recognition, Logical Reasoning, Adaptive Error Correction
L3: ",L2
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench evaluates interactive tool-using agents in realistic customer-support style domains (e.g., retail, airline, telecom) with policies and multi-turn user simulations. Agents must decide what to do next, call APIs correctly, follow policy constraints, and communicate clearly across multiple turns.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Working Memory, Adaptive Error Correction
L3: Inhibitory Control, Social Reasoning & Theory of Mind",L3
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI tests few-shot abstract reasoning on grid transformation puzzles where the rule must be inferred from only a handful of examples. It emphasizes generalization to novel patterns rather than recall, requiring flexible hypothesis generation and rule application.","L1: 
L2: Logical Reasoning, Working Memory, Attention, Spatial Representation & Mapping, Scene Understanding & Visual Reasoning
L3: Cognitive Flexibility",L3
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas evaluates real-world tool use via the Model Context Protocol, focusing on selecting the right tools and executing multi-step workflows over authentic APIs and data. Agents must manage tool schemas, handle errors, and compose outputs from multiple tool calls into coherent answers.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Working Memory, Adaptive Error Correction, Semantic Understanding & Context Recognition
L3: ",L2
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a broad, frontier-level benchmark covering hard questions across many domains, often requiring multi-step reasoning and (in some variants) multimodal understanding and tool use. It is designed to stress-test knowledge, reasoning, and synthesis rather than narrow skills.","L1: Language Comprehension, Language Production
L2: Logical Reasoning, Working Memory, Semantic Understanding & Context Recognition, Multisensory Integration, Planning
L3: ",L2
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 evaluates competition-style mathematics problems that typically require multi-step derivations, careful case analysis, and precise final answers. It is commonly used to gauge mathematical reasoning under test-like constraints, with and without tool assistance.","L1: 
L2: Logical Reasoning, Working Memory, Planning, Attention, Adaptive Error Correction
L3: ",L2
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a difficult multiple-choice science QA benchmark designed to be “Google-proof,” emphasizing reasoning over easy lookup. The Diamond subset targets high-quality questions where domain experts succeed and non-experts typically fail, stressing rigorous scientific understanding.","L1: 
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Attention, Decision-making
L3: ",L2
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends broad academic knowledge testing to multiple languages, measuring whether models can answer subject-matter questions beyond English. It probes both factual knowledge and reasoning consistency across languages and culturally varied phrasing.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory
L3: Cognitive Flexibility",L3
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro is a multimodal benchmark requiring models to answer expert-level questions grounded in images (e.g., diagrams, charts, scientific figures) alongside text. It emphasizes integrated perception and reasoning, often requiring interpreting visual evidence and combining it with domain knowledge.","L1: Visual Perception
L2: Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Working Memory
L3: ",L2
ScreenShot-Pro,https://gui-agent.github.io/grounding-leaderboard/,https://arxiv.org/abs/2504.07981,ScreenShot-Pro evaluates vision-enabled agents on understanding high-resolution screenshots from software interfaces and professional applications. Tasks require correctly grounding text instructions in UI elements and reasoning about layout to identify or act on the right targets.,"L1: Visual Perception
L2: Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Sensorimotor Coordination, Planning, Working Memory
L3: ",L2
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning tests reasoning over scientific documents and figures (commonly drawn from arXiv-style papers), where correct answers depend on interpreting charts, plots, or technical visuals. It stresses extracting quantitative/structural information from figures and integrating it with textual context.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Attention
L3: ",L2
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU extends multimodal academic reasoning to videos, requiring models to integrate information across time (multiple frames) along with text questions. It tests whether models can track events, spatial relations, and procedural steps in dynamic visual content.","L1: Visual Perception
L2: Attention, Working Memory, Scene Understanding & Visual Reasoning, Multisensory Integration
L3: Cognitive Timing & Predictive Modeling",L3
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 is a long-context evaluation where multiple similar “needle” items are embedded in a large “haystack,” and the model must retrieve and reproduce the correct referenced content; the 8-needle variant increases ambiguity and interference. It stresses robust retrieval, disambiguation, and consistency over long documents and multi-round context.","L1: 
L2: Working Memory, Episodic Memory, Attention, Semantic Understanding & Context Recognition, Logical Reasoning
L3: Inhibitory Control",L3
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval evaluates economically valuable, well-specified knowledge-work tasks spanning many occupations (e.g., drafting artifacts like presentations/spreadsheets, structured analysis, and planning deliverables), often judged by expert humans. It emphasizes end-to-end task execution quality, adherence to requirements, and usefulness of produced work products.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Semantic Understanding & Context Recognition, Working Memory
L3: Self-reflection",L3
SWE-Lancer,https://openai.com/index/swe-lancer/,https://arxiv.org/abs/2502.12115,"SWE-Lancer evaluates software-engineering capability on more agentic, end-to-end coding tasks, typically emphasizing realistic workflows beyond isolated bug fixes. Models must plan changes across a codebase, implement them correctly, and validate behavior with tests and tooling under practical constraints.","L1: Language Comprehension, Language Production
L2: Planning, Logical Reasoning, Working Memory, Adaptive Error Correction, Decision-making
L3: ",L2
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks tests long-context structured reasoning by embedding graph descriptions and querying for traversal-based outputs (e.g., reaching nodes, parents, BFS-like properties) that require following edges through the structure. It stresses faithful multi-step navigation through symbolic structure rather than surface-level pattern matching.","L1: 
L2: Spatial Representation & Mapping, Working Memory, Attention, Logical Reasoning, Planning
L3: ",L2
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon evaluates general tool-use competence across diverse tasks that require selecting, sequencing, and correctly parameterizing tool calls, then synthesizing results. It is intended to probe robust agent behavior under tool errors, partial information, and multi-step workflows.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory
L3: ",L2
HMMT,https://matharena.ai/?view=problem&comp=hmmt--hmmt_feb_2025,https://arxiv.org/abs/2505.23281,"HMMT problems are advanced competition mathematics questions that often require deeper proofs, creative constructions, and careful reasoning than standard exam-style items. As an evaluation set, it is used to measure high-end mathematical problem-solving and consistency under difficult constraints.","L1: 
L2: Logical Reasoning, Working Memory, Planning, Attention
L3: Cognitive Flexibility",L3
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath is designed to test expert-level mathematics, including problems intended to be challenging even for strong solvers, often spanning multiple tiers of difficulty. It emphasizes rigorous multi-step reasoning, precise symbolic manipulation, and, in tool-enabled settings, effective verification or computation support.","L1: 
L2: Logical Reasoning, Working Memory, Planning, Attention, Adaptive Error Correction
L3: Cognitive Flexibility",L3
