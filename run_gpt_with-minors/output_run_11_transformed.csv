Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates software-engineering agents on real GitHub issues by asking them to produce code patches that make the repository’s tests pass. The “Verified” subset emphasizes tasks confirmed solvable and reliably graded, focusing on end-to-end debugging, code understanding, and patch correctness.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Planning, Decision-making, Adaptive Error Correction, Working Memory
L3: ",L2
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro is a larger, harder successor aimed at measuring more contamination-resistant, industrially relevant software engineering across multiple languages and repositories. It stresses robust problem decomposition, codebase navigation, iterative fixing, and producing high-quality patches under realistic constraints.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Planning, Decision-making, Adaptive Error Correction, Working Memory
L3: Cognitive Flexibility",L3
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp measures a model’s ability to answer difficult questions by conducting web-style research over a controlled document collection, emphasizing reproducibility across systems. Agents must search, select evidence, synthesize findings, and avoid being misled by distractors or irrelevant sources.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Attention, Working Memory, Semantic Understanding & Context Recognition
L3: ",L2
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench evaluates interactive agent behavior in customer-service style environments (e.g., retail, airline, telecom) that require multi-turn dialogue, tool/API use, and policy compliance. The benchmark emphasizes consistent adherence to rules while still helping users effectively through complex workflows.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Adaptive Error Correction
L3: Inhibitory Control, Social Reasoning & Theory of Mind",L3
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,ARC-AGI tests “fluid intelligence” via small grid-based puzzles where the model must infer latent transformation rules from a few examples and apply them to new inputs. It is designed to reward generalizable abstraction and rule induction rather than domain memorization.,"L1: Visual Perception
L2: Logical Reasoning, Working Memory, Spatial Representation & Mapping, Scene Understanding & Visual Reasoning
L3: Cognitive Flexibility",L3
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas assesses real-world tool use through the Model Context Protocol (MCP), requiring models to discover tools, call APIs correctly, manage errors, and compose multi-step workflows. It targets reliability under realistic integration settings rather than single-call demonstrations.","L1: Language Comprehension
L2: Planning, Decision-making, Semantic Understanding & Context Recognition, Adaptive Error Correction, Working Memory
L3: ",L2
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a large, frontier-level multimodal benchmark intended to probe advanced reasoning and expert knowledge across many domains. Questions often require careful interpretation, multi-step inference, and (in tool-enabled settings) research-like synthesis from external evidence.","L1: Language Comprehension, Language Production
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Planning, Scene Understanding & Visual Reasoning, Working Memory
L3: ",L2
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 comprises competition-style mathematics problems that require multi-step derivations and precise symbolic manipulation. It is commonly used to evaluate mathematical reasoning depth, error avoidance, and the ability to maintain long solution threads.","L1: 
L2: Logical Reasoning, Working Memory, Planning, Adaptive Error Correction
L3: ",L2
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a curated subset of challenging graduate-level multiple-choice science questions designed to be difficult to answer via superficial pattern matching. It probes deep scientific reasoning, careful reading, and selection among plausible distractors.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Decision-making, Working Memory
L3: ",L2
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends broad academic testing into multiple languages, measuring whether knowledge and reasoning transfer beyond English. It evaluates multilingual understanding across many subjects while controlling for format consistency and breadth.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory
L3: Cognitive Flexibility",L3
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro is a high-difficulty multimodal benchmark spanning many disciplines, requiring models to reason jointly over images and text (e.g., diagrams, plots, problem statements). It targets expert-level visual-textual understanding and multi-step inference under multimodal ambiguity.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Working Memory, Attention
L3: ",L2
ScreenShot-Pro,https://gui-agent.github.io/grounding-leaderboard/,https://arxiv.org/abs/2504.07981,"ScreenShot-Pro evaluates screenshot understanding and GUI grounding, where models must interpret high-resolution interface images and answer questions or identify relevant elements. It stresses spatial layout understanding, visual-text alignment, and fine-grained attention to UI details.","L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Spatial Representation & Mapping, Visual Attention & Eye Movements, Attention
L3: ",L2
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning focuses on reasoning over scientific content from papers (e.g., charts/figures and associated context), requiring models to extract quantitative/structural information and answer questions that depend on correct interpretation. It emphasizes faithful figure reading and linking visual evidence to textual claims.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory
L3: ",L2
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU evaluates multimodal understanding and reasoning over videos, requiring integration of information across time and often across modalities of narration/text with visual events. Tasks probe temporal coherence, event tracking, and higher-level inference about actions and outcomes.","L1: Visual Perception
L2: Working Memory, Scene Understanding & Visual Reasoning, Attention, Multisensory Integration
L3: Cognitive Timing & Predictive Modeling",L3
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) tests long-context robustness via multi-round co-reference/needle-in-haystack style tasks, where multiple similar “needles” are embedded across a long dialogue/document. The model must retrieve and reproduce the correct referenced content, stressing precise context tracking under interference.","L1: 
L2: Working Memory, Attention, Episodic Memory, Semantic Understanding & Context Recognition
L3: Inhibitory Control",L3
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval measures performance on well-specified professional knowledge-work tasks across many occupations (e.g., producing spreadsheets, plans, reports), typically judged by expert humans. It emphasizes producing usable artifacts, following constraints, and making sound tradeoffs consistent with workplace objectives.","L1: Language Production, Language Comprehension
L2: Planning, Decision-making, Semantic Understanding & Context Recognition, Adaptive Error Correction
L3: Self-reflection",L3
SWE-Lancer,https://openai.com/index/swe-lancer/,https://arxiv.org/abs/2502.12115,"SWE-Lancer evaluates agentic software engineering in settings that more closely resemble commissioned work: understanding goals, making changes across a codebase, and delivering working solutions under practical constraints. It targets reliability, iterative debugging, and end-to-end completion rather than isolated coding skill.","L1: Language Comprehension
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition
L3: ",L2
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks tests structured reasoning over graph-like data presented in text, such as following edges, parent pointers, or traversal constraints to reach correct nodes. It probes the ability to maintain and manipulate discrete relational structure over long sequences without losing track.","L1: 
L2: Spatial Representation & Mapping, Logical Reasoning, Working Memory, Attention, Semantic Understanding & Context Recognition
L3: ",L2
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon evaluates general tool-use competence across diverse APIs and tasks, emphasizing selecting the right tool, calling it with correct arguments, and integrating results into a final answer. It targets robustness to tool errors, multi-step workflows, and faithful synthesis of tool outputs.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory
L3: ",L2
HMMT,https://matharena.ai/?view=problem&comp=hmmt--hmmt_feb_2025,https://arxiv.org/abs/2505.23281,HMMT problems (as used in modern model evals) are competition-level mathematics questions that require creative multi-step reasoning and careful case handling. The benchmark is used to assess advanced mathematical problem solving beyond routine textbook exercises.,"L1: 
L2: Logical Reasoning, Working Memory, Planning
L3: Cognitive Flexibility",L3
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath is an expert-level mathematics benchmark designed to probe the limits of formal and informal reasoning on difficult problems, often requiring nontrivial insight and long solution chains. It emphasizes correctness under deep reasoning pressure and resistance to hallucinated steps.","L1: 
L2: Logical Reasoning, Working Memory, Planning, Adaptive Error Correction
L3: Cognitive Flexibility",L3
