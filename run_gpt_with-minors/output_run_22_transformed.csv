Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates software engineering agents on real GitHub issues where the model must produce a patch that makes a provided test suite pass. The “Verified” subset consists of tasks vetted (e.g., by humans/automated checks) to ensure the issue is solvable and the evaluation is reliable under standardized harnesses.","L1: Language Comprehension
L2: Planning, Logical Reasoning, Working Memory, Adaptive Error Correction, Decision-making
L3: ",L2
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro is a harder, larger-scale software engineering benchmark designed to be more contamination-resistant and industrially representative (e.g., multiple languages and more complex repos/tasks). Models must implement fixes or features in real repositories under stricter evaluation protocols than SWE-bench Verified.","L1: Language Comprehension
L2: Planning, Logical Reasoning, Working Memory, Adaptive Error Correction, Decision-making
L3: ",L2
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp evaluates “deep research” style agents on information-seeking questions where the model must browse a controlled document collection (or web-like index) and synthesize a correct final answer. It measures the ability to search, read, cross-check sources, and integrate evidence across multiple documents under tool constraints.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Working Memory, Attention, Planning, Decision-making
L3: Self-reflection",L3
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench evaluates interactive customer-support agents operating in simulated environments (e.g., retail/airline/telecom) with APIs and policy constraints. The agent must conduct multi-turn dialogues, call tools correctly, follow policies, and resolve user goals despite ambiguity, exceptions, and long-horizon state tracking.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Working Memory, Reward Mechanisms
L3: Social Reasoning & Theory of Mind, Inhibitory Control",L3
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI measures fluid reasoning by requiring models to infer latent transformation rules from only a few input–output grid examples and apply them to new inputs. Tasks are intentionally novel and compact, emphasizing generalization and rule induction rather than memorized domain knowledge.","L1: 
L2: Logical Reasoning, Working Memory, Attention, Spatial Representation & Mapping, Scene Understanding & Visual Reasoning
L3: Cognitive Flexibility",L3
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas evaluates real-world tool use via the Model Context Protocol (MCP), where the model must discover tools, execute multi-step workflows, manage errors/retries, and synthesize responses from tool outputs. It targets practical agent reliability in production-like API ecosystems rather than single-call tool demos.","L1: Language Comprehension
L2: Planning, Decision-making, Working Memory, Adaptive Error Correction, Semantic Understanding & Context Recognition
L3: ",L2
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam (HLE) is a frontier, multi-domain benchmark intended to stress expert-level reasoning and knowledge, often including multimodal questions. It emphasizes hard, research-like problems where correct answers require synthesis, careful reasoning, and sometimes tool-supported analysis depending on the evaluation setting.","L1: Language Comprehension, Visual Perception
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Scene Understanding & Visual Reasoning, Planning, Multisensory Integration
L3: Self-reflection",L3
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 is a competition-math benchmark drawn from the 2025 American Invitational Mathematics Examination problems, typically requiring exact integer answers. It stresses multi-step symbolic reasoning, algebraic manipulation, and error-sensitive calculation under time/attempt constraints (often evaluated pass@1).","L1: 
L2: Logical Reasoning, Working Memory, Planning, Adaptive Error Correction, Attention
L3: ",L2
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a subset of GPQA containing the highest-quality graduate-level science multiple-choice questions designed to be “Google-proof.” It targets deep scientific reasoning and precise knowledge application, with distractors that punish shallow pattern matching.","L1: Language Comprehension
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Attention
L3: ",L2
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends MMLU to multiple languages, testing broad academic knowledge and reasoning across many subjects with multiple-choice questions. It probes whether models can transfer knowledge and reasoning across linguistic contexts rather than relying solely on English-centric training artifacts.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory
L3: Cognitive Flexibility",L3
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro is an advanced multimodal benchmark that requires reasoning over images (e.g., diagrams, charts, tables) and text across many expert disciplines. It emphasizes integration of visual evidence with domain knowledge and multi-step inference, often under challenging visual conditions.","L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Working Memory, Attention, Visual Attention & Eye Movements
L3: ",L2
ScreenShot-Pro,https://gui-agent.github.io/grounding-leaderboard/,https://arxiv.org/abs/2504.07981,"ScreenShot-Pro evaluates GUI understanding and grounding from high-resolution screenshots, requiring models to interpret interface layout and identify the correct elements or actions. It targets practical “computer use” competence such as locating controls, reading small text, and mapping intents to UI operations.","L1: Visual Perception, Language Comprehension
L2: Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Spatial Representation & Mapping, Planning, Decision-making, Sensorimotor Coordination, Working Memory
L3: ",L2
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning focuses on answering reasoning questions about scientific paper figures and content, commonly requiring chart/diagram interpretation and quantitative inference. It measures the ability to extract structured information from visual scientific artifacts and connect it to textual context and domain knowledge.","L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Logical Reasoning, Working Memory, Attention, Semantic Understanding & Context Recognition
L3: ",L2
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU extends multimodal understanding to the temporal domain, requiring reasoning over video frames plus text. It probes whether models can integrate evolving visual evidence, maintain temporal coherence, and answer questions that depend on events, state changes, or multi-step video understanding.","L1: Visual Perception, Language Comprehension
L2: Multisensory Integration, Working Memory, Attention, Scene Understanding & Visual Reasoning, Spatial Representation & Mapping
L3: Cognitive Timing & Predictive Modeling",L3
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) is a long-context evaluation where multiple similar “needle” queries and responses are embedded within large distractor contexts, and the model must retrieve the correct referenced response. It primarily measures robust long-range dependency tracking, resistance to interference, and precise recall under heavy context load.","L1: Language Comprehension
L2: Working Memory, Attention, Episodic Memory, Semantic Understanding & Context Recognition
L3: Inhibitory Control",L3
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval evaluates economically meaningful, well-specified professional knowledge-work tasks across many occupations, scored by expert human judges via pairwise comparisons. Tasks often require producing polished artifacts (e.g., plans, spreadsheets, presentations) and making pragmatic tradeoffs under constraints.","L1: Language Production, Language Comprehension
L2: Planning, Decision-making, Semantic Understanding & Context Recognition, Working Memory
L3: Self-reflection, Social Reasoning & Theory of Mind",L3
SWE-Lancer,https://openai.com/index/swe-lancer/,https://arxiv.org/abs/2502.12115,"SWE-Lancer targets agentic software engineering beyond single patches, emphasizing end-to-end work such as multi-step debugging, implementing changes across a codebase, and interacting with tooling. It is designed to reflect practical engineering workflows and longer-horizon task decomposition than simpler coding benchmarks.","L1: Language Comprehension
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory, Logical Reasoning
L3: Self-reflection",L3
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks tests long-context and algorithmic reasoning by requiring models to follow and query large graph-structured data embedded in text (e.g., BFS-like traversals, parent/neighbor queries). It emphasizes maintaining consistent internal state over many steps and executing discrete reasoning procedures reliably.","L1: 
L2: Spatial Representation & Mapping, Working Memory, Attention, Logical Reasoning, Planning
L3: Inhibitory Control",L3
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon evaluates tool-using agents on diverse multi-step tasks that require selecting appropriate tools, issuing correct calls, handling failures, and composing results into a final solution. It targets practical orchestration skills (workflow construction, verification, and recovery) rather than single-turn QA.","L1: Language Comprehension
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition
L3: Self-reflection",L3
HMMT,https://matharena.ai/?view=problem&comp=hmmt--hmmt_feb_2025,https://arxiv.org/abs/2505.23281,"HMMT comprises high-difficulty competition math problems (e.g., Harvard-MIT Mathematics Tournament sets) that often require creative problem solving and multi-lemma reasoning. It stresses reliable symbolic manipulation, strategic choice of approaches, and careful verification under strict answer correctness.","L1: 
L2: Logical Reasoning, Working Memory, Planning, Adaptive Error Correction, Attention
L3: Cognitive Flexibility",L3
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath is an expert-level mathematics benchmark intended to measure progress on problems closer to research-grade difficulty, often benefiting from tool-assisted computation and rigorous multi-step reasoning. It emphasizes depth, compositionality, and robustness against small logical or algebraic mistakes.","L1: 
L2: Logical Reasoning, Working Memory, Planning, Adaptive Error Correction, Attention
L3: Cognitive Flexibility, Self-reflection",L3
