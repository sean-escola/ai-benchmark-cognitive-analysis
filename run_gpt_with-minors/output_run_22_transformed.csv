Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 evaluates agentic coding and systems skills by placing a model in a real command-line environment where it must complete end-to-end tasks (e.g., debugging, running tools, editing files, and verifying outputs). Success depends on choosing correct shell actions, interpreting program feedback, and iterating until the task is solved under realistic constraints.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory
L3: ",L2
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp measures deep research and information-seeking by requiring models to answer challenging questions that typically need multi-step browsing and synthesis across multiple documents. It stresses evidence gathering, cross-checking sources, and producing a final answer consistent with retrieved information.","L1: Language Comprehension, Language Production
L2: Planning, Attention, Semantic Understanding & Context Recognition, Working Memory, Decision-making
L3: ",L2
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld evaluates “computer use” by having an agent operate within a desktop-like operating system to accomplish practical tasks across apps and web interfaces. It requires visually interpreting GUIs, executing multi-step procedures, recovering from mistakes, and coordinating perception with actions.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Sensorimotor Coordination, Planning, Adaptive Error Correction, Working Memory
L3: ",L2
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,ARC-AGI presents few-shot grid transformation puzzles where the model must infer hidden rules from a small number of input–output examples and generalize to a new input. The benchmark targets fluid reasoning under novelty rather than memorized knowledge.,"L1: Visual Perception
L2: Logical Reasoning, Working Memory, Planning, Spatial Representation & Mapping
L3: Cognitive Flexibility",L3
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 measures long-horizon agent competence in a simulated vending machine business over many decisions, such as sourcing inventory, setting prices, and handling changing market conditions. Performance depends on maintaining coherent strategy over time and adapting decisions based on outcomes.","L1: 
L2: Planning, Decision-making, Reward Mechanisms, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition
L3: ",L2
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity agents on tasks like finding and exploiting known vulnerabilities from descriptions and discovering new issues in real open-source codebases. It emphasizes code comprehension, hypothesis-driven debugging, and iterative testing in realistic vulnerability workflows.","L1: Language Comprehension
L2: Logical Reasoning, Planning, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition
L3: Inhibitory Control",L3
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"SpreadsheetBench tests whether models can navigate and manipulate real spreadsheets to produce correct outputs, including formula work, data cleaning, and structured transformations. It probes tool-using reliability and the ability to track multi-step state changes while avoiding cascading errors.","L1: 
L2: Logical Reasoning, Planning, Working Memory, Attention, Adaptive Error Correction, Semantic Understanding & Context Recognition
L3: ",L2
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam (HLE) is a difficult, broad, multimodal benchmark intended to stress frontier-level knowledge and reasoning across many disciplines. Questions often require integrating text with visual information and sustaining multi-step reasoning under uncertainty.","L1: Language Comprehension, Visual Perception
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Scene Understanding & Visual Reasoning, Planning
L3: ",L2
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,GPQA Diamond is a high-quality subset of GPQA consisting of very challenging multiple-choice science questions designed to resist superficial pattern matching and casual web lookup. It primarily tests deep scientific reasoning and careful reading under tight answer choices.,"L1: Language Comprehension
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Decision-making
L3: ",L2
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro evaluates expert-level multimodal understanding and reasoning across many academic and professional domains, combining images (e.g., diagrams, charts, figures) with text questions. It stresses grounding language in visual evidence and performing domain-style reasoning from that evidence.","L1: Visual Perception, Language Comprehension
L2: Multisensory Integration, Scene Understanding & Visual Reasoning, Logical Reasoning, Attention, Working Memory
L3: ",L2
MathArena Apex,https://matharena.ai/?view=problem&comp=apex--apex_2025,https://arxiv.org/abs/2505.23281,"MathArena Apex aggregates high-difficulty math problems to evaluate advanced mathematical reasoning under competitive settings. It emphasizes multi-step derivations, precise symbolic manipulation, and robustness across diverse problem styles.","L1: 
L2: Logical Reasoning, Planning, Working Memory
L3: Cognitive Flexibility",L3
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning tests whether models can answer questions that require interpreting scientific figures and charts drawn from research papers, connecting visual evidence to textual queries. It stresses quantitative and relational reasoning over complex visuals rather than simple caption reading.","L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Logical Reasoning, Attention, Working Memory
L3: ",L2
OmniDocBench 1.5,https://github.com/opendatalab/OmniDocBench,https://arxiv.org/abs/2412.07626,"OmniDocBench 1.5 evaluates document understanding and OCR-style extraction across heterogeneous layouts, including text blocks, formulas, tables, and reading order. It probes robustness to formatting, spatial layout, and structured transcription fidelity.","L1: Visual Perception, Language Production
L2: Scene Understanding & Visual Reasoning, Attention, Working Memory, Semantic Understanding & Context Recognition
L3: ",L2
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU extends multimodal reasoning to video, requiring models to answer questions that depend on events unfolding across time rather than a single frame. It stresses temporal integration, tracking entities and actions, and combining visual evidence with language reasoning.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Working Memory, Attention, Multisensory Integration
L3: Cognitive Timing & Predictive Modeling",L3
LiveCodeBench Pro,https://livecodebenchpro.com/projects/livecodebench-pro/leaderboard,https://arxiv.org/abs/2506.11928,"LiveCodeBench Pro evaluates coding ability on contemporary, competition-style programming tasks with standardized grading, often summarized via Elo-like ratings. It emphasizes generating correct, efficient programs, debugging through failures, and adapting solutions under time-like constraints.","L1: Language Comprehension, Language Production
L2: Logical Reasoning, Planning, Adaptive Error Correction, Working Memory
L3: ",L2
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"The FACTS Benchmark Suite systematically evaluates factuality by testing whether model outputs remain consistent with provided sources and real-world ground truth across multiple factuality-related tasks. It targets hallucination resistance, faithful summarization, and correctness under prompting pressure.","L1: Language Comprehension, Language Production
L2: Semantic Understanding & Context Recognition, Working Memory
L3: Inhibitory Control, Self-reflection",L3
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA evaluates physical commonsense and everyday interaction understanding across many languages, stressing whether models can reason about plausible actions and outcomes in the physical world. It targets robustness of commonsense reasoning beyond English-centric datasets.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Spatial Representation & Mapping
L3: Cognitive Flexibility",L3
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,MRCR v2 (8-needle) measures long-context retrieval and multi-round reference tracking by embedding multiple similar “needle” requests inside long “haystack” dialogues and asking the model to reproduce the correct associated response. It stresses sustained attention across long contexts and accurate retrieval under interference.,"L1: 
L2: Working Memory, Attention, Episodic Memory, Semantic Understanding & Context Recognition
L3: Inhibitory Control",L3
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval evaluates economically valuable, well-specified knowledge work by asking models to produce real work artifacts (e.g., slides, spreadsheets, plans) across many occupations and scoring them via expert comparison to human professionals. It stresses end-to-end task execution quality, instruction following, and professional judgment in deliverable creation.","L1: Language Production
L2: Planning, Decision-making, Semantic Understanding & Context Recognition, Adaptive Error Correction, Working Memory
L3: Social Reasoning & Theory of Mind",L3
SWE-Lancer,https://openai.com/index/swe-lancer/,https://arxiv.org/abs/2502.12115,SWE-Lancer evaluates agentic software engineering on realistic repository tasks where success requires implementing fixes or features consistent with project expectations and constraints. It emphasizes translating natural-language requirements into correct code changes and validating them against tests and repo structure.,"L1: Language Comprehension, Language Production
L2: Planning, Logical Reasoning, Adaptive Error Correction, Working Memory
L3: Self-reflection",L3
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks tests long-context structured reasoning by encoding graph traversal problems in text and requiring models to follow edges, perform BFS-like operations, or answer parent/neighbor queries. It emphasizes systematic multi-step traversal, bookkeeping, and error-free manipulation of discrete structures.","L1: 
L2: Logical Reasoning, Spatial Representation & Mapping, Working Memory, Attention, Planning
L3: ",L2
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon evaluates tool-using agents on tasks that require selecting the right tools, calling them with correct arguments, and composing results into a final answer under realistic tool constraints. It stresses orchestration across multiple steps, recovery from tool errors, and reliable state tracking.","L1: Language Comprehension
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition
L3: ",L2
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath is a hard mathematics benchmark aimed at measuring advanced problem solving near the frontier of current model capability, often requiring deep multi-step reasoning rather than routine computations. It stresses proof-like thinking, compositional strategy selection, and robustness to novel math formulations.","L1: 
L2: Logical Reasoning, Planning, Working Memory
L3: Cognitive Flexibility",L3
