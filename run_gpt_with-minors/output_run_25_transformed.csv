Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates a model’s ability to solve real GitHub software engineering issues by generating patches that make repository tests pass, on a curated set of tasks verified to be solvable. It emphasizes end-to-end debugging and code changes under realistic repository constraints rather than isolated coding puzzles.","L1: Language Comprehension
L2: Planning, Working Memory, Logical Reasoning, Adaptive Error Correction, Decision-making
L3: ",L2
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro is a larger and more difficult successor benchmark that expands beyond Python-only tasks to multiple programming languages and more complex, industry-style issues. It targets robustness to contamination and measures an agent’s ability to navigate larger codebases, reason about requirements, and produce correct multi-file fixes.","L1: Language Comprehension
L2: Planning, Working Memory, Logical Reasoning, Adaptive Error Correction, Decision-making
L3: Cognitive Flexibility",L3
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp measures deep-research performance where an agent must answer questions by searching and synthesizing information from a controlled web document collection (or reproducible browsing setup). It stresses source selection, evidence integration, and multi-step retrieval strategies rather than purely parametric recall.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Attention, Working Memory, Semantic Understanding & Context Recognition
L3: Self-reflection",L3
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench evaluates interactive agent behavior in simulated customer-service domains (e.g., retail, airline, telecom) where the model must converse with a user simulator, call tools/APIs, and follow policies consistently. It probes whether the agent can achieve user goals while respecting constraints and handling multi-turn state and exceptions.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Working Memory, Adaptive Error Correction, Attention
L3: Social Reasoning & Theory of Mind, Inhibitory Control",L3
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,ARC-AGI is a fluid intelligence benchmark based on abstract grid transformation tasks where the model must infer hidden rules from a few input–output examples and generalize to a new case. It is designed to minimize reliance on memorized knowledge and instead test novel pattern induction and compositional reasoning.,"L1: Visual Perception
L2: Logical Reasoning, Working Memory, Spatial Representation & Mapping, Scene Understanding & Visual Reasoning, Attention, Planning
L3: Cognitive Flexibility",L3
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas assesses real-world tool use through the Model Context Protocol, requiring models to discover appropriate tools, invoke them correctly, manage intermediate results, and integrate outputs into a final response. Tasks often require multi-step orchestration across multiple services, including retries and error handling.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Working Memory, Adaptive Error Correction, Semantic Understanding & Context Recognition, Attention
L3: ",L2
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam (HLE) is a frontier, multimodal benchmark spanning advanced questions intended to be difficult even for well-educated humans, often requiring multi-step reasoning and careful interpretation of prompts and evidence. It targets breadth across domains and the ability to combine knowledge with structured problem solving (sometimes with tools/search depending on the setup).","L1: Language Comprehension, Language Production
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Multisensory Integration, Scene Understanding & Visual Reasoning, Attention, Planning
L3: ",L2
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,AIME 2025 comprises competition-style mathematics problems that require creative algebraic and combinatorial reasoning under strict answer formats. It emphasizes precise symbolic manipulation and multi-step solution planning rather than short factual recall.,"L1: 
L2: Logical Reasoning, Working Memory, Planning, Attention
L3: Cognitive Flexibility",L3
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a high-quality subset of graduate-level, “Google-proof” multiple-choice science questions where non-experts typically fail while experts succeed. It stresses deep scientific understanding, careful elimination among distractors, and reasoning under domain knowledge constraints.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Attention, Decision-making
L3: ",L2
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends broad academic knowledge testing to many languages, evaluating whether models can comprehend and answer subject-matter questions beyond English. It probes multilingual generalization, cross-lingual transfer of concepts, and reasoning with localized terminology and phrasing.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Attention
L3: Cognitive Flexibility",L3
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro is a challenging multimodal benchmark covering expert-level questions that combine text with complex images (e.g., charts, diagrams, documents) across many disciplines. It evaluates whether models can ground language in visual evidence and perform multi-step reasoning over both modalities.","L1: Visual Perception, Language Comprehension, Language Production
L2: Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Working Memory, Attention
L3: ",L2
ScreenShot-Pro,https://gui-agent.github.io/grounding-leaderboard/,https://arxiv.org/abs/2504.07981,"ScreenShot-Pro evaluates screenshot understanding and GUI grounding, where models must interpret high-resolution interface images and answer questions or identify relevant on-screen elements. It targets spatially grounded reasoning about layout, controls, and visual affordances typical of real software tools.","L1: Visual Perception, Language Comprehension
L2: Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Spatial Representation & Mapping, Logical Reasoning, Working Memory, Attention, Decision-making
L3: ",L2
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning focuses on scientific-figure and paper-style visual reasoning, requiring models to interpret charts/figures and answer associated reasoning questions (often benefiting from quantitative analysis). It measures the ability to extract structured information from visuals and integrate it with scientific context to reach correct conclusions.","L1: Visual Perception, Language Comprehension, Language Production
L2: Scene Understanding & Visual Reasoning, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Attention
L3: ",L2
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU evaluates multimodal understanding and reasoning over videos paired with language questions, often requiring tracking events across time and integrating visual evidence with textual instructions. It stresses temporal coherence, event attribution, and multi-step inference from dynamic scenes.","L1: Visual Perception, Language Comprehension
L2: Attention, Working Memory, Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning
L3: Cognitive Timing & Predictive Modeling",L3
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 is a long-context benchmark for multi-round co-reference resolution, where multiple similar “needle” requests are embedded in a long “haystack” and the model must reproduce the response corresponding to a specific needle. The 8-needle setting probes robustness when many confusable targets exist across large token spans.","L1: Language Comprehension, Language Production
L2: Working Memory, Attention, Episodic Memory, Semantic Understanding & Context Recognition
L3: Inhibitory Control",L3
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval measures well-specified professional knowledge work across many occupations, with human judges comparing model outputs to expert work products (e.g., spreadsheets, presentations, plans). It emphasizes producing actionable artifacts and decisions under constraints, often rewarding coherent multi-step execution over long instructions.","L1: Language Production, Language Comprehension
L2: Planning, Decision-making, Working Memory, Adaptive Error Correction, Semantic Understanding & Context Recognition, Attention
L3: Self-reflection",L3
SWE-Lancer,https://openai.com/index/swe-lancer/,https://arxiv.org/abs/2502.12115,"SWE-Lancer evaluates software engineering performance on tasks framed around real-world contracting/freelance-style work, emphasizing practical fixes, codebase navigation, and meeting specification under realistic constraints. It targets agentic competence in understanding user intent, implementing changes, and validating behavior via tests or execution.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Working Memory, Logical Reasoning, Adaptive Error Correction, Attention
L3: ",L2
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks tests reasoning over graph-structured data described in text, such as following edges or computing properties like reachability/parent relationships across long contexts. It stresses systematic traversal, maintaining state across many steps, and resisting distractors in lengthy descriptions.","L1: 
L2: Spatial Representation & Mapping, Working Memory, Logical Reasoning, Attention, Planning
L3: Inhibitory Control",L3
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon evaluates general tool-use ability across diverse tasks where models must select tools, call them with correct arguments, and compose tool outputs into final answers. It measures robustness of tool planning, error recovery, and integration of external results into coherent reasoning chains.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Working Memory, Adaptive Error Correction, Attention, Semantic Understanding & Context Recognition
L3: ",L2
HMMT,https://matharena.ai/?view=problem&comp=hmmt--hmmt_feb_2025,https://arxiv.org/abs/2505.23281,"HMMT problems (as used in MathArena) are high-difficulty contest mathematics questions that demand creative insight, careful case analysis, and precise computation. The benchmark emphasizes sustained multi-step derivations and correctness under complex constraints.","L1: 
L2: Logical Reasoning, Working Memory, Planning, Attention
L3: Cognitive Flexibility",L3
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath is an expert-level mathematics benchmark designed to test genuine research-grade problem solving, often requiring sophisticated techniques and long derivations (with some settings allowing tool assistance such as Python). It targets deep mathematical reasoning, strategic decomposition of problems, and maintaining coherence over extended solution attempts.","L1: 
L2: Logical Reasoning, Planning, Working Memory, Attention, Adaptive Error Correction
L3: Cognitive Flexibility",L3
