Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates software engineering by asking models to generate patches that resolve real issues in open-source Python repositories, validated by running the project’s test suite. The “Verified” subset consists of tasks confirmed by human annotators to be solvable and unambiguous, reducing noise from underspecified bugs.","L1: Language Comprehension, Language Production
L2: Planning, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition
L3: ",L2
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro is a larger and more difficult software engineering benchmark designed to be more industrially representative and more resistant to contamination than earlier SWE-bench variants. It includes complex, multi-file changes and broader language coverage, requiring robust repository understanding and patch generation under realistic constraints.","L1: Language Comprehension, Language Production
L2: Planning, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition, Decision-making
L3: ",L2
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp evaluates “deep research” agents that must browse and synthesize information from a large web corpus to answer questions, often requiring multi-step exploration and evidence gathering. It emphasizes reliable retrieval, cross-document synthesis, and staying on task under long tool-using trajectories.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Attention, Working Memory, Episodic Memory, Semantic Understanding & Context Recognition
L3: ",L2
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench evaluates interactive customer-support agents in simulated domains (e.g., retail, airline, telecom) where the agent must converse with a user and call APIs while complying with domain policies. Success depends on maintaining dialogue state, following constraints, and executing correct multi-turn workflows despite distractions and edge cases.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Working Memory, Semantic Understanding & Context Recognition
L3: Social Reasoning & Theory of Mind, Inhibitory Control",L3
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,ARC-AGI measures fluid reasoning via abstract grid transformation puzzles: models infer hidden rules from a few input-output examples and must generalize to a new input. It is designed to reduce reliance on memorized knowledge by emphasizing novel pattern induction and compositional rule learning.,"L1: Visual Perception
L2: Logical Reasoning, Working Memory, Spatial Representation & Mapping, Scene Understanding & Visual Reasoning, Attention
L3: Cognitive Flexibility",L3
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas evaluates real-world tool use through the Model Context Protocol, requiring models to discover relevant tools, invoke them with correct arguments, handle errors, and integrate outputs into final answers. Tasks are multi-step and closer to production workflows, stressing reliable orchestration rather than single-call accuracy.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition
L3: Inhibitory Control",L3
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam (HLE) is a difficult multimodal benchmark intended to probe frontier-level knowledge and reasoning across many domains, including questions where shallow pattern matching is insufficient. Depending on the evaluation setup, models may need to combine long-form reasoning, tool use, and careful evidence-based responses.","L1: Language Comprehension, Language Production, Visual Perception
L2: Logical Reasoning, Working Memory, Semantic Understanding & Context Recognition, Scene Understanding & Visual Reasoning, Multisensory Integration, Planning
L3: ",L2
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 uses problems from the American Invitational Mathematics Examination, requiring multi-step competition-style mathematical reasoning under tight constraints. It tests symbolic manipulation, careful case analysis, and error-free execution over longer solution chains.","L1: 
L2: Logical Reasoning, Working Memory, Planning, Adaptive Error Correction
L3: ",L2
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a subset of GPQA consisting of especially challenging, graduate-level multiple-choice questions in the natural sciences designed to be difficult to answer by simple web search. It probes deep domain understanding and the ability to eliminate plausible distractors through reasoning.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Decision-making
L3: ",L2
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends MMLU-style broad academic evaluation into multiple languages, testing whether models retain knowledge and reasoning competence across multilingual prompts. It highlights cross-lingual generalization, instruction following, and robustness to linguistic variation.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory
L3: Cognitive Flexibility",L3
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro is a harder, “pro” variant of MMMU that evaluates multimodal understanding and expert-level reasoning across many disciplines using images paired with text questions. It stresses reading complex visuals (charts, diagrams, figures) and integrating them with domain knowledge to select correct answers.","L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Working Memory, Attention
L3: ",L2
ScreenShot-Pro,https://gui-agent.github.io/grounding-leaderboard/,https://arxiv.org/abs/2504.07981,"ScreenShot-Pro evaluates understanding of high-resolution screenshots of software interfaces, focusing on grounding: identifying relevant UI elements, interpreting layout, and answering or acting based on what is visible. It probes fine-grained spatial and visual reasoning in realistic computer-use settings.","L1: Visual Perception, Language Comprehension
L2: Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Spatial Representation & Mapping, Decision-making, Working Memory
L3: ",L2
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,CharXiv Reasoning evaluates whether models can answer questions that require interpreting and reasoning over scientific figures (and associated context) from research papers. It emphasizes extracting quantitative/structural information from plots and diagrams and combining it with textual constraints to reach correct conclusions.,"L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Logical Reasoning, Working Memory, Attention, Semantic Understanding & Context Recognition
L3: ",L2
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU extends multimodal reasoning evaluation to video, requiring models to integrate information over time (events, actions, and changes) and answer questions that depend on temporal context. It tests comprehension of dynamic scenes and the ability to retain and reason over longer visual sequences.","L1: Visual Perception, Language Comprehension
L2: Attention, Working Memory, Scene Understanding & Visual Reasoning, Multisensory Integration
L3: Cognitive Timing & Predictive Modeling",L3
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) tests long-context multi-round coreference and retrieval by embedding multiple similar “needle” interactions inside long “haystacks” and asking the model to reproduce the correct referenced response. The 8-needle variant increases distractors and ambiguity, stressing robust context tracking rather than surface matching.","L1: Language Comprehension
L2: Working Memory, Attention, Episodic Memory, Semantic Understanding & Context Recognition
L3: Inhibitory Control",L3
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval evaluates well-specified professional “knowledge work” across many occupations, where models must produce real work artifacts (e.g., plans, analyses, slides/spreadsheets) judged against human professionals. It emphasizes end-to-end task execution quality, including reasoning about constraints, formatting, and usefulness for downstream decision-making.","L1: Language Production, Language Comprehension
L2: Planning, Decision-making, Semantic Understanding & Context Recognition, Working Memory, Adaptive Error Correction
L3: Self-reflection",L3
SWE-Lancer,https://openai.com/index/swe-lancer/,https://arxiv.org/abs/2502.12115,"SWE-Lancer evaluates agentic software engineering in more end-to-end settings, emphasizing reliable bug fixing and implementation work that resembles real engineering tickets. Tasks often require understanding large codebases, making coordinated multi-file edits, and producing patches that pass automated checks.","L1: Language Comprehension, Language Production
L2: Planning, Adaptive Error Correction, Working Memory, Decision-making, Semantic Understanding & Context Recognition
L3: ",L2
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks evaluates algorithmic reasoning over graph-structured data presented in text, requiring models to follow traversal rules (e.g., BFS-style walks) and answer queries that depend on correct path/parent relationships. It stresses systematic step-by-step state tracking and resisting shortcuts when graphs are large or distractors are present.","L1: Language Comprehension
L2: Spatial Representation & Mapping, Logical Reasoning, Working Memory, Attention
L3: Inhibitory Control",L3
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon evaluates general tool-use competence across diverse tasks where models must select, call, and chain tools correctly, then synthesize outputs into a final solution. It emphasizes robustness to tool errors, argument formatting, and multi-step orchestration under realistic constraints.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory
L3: Inhibitory Control",L3
HMMT,https://matharena.ai/?view=problem&comp=hmmt--hmmt_feb_2025,https://arxiv.org/abs/2505.23281,"HMMT uses problems from the Harvard-MIT Mathematics Tournament, which are typically harder and more proof- and insight-oriented than standard competition questions. It evaluates multi-step reasoning, creative decomposition, and precision in symbolic manipulation across longer solution chains.","L1: 
L2: Logical Reasoning, Working Memory, Planning, Adaptive Error Correction
L3: Cognitive Flexibility",L3
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath is a curated set of expert-level mathematics problems designed to be challenging for frontier models and informative about progress on advanced mathematical reasoning. It emphasizes deep multi-step derivations, careful handling of definitions and edge cases, and sustained coherence over long proofs or computations.","L1: 
L2: Logical Reasoning, Working Memory, Planning, Adaptive Error Correction
L3: Cognitive Flexibility",L3
