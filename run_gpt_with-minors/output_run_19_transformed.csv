Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 evaluates agentic coding and system-administration skill by requiring models to complete real tasks inside a terminal environment (e.g., debugging, installing dependencies, running commands, and editing files). Success depends on choosing correct command sequences, interpreting outputs/errors, and iterating toward a working solution under realistic constraints.","L1: Language Comprehension
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory
L3: ",L2
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp measures deep-research performance on questions that require searching and synthesizing information from documents (often under controlled or reproducible retrieval setups). It emphasizes finding relevant evidence, reconciling sources, and producing a grounded final answer rather than relying on memorized knowledge.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Planning, Decision-making, Attention, Working Memory, Episodic Memory
L3: ",L2
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld evaluates multimodal “computer use” where an agent must complete tasks through a graphical desktop environment using screenshots and interactive actions. It tests end-to-end capability: perceiving UI state, planning multi-step workflows across applications, and recovering from mistakes and unexpected UI changes.","L1: Visual Perception
L2: Visual Attention & Eye Movements, Planning, Decision-making, Sensorimotor Coordination, Working Memory, Adaptive Error Correction
L3: ",L2
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI is a fluid-intelligence benchmark where models infer hidden transformation rules from a few example input–output grids and apply them to a new grid. It targets generalization to novel patterns with minimal examples, emphasizing compositional abstraction rather than domain knowledge.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Logical Reasoning, Working Memory
L3: Cognitive Flexibility",L3
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon agent coherence by simulating management of a vending-machine business over many decisions (pricing, inventory, supplier negotiation, etc.). The score reflects sustained strategy, adaptation to changing conditions, and avoiding compounding errors across an extended trajectory.","L1: 
L2: Planning, Decision-making, Reward Mechanisms, Working Memory, Adaptive Error Correction, Semantic Understanding & Context Recognition
L3: Motivational Drives",L3
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity agents on tasks such as finding known vulnerabilities from descriptions and discovering new vulnerabilities in real open-source projects. It stresses precise technical reasoning, hypothesis testing via tools, and careful iteration based on program behavior and evidence.","L1: 
L2: Logical Reasoning, Planning, Decision-making, Working Memory, Adaptive Error Correction, Semantic Understanding & Context Recognition
L3: ",L2
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"SpreadsheetBench tests whether models can navigate and manipulate complex spreadsheets to answer questions or produce correct transformations, often using realistic files and operations. It emphasizes structured data understanding, formula/logic consistency, and multi-step editing workflows.","L1: 
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Planning, Decision-making, Working Memory, Adaptive Error Correction
L3: ",L2
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam (HLE) is a difficult multimodal benchmark spanning frontier academic and professional knowledge, requiring reasoning rather than rote recall. Questions often demand combining domain knowledge with careful multi-step inference, and may be evaluated both with and without tool assistance depending on the setup.","L1: Language Comprehension, Visual Perception
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory
L3: ",L2
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,GPQA Diamond is a subset of extremely challenging graduate-level multiple-choice science questions designed to be resistant to shallow pattern matching. It assesses whether models can perform disciplined scientific reasoning and select the correct option under high distractor quality.,"L1: Language Comprehension
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory
L3: ",L2
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro is a multimodal benchmark for expert-level understanding and reasoning across many disciplines using text-plus-image questions. It probes whether models can integrate visual evidence (diagrams, plots, figures) with domain knowledge to answer complex multiple-choice problems.","L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Logical Reasoning, Multisensory Integration, Visual Attention & Eye Movements, Working Memory
L3: ",L2
MathArena Apex,https://matharena.ai/?view=problem&comp=apex--apex_2025,https://arxiv.org/abs/2505.23281,"MathArena Apex evaluates advanced mathematical problem solving, typically emphasizing rigorous multi-step derivations and correctness under competition-style constraints. It is designed to separate surface-level pattern matching from deeper mathematical reasoning and verification.","L1: 
L2: Logical Reasoning, Working Memory, Planning, Adaptive Error Correction
L3: ",L2
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning focuses on scientific figure understanding and reasoning over charts/plots from research papers, often requiring quantitative interpretation and cross-referencing visual elements with captions or text. It measures the ability to extract the right variables, relationships, and claims from dense visual scientific artifacts.","L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Visual Attention & Eye Movements
L3: ",L2
OmniDocBench 1.5,https://github.com/opendatalab/OmniDocBench,https://arxiv.org/abs/2412.07626,"OmniDocBench 1.5 benchmarks document understanding and OCR-style extraction across heterogeneous layouts, including text, formulas, and tables with reading-order requirements. It evaluates robust parsing of complex page structure and faithful reconstruction of document content.","L1: Visual Perception, Language Comprehension
L2: Visual Attention & Eye Movements, Semantic Understanding & Context Recognition, Spatial Representation & Mapping, Working Memory
L3: ",L2
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU evaluates multimodal reasoning over videos, requiring models to understand events, temporal relationships, and visual details across frames to answer questions. It emphasizes integrating information over time rather than relying on single-frame cues.","L1: Visual Perception
L2: Visual Attention & Eye Movements, Working Memory, Scene Understanding & Visual Reasoning, Multisensory Integration
L3: Cognitive Timing & Predictive Modeling",L3
LiveCodeBench Pro,https://livecodebenchpro.com/projects/livecodebench-pro/leaderboard,https://arxiv.org/abs/2506.11928,"LiveCodeBench Pro assesses real-world coding ability on fresh or continuously updated programming tasks, often emphasizing generalization beyond static training-era benchmarks. It targets writing correct code, debugging failures, and producing working solutions under constrained attempts.","L1: Language Comprehension, Language Production
L2: Logical Reasoning, Planning, Working Memory, Adaptive Error Correction
L3: ",L2
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"The FACTS Benchmark Suite systematically evaluates factuality by testing whether model outputs remain consistent with evidence, avoid unsupported claims, and correctly handle uncertainty. It aims to measure hallucination-related failure modes across diverse factuality settings rather than a single QA format.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Working Memory
L3: Inhibitory Control, Self-reflection",L3
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA evaluates pragmatic reasoning in everyday situations across many languages and cultural contexts, focusing on what action or interpretation is sensible given a scenario. It probes whether models can apply commonsense, context, and intent rather than only literal text matching.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Decision-making
L3: Social Reasoning & Theory of Mind, Cognitive Flexibility",L3
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,MRCR v2 (8-needle) tests long-context multi-round coreference resolution by embedding multiple similar “needle” interactions inside long “haystacks” and asking the model to reproduce the correct referenced content. It stresses robust retrieval and disambiguation under high interference and long-range dependencies.,"L1: Language Comprehension
L2: Working Memory, Attention, Episodic Memory, Semantic Understanding & Context Recognition
L3: ",L2
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval measures performance on well-specified professional “knowledge work” tasks across many occupations, typically judged via human comparisons to expert outputs. Tasks require producing real artifacts (e.g., plans, analyses, presentations/spreadsheets) with correctness, clarity, and practical usefulness.","L1: Language Production
L2: Planning, Decision-making, Semantic Understanding & Context Recognition, Working Memory
L3: Self-reflection",L3
SWE-Lancer,https://openai.com/index/swe-lancer/,https://arxiv.org/abs/2502.12115,"SWE-Lancer evaluates software engineering agents on realistic repository-based tasks that resemble professional development work, often including longer-horizon debugging and feature implementation. It emphasizes choosing effective engineering actions, maintaining context across files, and converging on a correct patch.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory, Logical Reasoning
L3: ",L2
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks benchmarks graph reasoning over sequences, such as following edges, retrieving parents, or performing BFS-style traversal under memory and distractor pressure. It tests whether models can maintain and manipulate structured relational state across many steps.","L1: 
L2: Spatial Representation & Mapping, Working Memory, Logical Reasoning, Attention, Planning
L3: ",L2
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon evaluates whether models can solve tasks by selecting, invoking, and chaining tools across heterogeneous tool ecosystems, with correctness depending on both tool choice and parameterization. It stresses robust orchestration, error handling, and integration of tool outputs into a coherent solution.","L1: Language Comprehension
L2: Planning, Decision-making, Working Memory, Adaptive Error Correction, Semantic Understanding & Context Recognition
L3: ",L2
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath targets expert-level mathematics beyond standard contest sets, designed to be difficult for both humans and models and to better reflect frontier reasoning demands. It emphasizes deep multi-step derivations, careful verification, and resilience against subtle errors.","L1: 
L2: Logical Reasoning, Working Memory, Planning, Adaptive Error Correction
L3: ",L2
