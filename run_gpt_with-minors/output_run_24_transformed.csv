Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates software-engineering agents by asking them to produce patches that resolve real issues in GitHub repositories, then running the project’s tests to verify correctness. The “Verified” subset uses tasks that have been filtered/validated to be solvable and to have reliable evaluation signals, making it a common headline metric for agentic coding.","L1: Language Comprehension
L2: Planning, Logical Reasoning, Working Memory, Adaptive Error Correction, Decision-making
L3: ",L2
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro is a harder, larger software-engineering benchmark intended to be more industrially representative and more resistant to shortcutting/contamination than earlier variants. It typically spans multiple programming languages and demands more robust debugging, codebase navigation, and patch synthesis under realistic constraints.","L1: Language Comprehension
L2: Planning, Logical Reasoning, Working Memory, Adaptive Error Correction, Decision-making
L3: Cognitive Flexibility",L3
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp evaluates “deep research” behavior: models must browse or retrieve information from documents/web sources to answer questions that require multi-step investigation and synthesis. It emphasizes citation-quality grounding, cross-document integration, and avoiding hallucinations when the answer requires lookup rather than recall.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Attention, Working Memory, Semantic Understanding & Context Recognition
L3: ",L2
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench evaluates interactive, policy-constrained agent behavior in multi-turn customer-support style environments (e.g., retail, airline, telecom) with simulated users and tool/API calls. Success requires correctly following domain policies while still resolving the user’s goal through appropriate tool use across several steps.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Attention, Working Memory
L3: Social Reasoning & Theory of Mind, Inhibitory Control",L3
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,ARC-AGI (Abstraction and Reasoning Corpus) tests fluid reasoning by giving a few input–output grid examples and requiring inference of the latent transformation rule for a new grid. It is designed to measure generalization to novel pattern-manipulation tasks with minimal training signal per task.,"L1: 
L2: Logical Reasoning, Working Memory, Attention, Spatial Representation & Mapping, Scene Understanding & Visual Reasoning
L3: Cognitive Flexibility",L3
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas measures real-world tool use via the Model Context Protocol (MCP), where models must discover, call, and chain tools hosted on MCP servers to complete tasks. It stresses reliable API invocation, multi-step workflow execution, and robust handling of tool errors and intermediate results.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Working Memory, Adaptive Error Correction, Attention
L3: ",L2
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a large, frontier-difficulty benchmark spanning many academic/professional domains (often including multimodal questions) intended to probe the limits of current generalist models. It is commonly reported both with and without tool access (e.g., web search, code) to separate pure reasoning/knowledge from tool-augmented performance.","L1: Language Comprehension, Language Production
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Planning, Decision-making, Scene Understanding & Visual Reasoning, Working Memory
L3: ",L2
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 uses problems from the American Invitational Mathematics Examination, requiring exact numerical answers (typically 0–999) and careful symbolic/quantitative reasoning. It is widely used to assess mathematical problem solving under competition-style constraints, sometimes also reported with tool support (e.g., Python) for verification.","L1: 
L2: Logical Reasoning, Working Memory, Attention
L3: Cognitive Flexibility",L3
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,GPQA Diamond is the highest-quality subset of GPQA: challenging graduate-level multiple-choice science questions designed to be “Google-proof” and to require real reasoning rather than memorized facts. Performance depends on integrating scientific knowledge with multi-step inference under distractor options.,"L1: 
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Attention, Decision-making
L3: ",L2
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends MMLU-style broad academic evaluation into multiple languages, testing knowledge and reasoning across many subjects with multilingual prompts. It is used to measure cross-lingual generalization and whether capabilities transfer beyond English.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Attention
L3: ",L2
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro evaluates multimodal, multi-discipline understanding and reasoning using images (e.g., diagrams, charts, figures) paired with text questions, often at expert level. Compared to simpler vision QA, it emphasizes cross-modal grounding and higher-level reasoning over visual evidence.","L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Attention, Working Memory
L3: ",L2
ScreenShot-Pro,https://gui-agent.github.io/grounding-leaderboard/,https://arxiv.org/abs/2504.07981,"ScreenShot-Pro tests GUI understanding from high-resolution screenshots, requiring models to interpret interface layout, identify relevant elements, and answer questions or take actions grounded in the screen. It is used as a proxy for computer-use competence and precise visual grounding in professional software contexts.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Spatial Representation & Mapping, Decision-making, Attention
L3: ",L2
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning evaluates reasoning over scientific figures (often charts/plots) from arXiv-style papers, requiring extracting quantitative/relational information from visuals and integrating it with the question. It is commonly used to measure chart/figure comprehension and multi-step inference from scientific visual evidence.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Logical Reasoning, Attention, Working Memory, Multisensory Integration, Semantic Understanding & Context Recognition
L3: ",L2
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU extends multimodal understanding to video, requiring models to integrate information across frames and time to answer questions about events, procedures, and visual evidence. It stresses temporal integration, robust perception, and reasoning grounded in dynamic scenes.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Attention, Working Memory, Multisensory Integration, Logical Reasoning
L3: Cognitive Timing & Predictive Modeling",L3
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) is a long-context evaluation where multiple similar “needle” turns are embedded in a large “haystack” of dialogue, and the model must retrieve the correct referenced response/attribute for a specified needle. It probes whether models can maintain and use precise referential information across very long contexts under strong distractor interference.","L1: Language Comprehension
L2: Working Memory, Attention, Semantic Understanding & Context Recognition
L3: Inhibitory Control",L3
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval evaluates economically valuable knowledge-work tasks across many occupations, where models produce real work artifacts (e.g., analyses, plans, documents, spreadsheets/slides) judged against professional baselines. It emphasizes end-to-end task completion quality, instruction following, and practical decision-making rather than only short-form Q&A.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Working Memory, Semantic Understanding & Context Recognition
L3: Self-reflection",L3
SWE-Lancer,https://openai.com/index/swe-lancer/,https://arxiv.org/abs/2502.12115,"SWE-Lancer is an agentic software-engineering benchmark focused on completing realistic engineering work beyond isolated bug fixes, often involving larger codebases, multi-step changes, and evaluation via tests or task-specific criteria. It targets reliability in end-to-end implementation and debugging under constraints closer to real development workflows.","L1: Language Comprehension
L2: Planning, Decision-making, Logical Reasoning, Working Memory, Adaptive Error Correction
L3: Cognitive Flexibility",L3
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks evaluates long-context reasoning by placing graph descriptions/edges in context and asking traversal queries (e.g., reachability, BFS-like steps, parent/neighbor relationships) that require systematic navigation rather than surface pattern matching. It stresses consistent multi-step computation over structured data embedded in text.","L1: 
L2: Spatial Representation & Mapping, Logical Reasoning, Working Memory, Attention, Planning
L3: ",L2
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon evaluates general tool-use competence across diverse APIs/tools, requiring models to select appropriate tools, sequence calls, and synthesize outputs into correct final answers. It emphasizes workflow reliability, error recovery, and the ability to coordinate heterogeneous tools under time/step constraints.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Working Memory, Adaptive Error Correction, Attention
L3: ",L2
HMMT,https://matharena.ai/?view=problem&comp=hmmt--hmmt_feb_2025,https://arxiv.org/abs/2505.23281,"HMMT uses problems from the Harvard-MIT Mathematics Tournament, typically harder and more proof/insight-oriented than many standard school exams. It tests deep mathematical reasoning, careful constraint handling, and sustained multi-step derivations under competition conditions.","L1: 
L2: Logical Reasoning, Working Memory, Attention
L3: Cognitive Flexibility",L3
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath is a frontier-level mathematics benchmark designed to probe expert and research-adjacent problem solving, often with hidden or carefully controlled sets and tiered difficulty. It emphasizes long-horizon reasoning, precise symbolic manipulation, and robustness against shallow pattern matching.","L1: 
L2: Logical Reasoning, Working Memory, Attention, Planning
L3: Cognitive Flexibility",L3
