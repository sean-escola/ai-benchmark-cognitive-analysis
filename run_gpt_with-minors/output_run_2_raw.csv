Benchmark,Website,Paper,Description,Cognitive Functions
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates end-to-end software engineering by asking a model to produce a patch for real GitHub issues and validating solutions by running repository test suites. The “Verified” set emphasizes tasks that have been confirmed solvable and reliably testable, reducing false negatives from flaky tests and underspecified bug reports.","Planning, Logical Reasoning, Working Memory, Adaptive Error Correction, Semantic Understanding & Context Recognition, Decision-making"
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro is a larger, harder successor benchmark that targets more realistic and contamination-resistant software engineering tasks, including greater diversity in repos and problem types. It emphasizes robustness on complex codebases where success depends on correctly diagnosing failures, making coherent multi-file edits, and satisfying automated checks.","Planning, Logical Reasoning, Working Memory, Adaptive Error Correction, Decision-making, Semantic Understanding & Context Recognition"
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp measures deep-research and browsing-style question answering where models must locate, integrate, and cite information from a large document collection (or web-like corpus) under realistic retrieval constraints. Strong performance requires decomposing an information need, searching iteratively, and synthesizing evidence into a grounded final response.","Planning, Attention, Working Memory, Episodic Memory, Language Comprehension, Language Production, Semantic Understanding & Context Recognition, Decision-making"
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench evaluates interactive agent behavior in multi-turn customer-support environments, where the agent must use tools/APIs and follow domain policies while helping a simulated user. The benchmark stresses reliability across long dialogues, correct tool invocation, and adherence to policy constraints rather than only producing a plausible answer.","Planning, Decision-making, Inhibitory Control, Social Reasoning & Theory of Mind, Language Comprehension, Language Production, Working Memory, Reward Mechanisms, Adaptive Error Correction"
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI tests fluid, few-shot abstract reasoning on grid-based transformation puzzles, where the model must infer the underlying rule from a handful of examples and apply it to a new input. It is designed to reduce reliance on memorized knowledge and instead probe novel pattern induction and compositional generalization.","Logical Reasoning, Working Memory, Cognitive Flexibility, Attention, Spatial Representation & Mapping, Scene Understanding & Visual Reasoning"
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas evaluates real-world tool use via the Model Context Protocol, focusing on whether models can discover tools, call them with correct arguments, handle errors, and compose multi-step workflows across services. Tasks resemble production integrations where success depends on reliable action selection and correct synthesis of tool outputs.","Planning, Decision-making, Working Memory, Adaptive Error Correction, Language Comprehension, Language Production, Semantic Understanding & Context Recognition"
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a difficult multimodal benchmark intended to sit near the frontier of human knowledge, spanning many domains and problem formats. It probes whether models can reason through unfamiliar, technical questions, sometimes benefiting from tool use (e.g., search or code) while maintaining grounded, coherent answers.","Language Comprehension, Language Production, Logical Reasoning, Working Memory, Semantic Understanding & Context Recognition, Scene Understanding & Visual Reasoning, Visual Perception, Planning, Decision-making"
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 is a competition-math evaluation based on the American Invitational Mathematics Examination, where each problem requires multi-step reasoning and a final integer answer. The tasks stress symbolic manipulation, careful case analysis, and error checking under tight problem statements.","Logical Reasoning, Working Memory, Planning, Adaptive Error Correction, Attention"
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a curated subset of graduate-level, “Google-proof” multiple-choice science questions designed to be difficult for non-experts. It emphasizes disciplined reasoning over physics/chemistry/biology content where shallow pattern matching tends to fail.","Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Attention, Decision-making"
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends broad academic knowledge testing into many non-English languages, covering dozens of subjects in a multiple-choice format. It measures whether models can maintain reasoning quality and factual reliability across languages and culturally varied formulations of similar concepts.","Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Attention"
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro is a challenging multimodal benchmark that requires models to answer expert-style questions by combining textual context with images such as diagrams, charts, figures, and problem setups across disciplines. It stresses visual grounding, cross-modal integration, and multi-step reasoning beyond simple captioning.","Visual Perception, Scene Understanding & Visual Reasoning, Multisensory Integration, Spatial Representation & Mapping, Language Comprehension, Logical Reasoning, Working Memory, Visual Attention & Eye Movements"
ScreenShot-Pro,https://gui-agent.github.io/grounding-leaderboard/,https://arxiv.org/abs/2504.07981,"ScreenShot-Pro evaluates models on understanding high-resolution screenshots from real software interfaces, requiring precise grounding in layout and UI semantics. Tasks often involve identifying relevant regions or controls and answering questions that depend on spatial relationships and interface conventions.","Visual Perception, Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Spatial Representation & Mapping, Multisensory Integration, Language Comprehension, Working Memory"
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning tests scientific figure and chart understanding drawn from research-paper contexts, requiring models to read visual encodings, legends, and annotations and then reason about trends or claims. It targets faithful extraction from dense scientific visuals and correct quantitative/structural interpretation rather than generic image description.","Visual Perception, Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Multisensory Integration, Language Comprehension, Logical Reasoning, Working Memory, Semantic Understanding & Context Recognition"
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU extends multimodal understanding to the temporal domain by asking questions that depend on events across video frames, audio/visual cues, and narrative context. It stresses integrating information over time, tracking entities/actions, and answering with coherent reasoning about dynamic scenes.","Visual Perception, Scene Understanding & Visual Reasoning, Multisensory Integration, Working Memory, Attention, Cognitive Timing & Predictive Modeling, Language Comprehension, Logical Reasoning, Spatial Representation & Mapping"
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) evaluates long-context retrieval and multi-round co-reference by embedding multiple similar “needle” interactions inside long “haystacks” and asking the model to reproduce the correct response for a specified needle. The 8-needle variant increases interference and distractor similarity, stressing robustness to confusion over long spans.","Working Memory, Episodic Memory, Attention, Semantic Understanding & Context Recognition, Logical Reasoning, Inhibitory Control"
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval measures performance on well-specified professional knowledge-work tasks across many occupations, judged by expert humans via head-to-head comparisons against human-produced artifacts. It emphasizes producing usable work products (e.g., spreadsheets, plans, analyses) with correct reasoning, appropriate structure, and practical completeness.","Planning, Decision-making, Language Comprehension, Language Production, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Adaptive Error Correction, Self-reflection, Social Reasoning & Theory of Mind"
SWE-Lancer,https://openai.com/index/swe-lancer/,https://arxiv.org/abs/2502.12115,"SWE-Lancer evaluates advanced, agentic software engineering on realistic tasks where models must understand a codebase, implement changes, and satisfy project-specific constraints and tests. It targets higher-level engineering competence such as coordinating multi-step edits, handling ambiguous requirements, and delivering patches that integrate cleanly.","Planning, Decision-making, Logical Reasoning, Working Memory, Adaptive Error Correction, Semantic Understanding & Context Recognition"
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks tests structured reasoning over graph descriptions, typically requiring multi-hop traversal (e.g., BFS-style walks) and correct tracking of visited nodes/paths under distracting context. It emphasizes precise state tracking and systematic exploration rather than world knowledge.","Logical Reasoning, Working Memory, Attention, Spatial Representation & Mapping, Planning, Inhibitory Control"
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon evaluates tool-using agents on tasks that require selecting appropriate tools, composing multi-step calls, and recovering from failures to reach a correct end result. It stresses reliable orchestration across heterogeneous tools and consistent integration of intermediate outputs into final answers.","Planning, Decision-making, Working Memory, Adaptive Error Correction, Language Comprehension, Language Production, Semantic Understanding & Context Recognition"
HMMT,https://matharena.ai/?view=problem&comp=hmmt--hmmt_feb_2025,https://arxiv.org/abs/2505.23281,"HMMT problems (e.g., Feb 2025 set) reflect high-difficulty competition mathematics from the Harvard-MIT Math Tournament, often requiring creative construction and non-routine proof or computation strategies. Success depends on sustained multi-step reasoning and careful verification of corner cases.","Logical Reasoning, Working Memory, Planning, Cognitive Flexibility, Adaptive Error Correction, Attention"
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath is an expert-level mathematics benchmark designed to be difficult and relatively contamination-resistant, with tiered problem difficulty reaching beyond standard contest math. It probes whether models can perform long-horizon mathematical reasoning, maintain correctness across many steps, and avoid subtle logical or algebraic errors.","Logical Reasoning, Working Memory, Planning, Adaptive Error Correction, Cognitive Flexibility, Attention"
