Benchmark,Website,Paper,Description,Cognitive Functions
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 evaluates autonomous coding and sysadmin-style problem solving inside real command-line environments. Models must interpret a task, execute shell commands, edit files, and iterate until tests or verifiers pass, emphasizing reliable tool-driven execution under environment constraints.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Language Comprehension, Language Production"
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp measures “deep research” performance where an agent must search, read, and synthesize information from a controlled document collection or web-like index to answer questions. It stresses multi-step retrieval, evidence aggregation, and minimizing citation or reasoning errors across long tool-augmented traces.","Planning, Attention, Semantic Understanding & Context Recognition, Working Memory, Language Comprehension, Decision-making"
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld benchmarks multimodal computer-use agents on realistic desktop tasks (e.g., navigating apps, configuring settings, filling forms) with a step limit. Models must perceive the screen, choose actions (mouse/keyboard-like), recover from mistakes, and complete goals in dynamic GUI environments.","Visual Perception, Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Planning, Decision-making, Sensorimotor Coordination, Adaptive Error Correction"
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI is a few-shot “fluid intelligence” benchmark using abstract grid-based transformation puzzles: given a handful of input–output examples, the model must infer the hidden rule and produce the correct output for a new input. It targets generalization to novel patterns rather than domain memorization.","Logical Reasoning, Cognitive Flexibility, Working Memory, Spatial Representation & Mapping, Visual Perception, Scene Understanding & Visual Reasoning"
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon autonomous agency by running a simulated vending machine business over an extended period. Agents must plan inventory, pricing, procurement, and negotiation-like interactions while maintaining coherence and adapting strategy to changing conditions.","Planning, Decision-making, Reward Mechanisms, Working Memory, Cognitive Timing & Predictive Modeling, Adaptive Error Correction, Self-reflection"
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym tests cybersecurity agent capability on tasks such as locating known vulnerabilities in real open-source projects and discovering previously unknown issues. It emphasizes structured investigation, codebase navigation, hypothesis testing, and producing correct exploit-relevant findings or patches under time/interaction constraints.","Logical Reasoning, Planning, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition, Decision-making"
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"SpreadsheetBench evaluates the ability to understand, edit, and compute over complex spreadsheets resembling real workplace artifacts. Tasks often require reading tables, applying correct formulas/transformations, and producing verifiable outputs while managing multi-step dependencies across cells and sheets.","Logical Reasoning, Planning, Working Memory, Adaptive Error Correction, Language Comprehension, Decision-making"
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a frontier, multimodal exam-style benchmark spanning advanced academic and professional topics. It requires integrating text, figures, and sometimes tool-like reasoning to answer challenging questions that are designed to stress general reasoning and broad knowledge under low contamination tolerance.","Language Comprehension, Logical Reasoning, Visual Perception, Scene Understanding & Visual Reasoning, Working Memory, Semantic Understanding & Context Recognition"
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,GPQA Diamond is a high-quality subset of extremely difficult graduate-level science multiple-choice questions designed to be “Google-proof.” It targets deep scientific reasoning and careful elimination among plausible distractors rather than simple recall.,"Language Comprehension, Logical Reasoning, Working Memory, Semantic Understanding & Context Recognition"
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro is a harder, professional-grade variant of MMMU that tests multidisciplinary multimodal understanding via questions grounded in images (charts, diagrams, screenshots) plus text. It stresses visual grounding, cross-domain knowledge, and robust reasoning in settings closer to real expert work.","Visual Perception, Scene Understanding & Visual Reasoning, Language Comprehension, Multisensory Integration, Logical Reasoning, Visual Attention & Eye Movements, Working Memory"
MathArena Apex,https://matharena.ai/?view=problem&comp=apex--apex_2025,https://arxiv.org/abs/2505.23281,"MathArena Apex evaluates advanced mathematical problem solving with competition-style questions and rigorous scoring. It emphasizes multi-step derivations, selecting the right technique, and maintaining correctness under complex symbolic constraints.","Logical Reasoning, Working Memory, Planning, Adaptive Error Correction"
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning evaluates reasoning over scientific figures and related context from arXiv-style papers, often involving charts and plotted results. Models must extract quantitative/qualitative information from visuals, align it with text, and answer higher-level inference questions; tool use (e.g., Python) can be enabled in some settings.","Visual Perception, Scene Understanding & Visual Reasoning, Semantic Understanding & Context Recognition, Logical Reasoning, Multisensory Integration, Working Memory"
OmniDocBench 1.5,https://github.com/opendatalab/OmniDocBench,https://arxiv.org/abs/2412.07626,"OmniDocBench 1.5 evaluates document understanding and reconstruction across heterogeneous document types, including text, formulas, tables, and reading order. It emphasizes accurate visual-to-structured extraction and layout-aware interpretation rather than only plain OCR.","Visual Perception, Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Language Comprehension, Working Memory"
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU benchmarks video understanding and multimodal reasoning across diverse domains, requiring models to integrate information across time. Tasks often depend on tracking events, recognizing salient frames, and answering questions that require temporal and causal inference.","Visual Perception, Multisensory Integration, Cognitive Timing & Predictive Modeling, Scene Understanding & Visual Reasoning, Working Memory, Attention"
LiveCodeBench Pro,https://livecodebenchpro.com/projects/livecodebench-pro/leaderboard,https://arxiv.org/abs/2506.11928,"LiveCodeBench Pro evaluates code generation and debugging on up-to-date programming tasks under standardized judging, often summarized as an ELO-style rating. It emphasizes producing correct, runnable solutions under time pressure and reducing failure modes like off-by-one errors or incorrect API usage.","Logical Reasoning, Planning, Adaptive Error Correction, Working Memory, Language Comprehension, Language Production"
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"FACTS Benchmark Suite evaluates LLM factuality across a collection of tasks designed to measure correctness, grounding, and resistance to hallucination. It emphasizes truthfulness under uncertainty, careful attribution to sources when available, and avoiding confident fabrication.","Semantic Understanding & Context Recognition, Language Comprehension, Inhibitory Control, Self-reflection, Working Memory"
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA extends physical commonsense reasoning evaluation to many languages with non-parallel data, probing whether models can generalize intuitive physics and everyday affordances beyond English. It tests whether models choose the more plausible physical interaction outcome given a situation description.","Language Comprehension, Logical Reasoning, Semantic Understanding & Context Recognition, Spatial Representation & Mapping, Cognitive Flexibility"
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) is a long-context multi-round coreference/retrieval evaluation where multiple similar “needle” requests are embedded within a large “haystack” dialogue/document. The model must reproduce the correct response associated with a specified needle, testing robust long-range dependency handling and interference resistance.","Working Memory, Attention, Language Comprehension, Episodic Memory, Semantic Understanding & Context Recognition"
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval evaluates economically valuable, well-specified knowledge work across many occupations, with outputs judged against professional baselines (often via pairwise comparisons). Tasks include producing realistic work artifacts (e.g., spreadsheets, plans, presentations), stressing end-to-end project execution quality rather than short QA.","Planning, Decision-making, Language Production, Language Comprehension, Self-reflection, Social Reasoning & Theory of Mind, Working Memory"
SWE-Lancer,https://openai.com/index/swe-lancer/,https://arxiv.org/abs/2502.12115,"SWE-Lancer evaluates agentic software engineering on realistic repositories and tasks that resemble professional bug-fixing and feature work, emphasizing robustness and practical impact. It stresses translating natural-language requirements into correct patches and handling tooling, tests, and multi-file changes reliably.","Logical Reasoning, Planning, Adaptive Error Correction, Working Memory, Language Comprehension, Decision-making"
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks tests long-context reasoning over graph-structured data encoded as sequences, such as following edges, tracking parents, or performing BFS-like traversals. It probes whether models can maintain consistent state over many steps and avoid drift in structured multi-hop reasoning.","Working Memory, Spatial Representation & Mapping, Logical Reasoning, Attention, Planning"
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon measures general tool-use competence across varied APIs and multi-step workflows, focusing on selecting the right tools, calling them correctly, and recovering from tool errors. It emphasizes orchestration—keeping goals, intermediate results, and constraints aligned across a long action trace.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Language Comprehension"
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath evaluates expert-level mathematics beyond standard contest sets, often requiring deeper multi-step reasoning and proof-like derivations; some tiers permit computational assistance. It is designed to probe the limits of mathematical reasoning and error-free symbolic manipulation under hard, novel problems.","Logical Reasoning, Working Memory, Planning, Adaptive Error Correction"
