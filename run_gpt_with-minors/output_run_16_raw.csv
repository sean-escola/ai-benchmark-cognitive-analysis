Benchmark,Website,Paper,Description,Cognitive Functions
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates LLM-based software engineering by asking a model to produce patches that fix real GitHub issues, then running repository tests to verify correctness. The “Verified” split emphasizes tasks that have been validated as solvable and aims to reduce evaluation noise by using robust, reproducible test harnesses.","Planning, Logical Reasoning, Working Memory, Adaptive Error Correction, Semantic Understanding & Context Recognition, Decision-making"
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro is a harder, more contamination-resistant software engineering benchmark with a larger and more diverse set of real-world coding tasks (including multiple languages) and stricter evaluation. It stresses end-to-end problem solving over larger codebases, requiring reliable debugging, refactoring, and test-driven iteration.","Planning, Logical Reasoning, Working Memory, Adaptive Error Correction, Semantic Understanding & Context Recognition, Decision-making"
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp evaluates “deep research” abilities by requiring models/agents to answer questions using web browsing and retrieval, typically with citations or evidence from sources. Success depends on decomposing a query, searching effectively, reading selectively, and synthesizing a grounded final answer.","Planning, Decision-making, Attention, Working Memory, Semantic Understanding & Context Recognition, Language Comprehension, Self-reflection"
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench evaluates interactive, tool-using customer-support agents in simulated domains (e.g., retail, airline, telecom) with policies that must be followed across multi-turn dialogues. It measures whether an agent can maintain state, comply with constraints, call APIs correctly, and resolve user goals in realistic conversations.","Language Comprehension, Language Production, Social Reasoning & Theory of Mind, Inhibitory Control, Planning, Decision-making, Working Memory, Empathy, Semantic Understanding & Context Recognition"
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI tests fluid reasoning by presenting few-shot input–output grid transformation puzzles that require inferring hidden rules and applying them to new cases. The benchmark is designed to emphasize novelty and abstraction over memorized knowledge, with performance reflecting generalization from minimal examples.","Logical Reasoning, Cognitive Flexibility, Spatial Representation & Mapping, Visual Perception, Working Memory, Attention"
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas evaluates real-world tool use via the Model Context Protocol (MCP), where models must discover and invoke tools across multi-step workflows and integrate results into a correct response. It stresses robustness to tool errors, correct parameterization, and orchestration across heterogeneous services.","Planning, Decision-making, Working Memory, Adaptive Error Correction, Language Comprehension, Semantic Understanding & Context Recognition, Inhibitory Control"
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam (HLE) is a large, frontier-difficulty benchmark spanning advanced academic and professional questions, including multimodal items, intended to probe the limits of model reasoning and knowledge. It is often run with optional tools (e.g., search or code) to assess how well models can augment reasoning with external resources while staying grounded.","Language Comprehension, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Planning, Decision-making, Scene Understanding & Visual Reasoning, Visual Perception"
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,AIME 2025 consists of competition-style mathematics problems that require multi-step reasoning under tight answer formats (typically integer answers). It probes a model’s ability to carry out precise symbolic and numerical reasoning rather than relying on retrieval or vague explanations.,"Logical Reasoning, Working Memory, Planning, Attention"
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a challenging multiple-choice science benchmark curated to be “Google-proof,” emphasizing questions where experts succeed and non-experts struggle. It targets deep conceptual understanding and careful elimination of distractors across biology, chemistry, and physics.","Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Decision-making, Attention"
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends broad academic knowledge and reasoning evaluation across many subjects into multiple languages, testing whether competence transfers beyond English. It probes multilingual comprehension and culturally/linguistically robust knowledge application under standardized question formats.","Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Cognitive Flexibility"
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro is an expert-level multimodal benchmark where models must answer questions that require combining text with images such as diagrams, charts, and scientific figures. It emphasizes rigorous reasoning over visual evidence and reduces shortcutting by using harder items and evaluation protocols.","Visual Perception, Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Working Memory, Attention, Semantic Understanding & Context Recognition"
ScreenShot-Pro,https://gui-agent.github.io/grounding-leaderboard/,https://arxiv.org/abs/2504.07981,"ScreenShot-Pro evaluates screenshot understanding for GUI grounding, where the model must interpret high-resolution interface images and answer questions or identify actionable targets. It stresses spatial/layout reasoning over UI elements and mapping visual cues to correct semantic actions.","Visual Perception, Visual Attention & Eye Movements, Spatial Representation & Mapping, Scene Understanding & Visual Reasoning, Decision-making, Planning, Working Memory"
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning focuses on reasoning over scientific content extracted from arXiv-style documents, often involving complex figures, plots, and accompanying text. The task rewards careful interpretation of visual encodings and integration with technical language to derive correct answers.","Scene Understanding & Visual Reasoning, Visual Perception, Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Attention"
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU evaluates multimodal understanding and reasoning over video inputs, requiring integration of information across time rather than single frames. It stresses temporal coherence, event understanding, and multi-step reasoning grounded in dynamic visual context.","Visual Perception, Scene Understanding & Visual Reasoning, Cognitive Timing & Predictive Modeling, Attention, Working Memory, Multisensory Integration, Logical Reasoning"
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 is a long-context multi-round coreference and retrieval-style benchmark where many similar “needle” requests are embedded in long conversational haystacks. The 8-needle setting tests whether a model can reliably track, retrieve, and reproduce the correct referenced content under heavy distractors.","Working Memory, Attention, Episodic Memory, Language Comprehension, Semantic Understanding & Context Recognition, Inhibitory Control"
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval evaluates economically valuable professional knowledge work across many occupations by having models produce real work artifacts (e.g., plans, analyses, spreadsheets, presentations) judged against expert outputs. It emphasizes end-to-end task completion quality, instruction adherence, and pragmatic decision-making under realistic constraints.","Planning, Decision-making, Language Production, Language Comprehension, Semantic Understanding & Context Recognition, Self-reflection, Working Memory"
SWE-Lancer,https://openai.com/index/swe-lancer/,https://arxiv.org/abs/2502.12115,"SWE-Lancer evaluates agentic software engineering on more open-ended, real-world development tasks akin to contracting or freelance work, where success involves selecting approaches, implementing changes, and validating outcomes. Compared to narrower bug-fix tasks, it stresses longer-horizon execution and reliability over complex repos.","Planning, Decision-making, Working Memory, Adaptive Error Correction, Logical Reasoning, Semantic Understanding & Context Recognition"
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks evaluates reasoning over graph-structured data by requiring models to follow paths, apply traversal rules, and answer queries that depend on correct multi-step navigation. It targets systematic state tracking and resistance to distractors when intermediate steps must be maintained accurately.","Spatial Representation & Mapping, Working Memory, Logical Reasoning, Attention, Planning"
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon evaluates general tool-use competence across diverse APIs and multi-step workflows, typically requiring correct tool selection, argument construction, and iterative recovery from tool failures. It is designed to measure whether models can behave like reliable “operators” in tool-rich environments rather than only generating text.","Planning, Decision-making, Working Memory, Adaptive Error Correction, Inhibitory Control, Semantic Understanding & Context Recognition, Language Comprehension"
HMMT,https://matharena.ai/?view=problem&comp=hmmt--hmmt_feb_2025,https://arxiv.org/abs/2505.23281,"HMMT problems (e.g., Feb 2025 set) are high-difficulty contest mathematics tasks that often require creative multi-step derivations and tight control of algebraic manipulation. They probe robustness on non-routine problem solving beyond standard competition sets.","Logical Reasoning, Working Memory, Planning, Cognitive Flexibility, Attention"
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath is an expert-level mathematics benchmark intended to measure progress on problems closer to research-grade difficulty, often requiring long chains of precise reasoning and careful verification. It emphasizes correctness under complex symbolic structure, where small errors can invalidate an entire solution.","Logical Reasoning, Working Memory, Planning, Attention, Adaptive Error Correction, Cognitive Flexibility"
