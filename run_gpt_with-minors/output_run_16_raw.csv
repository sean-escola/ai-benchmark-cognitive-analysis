Benchmark,Website,Paper,Description,Cognitive Functions
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 evaluates agentic coding and system-operation ability in real command-line environments. Models must plan and execute sequences of shell commands, inspect and modify files, and recover from mistakes under realistic tooling constraints.","Planning, Decision-making, Working Memory, Adaptive Error Correction, Language Comprehension, Language Production"
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp measures deep-research performance: answering questions that require locating, reading, and synthesizing evidence from a large document collection (often via a search tool). Success depends on decomposing the query, iteratively refining searches, and producing a grounded final answer.","Planning, Decision-making, Working Memory, Attention, Language Comprehension, Semantic Understanding & Context Recognition"
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld evaluates computer-use agents in a desktop-like operating system setting, where tasks require navigating GUIs and applications (e.g., settings, browsers, file managers). It stresses robust perception of screens plus multi-step interaction policies over long action sequences.","Visual Perception, Visual Attention & Eye Movements, Planning, Decision-making, Sensorimotor Coordination, Working Memory"
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI tests fluid, example-driven abstraction: given a few input–output grid demonstrations, the model must infer the hidden rule and apply it to a novel grid. It emphasizes out-of-distribution pattern induction rather than memorized knowledge.","Logical Reasoning, Cognitive Flexibility, Working Memory, Attention"
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon autonomy by having an agent run a simulated vending machine business over an extended period. The agent must manage inventory, pricing, supplier negotiation, and strategy while maintaining coherence across thousands of decisions.","Planning, Decision-making, Working Memory, Reward Mechanisms, Cognitive Timing & Predictive Modeling"
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity agent capabilities on real-world software: identifying known vulnerabilities from descriptions and discovering new vulnerabilities. It requires reading code and tool outputs, forming hypotheses, testing them, and iterating toward an exploit or fix.","Logical Reasoning, Planning, Working Memory, Adaptive Error Correction, Attention"
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"SpreadsheetBench measures an agent’s ability to understand, edit, and compute with complex spreadsheets derived from real tasks. Problems often require locating relevant cells/sheets, applying correct formulas or transformations, and validating outputs against constraints.","Planning, Working Memory, Attention, Logical Reasoning, Adaptive Error Correction"
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a frontier, multimodal academic benchmark designed to probe broad expert knowledge and difficult reasoning. Questions can require synthesizing information, interpreting visuals, and (in some settings) using tools like search or code to verify answers.","Language Comprehension, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Visual Perception, Scene Understanding & Visual Reasoning"
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a difficult multiple-choice science QA benchmark intended to be “Google-proof,” emphasizing reasoning over rote recall. The Diamond subset filters for especially high-quality questions that reliably separate expert from non-expert performance.","Language Comprehension, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory"
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro evaluates expert-level multimodal understanding and reasoning across many disciplines, using images paired with text questions and structured answer formats. It stresses interpreting diagrams, charts, tables, and scientific visuals while performing multi-step reasoning.","Visual Perception, Scene Understanding & Visual Reasoning, Multisensory Integration, Language Comprehension, Logical Reasoning, Working Memory"
MathArena Apex,https://matharena.ai/?view=problem&comp=apex--apex_2025,https://arxiv.org/abs/2505.23281,MathArena Apex aggregates challenging competition-style mathematics problems to evaluate advanced mathematical reasoning. It emphasizes correct multi-step derivations and robustness across diverse topics rather than memorization of standard templates.,"Logical Reasoning, Working Memory, Planning, Adaptive Error Correction, Attention"
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning evaluates reasoning over figures from scientific papers (e.g., plots, charts, and visual evidence) combined with natural-language questions. It tests whether a model can extract quantitative/structural information from visuals and connect it to the textual prompt to answer correctly.","Visual Perception, Scene Understanding & Visual Reasoning, Language Comprehension, Logical Reasoning, Working Memory, Attention"
OmniDocBench 1.5,https://github.com/opendatalab/OmniDocBench,https://arxiv.org/abs/2412.07626,"OmniDocBench 1.5 evaluates document understanding and OCR across heterogeneous document elements such as text blocks, formulas, tables, and reading order. It measures end-to-end extraction quality under real layout and rendering variability.","Visual Perception, Attention, Scene Understanding & Visual Reasoning, Language Comprehension, Language Production, Working Memory"
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU evaluates multimodal understanding and reasoning from videos paired with questions, requiring temporal integration across frames. Tasks often test event understanding, causal/goal inference, and fine-grained visual details over time.","Visual Perception, Scene Understanding & Visual Reasoning, Cognitive Timing & Predictive Modeling, Working Memory, Attention, Multisensory Integration"
LiveCodeBench Pro,https://livecodebenchpro.com/projects/livecodebench-pro/leaderboard,https://arxiv.org/abs/2506.11928,"LiveCodeBench Pro evaluates coding ability on competitive-programming-style tasks and practical coding problems, often scored with pass@k or Elo-style ratings. It stresses generating correct, executable solutions, handling edge cases, and debugging failures.","Logical Reasoning, Planning, Working Memory, Adaptive Error Correction, Language Comprehension, Language Production"
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"FACTS Benchmark Suite systematically evaluates factuality and grounding across a collection of tests targeting hallucinations, unsupported claims, and citation/attribution failures. It aims to characterize when models are accurate, when they confabulate, and how reliably they separate known from unknown.","Semantic Understanding & Context Recognition, Language Comprehension, Inhibitory Control, Self-reflection, Working Memory"
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,Global PIQA tests practical commonsense reasoning about everyday physical interactions and outcomes across many languages and locales. It focuses on whether a model can choose plausible actions/effects and generalize commonsense beyond English-centric distributions.,"Language Comprehension, Logical Reasoning, Semantic Understanding & Context Recognition, Cognitive Flexibility"
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) is a long-context evaluation for multi-round coreference and retrieval within extended conversations/documents. It inserts multiple similar “needle” requests into long “haystacks” and asks the model to reproduce the correct referenced response, stressing interference resistance.","Working Memory, Attention, Language Comprehension, Semantic Understanding & Context Recognition"
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval evaluates economically valuable, well-specified professional knowledge-work tasks (e.g., producing spreadsheets, presentations, schedules, or analyses) judged against strong human baselines. It emphasizes end-to-end artifact quality, instruction adherence, and practical decision quality under constraints.","Planning, Decision-making, Language Comprehension, Language Production, Working Memory, Self-reflection, Social Reasoning & Theory of Mind"
SWE-Lancer,https://openai.com/index/swe-lancer/,https://arxiv.org/abs/2502.12115,"SWE-Lancer evaluates real-world software engineering work representative of professional “gig” or contracting tasks, emphasizing end-to-end delivery and correctness. It typically requires understanding a repo, making coordinated code changes, and iterating based on tests or feedback signals.","Planning, Decision-making, Logical Reasoning, Adaptive Error Correction, Working Memory, Language Comprehension, Language Production"
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks evaluates a model’s ability to follow graph-structured relationships described in text and answer traversal queries (e.g., BFS-style reachability or parent-chain reconstruction), often at long context lengths. It stresses faithful state tracking through many hops and resistance to distractors.","Spatial Representation & Mapping, Logical Reasoning, Working Memory, Attention, Cognitive Timing & Predictive Modeling"
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon evaluates tool-use competence across diverse APIs and multi-step workflows, emphasizing correct tool selection, argument construction, and result integration. It probes whether models can reliably orchestrate calls and recover from tool errors or ambiguous outputs.","Planning, Decision-making, Working Memory, Adaptive Error Correction, Language Comprehension, Attention"
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath evaluates advanced mathematics at and beyond typical competition difficulty, with tiered problem sets that stress novelty and depth. It measures sustained multi-step reasoning and precision, often benefiting from verification via computation but not reducible to it.","Logical Reasoning, Working Memory, Planning, Adaptive Error Correction, Attention"
