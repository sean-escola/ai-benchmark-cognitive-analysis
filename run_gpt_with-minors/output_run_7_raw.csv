Benchmark,Website,Paper,Description,Cognitive Functions
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 evaluates agentic command-line performance in sandboxed terminal environments, where models must complete real operational tasks by issuing shell commands, editing files, and interpreting tool outputs. It stresses end-to-end autonomy under an execution loop (observe → act → verify → iterate) rather than pure text-only coding.","Adaptive Error Correction, Decision-making, Language Comprehension, Planning, Semantic Understanding & Context Recognition, Working Memory"
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp measures “deep research” and web-browsing competence on information-seeking problems that require searching, reading, and synthesizing across sources. It emphasizes choosing what to search, when to open/inspect documents, and how to aggregate evidence into a final grounded answer.","Attention, Decision-making, Episodic Memory, Language Comprehension, Planning, Semantic Understanding & Context Recognition, Working Memory"
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld evaluates computer-use agents in a realistic desktop OS environment, requiring interaction with graphical interfaces (apps, settings, web pages) to accomplish multi-step tasks. It tests perception-driven action selection, error recovery from UI misclicks or wrong navigation paths, and maintaining task state over long trajectories.","Adaptive Error Correction, Decision-making, Planning, Scene Understanding & Visual Reasoning, Sensorimotor Coordination, Visual Attention & Eye Movements, Visual Perception, Working Memory"
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,ARC-AGI is a few-shot abstract reasoning benchmark where models infer transformation rules from a small set of input–output grid examples and apply them to a new grid. The tasks are designed to probe novel rule induction and compositional pattern reasoning rather than memorized knowledge.,"Cognitive Flexibility, Logical Reasoning, Planning, Scene Understanding & Visual Reasoning, Spatial Representation & Mapping, Visual Perception, Working Memory"
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 measures long-horizon agent coherence by placing a model in a simulated vending machine business it must run over extended time (e.g., a year), making many sequential decisions. Success requires strategy, tracking inventory/finances, adapting to changing conditions, and balancing short-term actions against long-term outcomes.","Decision-making, Episodic Memory, Planning, Reward Mechanisms, Semantic Understanding & Context Recognition, Working Memory"
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity agency in realistic software contexts, including finding known vulnerabilities and discovering new ones in open-source codebases. It stresses hypothesis-driven investigation, interpreting program behavior, and iterating on attempts under constraints typical of security workflows.","Adaptive Error Correction, Decision-making, Language Comprehension, Logical Reasoning, Planning, Semantic Understanding & Context Recognition, Working Memory"
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"SpreadsheetBench tests whether models can navigate and manipulate complex spreadsheets to produce correct computed outputs and edits, often requiring multi-step operations and formula understanding. It emphasizes structured data reasoning, careful attention to layout/semantics, and verification of results through tool-assisted execution.","Adaptive Error Correction, Attention, Decision-making, Logical Reasoning, Planning, Semantic Understanding & Context Recognition, Working Memory"
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam (HLE) is a large, frontier-difficulty benchmark of expert-level questions (including multimodal items) spanning many domains, intended to stress general reasoning and knowledge at the edge of current models. Many problems require synthesis across concepts, careful reading, and sometimes visual interpretation rather than retrieval of a single fact.","Language Comprehension, Language Production, Logical Reasoning, Scene Understanding & Visual Reasoning, Semantic Understanding & Context Recognition, Visual Perception, Working Memory"
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a curated subset of extremely difficult graduate-level multiple-choice science questions designed to be “Google-proof,” emphasizing reasoning over superficial recall. It probes whether models can use domain knowledge to discriminate between close distractors and maintain consistency under uncertainty.","Decision-making, Language Comprehension, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory"
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro is an advanced multimodal benchmark targeting expert-level understanding and reasoning across many disciplines, combining text with challenging images/figures. It stresses visual interpretation, cross-modal grounding, and multi-step reasoning needed to answer questions about diagrams, charts, and scientific visuals.","Language Comprehension, Logical Reasoning, Multisensory Integration, Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Visual Perception, Working Memory"
MathArena Apex,https://matharena.ai/?view=problem&comp=apex--apex_2025,https://arxiv.org/abs/2505.23281,"MathArena Apex evaluates difficult mathematical problem solving in a competitive setting, focusing on robust multi-step derivations rather than short arithmetic. It is intended to separate shallow pattern-matching from systematic reasoning, often requiring careful intermediate-state tracking and strategy selection.","Decision-making, Logical Reasoning, Planning, Working Memory"
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,CharXiv Reasoning evaluates scientific-figure understanding and paper-style reasoning by asking questions that require interpreting information from research-paper visuals and accompanying context. It targets grounded analytical reasoning about plots/figures and benefits from tool-assisted calculation when available.,"Language Comprehension, Logical Reasoning, Multisensory Integration, Scene Understanding & Visual Reasoning, Visual Perception, Working Memory"
OmniDocBench 1.5,https://github.com/opendatalab/OmniDocBench,https://arxiv.org/abs/2412.07626,"OmniDocBench 1.5 evaluates document understanding/OCR pipelines across diverse document elements (text, formulas, tables, and reading order). It stresses accurate perception of structured layouts and faithful transcription/extraction into machine-usable representations.","Attention, Language Comprehension, Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Visual Perception, Working Memory"
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU extends multimodal understanding to videos, requiring temporal integration of visual evidence with language to answer questions about events, actions, and context. It probes whether models can maintain coherent situation models across time and leverage fine-grained visual cues.","Attention, Cognitive Timing & Predictive Modeling, Language Comprehension, Multisensory Integration, Scene Understanding & Visual Reasoning, Visual Perception, Working Memory"
LiveCodeBench Pro,https://livecodebenchpro.com/projects/livecodebench-pro/leaderboard,https://arxiv.org/abs/2506.11928,"LiveCodeBench Pro measures code generation and interactive programming performance on challenging, time-relevant tasks, often scored via competitive-programming-style execution and comparison. It emphasizes writing correct, efficient code under single-attempt constraints and debugging through iterative refinement when permitted by the harness.","Adaptive Error Correction, Decision-making, Language Comprehension, Language Production, Logical Reasoning, Planning, Working Memory"
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"The FACTS Benchmark Suite systematically evaluates LLM factuality across multiple settings, stressing whether generated statements are supported by evidence and whether models avoid hallucinating. It targets reliability under information uncertainty and encourages calibrated, constraint-respecting generation.","Adaptive Error Correction, Inhibitory Control, Language Comprehension, Language Production, Semantic Understanding & Context Recognition, Working Memory"
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA evaluates commonsense and pragmatic reasoning across many languages and cultural contexts, aiming to test whether models retain robust everyday reasoning beyond English-centric distributions. It emphasizes understanding intent, context, and plausible actions/interpretations under linguistic variation.","Language Comprehension, Logical Reasoning, Semantic Understanding & Context Recognition, Social Reasoning & Theory of Mind, Working Memory"
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) tests long-context, multi-round co-reference and retrieval by embedding multiple similar “needle” interactions within long “haystacks” and asking the model to reproduce the correct referenced response. It stresses precise attention control, resisting interference from near-duplicates, and maintaining consistent context bindings over very long inputs.","Attention, Episodic Memory, Inhibitory Control, Language Comprehension, Semantic Understanding & Context Recognition, Working Memory"
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval evaluates economically valuable “knowledge work” by having models produce real work artifacts (e.g., spreadsheets, presentations, plans) across many occupations and comparing them to expert human work via judges. It probes end-to-end professional task execution: interpreting specifications, producing structured outputs, and making tradeoffs aligned with task goals.","Decision-making, Language Comprehension, Language Production, Planning, Self-reflection, Semantic Understanding & Context Recognition, Working Memory"
SWE-Lancer,https://openai.com/index/swe-lancer/,https://arxiv.org/abs/2502.12115,"SWE-Lancer evaluates software engineering capability on realistic repository-level tasks that require understanding codebases, implementing changes, and producing correct patches. It emphasizes agentic engineering behaviors such as planning an approach, iterating based on test feedback, and minimizing regressions.","Adaptive Error Correction, Decision-making, Language Comprehension, Language Production, Logical Reasoning, Planning, Working Memory"
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks evaluates long-context structured reasoning by embedding graph descriptions and querying traversal-related properties (e.g., following edges, BFS-style reasoning, parent relationships). It stresses maintaining and updating an internal structured state and performing multi-step symbolic-like traversal over large inputs.","Attention, Logical Reasoning, Planning, Spatial Representation & Mapping, Working Memory"
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon evaluates general tool-use competence across diverse tasks requiring selecting tools, calling them with correct arguments, handling failures, and integrating results into a final answer. It emphasizes agent loop robustness, including when to stop, how to recover from tool errors, and how to manage intermediate state across steps.","Adaptive Error Correction, Decision-making, Language Comprehension, Planning, Semantic Understanding & Context Recognition, Working Memory"
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath is a difficult, expert-oriented mathematics benchmark intended to measure progress on research-level or near-research-level quantitative reasoning, often stratified by tiers of difficulty. It stresses long multi-step derivations, careful bookkeeping, and (when tools are allowed) integrating computation as part of a reasoning workflow.","Decision-making, Logical Reasoning, Planning, Working Memory"
