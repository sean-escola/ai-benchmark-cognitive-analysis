Benchmark,Website,Paper,Description,Cognitive Functions
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,SWE-bench Verified evaluates an LLM’s ability to solve real GitHub issues by producing a patch that passes a project’s tests. The “Verified” subset emphasizes tasks that have been validated as solvable and uses rigorous checking to reduce noise in problem statements and expected fixes.,"Language Comprehension, Language Production, Planning, Working Memory, Adaptive Error Correction, Decision-making, Logical Reasoning, Semantic Understanding & Context Recognition"
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro is a harder, more contamination-resistant software engineering benchmark that expands task diversity and difficulty beyond the original SWE-bench setting. Models must generate correct patches across multiple languages and real repositories under stricter evaluation and broader task coverage.","Language Comprehension, Language Production, Planning, Working Memory, Adaptive Error Correction, Decision-making, Logical Reasoning, Semantic Understanding & Context Recognition"
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp measures how well models answer questions that require multi-step web research, typically involving searching, reading, and synthesizing evidence from multiple documents. It is designed to evaluate browsing-style information foraging and the ability to produce supported answers rather than relying on parametric memory alone.","Planning, Decision-making, Attention, Working Memory, Episodic Memory, Language Comprehension, Language Production, Semantic Understanding & Context Recognition, Self-reflection"
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench evaluates interactive agents in simulated customer-support domains (e.g., retail, airline, telecom) where the agent must call APIs and converse over multiple turns. The benchmark emphasizes policy compliance, robust tool use, and resolving user goals under constraints and ambiguous conversational context.","Social Reasoning & Theory of Mind, Empathy, Inhibitory Control, Decision-making, Planning, Working Memory, Language Comprehension, Language Production, Adaptive Error Correction, Reward Mechanisms"
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI tests “fluid” reasoning on novel grid-transformation puzzles where only a few input-output examples are provided per task. Success requires inferring the underlying rule and generalizing to a new input, emphasizing abstraction and systematic generalization rather than memorized knowledge.","Logical Reasoning, Cognitive Flexibility, Working Memory, Attention, Spatial Representation & Mapping, Scene Understanding & Visual Reasoning, Planning"
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas assesses real-world tool use via the Model Context Protocol, requiring models to discover, invoke, and chain tools to complete tasks. It stresses correct API selection/parameterization, multi-step workflow execution, and recovery from tool errors or partial failures.","Planning, Decision-making, Working Memory, Adaptive Error Correction, Inhibitory Control, Language Comprehension, Language Production, Semantic Understanding & Context Recognition"
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a frontier, multi-domain benchmark intended to probe advanced reasoning and knowledge at the edge of expert difficulty, often with multimodal questions. It targets robust synthesis and problem solving under uncertainty, where shallow pattern matching is less likely to succeed.","Logical Reasoning, Language Comprehension, Language Production, Working Memory, Semantic Understanding & Context Recognition, Scene Understanding & Visual Reasoning, Visual Perception, Planning, Self-reflection"
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 consists of competition-style mathematics problems that require multi-step derivations and careful symbolic manipulation. It is commonly used to measure mathematical reasoning accuracy, consistency, and the ability to carry intermediate results without arithmetic or logical slips.","Logical Reasoning, Working Memory, Planning, Attention, Adaptive Error Correction"
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a high-quality subset of graduate-level, “Google-proof” multiple-choice science questions designed to be difficult for non-experts. It emphasizes deep scientific reasoning and precise reading of technical detail rather than retrieval of simple facts.","Language Comprehension, Logical Reasoning, Working Memory, Semantic Understanding & Context Recognition, Decision-making, Attention"
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends MMLU-style breadth evaluation to multiple languages, testing knowledge and reasoning across many academic subjects in non-English settings. It probes whether models maintain consistent understanding and reasoning when the same kinds of questions are posed across diverse linguistic contexts.","Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Attention"
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro is a multimodal benchmark requiring expert-level reasoning over text and images across many disciplines, with an emphasis on robust evaluation settings. Questions often require interpreting diagrams, charts, or figures and integrating them with domain knowledge to select or produce correct answers.","Visual Perception, Scene Understanding & Visual Reasoning, Multisensory Integration, Language Comprehension, Language Production, Logical Reasoning, Working Memory, Attention"
ScreenShot-Pro,https://gui-agent.github.io/grounding-leaderboard/,https://arxiv.org/abs/2504.07981,"ScreenShot-Pro evaluates GUI understanding and grounding from screenshots, where a model must reason about interface elements and their spatial relationships to identify targets or actions. It measures practical visual UI comprehension under realistic, high-resolution screens and varied application layouts.","Visual Perception, Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Spatial Representation & Mapping, Sensorimotor Coordination, Language Comprehension, Decision-making, Working Memory"
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning tests whether models can answer questions about scientific papers using the figures (and often accompanying captions/text), focusing on chart/plot interpretation and scientific visual reasoning. It emphasizes extracting quantitative/relational information from figures and connecting it to the question context.","Visual Perception, Scene Understanding & Visual Reasoning, Language Comprehension, Language Production, Multisensory Integration, Logical Reasoning, Working Memory, Attention"
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU evaluates multimodal understanding over videos, requiring models to integrate information across time (multiple frames/scenes) with textual prompts. Tasks often demand tracking events, recognizing changes, and using temporal context to answer complex questions.","Visual Perception, Scene Understanding & Visual Reasoning, Multisensory Integration, Working Memory, Attention, Cognitive Timing & Predictive Modeling, Language Comprehension, Language Production, Logical Reasoning"
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 is a long-context, multi-round coreference-style evaluation where multiple similar “needle” interactions are embedded in a long “haystack” dialogue/log, and the model must retrieve the correct referenced response. The 8-needle variant stresses robust attention control and interference resistance across many competing, near-duplicate targets.","Working Memory, Attention, Episodic Memory, Language Comprehension, Language Production, Inhibitory Control, Semantic Understanding & Context Recognition"
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval evaluates economically meaningful, well-specified professional tasks across many occupations, judged by expert humans comparing model outputs to professional baselines. It focuses on producing usable work artifacts (e.g., plans, analyses, structured documents) under real-world constraints and quality standards.","Planning, Decision-making, Language Production, Language Comprehension, Semantic Understanding & Context Recognition, Working Memory, Self-reflection, Adaptive Error Correction, Social Reasoning & Theory of Mind"
SWE-Lancer,https://openai.com/index/swe-lancer/,https://arxiv.org/abs/2502.12115,"SWE-Lancer measures autonomous software engineering ability on tasks resembling contract/freelance work, often requiring understanding requirements, making multi-file changes, and delivering a working solution. It emphasizes end-to-end task completion and practical engineering judgment beyond isolated bug fixes.","Language Comprehension, Language Production, Planning, Decision-making, Working Memory, Adaptive Error Correction, Semantic Understanding & Context Recognition, Self-reflection"
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks tests long-context reasoning by requiring models to traverse graph structures described in text and answer queries about reachability, parent pointers, or paths. It stresses consistent stepwise tracking of states over many hops and resistance to distraction in large inputs.","Spatial Representation & Mapping, Working Memory, Attention, Logical Reasoning, Planning, Inhibitory Control, Language Comprehension"
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon evaluates how reliably models can solve tasks by selecting and composing tools across multi-step workflows, often under time/interaction constraints and with noisy tool outputs. It targets practical orchestration: choosing the right tool, calling it correctly, and integrating results into a coherent final response.","Planning, Decision-making, Working Memory, Adaptive Error Correction, Inhibitory Control, Language Comprehension, Language Production, Semantic Understanding & Context Recognition"
HMMT,https://matharena.ai/?view=problem&comp=hmmt--hmmt_feb_2025,https://arxiv.org/abs/2505.23281,"HMMT problems are high-difficulty contest mathematics questions that typically require creative insights, multi-step proofs/derivations, and careful case analysis. As an evaluation, it targets advanced mathematical reasoning and the ability to maintain correctness over long solution chains.","Logical Reasoning, Working Memory, Planning, Attention, Adaptive Error Correction, Cognitive Flexibility"
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath is an expert-level mathematics benchmark designed to measure progress on difficult, research-adjacent math problems across tiers of difficulty. It emphasizes deep multi-step reasoning, rigorous symbolic/quantitative accuracy, and sustained coherence on problems that resist short heuristic solutions.","Logical Reasoning, Working Memory, Planning, Attention, Adaptive Error Correction, Cognitive Flexibility"
