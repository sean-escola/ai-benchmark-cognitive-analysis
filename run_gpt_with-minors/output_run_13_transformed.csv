Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 evaluates autonomous agents in real command-line environments, requiring them to navigate filesystems, run shell commands, install/use tools, and complete multi-step tasks end-to-end. It emphasizes robust execution under noisy real-world conditions (e.g., dependency issues, runtime errors) rather than purely writing code snippets.","L1: 
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition
L3: ",L2
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp evaluates deep-research style web browsing where models must search, gather evidence, and synthesize answers across multiple documents. The benchmark focuses on reliable information seeking and multi-step reasoning grounded in retrieved sources, often with constraints to reduce reliance on memorization.","L1: Language Comprehension
L2: Planning, Attention, Working Memory, Semantic Understanding & Context Recognition, Logical Reasoning
L3: ",L2
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld benchmarks “computer use” in realistic desktop-like operating system environments, where agents must complete tasks by interacting with GUI applications across multiple steps. It tests whether a model can perceive interface state and execute correct action sequences under partial observability and changing UI conditions.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Planning, Decision-making, Sensorimotor Coordination, Working Memory, Attention
L3: ",L2
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI measures abstract pattern discovery and few-shot generalization using small grid-based puzzles, where the system must infer a hidden transformation rule from a few examples. It is designed to emphasize fluid reasoning and compositional generalization over training-set memorization.","L1: 
L2: Logical Reasoning, Working Memory, Planning, Attention
L3: Cognitive Flexibility",L3
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon agent coherence and strategy by simulating the management of a vending-machine business over an extended period. Agents must make repeated decisions (pricing, inventory, supplier interactions, budgeting) and adapt to market dynamics to maximize outcomes.","L1: 
L2: Planning, Decision-making, Reward Mechanisms, Working Memory, Adaptive Error Correction, Semantic Understanding & Context Recognition
L3: ",L2
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity capability on real-world software vulnerability tasks, including identifying known vulnerabilities from descriptions and sometimes discovering new issues. It emphasizes practical reasoning about codebases, exploitation conditions, and remediation under realistic constraints.","L1: Language Comprehension
L2: Logical Reasoning, Planning, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition
L3: ",L2
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"SpreadsheetBench tests the ability to understand, edit, and generate complex spreadsheets using realistic artifacts and tasks (e.g., formulas, tables, formatting, and multi-sheet logic). It targets reliable structured manipulation where small mistakes can cascade into incorrect outputs.","L1: 
L2: Logical Reasoning, Working Memory, Attention, Planning, Adaptive Error Correction, Semantic Understanding & Context Recognition
L3: ",L2
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a broad, frontier-difficulty benchmark spanning many academic and professional domains, including multimodal questions. It is intended to probe advanced reasoning and knowledge integration, often benefiting from careful tool use and cross-source synthesis when tools are allowed.","L1: Language Comprehension, Visual Perception
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Planning, Multisensory Integration
L3: ",L2
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a high-quality subset of challenging, graduate-level science multiple-choice questions designed to be difficult for non-experts and resistant to superficial pattern matching. It probes deep conceptual understanding and careful reasoning under tight answer constraints.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Attention
L3: ",L2
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro evaluates expert-level multimodal understanding across diverse disciplines, requiring models to answer questions grounded in images, diagrams, and accompanying text. It stresses fine-grained visual interpretation plus cross-modal reasoning rather than text-only recall.","L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Attention, Working Memory
L3: ",L2
MathArena Apex,https://matharena.ai/?view=problem&comp=apex--apex_2025,https://arxiv.org/abs/2505.23281,"MathArena Apex aggregates difficult competition-style mathematics problems and evaluates high-end mathematical reasoning performance. It emphasizes multi-step derivations, precision, and robustness across varied problem types under standardized evaluation.","L1: 
L2: Logical Reasoning, Planning, Working Memory, Adaptive Error Correction, Attention
L3: ",L2
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning evaluates reasoning over scientific figures and paper-style visual content, often requiring interpretation of plots, annotations, and visual structure. It targets grounded analysis where correct answers depend on extracting and integrating visual evidence with technical context.","L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Logical Reasoning, Attention, Working Memory, Multisensory Integration
L3: ",L2
OmniDocBench 1.5,https://github.com/opendatalab/OmniDocBench,https://arxiv.org/abs/2412.07626,"OmniDocBench 1.5 evaluates document understanding and OCR-style extraction across heterogeneous document elements such as text blocks, formulas, tables, and reading order. It emphasizes accurate perception and structured reconstruction of complex page layouts.","L1: Visual Perception, Language Comprehension
L2: Visual Attention & Eye Movements, Attention, Semantic Understanding & Context Recognition, Working Memory
L3: ",L2
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU evaluates multimodal understanding and reasoning over videos, requiring models to integrate information across time (events, actions, and context) alongside text prompts. It probes temporal grounding and cross-frame consistency rather than single-image recognition.","L1: Visual Perception
L2: Multisensory Integration, Working Memory, Attention, Scene Understanding & Visual Reasoning
L3: Cognitive Timing & Predictive Modeling",L3
LiveCodeBench Pro,https://livecodebenchpro.com/projects/livecodebench-pro/leaderboard,https://arxiv.org/abs/2506.11928,"LiveCodeBench Pro benchmarks coding in a setting closer to interactive development, emphasizing correct program behavior and iterative problem solving rather than one-shot code generation. It is designed to reflect practical software engineering competence under time-evolving tasks.","L1: Language Production, Language Comprehension
L2: Planning, Logical Reasoning, Adaptive Error Correction, Working Memory, Decision-making
L3: ",L2
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"FACTS Benchmark Suite systematically evaluates factuality and grounding, testing whether models produce accurate claims, appropriately express uncertainty, and avoid unsupported statements. It targets reliability across diverse factuality failure modes, including subtle distortions and overconfident errors.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Attention
L3: Inhibitory Control, Self-reflection",L3
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA evaluates practical reasoning about everyday physical and social situations across many languages and locales, aiming to reduce English-centric bias. It probes whether models can select plausible actions/solutions using common-sense constraints rather than niche factual recall.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Attention
L3: Social Reasoning & Theory of Mind",L3
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,MRCR v2 (8-needle) tests long-context retrieval and multi-round coreference by embedding multiple similar “needle” interactions within long “haystack” conversations and asking the model to reproduce the correct referenced content. It emphasizes accurate tracking amid distractors and extended contexts.,"L1: Language Comprehension
L2: Working Memory, Attention, Episodic Memory, Semantic Understanding & Context Recognition
L3: ",L2
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval evaluates well-specified, economically meaningful knowledge-work tasks across many occupations, often judged by expert humans via head-to-head comparisons. Tasks emphasize producing usable work artifacts (e.g., plans, analyses, slides/spreadsheets) with real-world constraints and quality standards.","L1: Language Production
L2: Planning, Decision-making, Semantic Understanding & Context Recognition, Working Memory
L3: Self-reflection, Social Reasoning & Theory of Mind",L3
SWE-Lancer,https://openai.com/index/swe-lancer/,https://arxiv.org/abs/2502.12115,"SWE-Lancer evaluates software engineering performance in more agentic, end-to-end scenarios that resemble contracting or “task completion for a client,” typically involving navigating a repo, implementing changes, and validating results. It emphasizes reliability, iteration, and producing shippable patches under realistic constraints.","L1: Language Comprehension, Language Production
L2: Planning, Logical Reasoning, Adaptive Error Correction, Working Memory, Decision-making
L3: ",L2
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks evaluates reasoning over graph-structured data embedded in text, such as following paths, tracking parent/child relations, or performing constrained traversals. It targets systematic multi-step relational reasoning and resistance to distraction over longer sequences.","L1: 
L2: Spatial Representation & Mapping, Working Memory, Attention, Logical Reasoning, Planning
L3: ",L2
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon evaluates tool-use competence across a variety of tasks where models must decide when and how to call tools, interpret results, and recover from failures. It emphasizes robust orchestration, correct parameterization, and multi-step execution rather than pure language generation.","L1: 
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory, Attention, Semantic Understanding & Context Recognition
L3: ",L2
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath evaluates expert-level mathematics, emphasizing difficult problems that require long multi-step reasoning and careful verification, often with optional tool assistance (e.g., Python). It is intended to measure progress on advanced mathematical problem solving near the frontier of model capability.","L1: 
L2: Logical Reasoning, Planning, Working Memory, Adaptive Error Correction, Attention
L3: ",L2
