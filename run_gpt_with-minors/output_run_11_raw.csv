Benchmark,Website,Paper,Description,Cognitive Functions
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates software-engineering agents on real GitHub issues by asking them to produce code patches that make the repository’s tests pass. The “Verified” subset emphasizes tasks confirmed solvable and reliably graded, focusing on end-to-end debugging, code understanding, and patch correctness.","Language Comprehension, Semantic Understanding & Context Recognition, Planning, Decision-making, Adaptive Error Correction, Working Memory"
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro is a larger, harder successor aimed at measuring more contamination-resistant, industrially relevant software engineering across multiple languages and repositories. It stresses robust problem decomposition, codebase navigation, iterative fixing, and producing high-quality patches under realistic constraints.","Language Comprehension, Semantic Understanding & Context Recognition, Planning, Decision-making, Adaptive Error Correction, Working Memory, Cognitive Flexibility"
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp measures a model’s ability to answer difficult questions by conducting web-style research over a controlled document collection, emphasizing reproducibility across systems. Agents must search, select evidence, synthesize findings, and avoid being misled by distractors or irrelevant sources.","Planning, Decision-making, Attention, Working Memory, Semantic Understanding & Context Recognition, Language Comprehension, Language Production"
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench evaluates interactive agent behavior in customer-service style environments (e.g., retail, airline, telecom) that require multi-turn dialogue, tool/API use, and policy compliance. The benchmark emphasizes consistent adherence to rules while still helping users effectively through complex workflows.","Language Comprehension, Language Production, Planning, Decision-making, Inhibitory Control, Social Reasoning & Theory of Mind, Adaptive Error Correction"
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,ARC-AGI tests “fluid intelligence” via small grid-based puzzles where the model must infer latent transformation rules from a few examples and apply them to new inputs. It is designed to reward generalizable abstraction and rule induction rather than domain memorization.,"Logical Reasoning, Cognitive Flexibility, Working Memory, Spatial Representation & Mapping, Visual Perception, Scene Understanding & Visual Reasoning"
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas assesses real-world tool use through the Model Context Protocol (MCP), requiring models to discover tools, call APIs correctly, manage errors, and compose multi-step workflows. It targets reliability under realistic integration settings rather than single-call demonstrations.","Planning, Decision-making, Language Comprehension, Semantic Understanding & Context Recognition, Adaptive Error Correction, Working Memory"
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a large, frontier-level multimodal benchmark intended to probe advanced reasoning and expert knowledge across many domains. Questions often require careful interpretation, multi-step inference, and (in tool-enabled settings) research-like synthesis from external evidence.","Language Comprehension, Language Production, Logical Reasoning, Semantic Understanding & Context Recognition, Planning, Scene Understanding & Visual Reasoning, Working Memory"
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 comprises competition-style mathematics problems that require multi-step derivations and precise symbolic manipulation. It is commonly used to evaluate mathematical reasoning depth, error avoidance, and the ability to maintain long solution threads.","Logical Reasoning, Working Memory, Planning, Adaptive Error Correction"
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a curated subset of challenging graduate-level multiple-choice science questions designed to be difficult to answer via superficial pattern matching. It probes deep scientific reasoning, careful reading, and selection among plausible distractors.","Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning, Decision-making, Working Memory"
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends broad academic testing into multiple languages, measuring whether knowledge and reasoning transfer beyond English. It evaluates multilingual understanding across many subjects while controlling for format consistency and breadth.","Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Cognitive Flexibility"
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro is a high-difficulty multimodal benchmark spanning many disciplines, requiring models to reason jointly over images and text (e.g., diagrams, plots, problem statements). It targets expert-level visual-textual understanding and multi-step inference under multimodal ambiguity.","Visual Perception, Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Working Memory, Attention"
ScreenShot-Pro,https://gui-agent.github.io/grounding-leaderboard/,https://arxiv.org/abs/2504.07981,"ScreenShot-Pro evaluates screenshot understanding and GUI grounding, where models must interpret high-resolution interface images and answer questions or identify relevant elements. It stresses spatial layout understanding, visual-text alignment, and fine-grained attention to UI details.","Visual Perception, Scene Understanding & Visual Reasoning, Spatial Representation & Mapping, Visual Attention & Eye Movements, Attention, Language Comprehension"
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning focuses on reasoning over scientific content from papers (e.g., charts/figures and associated context), requiring models to extract quantitative/structural information and answer questions that depend on correct interpretation. It emphasizes faithful figure reading and linking visual evidence to textual claims.","Visual Perception, Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory"
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU evaluates multimodal understanding and reasoning over videos, requiring integration of information across time and often across modalities of narration/text with visual events. Tasks probe temporal coherence, event tracking, and higher-level inference about actions and outcomes.","Visual Perception, Cognitive Timing & Predictive Modeling, Working Memory, Scene Understanding & Visual Reasoning, Attention, Multisensory Integration"
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) tests long-context robustness via multi-round co-reference/needle-in-haystack style tasks, where multiple similar “needles” are embedded across a long dialogue/document. The model must retrieve and reproduce the correct referenced content, stressing precise context tracking under interference.","Working Memory, Attention, Episodic Memory, Semantic Understanding & Context Recognition, Inhibitory Control"
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval measures performance on well-specified professional knowledge-work tasks across many occupations (e.g., producing spreadsheets, plans, reports), typically judged by expert humans. It emphasizes producing usable artifacts, following constraints, and making sound tradeoffs consistent with workplace objectives.","Planning, Decision-making, Language Production, Language Comprehension, Semantic Understanding & Context Recognition, Self-reflection, Adaptive Error Correction"
SWE-Lancer,https://openai.com/index/swe-lancer/,https://arxiv.org/abs/2502.12115,"SWE-Lancer evaluates agentic software engineering in settings that more closely resemble commissioned work: understanding goals, making changes across a codebase, and delivering working solutions under practical constraints. It targets reliability, iterative debugging, and end-to-end completion rather than isolated coding skill.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition, Language Comprehension"
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks tests structured reasoning over graph-like data presented in text, such as following edges, parent pointers, or traversal constraints to reach correct nodes. It probes the ability to maintain and manipulate discrete relational structure over long sequences without losing track.","Spatial Representation & Mapping, Logical Reasoning, Working Memory, Attention, Semantic Understanding & Context Recognition"
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon evaluates general tool-use competence across diverse APIs and tasks, emphasizing selecting the right tool, calling it with correct arguments, and integrating results into a final answer. It targets robustness to tool errors, multi-step workflows, and faithful synthesis of tool outputs.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Language Comprehension, Language Production"
HMMT,https://matharena.ai/?view=problem&comp=hmmt--hmmt_feb_2025,https://arxiv.org/abs/2505.23281,HMMT problems (as used in modern model evals) are competition-level mathematics questions that require creative multi-step reasoning and careful case handling. The benchmark is used to assess advanced mathematical problem solving beyond routine textbook exercises.,"Logical Reasoning, Working Memory, Planning, Cognitive Flexibility"
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath is an expert-level mathematics benchmark designed to probe the limits of formal and informal reasoning on difficult problems, often requiring nontrivial insight and long solution chains. It emphasizes correctness under deep reasoning pressure and resistance to hallucinated steps.","Logical Reasoning, Working Memory, Planning, Cognitive Flexibility, Adaptive Error Correction"
