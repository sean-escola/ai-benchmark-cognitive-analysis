Benchmark,Website,Paper,Description,Cognitive Functions
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 evaluates agentic software engineering in real command-line environments, where models must navigate repositories, run programs, inspect outputs, and apply fixes. It stresses autonomous tool use under realistic constraints (stateful shells, file systems, package managers) and scores success on end-to-end task completion.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Logical Reasoning, Language Comprehension, Language Production"
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp measures an agent’s ability to answer difficult questions by searching and synthesizing information from a controlled web-like corpus. It emphasizes iterative query formulation, evidence gathering, and final answer justification under retrieval and context constraints.","Planning, Decision-making, Attention, Working Memory, Semantic Understanding & Context Recognition, Logical Reasoning, Language Comprehension, Language Production"
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld evaluates multimodal computer-use agents operating a desktop OS to complete realistic tasks (e.g., navigating GUIs, editing files, using applications). The benchmark probes perception-to-action grounding, interface navigation, and robust execution across long, multi-step workflows.","Visual Perception, Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Sensorimotor Coordination, Planning, Decision-making, Adaptive Error Correction, Working Memory"
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI tests fluid reasoning on novel grid-based pattern transformation tasks with only a few examples per problem. Models must infer latent rules, generalize to new inputs, and produce correct transformed grids, with strong penalties for brittle memorization.","Logical Reasoning, Cognitive Flexibility, Working Memory, Spatial Representation & Mapping, Scene Understanding & Visual Reasoning, Visual Perception"
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon autonomous business management in a simulated environment, where agents run a vending operation over extended time with budgeting, procurement, pricing, and adaptation to market dynamics. Performance is typically scored by final financial outcome and operational stability across many decisions.","Planning, Decision-making, Reward Mechanisms, Working Memory, Adaptive Error Correction, Semantic Understanding & Context Recognition, Self-reflection"
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates agents on cybersecurity tasks over real software projects, including locating known vulnerabilities from high-level descriptions and discovering new ones. It stresses systematic investigation, hypothesis testing, and safe-but-effective tool-based workflows under uncertainty.","Logical Reasoning, Planning, Decision-making, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition, Inhibitory Control"
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"SpreadsheetBench measures an agent’s ability to understand, edit, and compute with complex spreadsheets derived from real-world scenarios. Tasks require reading structure, applying transformations or formulas, and verifying outputs across multi-step operations.","Semantic Understanding & Context Recognition, Working Memory, Logical Reasoning, Planning, Decision-making, Adaptive Error Correction, Language Comprehension"
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a broad, frontier-focused benchmark spanning difficult questions (often multimodal) intended to probe advanced reasoning and expert knowledge. It assesses whether models can integrate information, follow constraints, and produce correct answers on challenging, high-variance items.","Language Comprehension, Language Production, Logical Reasoning, Working Memory, Semantic Understanding & Context Recognition, Multisensory Integration, Scene Understanding & Visual Reasoning"
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a high-difficulty, multiple-choice science QA benchmark designed to be “Google-proof,” emphasizing questions that require deep understanding rather than easy lookup. The Diamond subset targets the highest-quality items where experts reliably answer correctly and non-experts struggle.","Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory"
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro evaluates multimodal, multi-discipline reasoning using images and text with expert-level questions across domains. It emphasizes grounding answers in visual evidence, cross-referencing textual context, and handling challenging distractors in structured evaluation formats.","Visual Perception, Scene Understanding & Visual Reasoning, Multisensory Integration, Visual Attention & Eye Movements, Logical Reasoning, Working Memory, Semantic Understanding & Context Recognition"
MathArena Apex,https://matharena.ai/?view=problem&comp=apex--apex_2025,https://arxiv.org/abs/2505.23281,MathArena Apex is a competitive mathematics evaluation emphasizing hard problems and robust scoring aimed at differentiating strong reasoning models. It probes the ability to execute multi-step derivations reliably and avoid subtle logical or arithmetic errors under time/attempt constraints.,"Logical Reasoning, Working Memory, Planning, Adaptive Error Correction, Cognitive Flexibility"
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning assesses reasoning over scientific figures and paper-style visual content, often requiring interpreting plots, diagrams, and captions to answer questions. It emphasizes precise visual grounding, quantitative/relational inference, and faithful extraction of relevant evidence from complex layouts.","Visual Perception, Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Multisensory Integration, Logical Reasoning, Working Memory, Semantic Understanding & Context Recognition"
OmniDocBench 1.5,https://github.com/opendatalab/OmniDocBench,https://arxiv.org/abs/2412.07626,"OmniDocBench 1.5 evaluates document understanding and OCR-style extraction across text, formulas, tables, and reading order. It probes whether models can preserve structure and fidelity when converting rich, formatted documents into accurate machine-readable representations.","Visual Perception, Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Working Memory, Semantic Understanding & Context Recognition, Language Production"
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU evaluates multimodal understanding and reasoning over videos paired with text questions, requiring integration across multiple frames and events. It stresses temporal coherence, event inference, and using visual context to resolve ambiguous or evolving situations.","Visual Perception, Visual Attention & Eye Movements, Cognitive Timing & Predictive Modeling, Scene Understanding & Visual Reasoning, Multisensory Integration, Working Memory, Logical Reasoning"
LiveCodeBench Pro,https://livecodebenchpro.com/projects/livecodebench-pro/leaderboard,https://arxiv.org/abs/2506.11928,"LiveCodeBench Pro evaluates coding ability on programming tasks with an emphasis on realistic, competitive settings (e.g., measuring with an Elo-style rating). It probes algorithmic reasoning, implementation correctness, debugging, and the ability to produce executable solutions under single-attempt constraints.","Planning, Logical Reasoning, Adaptive Error Correction, Working Memory, Language Comprehension, Language Production"
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"FACTS Benchmark Suite systematically evaluates factuality by testing whether model outputs remain grounded, consistent, and accurate across diverse factuality-focused tasks. It highlights hallucination tendencies and the ability to maintain correctness under prompting pressure and long-form generation.","Semantic Understanding & Context Recognition, Language Comprehension, Language Production, Working Memory, Inhibitory Control, Self-reflection, Logical Reasoning"
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA evaluates practical, commonsense physical reasoning across many languages and locales, aiming to measure whether models generalize beyond English-centric priors. It probes everyday reasoning about actions and outcomes and robustness to multilingual surface forms.","Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning, Cognitive Flexibility, Working Memory"
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,MRCR v2 (8-needle) tests long-context multi-round coreference and retrieval by embedding multiple similar “needle” interactions within large “haystacks” of dialogue and asking the model to reproduce the correct referenced response. It stresses precise retrieval among confounders and resistance to interference across very long contexts.,"Working Memory, Attention, Episodic Memory, Language Comprehension, Cognitive Timing & Predictive Modeling"
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval evaluates economically valuable, well-specified knowledge-work tasks across many occupations via side-by-side comparisons against skilled human outputs. It emphasizes producing usable artifacts (e.g., spreadsheets, plans, analyses) under practical constraints and being judged on real-world quality criteria.","Planning, Decision-making, Language Production, Semantic Understanding & Context Recognition, Social Reasoning & Theory of Mind, Self-reflection, Working Memory"
SWE-Lancer,https://openai.com/index/swe-lancer/,https://arxiv.org/abs/2502.12115,"SWE-Lancer evaluates advanced software engineering performance on real repositories, emphasizing end-to-end patch generation and integration-quality fixes. It probes task decomposition, codebase navigation, debugging, and reliability across complex engineering constraints and specifications.","Planning, Decision-making, Logical Reasoning, Adaptive Error Correction, Working Memory, Language Comprehension, Language Production"
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks evaluates reasoning over graph-structured data by requiring models to follow specified traversal rules (e.g., BFS-like procedures) and answer queries about nodes/paths/relationships. It stresses maintaining state over multi-step traversals and avoiding errors from distractor structure.","Spatial Representation & Mapping, Working Memory, Logical Reasoning, Attention, Planning"
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon evaluates an agent’s ability to solve tasks by selecting and using tools across heterogeneous environments, often requiring multi-step orchestration and error recovery. It emphasizes correct tool choice, parameterization, iterative refinement, and robust completion under realistic tool failures or partial observability.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Language Comprehension, Inhibitory Control"
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath evaluates expert-level mathematics with problems intended to be difficult for current models and more resistant to memorization. It probes deep multi-step reasoning, symbolic manipulation, and the ability to sustain correct derivations on unfamiliar problem families (often with optional tool assistance).","Logical Reasoning, Working Memory, Planning, Adaptive Error Correction, Cognitive Flexibility"
