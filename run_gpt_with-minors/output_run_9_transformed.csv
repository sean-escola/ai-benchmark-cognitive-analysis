Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates software engineering agents on real GitHub issues by requiring them to produce a patch that makes a project’s tests pass. The Verified subset uses tasks that have been validated as solvable and includes stronger, more reliable evaluation of correctness via tests and human verification.","L1: Language Comprehension, Language Production
L2: Planning, Logical Reasoning, Working Memory, Adaptive Error Correction, Decision-making, Semantic Understanding & Context Recognition
L3: ",L2
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro is a more challenging software engineering benchmark with a larger and harder set of real-world issues, designed to be more contamination-resistant and representative of professional development work. It emphasizes end-to-end problem solving in realistic repositories, often spanning multiple files and requiring careful adherence to project conventions and tests.","L1: Language Comprehension, Language Production
L2: Planning, Logical Reasoning, Working Memory, Adaptive Error Correction, Decision-making, Semantic Understanding & Context Recognition
L3: ",L2
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp measures a model’s ability to do “deep research” by finding and synthesizing information from the web (or a controlled web-like corpus) to answer difficult questions. Success depends on decomposing queries, selecting sources, cross-checking evidence, and producing a supported final response.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Attention, Working Memory, Episodic Memory, Semantic Understanding & Context Recognition
L3: Self-reflection",L3
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench evaluates interactive agent behavior in multi-turn customer-support scenarios (e.g., retail, airline, telecom) with simulated users and APIs, emphasizing policy compliance and task completion. It stresses whether agents can use tools correctly over long dialogues while following domain constraints and avoiding loopholes.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Working Memory, Adaptive Error Correction, Attention
L3: Social Reasoning & Theory of Mind, Inhibitory Control",L3
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,ARC-AGI tests fluid reasoning via abstract grid-based puzzles where the model must infer a hidden transformation rule from a few examples and apply it to a new input. It is intended to reward generalization and novel pattern induction rather than memorized knowledge.,"L1: Visual Perception
L2: Logical Reasoning, Working Memory, Spatial Representation & Mapping, Scene Understanding & Visual Reasoning, Attention
L3: Cognitive Flexibility",L3
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas measures real-world tool-use competency through the Model Context Protocol (MCP), requiring models to discover tools, call them with correct arguments, handle errors, and synthesize outputs. Tasks are multi-step workflows resembling production API usage across heterogeneous services.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Working Memory, Adaptive Error Correction, Attention, Semantic Understanding & Context Recognition
L3: ",L2
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a difficult, frontier-knowledge benchmark spanning many domains and including multimodal questions, designed to probe capabilities near the limits of current models. It emphasizes robust reasoning and synthesis under sparse or non-obvious cues, sometimes with optional tool use depending on the evaluation setting.","L1: Language Comprehension, Language Production, Visual Perception
L2: Logical Reasoning, Working Memory, Semantic Understanding & Context Recognition, Scene Understanding & Visual Reasoning, Multisensory Integration, Attention
L3: ",L2
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 evaluates competition-style mathematics on problems that typically require multi-step derivations, algebraic manipulation, and careful case analysis. It is often used to test non-trivial symbolic reasoning under time/attempt constraints (e.g., pass@1).","L1: 
L2: Logical Reasoning, Working Memory, Planning, Attention
L3: Cognitive Flexibility",L3
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,GPQA Diamond is a curated subset of extremely challenging graduate-level science multiple-choice questions intended to be “Google-proof” and to require real scientific reasoning. The Diamond split focuses on high-quality items where experts reliably answer correctly while non-experts tend to fail.,"L1: Language Comprehension
L2: Logical Reasoning, Working Memory, Semantic Understanding & Context Recognition, Attention
L3: ",L2
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends broad academic knowledge and reasoning evaluation into many non-English languages across dozens of subjects. It tests whether models can understand and answer subject-matter questions consistently across languages, not just in English.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Attention
L3: ",L2
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro is a harder, more evaluation-robust variant of MMMU that tests expert-level multimodal understanding across many disciplines using images paired with text questions. It emphasizes reasoning over diagrams, charts, scientific figures, and other visuals that require structured interpretation.","L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Multisensory Integration, Visual Attention & Eye Movements, Logical Reasoning, Working Memory, Attention
L3: ",L2
ScreenShot-Pro,https://gui-agent.github.io/grounding-leaderboard/,https://arxiv.org/abs/2504.07981,"ScreenShot-Pro evaluates screenshot understanding for software/UI contexts, requiring models to interpret high-resolution interface images and answer questions that depend on layout, controls, and visual affordances. It targets visually grounded reasoning that is critical for computer-use agents and GUI automation.","L1: Visual Perception
L2: Visual Attention & Eye Movements, Spatial Representation & Mapping, Scene Understanding & Visual Reasoning, Decision-making, Planning, Working Memory, Attention
L3: ",L2
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning evaluates scientific figure understanding and reasoning over charts and visual elements from arXiv-style papers, often requiring quantitative interpretation and linking captions/text to visual evidence. It is designed to measure faithful, grounded reasoning about complex academic visuals, sometimes with optional computation tools.","L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Working Memory, Semantic Understanding & Context Recognition, Attention
L3: ",L2
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU extends multimodal reasoning to the temporal domain by asking questions that require understanding events and context across video clips (often with accompanying text). It stresses integrating information across time, tracking entities, and reasoning about changes and causality.","L1: Visual Perception, Language Comprehension
L2: Multisensory Integration, Working Memory, Attention, Scene Understanding & Visual Reasoning
L3: Cognitive Timing & Predictive Modeling",L3
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) is a long-context retrieval and co-reference benchmark where multiple similar “needle” requests are embedded in a long “haystack” of text, and the model must recover the correct referenced answer. The 8-needle variant increases interference, testing robust context tracking rather than short-window pattern matching.","L1: Language Comprehension
L2: Working Memory, Attention, Episodic Memory, Semantic Understanding & Context Recognition
L3: Inhibitory Control",L3
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval evaluates well-specified professional knowledge-work tasks across many occupations by having models produce real work artifacts (e.g., spreadsheets, presentations, plans) judged against human professional performance. It targets practical, end-to-end execution quality, not just question answering, under constraints typical of workplace deliverables.","L1: Language Production, Language Comprehension
L2: Planning, Decision-making, Working Memory, Semantic Understanding & Context Recognition, Adaptive Error Correction
L3: Self-reflection, Social Reasoning & Theory of Mind",L3
SWE-Lancer,https://openai.com/index/swe-lancer/,https://arxiv.org/abs/2502.12115,"SWE-Lancer evaluates agentic software engineering in a “freelance” style setting where models must complete coding tasks that resemble paid engineering work, often involving ambiguous requirements and iterative refinement. It emphasizes realistic development behaviors like understanding requirements, implementing changes, and validating via tests or tooling.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Logical Reasoning, Working Memory, Adaptive Error Correction, Semantic Understanding & Context Recognition
L3: ",L2
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks tests long-context reasoning over graph-structured data encoded in text, requiring models to follow edges, perform traversals (e.g., BFS-like queries), and answer reachability or parent/ancestor questions. Performance depends on reliably maintaining state over many hops and resisting distractors.","L1: 
L2: Spatial Representation & Mapping, Working Memory, Logical Reasoning, Attention, Semantic Understanding & Context Recognition
L3: Inhibitory Control",L3
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon evaluates tool-using agents on multi-step tasks that require selecting appropriate tools, calling them correctly, handling failures, and integrating tool outputs into final answers. It emphasizes reliability under tool friction (errors, missing fields, retries) and correct orchestration over extended interaction traces.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Working Memory, Adaptive Error Correction, Attention, Semantic Understanding & Context Recognition
L3: ",L2
HMMT,https://matharena.ai/?view=problem&comp=hmmt--hmmt_feb_2025,https://arxiv.org/abs/2505.23281,"HMMT is a high-difficulty mathematics competition benchmark that probes multi-step problem solving, creative decomposition, and careful verification under strict answer formats. Compared with many standard math sets, problems often require deeper insight and longer chains of reasoning.","L1: 
L2: Logical Reasoning, Working Memory, Planning, Attention
L3: Cognitive Flexibility",L3
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath is an expert-level mathematics benchmark aimed at measuring progress on problems closer to current research and advanced olympiad-style reasoning, often requiring substantial derivations and proof-like thinking. It is designed to be difficult for models and to better reflect genuine mathematical problem-solving ability.","L1: 
L2: Logical Reasoning, Planning, Working Memory, Attention
L3: Cognitive Flexibility, Self-reflection",L3
