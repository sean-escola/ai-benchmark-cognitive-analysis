Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates whether a model can fix real GitHub issues by producing a patch that makes a repository’s tests pass. The “Verified” subset consists of tasks that have been validated as solvable and reliably testable, emphasizing correctness and practical software-engineering competence.","L1: 
L2: Planning, Logical Reasoning, Working Memory, Adaptive Error Correction, Decision-making
L3: ",L2
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro is a harder, more contamination-resistant software engineering benchmark spanning multiple programming languages and larger, more realistic tasks. It measures an agent’s ability to understand a codebase, implement or repair functionality, and satisfy rigorous test suites under realistic constraints.","L1: 
L2: Planning, Logical Reasoning, Working Memory, Adaptive Error Correction, Decision-making
L3: Cognitive Flexibility",L3
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp evaluates web research agents on information-seeking questions that require searching, reading, and synthesizing evidence from multiple web sources. It stresses reliable navigation and answer construction rather than memorizing facts, often requiring cross-checking and source integration.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Attention, Working Memory, Semantic Understanding & Context Recognition, Adaptive Error Correction
L3: ",L2
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench measures interactive, policy-constrained tool use in customer-service-like simulations (e.g., retail, airline, telecom). The agent must carry a multi-turn dialogue, call APIs correctly, follow domain rules, and resolve the user’s request without policy violations.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Working Memory, Reward Mechanisms
L3: Social Reasoning & Theory of Mind, Inhibitory Control",L3
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI tests abstract pattern induction and generalization from a few demonstrations using grid-based puzzles. Success requires discovering latent rules from small examples and applying them to novel inputs, emphasizing fluid reasoning over domain knowledge.","L1: 
L2: Logical Reasoning, Working Memory, Planning, Spatial Representation & Mapping, Attention
L3: Cognitive Flexibility",L3
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas evaluates real-world tool use through the Model Context Protocol (MCP), requiring models to select, call, and chain tools across multi-step workflows. It emphasizes robustness to API structure, error handling, and composing intermediate results into a correct final output.","L1: 
L2: Planning, Decision-making, Working Memory, Adaptive Error Correction, Semantic Understanding & Context Recognition
L3: Inhibitory Control",L3
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam (HLE) is a frontier, multi-domain benchmark designed to probe advanced reasoning and specialized knowledge, often in multimodal formats. Questions are intended to be difficult, requiring careful interpretation, multi-step inference, and sometimes tool-augmented verification.","L1: Language Comprehension, Language Production, Visual Perception
L2: Logical Reasoning, Working Memory, Semantic Understanding & Context Recognition, Scene Understanding & Visual Reasoning
L3: ",L2
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,AIME 2025 evaluates mathematical problem solving on contest-style questions that typically require multi-step derivations and careful algebraic or combinatorial reasoning. It is commonly used to gauge symbolic manipulation skill and error-prone reasoning chains under time-like constraints.,"L1: 
L2: Logical Reasoning, Working Memory, Planning, Adaptive Error Correction, Attention
L3: ",L2
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a high-difficulty multiple-choice benchmark of graduate-level science questions designed to be “Google-proof.” The Diamond subset focuses on the most reliable, expert-verified items, stressing deep scientific reasoning and careful distractor discrimination.","L1: Language Comprehension
L2: Logical Reasoning, Working Memory, Semantic Understanding & Context Recognition, Decision-making
L3: ",L2
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends broad academic knowledge testing to multiple languages, covering many subjects across diverse linguistic contexts. It emphasizes robust multilingual understanding, transfer of knowledge across languages, and resisting superficial pattern matching.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory
L3: Cognitive Flexibility",L3
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro is a challenging multimodal benchmark that combines text with images (e.g., diagrams, charts, documents) across many disciplines. It measures whether models can integrate visual evidence with domain knowledge to answer expert-level questions reliably.","L1: Visual Perception, Language Comprehension
L2: Multisensory Integration, Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Logical Reasoning, Working Memory
L3: ",L2
ScreenShot-Pro,https://gui-agent.github.io/grounding-leaderboard/,https://arxiv.org/abs/2504.07981,"ScreenShot-Pro evaluates UI grounding and screenshot understanding, where models must interpret high-resolution interface images and answer questions or locate elements. It stresses spatial layout reasoning, fine-grained visual discrimination, and mapping visual cues to precise actions or references.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Spatial Representation & Mapping, Decision-making, Working Memory
L3: ",L2
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning tests reasoning over scientific figures and paper content, often requiring interpretation of plots, diagrams, and accompanying textual context. It measures whether models can extract structured information from scientific visuals and perform multi-step inference grounded in that evidence.","L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Working Memory, Attention
L3: ",L2
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU evaluates multimodal understanding and reasoning over video, requiring models to integrate temporal visual information with text prompts. Tasks often depend on tracking events across frames and using contextual cues to answer questions about actions, states, or outcomes.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Multisensory Integration, Working Memory, Attention
L3: Cognitive Timing & Predictive Modeling",L3
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) evaluates long-context retrieval and multi-round coreference resolution by embedding multiple similar “needle” interactions inside long “haystacks.” The model must locate the correct referenced needle and reproduce or select the appropriate response, stressing robust retrieval under interference.","L1: 
L2: Working Memory, Episodic Memory, Attention, Semantic Understanding & Context Recognition, Adaptive Error Correction
L3: ",L2
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval evaluates well-specified professional knowledge-work tasks (e.g., creating business artifacts such as plans, spreadsheets, presentations) judged against expert human performance. It emphasizes producing usable deliverables under constraints, aligning outputs to a goal, and making tradeoffs across quality, correctness, and clarity.","L1: Language Production, Language Comprehension
L2: Planning, Decision-making, Semantic Understanding & Context Recognition, Working Memory
L3: Self-reflection",L3
SWE-Lancer,https://openai.com/index/swe-lancer/,https://arxiv.org/abs/2502.12115,"SWE-Lancer evaluates agentic software engineering in a freelancing-style setting, focusing on completing practical coding tasks end-to-end in realistic repos. It stresses requirement interpretation, implementation quality, iterative debugging, and producing solutions that satisfy external acceptance criteria.","L1: 
L2: Planning, Logical Reasoning, Working Memory, Adaptive Error Correction, Decision-making, Semantic Understanding & Context Recognition
L3: ",L2
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks tests structured reasoning over graph-structured data by requiring models to traverse or query relationships in a graph (e.g., BFS-like tasks or parent-tracing). It measures whether models can maintain and manipulate relational structure across steps without losing the traversal state.","L1: 
L2: Spatial Representation & Mapping, Working Memory, Logical Reasoning, Planning, Attention
L3: ",L2
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon evaluates general tool-use competence across diverse tools and workflows, emphasizing correct tool selection, parameterization, and multi-step orchestration. It typically stresses reliability across long action sequences, including recovery from tool errors and composing outputs into a final answer.","L1: 
L2: Planning, Decision-making, Working Memory, Adaptive Error Correction, Semantic Understanding & Context Recognition
L3: Inhibitory Control",L3
HMMT,https://matharena.ai/?view=problem&comp=hmmt--hmmt_feb_2025,https://arxiv.org/abs/2505.23281,"HMMT evaluates high-level contest mathematics with problems that often require creative problem decomposition, proof-like reasoning, and careful bookkeeping. It is used to probe deep mathematical reasoning and robustness against subtle logical or arithmetic mistakes.","L1: 
L2: Logical Reasoning, Working Memory, Planning, Adaptive Error Correction, Attention
L3: ",L2
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath targets expert-level mathematics beyond routine contest problems, aiming to measure genuine research-adjacent reasoning difficulty across tiers. It stresses long chains of deduction, choosing appropriate techniques, and avoiding compounding errors over extended solutions.","L1: 
L2: Logical Reasoning, Working Memory, Planning, Adaptive Error Correction, Attention
L3: Cognitive Flexibility",L3
