Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 evaluates agentic coding in real command-line environments, where models must execute multi-step workflows (e.g., installing dependencies, editing files, running tests, and debugging) to complete tasks. It emphasizes end-to-end reliability under tool constraints, where small mistakes can compound across steps.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory
L3: ",L2
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp evaluates deep research and information synthesis by requiring models to browse and extract evidence from a controlled document collection to answer difficult questions. It stresses grounding, cross-document integration, and producing final answers supported by retrieved sources rather than free-form recall.","L1: Language Comprehension, Language Production
L2: Planning, Attention, Working Memory, Semantic Understanding & Context Recognition, Logical Reasoning
L3: ",L2
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld benchmarks multimodal “computer use” by having agents interact with a desktop-like operating system to accomplish realistic tasks (navigation, form filling, settings changes, file operations). Success requires interpreting screenshots/GUI state and choosing correct action sequences under step limits.","L1: Visual Perception
L2: Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Planning, Decision-making, Sensorimotor Coordination, Adaptive Error Correction, Working Memory
L3: ",L2
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI measures abstraction and few-shot generalization using grid transformation puzzles, where the rule must be inferred from only a handful of examples. It is designed to reduce dependence on memorized knowledge and instead test flexible induction over novel patterns.","L1: 
L2: Logical Reasoning, Working Memory, Spatial Representation & Mapping, Scene Understanding & Visual Reasoning, Attention
L3: Cognitive Flexibility",L3
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon autonomous agency by simulating management of a vending-machine business over extended time (inventory, pricing, supplier negotiation, budgeting). It probes whether an agent can maintain coherent goals, adapt strategy to feedback, and avoid failure modes over many decisions.","L1: 
L2: Planning, Decision-making, Reward Mechanisms, Working Memory, Adaptive Error Correction
L3: Self-reflection",L3
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym tests cybersecurity capabilities by asking agents to identify known vulnerabilities in real projects and to discover new ones, typically by reading code, running tools, and validating hypotheses. It emphasizes systematic investigation, exploit reasoning, and careful iteration under realistic constraints.","L1: Language Comprehension, Language Production
L2: Logical Reasoning, Planning, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition, Attention
L3: ",L2
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"SpreadsheetBench evaluates the ability to understand and manipulate complex spreadsheets, including formulas, tables, and multi-sheet dependencies. Models must execute accurate, multi-step edits and computations where mistakes can silently propagate through downstream cells.","L1: Language Comprehension, Language Production
L2: Logical Reasoning, Working Memory, Planning, Adaptive Error Correction, Attention
L3: ",L2
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a difficult, frontier-knowledge benchmark spanning many academic areas and often requiring multi-step reasoning and synthesis rather than simple fact recall. It is frequently used with tool-enabled settings (e.g., search or code), highlighting the gap between raw model knowledge and grounded problem solving.","L1: Language Comprehension, Language Production, Visual Perception
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Multisensory Integration
L3: ",L2
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,GPQA Diamond is a high-quality subset of GPQA consisting of very challenging graduate-level science multiple-choice questions designed to be resistant to shallow web lookup. It targets deep conceptual understanding and careful discrimination between close answer choices.,"L1: Language Comprehension
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Decision-making
L3: ",L2
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro evaluates expert-level multimodal understanding and reasoning across many disciplines using images paired with text questions (often multiple-choice). It stresses integrating visual evidence with domain knowledge and performing nontrivial reasoning over diagrams, plots, and figures.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Attention, Working Memory, Semantic Understanding & Context Recognition
L3: ",L2
MathArena Apex,https://matharena.ai/?view=problem&comp=apex--apex_2025,https://arxiv.org/abs/2505.23281,"MathArena Apex aggregates difficult competition-style math problems and evaluates advanced mathematical reasoning under standardized settings. It is used to compare models’ ability to carry out multi-step derivations, maintain consistency, and avoid brittle arithmetic or logic slips.","L1: 
L2: Logical Reasoning, Working Memory, Planning, Adaptive Error Correction
L3: Cognitive Flexibility",L3
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning evaluates scientific figure understanding and reasoning, typically requiring models to interpret plots/diagrams from papers and answer questions that depend on the visual evidence. It probes whether models can connect figure content to the accompanying scientific context and perform quantitative or relational reasoning.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Logical Reasoning, Attention, Working Memory, Semantic Understanding & Context Recognition, Multisensory Integration
L3: ",L2
OmniDocBench 1.5,https://github.com/opendatalab/OmniDocBench,https://arxiv.org/abs/2412.07626,"OmniDocBench 1.5 evaluates document understanding and OCR robustness across heterogeneous layouts, including text blocks, tables, formulas, and reading order. The goal is faithful extraction and structured reconstruction rather than merely recognizing isolated text lines.","L1: Visual Perception
L2: Visual Attention & Eye Movements, Semantic Understanding & Context Recognition, Working Memory, Attention
L3: ",L2
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU tests video understanding and multimodal reasoning by asking questions that require integrating information across frames and time. It stresses temporal grounding (events, causality, state changes) rather than single-image recognition.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Working Memory, Attention, Multisensory Integration, Logical Reasoning
L3: Cognitive Timing & Predictive Modeling",L3
LiveCodeBench Pro,https://livecodebenchpro.com/projects/livecodebench-pro/leaderboard,https://arxiv.org/abs/2506.11928,"LiveCodeBench Pro evaluates coding ability on problems designed to reduce contamination and better reflect modern development workflows, often emphasizing correctness under execution-based grading. It probes the ability to design algorithms, implement them reliably, and debug under time/attempt constraints.","L1: Language Comprehension, Language Production
L2: Planning, Logical Reasoning, Adaptive Error Correction, Working Memory, Decision-making
L3: ",L2
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"FACTS Benchmark Suite systematically evaluates factuality by testing whether model outputs remain consistent with provided evidence and whether they avoid introducing unsupported claims. It targets grounded generation behaviors relevant to real deployments, such as citation faithfulness and robustness against hallucination.","L1: Language Comprehension, Language Production
L2: Semantic Understanding & Context Recognition, Working Memory, Attention
L3: Inhibitory Control, Self-reflection",L3
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA extends physical commonsense reasoning evaluation across many languages and cultural contexts, aiming to test whether models can reason about everyday interactions with objects and environments beyond English. It emphasizes plausible action/affordance understanding and robust multilingual comprehension.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Spatial Representation & Mapping, Decision-making
L3: ",L2
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,MRCR v2 (8-needle) evaluates long-context multi-round coreference and retrieval by embedding multiple similar “needle” interactions inside long “haystacks” and asking for the correct referenced response. It stresses maintaining and selecting the right referent amid distractors over very long inputs.,"L1: Language Comprehension
L2: Working Memory, Attention, Episodic Memory, Logical Reasoning
L3: Inhibitory Control",L3
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval evaluates economically valuable, well-specified knowledge-work tasks across many occupations, often requiring production of artifacts like plans, spreadsheets, presentations, and analyses. It is judged via expert comparisons, focusing on usefulness, correctness, and professional quality rather than short-form QA.","L1: Language Production, Language Comprehension
L2: Planning, Decision-making, Semantic Understanding & Context Recognition, Working Memory
L3: Social Reasoning & Theory of Mind, Self-reflection",L3
SWE-Lancer,https://openai.com/index/swe-lancer/,https://arxiv.org/abs/2502.12115,"SWE-Lancer evaluates software engineering tasks framed around real-world impact and difficulty, emphasizing end-to-end patch generation, codebase understanding, and robust completion. It is intended to better reflect practical engineering work where planning, testing, and iterative fixes matter.","L1: Language Comprehension, Language Production
L2: Planning, Logical Reasoning, Adaptive Error Correction, Working Memory, Decision-making
L3: ",L2
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks tests reasoning over graph-structured data by requiring models to follow edges, track paths, and answer questions about reachability or node relationships from long, structured descriptions. It stresses precise state tracking and resistance to distractor structure as graphs scale.","L1: 
L2: Spatial Representation & Mapping, Working Memory, Logical Reasoning, Attention, Planning
L3: ",L2
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon evaluates general tool-use competence across diverse APIs and tasks, emphasizing correct tool selection, parameterization, and multi-step orchestration. It targets reliability in agent loops where errors in calling, interpreting, or sequencing tools can derail outcomes.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory, Attention
L3: ",L2
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath evaluates advanced mathematics at or beyond typical graduate and research levels, with tiers intended to measure progress at the frontier. It stresses rigorous multi-step reasoning, precise symbolic manipulation, and robustness to very hard problems where partial progress is common.","L1: 
L2: Logical Reasoning, Working Memory, Planning, Adaptive Error Correction
L3: Cognitive Flexibility",L3
