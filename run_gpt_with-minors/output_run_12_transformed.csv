Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates autonomous software engineering by asking a model to produce a patch that fixes real issues in open-source Python repositories given an issue description and tests. The “Verified” subset filters to tasks that are confirmed solvable and reliably graded, emphasizing end-to-end debugging rather than code-writing in isolation.","L1: 
L2: Planning, Logical Reasoning, Working Memory, Adaptive Error Correction, Semantic Understanding & Context Recognition
L3: ",L2
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro is a larger and harder successor-style benchmark focused on real-world software engineering tasks across multiple programming languages and more contamination-resistant settings. It stresses robust repository understanding, multi-file edits, and producing changes that satisfy hidden tests under realistic constraints.","L1: 
L2: Planning, Logical Reasoning, Working Memory, Adaptive Error Correction, Decision-making
L3: Cognitive Flexibility",L3
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp evaluates “deep research” behavior by requiring models to answer questions using information found by browsing a fixed web-like corpus or indexed document collection. It emphasizes iterative search, evidence gathering, and synthesis of scattered sources into a final, grounded answer.","L1: Language Comprehension, Language Production
L2: Planning, Attention, Working Memory, Semantic Understanding & Context Recognition, Decision-making
L3: ",L2
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench measures agentic tool use in multi-turn, policy-constrained customer support scenarios (e.g., retail, airline, telecom) with simulated users and backend APIs. Success depends on maintaining dialogue state, following policies, and correctly calling tools to resolve cases end-to-end.","L1: Language Comprehension, Language Production
L2: Working Memory, Planning, Decision-making
L3: Social Reasoning & Theory of Mind, Inhibitory Control",L3
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,ARC-AGI is a few-shot abstract reasoning benchmark where models infer hidden rules from a small number of grid-based input–output examples and apply them to new inputs. It targets fluid reasoning and generalization to novel tasks rather than recall of domain knowledge.,"L1: Visual Perception
L2: Logical Reasoning, Working Memory, Spatial Representation & Mapping, Scene Understanding & Visual Reasoning
L3: Cognitive Flexibility",L3
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas evaluates real-world tool use via the Model Context Protocol, requiring models to discover tools, invoke them correctly, handle errors, and compose multi-step workflows across services. The focus is on reliable orchestration over tool catalogs and realistic API interactions.","L1: Language Comprehension
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition
L3: ",L2
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam (HLE) is a large, frontier-level benchmark spanning advanced academic and professional questions, often multimodal, intended to probe capability near the limits of current models. It emphasizes difficult reasoning and broad knowledge integration under strict grading.","L1: Language Comprehension, Language Production, Visual Perception
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Scene Understanding & Visual Reasoning
L3: ",L2
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,AIME 2025 consists of competition-style mathematics problems designed to test multi-step quantitative reasoning under time-like constraints and minimal scaffolding. Performance depends on translating word problems into formal structure and executing correct symbolic manipulations.,"L1: 
L2: Logical Reasoning, Working Memory, Planning, Adaptive Error Correction
L3: ",L2
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a high-difficulty, multiple-choice science question benchmark curated to be “Google-proof” and discriminative for expert reasoning. The Diamond subset emphasizes question quality and requires combining scientific knowledge with careful elimination and inference.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Decision-making
L3: ",L2
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends the MMLU format to multiple languages, testing broad academic knowledge and reasoning across many subjects under multilingual prompts. It probes whether models can maintain consistent competence and calibration across languages, not just English.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory
L3: ",L2
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro is a challenging multimodal benchmark where models answer expert-level questions grounded in images (e.g., charts, diagrams, screenshots) plus text. It emphasizes robust visual understanding and cross-modal reasoning with reduced shortcut opportunities compared to simpler VQA.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Working Memory, Visual Attention & Eye Movements
L3: ",L2
ScreenShot-Pro,https://gui-agent.github.io/grounding-leaderboard/,https://arxiv.org/abs/2504.07981,"ScreenShot-Pro evaluates UI understanding and visual grounding on high-resolution screenshots, typically requiring identifying interface elements, their relationships, and intended actions. It stresses precise spatial localization and interpreting affordances in professional software interfaces.","L1: Visual Perception
L2: Visual Attention & Eye Movements, Spatial Representation & Mapping, Scene Understanding & Visual Reasoning, Planning, Sensorimotor Coordination
L3: ",L2
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning tests reasoning over scientific figures from papers, asking models to interpret plots, diagrams, and figure captions to answer questions. It emphasizes extracting quantitative/structural information from visuals and linking it with scientific context.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Multisensory Integration
L3: ",L2
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU extends multimodal understanding to video, requiring models to answer questions that depend on temporal events, actions, and evolving context. It probes integration of information across frames and maintaining task-relevant details over time.","L1: Visual Perception
L2: Attention, Working Memory, Scene Understanding & Visual Reasoning, Multisensory Integration
L3: Cognitive Timing & Predictive Modeling",L3
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) evaluates long-context retrieval and multi-round coreference resolution by embedding multiple similar “needle” interactions in a long “haystack” of dialogue-like content. The model must identify and reproduce the correct referenced response, stressing robustness to interference and distractors.","L1: Language Comprehension
L2: Working Memory, Attention, Semantic Understanding & Context Recognition, Episodic Memory
L3: Inhibitory Control",L3
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval measures performance on well-specified, economically relevant knowledge-work tasks across many occupations, judged by expert humans via head-to-head comparisons against professional outputs. Tasks often require producing structured artifacts (e.g., slides, spreadsheets, plans) and making tradeoffs under constraints.","L1: Language Production
L2: Planning, Decision-making, Semantic Understanding & Context Recognition, Working Memory
L3: Self-reflection",L3
SWE-Lancer,https://openai.com/index/swe-lancer/,https://arxiv.org/abs/2502.12115,"SWE-Lancer evaluates agentic software engineering on freelancing-style tasks that resemble contracted work, often involving ambiguous requirements and realistic repository contexts. It emphasizes selecting an approach, implementing changes, and delivering outcomes aligned with user intent and acceptance criteria.","L1: 
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition
L3: Cognitive Flexibility",L3
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks tests systematic reasoning over graph-structured data by requiring models to follow traversal rules (e.g., BFS-style walks) and answer queries that depend on correct path/parent relationships. It stresses faithful multi-step state tracking under combinatorial branching.","L1: 
L2: Spatial Representation & Mapping, Working Memory, Logical Reasoning, Attention
L3: Cognitive Timing & Predictive Modeling",L3
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon evaluates how well models can use diverse tools/APIs to complete multi-step tasks, often requiring correct tool selection, argument construction, and iterative repair after failures. It targets practical agent reliability rather than purely linguistic performance.","L1: 
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition
L3: Inhibitory Control",L3
HMMT,https://matharena.ai/?view=problem&comp=hmmt--hmmt_feb_2025,https://arxiv.org/abs/2505.23281,HMMT consists of high-difficulty competition mathematics problems that require creative multi-step derivations and careful handling of edge cases. It probes depth of mathematical reasoning beyond routine curriculum exercises.,"L1: 
L2: Logical Reasoning, Working Memory, Planning, Adaptive Error Correction
L3: Cognitive Flexibility",L3
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath is an expert-level mathematics benchmark designed to be hard for modern models, emphasizing novel problem solving, proof-like reasoning, and long chains of inference. It aims to reduce contamination and reward genuine mathematical generalization under strict evaluation.","L1: 
L2: Logical Reasoning, Planning, Working Memory, Adaptive Error Correction
L3: Cognitive Flexibility",L3
