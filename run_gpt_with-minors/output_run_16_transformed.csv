Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates LLM-based software engineering by asking a model to produce patches that fix real GitHub issues, then running repository tests to verify correctness. The “Verified” split emphasizes tasks that have been validated as solvable and aims to reduce evaluation noise by using robust, reproducible test harnesses.","L1: 
L2: Planning, Logical Reasoning, Working Memory, Adaptive Error Correction, Semantic Understanding & Context Recognition, Decision-making
L3: ",L2
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro is a harder, more contamination-resistant software engineering benchmark with a larger and more diverse set of real-world coding tasks (including multiple languages) and stricter evaluation. It stresses end-to-end problem solving over larger codebases, requiring reliable debugging, refactoring, and test-driven iteration.","L1: 
L2: Planning, Logical Reasoning, Working Memory, Adaptive Error Correction, Semantic Understanding & Context Recognition, Decision-making
L3: ",L2
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp evaluates “deep research” abilities by requiring models/agents to answer questions using web browsing and retrieval, typically with citations or evidence from sources. Success depends on decomposing a query, searching effectively, reading selectively, and synthesizing a grounded final answer.","L1: Language Comprehension
L2: Planning, Decision-making, Attention, Working Memory, Semantic Understanding & Context Recognition
L3: Self-reflection",L3
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench evaluates interactive, tool-using customer-support agents in simulated domains (e.g., retail, airline, telecom) with policies that must be followed across multi-turn dialogues. It measures whether an agent can maintain state, comply with constraints, call APIs correctly, and resolve user goals in realistic conversations.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Working Memory, Semantic Understanding & Context Recognition
L3: Social Reasoning & Theory of Mind, Inhibitory Control, Empathy",L3
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI tests fluid reasoning by presenting few-shot input–output grid transformation puzzles that require inferring hidden rules and applying them to new cases. The benchmark is designed to emphasize novelty and abstraction over memorized knowledge, with performance reflecting generalization from minimal examples.","L1: Visual Perception
L2: Logical Reasoning, Spatial Representation & Mapping, Working Memory, Attention
L3: Cognitive Flexibility",L3
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas evaluates real-world tool use via the Model Context Protocol (MCP), where models must discover and invoke tools across multi-step workflows and integrate results into a correct response. It stresses robustness to tool errors, correct parameterization, and orchestration across heterogeneous services.","L1: Language Comprehension
L2: Planning, Decision-making, Working Memory, Adaptive Error Correction, Semantic Understanding & Context Recognition
L3: Inhibitory Control",L3
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam (HLE) is a large, frontier-difficulty benchmark spanning advanced academic and professional questions, including multimodal items, intended to probe the limits of model reasoning and knowledge. It is often run with optional tools (e.g., search or code) to assess how well models can augment reasoning with external resources while staying grounded.","L1: Language Comprehension, Visual Perception
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Planning, Decision-making, Scene Understanding & Visual Reasoning
L3: ",L2
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,AIME 2025 consists of competition-style mathematics problems that require multi-step reasoning under tight answer formats (typically integer answers). It probes a model’s ability to carry out precise symbolic and numerical reasoning rather than relying on retrieval or vague explanations.,"L1: 
L2: Logical Reasoning, Working Memory, Planning, Attention
L3: ",L2
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a challenging multiple-choice science benchmark curated to be “Google-proof,” emphasizing questions where experts succeed and non-experts struggle. It targets deep conceptual understanding and careful elimination of distractors across biology, chemistry, and physics.","L1: 
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Decision-making, Attention
L3: ",L2
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends broad academic knowledge and reasoning evaluation across many subjects into multiple languages, testing whether competence transfers beyond English. It probes multilingual comprehension and culturally/linguistically robust knowledge application under standardized question formats.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory
L3: Cognitive Flexibility",L3
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro is an expert-level multimodal benchmark where models must answer questions that require combining text with images such as diagrams, charts, and scientific figures. It emphasizes rigorous reasoning over visual evidence and reduces shortcutting by using harder items and evaluation protocols.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Working Memory, Attention, Semantic Understanding & Context Recognition
L3: ",L2
ScreenShot-Pro,https://gui-agent.github.io/grounding-leaderboard/,https://arxiv.org/abs/2504.07981,"ScreenShot-Pro evaluates screenshot understanding for GUI grounding, where the model must interpret high-resolution interface images and answer questions or identify actionable targets. It stresses spatial/layout reasoning over UI elements and mapping visual cues to correct semantic actions.","L1: Visual Perception
L2: Visual Attention & Eye Movements, Spatial Representation & Mapping, Scene Understanding & Visual Reasoning, Decision-making, Planning, Working Memory
L3: ",L2
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning focuses on reasoning over scientific content extracted from arXiv-style documents, often involving complex figures, plots, and accompanying text. The task rewards careful interpretation of visual encodings and integration with technical language to derive correct answers.","L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Attention
L3: ",L2
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU evaluates multimodal understanding and reasoning over video inputs, requiring integration of information across time rather than single frames. It stresses temporal coherence, event understanding, and multi-step reasoning grounded in dynamic visual context.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Attention, Working Memory, Multisensory Integration, Logical Reasoning
L3: Cognitive Timing & Predictive Modeling",L3
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 is a long-context multi-round coreference and retrieval-style benchmark where many similar “needle” requests are embedded in long conversational haystacks. The 8-needle setting tests whether a model can reliably track, retrieve, and reproduce the correct referenced content under heavy distractors.","L1: Language Comprehension
L2: Working Memory, Attention, Episodic Memory, Semantic Understanding & Context Recognition
L3: Inhibitory Control",L3
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval evaluates economically valuable professional knowledge work across many occupations by having models produce real work artifacts (e.g., plans, analyses, spreadsheets, presentations) judged against expert outputs. It emphasizes end-to-end task completion quality, instruction adherence, and pragmatic decision-making under realistic constraints.","L1: Language Production, Language Comprehension
L2: Planning, Decision-making, Semantic Understanding & Context Recognition, Working Memory
L3: Self-reflection",L3
SWE-Lancer,https://openai.com/index/swe-lancer/,https://arxiv.org/abs/2502.12115,"SWE-Lancer evaluates agentic software engineering on more open-ended, real-world development tasks akin to contracting or freelance work, where success involves selecting approaches, implementing changes, and validating outcomes. Compared to narrower bug-fix tasks, it stresses longer-horizon execution and reliability over complex repos.","L1: 
L2: Planning, Decision-making, Working Memory, Adaptive Error Correction, Logical Reasoning, Semantic Understanding & Context Recognition
L3: ",L2
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks evaluates reasoning over graph-structured data by requiring models to follow paths, apply traversal rules, and answer queries that depend on correct multi-step navigation. It targets systematic state tracking and resistance to distractors when intermediate steps must be maintained accurately.","L1: 
L2: Spatial Representation & Mapping, Working Memory, Logical Reasoning, Attention, Planning
L3: ",L2
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon evaluates general tool-use competence across diverse APIs and multi-step workflows, typically requiring correct tool selection, argument construction, and iterative recovery from tool failures. It is designed to measure whether models can behave like reliable “operators” in tool-rich environments rather than only generating text.","L1: Language Comprehension
L2: Planning, Decision-making, Working Memory, Adaptive Error Correction, Semantic Understanding & Context Recognition
L3: Inhibitory Control",L3
HMMT,https://matharena.ai/?view=problem&comp=hmmt--hmmt_feb_2025,https://arxiv.org/abs/2505.23281,"HMMT problems (e.g., Feb 2025 set) are high-difficulty contest mathematics tasks that often require creative multi-step derivations and tight control of algebraic manipulation. They probe robustness on non-routine problem solving beyond standard competition sets.","L1: 
L2: Logical Reasoning, Working Memory, Planning, Attention
L3: Cognitive Flexibility",L3
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath is an expert-level mathematics benchmark intended to measure progress on problems closer to research-grade difficulty, often requiring long chains of precise reasoning and careful verification. It emphasizes correctness under complex symbolic structure, where small errors can invalidate an entire solution.","L1: 
L2: Logical Reasoning, Working Memory, Planning, Attention, Adaptive Error Correction
L3: Cognitive Flexibility",L3
