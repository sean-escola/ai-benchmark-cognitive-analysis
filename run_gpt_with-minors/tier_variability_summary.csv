Benchmark,L1_count,L2_count,L3_count,Total_runs,Mode_tier,Distinct_tiers
SWE-bench Verified,0,25,0,25,L2,1
SWE-bench Pro,0,10,15,25,L3,2
BrowseComp,0,17,8,25,L2,2
τ2-bench,0,0,25,25,L3,1
ARC-AGI,0,0,25,25,L3,1
MCP-Atlas,0,14,11,25,L2,2
Humanity’s Last Exam,0,23,2,25,L2,2
AIME 2025,0,20,5,25,L2,2
GPQA Diamond,0,25,0,25,L2,1
MMMLU,0,14,11,25,L2,2
MMMU-Pro,0,25,0,25,L2,1
ScreenShot-Pro,0,25,0,25,L2,1
CharXiv Reasoning,0,25,0,25,L2,1
Video-MMMU,0,0,25,25,L3,1
MRCR v2 (8-needle),0,9,16,25,L3,2
GDPval,0,1,24,25,L3,2
SWE-Lancer,0,14,11,25,L2,2
Graphwalks,0,15,10,25,L2,2
Toolathon,0,11,14,25,L3,2
HMMT,0,3,22,25,L3,2
FrontierMath,0,1,24,25,L3,2
