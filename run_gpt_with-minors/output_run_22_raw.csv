Benchmark,Website,Paper,Description,Cognitive Functions
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates software engineering agents on real GitHub issues where the model must produce a patch that makes a provided test suite pass. The “Verified” subset consists of tasks vetted (e.g., by humans/automated checks) to ensure the issue is solvable and the evaluation is reliable under standardized harnesses.","Language Comprehension, Planning, Logical Reasoning, Working Memory, Adaptive Error Correction, Decision-making"
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro is a harder, larger-scale software engineering benchmark designed to be more contamination-resistant and industrially representative (e.g., multiple languages and more complex repos/tasks). Models must implement fixes or features in real repositories under stricter evaluation protocols than SWE-bench Verified.","Language Comprehension, Planning, Logical Reasoning, Working Memory, Adaptive Error Correction, Decision-making"
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp evaluates “deep research” style agents on information-seeking questions where the model must browse a controlled document collection (or web-like index) and synthesize a correct final answer. It measures the ability to search, read, cross-check sources, and integrate evidence across multiple documents under tool constraints.","Language Comprehension, Semantic Understanding & Context Recognition, Working Memory, Attention, Planning, Decision-making, Self-reflection"
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench evaluates interactive customer-support agents operating in simulated environments (e.g., retail/airline/telecom) with APIs and policy constraints. The agent must conduct multi-turn dialogues, call tools correctly, follow policies, and resolve user goals despite ambiguity, exceptions, and long-horizon state tracking.","Language Comprehension, Language Production, Social Reasoning & Theory of Mind, Planning, Decision-making, Working Memory, Inhibitory Control, Reward Mechanisms"
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI measures fluid reasoning by requiring models to infer latent transformation rules from only a few input–output grid examples and apply them to new inputs. Tasks are intentionally novel and compact, emphasizing generalization and rule induction rather than memorized domain knowledge.","Logical Reasoning, Cognitive Flexibility, Working Memory, Attention, Spatial Representation & Mapping, Scene Understanding & Visual Reasoning"
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas evaluates real-world tool use via the Model Context Protocol (MCP), where the model must discover tools, execute multi-step workflows, manage errors/retries, and synthesize responses from tool outputs. It targets practical agent reliability in production-like API ecosystems rather than single-call tool demos.","Planning, Decision-making, Working Memory, Language Comprehension, Adaptive Error Correction, Semantic Understanding & Context Recognition"
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam (HLE) is a frontier, multi-domain benchmark intended to stress expert-level reasoning and knowledge, often including multimodal questions. It emphasizes hard, research-like problems where correct answers require synthesis, careful reasoning, and sometimes tool-supported analysis depending on the evaluation setting.","Language Comprehension, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Scene Understanding & Visual Reasoning, Visual Perception, Planning, Self-reflection, Multisensory Integration"
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 is a competition-math benchmark drawn from the 2025 American Invitational Mathematics Examination problems, typically requiring exact integer answers. It stresses multi-step symbolic reasoning, algebraic manipulation, and error-sensitive calculation under time/attempt constraints (often evaluated pass@1).","Logical Reasoning, Working Memory, Planning, Adaptive Error Correction, Attention"
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a subset of GPQA containing the highest-quality graduate-level science multiple-choice questions designed to be “Google-proof.” It targets deep scientific reasoning and precise knowledge application, with distractors that punish shallow pattern matching.","Language Comprehension, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Attention"
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends MMLU to multiple languages, testing broad academic knowledge and reasoning across many subjects with multiple-choice questions. It probes whether models can transfer knowledge and reasoning across linguistic contexts rather than relying solely on English-centric training artifacts.","Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Cognitive Flexibility"
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro is an advanced multimodal benchmark that requires reasoning over images (e.g., diagrams, charts, tables) and text across many expert disciplines. It emphasizes integration of visual evidence with domain knowledge and multi-step inference, often under challenging visual conditions.","Visual Perception, Scene Understanding & Visual Reasoning, Multisensory Integration, Language Comprehension, Logical Reasoning, Working Memory, Attention, Visual Attention & Eye Movements"
ScreenShot-Pro,https://gui-agent.github.io/grounding-leaderboard/,https://arxiv.org/abs/2504.07981,"ScreenShot-Pro evaluates GUI understanding and grounding from high-resolution screenshots, requiring models to interpret interface layout and identify the correct elements or actions. It targets practical “computer use” competence such as locating controls, reading small text, and mapping intents to UI operations.","Visual Perception, Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Language Comprehension, Spatial Representation & Mapping, Planning, Decision-making, Sensorimotor Coordination, Working Memory"
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning focuses on answering reasoning questions about scientific paper figures and content, commonly requiring chart/diagram interpretation and quantitative inference. It measures the ability to extract structured information from visual scientific artifacts and connect it to textual context and domain knowledge.","Visual Perception, Scene Understanding & Visual Reasoning, Language Comprehension, Logical Reasoning, Working Memory, Attention, Semantic Understanding & Context Recognition"
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU extends multimodal understanding to the temporal domain, requiring reasoning over video frames plus text. It probes whether models can integrate evolving visual evidence, maintain temporal coherence, and answer questions that depend on events, state changes, or multi-step video understanding.","Visual Perception, Multisensory Integration, Working Memory, Attention, Cognitive Timing & Predictive Modeling, Scene Understanding & Visual Reasoning, Language Comprehension, Spatial Representation & Mapping"
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) is a long-context evaluation where multiple similar “needle” queries and responses are embedded within large distractor contexts, and the model must retrieve the correct referenced response. It primarily measures robust long-range dependency tracking, resistance to interference, and precise recall under heavy context load.","Working Memory, Attention, Episodic Memory, Language Comprehension, Inhibitory Control, Semantic Understanding & Context Recognition"
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval evaluates economically meaningful, well-specified professional knowledge-work tasks across many occupations, scored by expert human judges via pairwise comparisons. Tasks often require producing polished artifacts (e.g., plans, spreadsheets, presentations) and making pragmatic tradeoffs under constraints.","Planning, Decision-making, Language Production, Language Comprehension, Semantic Understanding & Context Recognition, Working Memory, Self-reflection, Social Reasoning & Theory of Mind"
SWE-Lancer,https://openai.com/index/swe-lancer/,https://arxiv.org/abs/2502.12115,"SWE-Lancer targets agentic software engineering beyond single patches, emphasizing end-to-end work such as multi-step debugging, implementing changes across a codebase, and interacting with tooling. It is designed to reflect practical engineering workflows and longer-horizon task decomposition than simpler coding benchmarks.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Language Comprehension, Logical Reasoning, Self-reflection"
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks tests long-context and algorithmic reasoning by requiring models to follow and query large graph-structured data embedded in text (e.g., BFS-like traversals, parent/neighbor queries). It emphasizes maintaining consistent internal state over many steps and executing discrete reasoning procedures reliably.","Spatial Representation & Mapping, Working Memory, Attention, Logical Reasoning, Planning, Inhibitory Control"
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon evaluates tool-using agents on diverse multi-step tasks that require selecting appropriate tools, issuing correct calls, handling failures, and composing results into a final solution. It targets practical orchestration skills (workflow construction, verification, and recovery) rather than single-turn QA.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Language Comprehension, Self-reflection, Semantic Understanding & Context Recognition"
HMMT,https://matharena.ai/?view=problem&comp=hmmt--hmmt_feb_2025,https://arxiv.org/abs/2505.23281,"HMMT comprises high-difficulty competition math problems (e.g., Harvard-MIT Mathematics Tournament sets) that often require creative problem solving and multi-lemma reasoning. It stresses reliable symbolic manipulation, strategic choice of approaches, and careful verification under strict answer correctness.","Logical Reasoning, Working Memory, Planning, Adaptive Error Correction, Attention, Cognitive Flexibility"
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath is an expert-level mathematics benchmark intended to measure progress on problems closer to research-grade difficulty, often benefiting from tool-assisted computation and rigorous multi-step reasoning. It emphasizes depth, compositionality, and robustness against small logical or algebraic mistakes.","Logical Reasoning, Working Memory, Planning, Adaptive Error Correction, Attention, Cognitive Flexibility, Self-reflection"
