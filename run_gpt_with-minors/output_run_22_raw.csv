Benchmark,Website,Paper,Description,Cognitive Functions
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 evaluates agentic coding and systems skills by placing a model in a real command-line environment where it must complete end-to-end tasks (e.g., debugging, running tools, editing files, and verifying outputs). Success depends on choosing correct shell actions, interpreting program feedback, and iterating until the task is solved under realistic constraints.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Language Comprehension, Language Production"
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp measures deep research and information-seeking by requiring models to answer challenging questions that typically need multi-step browsing and synthesis across multiple documents. It stresses evidence gathering, cross-checking sources, and producing a final answer consistent with retrieved information.","Planning, Attention, Semantic Understanding & Context Recognition, Working Memory, Decision-making, Language Comprehension, Language Production"
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld evaluates “computer use” by having an agent operate within a desktop-like operating system to accomplish practical tasks across apps and web interfaces. It requires visually interpreting GUIs, executing multi-step procedures, recovering from mistakes, and coordinating perception with actions.","Visual Perception, Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Sensorimotor Coordination, Planning, Adaptive Error Correction, Working Memory"
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,ARC-AGI presents few-shot grid transformation puzzles where the model must infer hidden rules from a small number of input–output examples and generalize to a new input. The benchmark targets fluid reasoning under novelty rather than memorized knowledge.,"Logical Reasoning, Cognitive Flexibility, Working Memory, Planning, Spatial Representation & Mapping, Visual Perception"
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 measures long-horizon agent competence in a simulated vending machine business over many decisions, such as sourcing inventory, setting prices, and handling changing market conditions. Performance depends on maintaining coherent strategy over time and adapting decisions based on outcomes.","Planning, Decision-making, Reward Mechanisms, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition"
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity agents on tasks like finding and exploiting known vulnerabilities from descriptions and discovering new issues in real open-source codebases. It emphasizes code comprehension, hypothesis-driven debugging, and iterative testing in realistic vulnerability workflows.","Logical Reasoning, Planning, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition, Inhibitory Control, Language Comprehension"
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"SpreadsheetBench tests whether models can navigate and manipulate real spreadsheets to produce correct outputs, including formula work, data cleaning, and structured transformations. It probes tool-using reliability and the ability to track multi-step state changes while avoiding cascading errors.","Logical Reasoning, Planning, Working Memory, Attention, Adaptive Error Correction, Semantic Understanding & Context Recognition"
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam (HLE) is a difficult, broad, multimodal benchmark intended to stress frontier-level knowledge and reasoning across many disciplines. Questions often require integrating text with visual information and sustaining multi-step reasoning under uncertainty.","Language Comprehension, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Visual Perception, Scene Understanding & Visual Reasoning, Planning"
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,GPQA Diamond is a high-quality subset of GPQA consisting of very challenging multiple-choice science questions designed to resist superficial pattern matching and casual web lookup. It primarily tests deep scientific reasoning and careful reading under tight answer choices.,"Language Comprehension, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Decision-making"
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro evaluates expert-level multimodal understanding and reasoning across many academic and professional domains, combining images (e.g., diagrams, charts, figures) with text questions. It stresses grounding language in visual evidence and performing domain-style reasoning from that evidence.","Multisensory Integration, Visual Perception, Scene Understanding & Visual Reasoning, Language Comprehension, Logical Reasoning, Attention, Working Memory"
MathArena Apex,https://matharena.ai/?view=problem&comp=apex--apex_2025,https://arxiv.org/abs/2505.23281,"MathArena Apex aggregates high-difficulty math problems to evaluate advanced mathematical reasoning under competitive settings. It emphasizes multi-step derivations, precise symbolic manipulation, and robustness across diverse problem styles.","Logical Reasoning, Planning, Working Memory, Cognitive Flexibility"
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning tests whether models can answer questions that require interpreting scientific figures and charts drawn from research papers, connecting visual evidence to textual queries. It stresses quantitative and relational reasoning over complex visuals rather than simple caption reading.","Visual Perception, Scene Understanding & Visual Reasoning, Language Comprehension, Logical Reasoning, Attention, Working Memory"
OmniDocBench 1.5,https://github.com/opendatalab/OmniDocBench,https://arxiv.org/abs/2412.07626,"OmniDocBench 1.5 evaluates document understanding and OCR-style extraction across heterogeneous layouts, including text blocks, formulas, tables, and reading order. It probes robustness to formatting, spatial layout, and structured transcription fidelity.","Visual Perception, Scene Understanding & Visual Reasoning, Attention, Working Memory, Semantic Understanding & Context Recognition, Language Production"
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU extends multimodal reasoning to video, requiring models to answer questions that depend on events unfolding across time rather than a single frame. It stresses temporal integration, tracking entities and actions, and combining visual evidence with language reasoning.","Visual Perception, Scene Understanding & Visual Reasoning, Working Memory, Attention, Cognitive Timing & Predictive Modeling, Multisensory Integration"
LiveCodeBench Pro,https://livecodebenchpro.com/projects/livecodebench-pro/leaderboard,https://arxiv.org/abs/2506.11928,"LiveCodeBench Pro evaluates coding ability on contemporary, competition-style programming tasks with standardized grading, often summarized via Elo-like ratings. It emphasizes generating correct, efficient programs, debugging through failures, and adapting solutions under time-like constraints.","Logical Reasoning, Planning, Adaptive Error Correction, Working Memory, Language Comprehension, Language Production"
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"The FACTS Benchmark Suite systematically evaluates factuality by testing whether model outputs remain consistent with provided sources and real-world ground truth across multiple factuality-related tasks. It targets hallucination resistance, faithful summarization, and correctness under prompting pressure.","Semantic Understanding & Context Recognition, Language Comprehension, Language Production, Inhibitory Control, Self-reflection, Working Memory"
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA evaluates physical commonsense and everyday interaction understanding across many languages, stressing whether models can reason about plausible actions and outcomes in the physical world. It targets robustness of commonsense reasoning beyond English-centric datasets.","Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning, Spatial Representation & Mapping, Cognitive Flexibility"
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,MRCR v2 (8-needle) measures long-context retrieval and multi-round reference tracking by embedding multiple similar “needle” requests inside long “haystack” dialogues and asking the model to reproduce the correct associated response. It stresses sustained attention across long contexts and accurate retrieval under interference.,"Working Memory, Attention, Episodic Memory, Semantic Understanding & Context Recognition, Inhibitory Control"
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval evaluates economically valuable, well-specified knowledge work by asking models to produce real work artifacts (e.g., slides, spreadsheets, plans) across many occupations and scoring them via expert comparison to human professionals. It stresses end-to-end task execution quality, instruction following, and professional judgment in deliverable creation.","Planning, Decision-making, Language Production, Semantic Understanding & Context Recognition, Adaptive Error Correction, Working Memory, Social Reasoning & Theory of Mind"
SWE-Lancer,https://openai.com/index/swe-lancer/,https://arxiv.org/abs/2502.12115,SWE-Lancer evaluates agentic software engineering on realistic repository tasks where success requires implementing fixes or features consistent with project expectations and constraints. It emphasizes translating natural-language requirements into correct code changes and validating them against tests and repo structure.,"Planning, Logical Reasoning, Adaptive Error Correction, Working Memory, Language Comprehension, Language Production, Self-reflection"
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks tests long-context structured reasoning by encoding graph traversal problems in text and requiring models to follow edges, perform BFS-like operations, or answer parent/neighbor queries. It emphasizes systematic multi-step traversal, bookkeeping, and error-free manipulation of discrete structures.","Logical Reasoning, Spatial Representation & Mapping, Working Memory, Attention, Planning"
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon evaluates tool-using agents on tasks that require selecting the right tools, calling them with correct arguments, and composing results into a final answer under realistic tool constraints. It stresses orchestration across multiple steps, recovery from tool errors, and reliable state tracking.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Language Comprehension, Semantic Understanding & Context Recognition"
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath is a hard mathematics benchmark aimed at measuring advanced problem solving near the frontier of current model capability, often requiring deep multi-step reasoning rather than routine computations. It stresses proof-like thinking, compositional strategy selection, and robustness to novel math formulations.","Logical Reasoning, Planning, Working Memory, Cognitive Flexibility"
