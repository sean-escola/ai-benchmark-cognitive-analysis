Benchmark,Website,Paper,Description,Cognitive Functions
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 evaluates autonomous agentic coding and systems tasks inside a real command-line environment (e.g., running programs, inspecting logs, editing files, installing dependencies, and debugging). Success requires correctly sequencing shell actions and recovering from errors under realistic tool and resource constraints.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Language Comprehension"
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp measures deep-research and browsing competence: models must search a fixed corpus (or web-like index), retrieve relevant documents, and synthesize answers to complex questions. It emphasizes evidence aggregation, query refinement, and maintaining correctness despite partial, noisy, or redundant information.","Planning, Decision-making, Attention, Working Memory, Semantic Understanding & Context Recognition, Language Comprehension, Language Production"
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld tests multimodal computer-use agents on realistic desktop tasks (e.g., navigating GUIs, filling forms, managing files, and using applications). Models must interpret screen content and execute multi-step actions to accomplish goals across varying interfaces.","Visual Perception, Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Sensorimotor Coordination, Planning, Decision-making, Working Memory"
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI evaluates “fluid” abstract reasoning from few examples using grid-based input-output puzzles, where the system must infer the underlying transformation rule. It targets generalization to novel tasks rather than memorization, emphasizing rule discovery and compositional reasoning.","Logical Reasoning, Working Memory, Cognitive Flexibility, Spatial Representation & Mapping, Visual Perception"
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon autonomy by having agents run a simulated vending-machine business over an extended period, optimizing inventory, pricing, and supplier interactions. Performance reflects sustained coherence, strategic adaptation to changing conditions, and goal-directed decision-making across many steps.","Planning, Decision-making, Reward Mechanisms, Working Memory, Adaptive Error Correction, Semantic Understanding & Context Recognition"
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity agents on vulnerability identification and exploitation/repair tasks grounded in real-world software vulnerabilities and workflows. It rewards accurate reasoning about code, systems behavior, and attacker/defender actions under realistic constraints.","Logical Reasoning, Planning, Decision-making, Adaptive Error Correction, Semantic Understanding & Context Recognition, Working Memory"
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"SpreadsheetBench measures an agent’s ability to understand and manipulate complex spreadsheets, including formula logic, table operations, and multi-step transformations. Tasks often require programmatic tool use (e.g., Python or office tooling) alongside careful tracking of constraints and intermediate results.","Logical Reasoning, Working Memory, Planning, Decision-making, Attention, Semantic Understanding & Context Recognition"
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a frontier multimodal benchmark spanning expert-level questions across many disciplines, often requiring multi-step reasoning and synthesis rather than recall. It is designed to probe the limits of general problem solving on hard, knowledge-intensive tasks (with and without tools/search in some settings).","Language Comprehension, Language Production, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Visual Perception, Multisensory Integration"
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a difficult, “Google-proof” multiple-choice science benchmark emphasizing deep conceptual understanding and careful elimination of distractors. The Diamond subset focuses on high-quality questions where experts succeed and non-experts frequently fail.","Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Language Comprehension"
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro evaluates multimodal understanding and reasoning across expert-level subjects using images and text (e.g., diagrams, plots, tables, and domain-specific visuals). It stresses grounded reasoning from visual evidence, not just language priors.","Multisensory Integration, Visual Perception, Scene Understanding & Visual Reasoning, Logical Reasoning, Attention, Working Memory"
MathArena Apex,https://matharena.ai/?view=problem&comp=apex--apex_2025,https://arxiv.org/abs/2505.23281,"MathArena Apex aggregates challenging math problems intended to differentiate advanced reasoning systems, often with rigorous verification of final answers. It focuses on complex multi-step derivations, symbolic manipulation, and robust problem-solving under tight correctness criteria.","Logical Reasoning, Working Memory, Planning, Adaptive Error Correction"
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning evaluates scientific figure and paper-centric reasoning, requiring models to interpret research artifacts and answer questions that depend on understanding plots, diagrams, and accompanying text. It targets grounded scientific comprehension and multi-step inference from structured visual evidence.","Scene Understanding & Visual Reasoning, Visual Perception, Language Comprehension, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory"
OmniDocBench 1.5,https://github.com/opendatalab/OmniDocBench,https://arxiv.org/abs/2412.07626,"OmniDocBench 1.5 benchmarks document understanding/OCR across heterogeneous layouts, including text blocks, formulas, tables, and reading order. It measures how well models reconstruct and interpret structured documents rather than plain text alone.","Visual Perception, Visual Attention & Eye Movements, Attention, Language Comprehension, Semantic Understanding & Context Recognition, Working Memory"
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU extends multimodal understanding to video, requiring models to track events over time and answer questions grounded in temporal visual content plus text. It emphasizes integrating cues across frames and maintaining coherent situational understanding.","Visual Perception, Scene Understanding & Visual Reasoning, Cognitive Timing & Predictive Modeling, Working Memory, Multisensory Integration, Attention"
LiveCodeBench Pro,https://livecodebenchpro.com/projects/livecodebench-pro/leaderboard,https://arxiv.org/abs/2506.11928,"LiveCodeBench Pro evaluates code generation and debugging on continuously updated programming tasks, aiming to reduce contamination and better reflect current coding ability. It typically scores functional correctness under time/interaction constraints, emphasizing robust solution construction and repair.","Logical Reasoning, Planning, Adaptive Error Correction, Working Memory, Language Comprehension, Language Production"
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"The FACTS Benchmark Suite systematically evaluates factuality and grounding, measuring whether model outputs remain consistent with provided sources and with reality under adversarial or confusing prompts. It stresses faithful citation/attribution behaviors and resistance to hallucination.","Semantic Understanding & Context Recognition, Language Comprehension, Language Production, Inhibitory Control, Self-reflection, Attention"
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA tests practical and physical commonsense reasoning across many languages, focusing on everyday procedural and intuitive physics knowledge. It probes whether models can generalize pragmatic understanding beyond English-centric cues.","Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning, Cognitive Flexibility"
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,MRCR v2 (8-needle) evaluates long-context multi-round coreference resolution by embedding repeated “needle” interactions in long “haystacks” and requiring retrieval of the correct referenced response. It stresses precise discourse tracking under extreme context lengths.,"Working Memory, Attention, Episodic Memory, Semantic Understanding & Context Recognition, Language Comprehension"
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval evaluates economically meaningful knowledge work across many occupations via side-by-side comparisons against professional outputs, including producing artifacts like spreadsheets, slides, and analyses. It emphasizes end-to-end task execution quality, adherence to specifications, and usefulness to human reviewers.","Planning, Decision-making, Language Production, Language Comprehension, Semantic Understanding & Context Recognition, Working Memory, Social Reasoning & Theory of Mind"
SWE-Lancer,https://openai.com/index/swe-lancer/,https://arxiv.org/abs/2502.12115,"SWE-Lancer evaluates agentic software engineering on tasks that resemble real developer work, often involving iterative debugging, patch creation, and integration with tooling. It is designed to capture reliability and autonomy in completing engineering objectives beyond single-file code writing.","Planning, Decision-making, Adaptive Error Correction, Logical Reasoning, Working Memory, Language Comprehension"
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks tests reasoning over graph-structured data by requiring multi-step traversal, relationship tracking, and correct retrieval of nodes/parents/paths. It emphasizes systematic exploration and state tracking rather than pattern-matching short contexts.","Spatial Representation & Mapping, Working Memory, Attention, Logical Reasoning, Planning"
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon evaluates tool-using agents on multi-step tasks that require selecting, calling, and composing tools reliably to reach a correct end state. It stresses robust orchestration (including error recovery) across diverse tool APIs and intermediate representations.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Attention, Language Comprehension"
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath is an expert-level mathematics benchmark intended to be challenging even for strong models, with rigorous verification and tiering by difficulty. It targets deep mathematical reasoning, long proofs/derivations, and careful handling of edge cases.","Logical Reasoning, Working Memory, Planning, Adaptive Error Correction"
