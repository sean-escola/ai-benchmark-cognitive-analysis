Benchmark,Website,Paper,Description,Cognitive Functions
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates whether a model can fix real GitHub issues by producing a patch that makes a repository’s tests pass. The “Verified” subset consists of tasks that have been validated as solvable and reliably testable, emphasizing correctness and practical software-engineering competence.","Planning, Logical Reasoning, Working Memory, Adaptive Error Correction, Decision-making"
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro is a harder, more contamination-resistant software engineering benchmark spanning multiple programming languages and larger, more realistic tasks. It measures an agent’s ability to understand a codebase, implement or repair functionality, and satisfy rigorous test suites under realistic constraints.","Planning, Logical Reasoning, Working Memory, Adaptive Error Correction, Decision-making, Cognitive Flexibility"
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp evaluates web research agents on information-seeking questions that require searching, reading, and synthesizing evidence from multiple web sources. It stresses reliable navigation and answer construction rather than memorizing facts, often requiring cross-checking and source integration.","Planning, Decision-making, Attention, Working Memory, Semantic Understanding & Context Recognition, Language Comprehension, Language Production, Adaptive Error Correction"
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench measures interactive, policy-constrained tool use in customer-service-like simulations (e.g., retail, airline, telecom). The agent must carry a multi-turn dialogue, call APIs correctly, follow domain rules, and resolve the user’s request without policy violations.","Social Reasoning & Theory of Mind, Language Comprehension, Language Production, Planning, Decision-making, Inhibitory Control, Working Memory, Reward Mechanisms"
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI tests abstract pattern induction and generalization from a few demonstrations using grid-based puzzles. Success requires discovering latent rules from small examples and applying them to novel inputs, emphasizing fluid reasoning over domain knowledge.","Logical Reasoning, Working Memory, Cognitive Flexibility, Planning, Spatial Representation & Mapping, Attention"
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas evaluates real-world tool use through the Model Context Protocol (MCP), requiring models to select, call, and chain tools across multi-step workflows. It emphasizes robustness to API structure, error handling, and composing intermediate results into a correct final output.","Planning, Decision-making, Working Memory, Adaptive Error Correction, Semantic Understanding & Context Recognition, Inhibitory Control"
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam (HLE) is a frontier, multi-domain benchmark designed to probe advanced reasoning and specialized knowledge, often in multimodal formats. Questions are intended to be difficult, requiring careful interpretation, multi-step inference, and sometimes tool-augmented verification.","Language Comprehension, Language Production, Logical Reasoning, Working Memory, Semantic Understanding & Context Recognition, Scene Understanding & Visual Reasoning, Visual Perception"
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,AIME 2025 evaluates mathematical problem solving on contest-style questions that typically require multi-step derivations and careful algebraic or combinatorial reasoning. It is commonly used to gauge symbolic manipulation skill and error-prone reasoning chains under time-like constraints.,"Logical Reasoning, Working Memory, Planning, Adaptive Error Correction, Attention"
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a high-difficulty multiple-choice benchmark of graduate-level science questions designed to be “Google-proof.” The Diamond subset focuses on the most reliable, expert-verified items, stressing deep scientific reasoning and careful distractor discrimination.","Language Comprehension, Logical Reasoning, Working Memory, Semantic Understanding & Context Recognition, Decision-making"
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends broad academic knowledge testing to multiple languages, covering many subjects across diverse linguistic contexts. It emphasizes robust multilingual understanding, transfer of knowledge across languages, and resisting superficial pattern matching.","Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Cognitive Flexibility"
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro is a challenging multimodal benchmark that combines text with images (e.g., diagrams, charts, documents) across many disciplines. It measures whether models can integrate visual evidence with domain knowledge to answer expert-level questions reliably.","Multisensory Integration, Visual Perception, Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Language Comprehension, Logical Reasoning, Working Memory"
ScreenShot-Pro,https://gui-agent.github.io/grounding-leaderboard/,https://arxiv.org/abs/2504.07981,"ScreenShot-Pro evaluates UI grounding and screenshot understanding, where models must interpret high-resolution interface images and answer questions or locate elements. It stresses spatial layout reasoning, fine-grained visual discrimination, and mapping visual cues to precise actions or references.","Visual Perception, Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Spatial Representation & Mapping, Decision-making, Working Memory"
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning tests reasoning over scientific figures and paper content, often requiring interpretation of plots, diagrams, and accompanying textual context. It measures whether models can extract structured information from scientific visuals and perform multi-step inference grounded in that evidence.","Scene Understanding & Visual Reasoning, Visual Perception, Multisensory Integration, Language Comprehension, Logical Reasoning, Working Memory, Attention"
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU evaluates multimodal understanding and reasoning over video, requiring models to integrate temporal visual information with text prompts. Tasks often depend on tracking events across frames and using contextual cues to answer questions about actions, states, or outcomes.","Visual Perception, Scene Understanding & Visual Reasoning, Multisensory Integration, Working Memory, Attention, Cognitive Timing & Predictive Modeling"
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) evaluates long-context retrieval and multi-round coreference resolution by embedding multiple similar “needle” interactions inside long “haystacks.” The model must locate the correct referenced needle and reproduce or select the appropriate response, stressing robust retrieval under interference.","Working Memory, Episodic Memory, Attention, Semantic Understanding & Context Recognition, Adaptive Error Correction"
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval evaluates well-specified professional knowledge-work tasks (e.g., creating business artifacts such as plans, spreadsheets, presentations) judged against expert human performance. It emphasizes producing usable deliverables under constraints, aligning outputs to a goal, and making tradeoffs across quality, correctness, and clarity.","Planning, Decision-making, Language Production, Language Comprehension, Semantic Understanding & Context Recognition, Working Memory, Self-reflection"
SWE-Lancer,https://openai.com/index/swe-lancer/,https://arxiv.org/abs/2502.12115,"SWE-Lancer evaluates agentic software engineering in a freelancing-style setting, focusing on completing practical coding tasks end-to-end in realistic repos. It stresses requirement interpretation, implementation quality, iterative debugging, and producing solutions that satisfy external acceptance criteria.","Planning, Logical Reasoning, Working Memory, Adaptive Error Correction, Decision-making, Semantic Understanding & Context Recognition"
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks tests structured reasoning over graph-structured data by requiring models to traverse or query relationships in a graph (e.g., BFS-like tasks or parent-tracing). It measures whether models can maintain and manipulate relational structure across steps without losing the traversal state.","Spatial Representation & Mapping, Working Memory, Logical Reasoning, Planning, Attention"
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon evaluates general tool-use competence across diverse tools and workflows, emphasizing correct tool selection, parameterization, and multi-step orchestration. It typically stresses reliability across long action sequences, including recovery from tool errors and composing outputs into a final answer.","Planning, Decision-making, Working Memory, Adaptive Error Correction, Inhibitory Control, Semantic Understanding & Context Recognition"
HMMT,https://matharena.ai/?view=problem&comp=hmmt--hmmt_feb_2025,https://arxiv.org/abs/2505.23281,"HMMT evaluates high-level contest mathematics with problems that often require creative problem decomposition, proof-like reasoning, and careful bookkeeping. It is used to probe deep mathematical reasoning and robustness against subtle logical or arithmetic mistakes.","Logical Reasoning, Working Memory, Planning, Adaptive Error Correction, Attention"
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath targets expert-level mathematics beyond routine contest problems, aiming to measure genuine research-adjacent reasoning difficulty across tiers. It stresses long chains of deduction, choosing appropriate techniques, and avoiding compounding errors over extended solutions.","Logical Reasoning, Working Memory, Planning, Adaptive Error Correction, Cognitive Flexibility, Attention"
