Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates end-to-end software engineering by asking a model to generate patches that fix real issues in open-source Python repositories, with tasks curated to be solvable and verified. It measures whether an agent can understand a codebase, localize bugs, implement correct changes, and satisfy tests under a constrained, single-attempt setting.","L1: Language Comprehension, Language Production
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Planning, Working Memory, Adaptive Error Correction
L3: ",L2
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro is a larger and more difficult successor-style evaluation that tests realistic software engineering across multiple programming languages and more diverse industrial tasks. It emphasizes higher difficulty, stronger contamination resistance, and broader repository understanding than prior SWE-bench variants.","L1: Language Comprehension, Language Production
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Planning, Working Memory, Adaptive Error Correction
L3: Cognitive Flexibility",L3
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp measures an agent’s ability to answer questions by performing web-style information seeking and synthesis over a controlled document collection, stressing deep research rather than memorization. Successful performance requires decomposing a query, searching effectively, verifying evidence, and producing a grounded final response.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Attention, Working Memory, Semantic Understanding & Context Recognition, Adaptive Error Correction
L3: ",L2
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench evaluates interactive customer-support agents operating in simulated domains (e.g., retail, airline, telecom) that must follow policies while using tools/APIs and handling multi-turn conversations. It probes whether models can reliably execute tool calls, maintain dialogue state, and adhere to constraints under realistic interaction pressure.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Working Memory, Adaptive Error Correction, Attention
L3: Inhibitory Control, Social Reasoning & Theory of Mind",L3
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI tests fluid reasoning through few-shot induction on grid-based puzzles where the model must infer hidden transformation rules from a small set of examples. It emphasizes generalization to novel tasks, rule discovery, and compositional reasoning rather than domain knowledge.","L1: Visual Perception
L2: Logical Reasoning, Working Memory, Attention, Spatial Representation & Mapping, Scene Understanding & Visual Reasoning, Planning
L3: Cognitive Flexibility",L3
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas evaluates real-world tool use via the Model Context Protocol, requiring models to discover appropriate tools, call them correctly, and complete multi-step workflows across authentic server environments. It stresses robustness to tool errors, API semantics, and multi-hop integration across multiple tool calls.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Working Memory, Semantic Understanding & Context Recognition, Adaptive Error Correction, Attention
L3: Inhibitory Control",L3
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam (HLE) is a broad, frontier-knowledge benchmark with difficult questions (often multimodal) intended to measure advanced reasoning and expert-level competence. It typically stresses synthesis across domains, careful reading of prompts, and producing correct, well-justified answers under limited attempts.","L1: Language Comprehension, Language Production, Visual Perception
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Scene Understanding & Visual Reasoning, Attention
L3: ",L2
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 uses problems from the American Invitational Mathematics Examination to test competition-style mathematical reasoning. It emphasizes multi-step derivations, symbolic manipulation, and precise final answers rather than retrieval of facts.","L1: 
L2: Logical Reasoning, Planning, Working Memory, Adaptive Error Correction, Attention
L3: ",L2
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a hard subset of GPQA consisting of high-quality, graduate-level multiple-choice science questions designed to be difficult to answer by simple web search. It targets scientific reasoning, careful constraint tracking, and disambiguation across closely related options.","L1: Language Comprehension
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Attention
L3: ",L2
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends broad academic knowledge testing into multiple languages, assessing whether a model can answer questions across many subjects while operating in non-English contexts. It probes multilingual comprehension, transfer of knowledge across languages, and reasoning under linguistic variation.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Attention
L3: Cognitive Flexibility",L3
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro is a stronger, more challenging multimodal benchmark where models must combine text with images (e.g., diagrams, charts, scientific figures) to answer expert-level questions. It emphasizes multimodal grounding and reasoning beyond simple visual recognition.","L1: Visual Perception, Language Comprehension, Language Production
L2: Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Working Memory, Attention, Spatial Representation & Mapping
L3: ",L2
ScreenShot-Pro,https://gui-agent.github.io/grounding-leaderboard/,https://arxiv.org/abs/2504.07981,"ScreenShot-Pro evaluates screenshot understanding for GUI-centric tasks, where models must interpret complex interfaces and answer questions or take actions grounded in on-screen elements. It stresses layout understanding, precise referencing of UI components, and robust perception under real-world visual clutter.","L1: Visual Perception, Language Comprehension, Language Production
L2: Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Spatial Representation & Mapping, Planning, Decision-making, Working Memory, Sensorimotor Coordination
L3: ",L2
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning tests whether models can answer reasoning questions about scientific paper figures, combining visual evidence (plots, tables, diagrams) with accompanying text or scientific context. It focuses on extracting quantitative/structural information from figures and drawing correct inferences.","L1: Visual Perception, Language Comprehension, Language Production
L2: Scene Understanding & Visual Reasoning, Logical Reasoning, Working Memory, Attention, Semantic Understanding & Context Recognition, Multisensory Integration
L3: ",L2
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU extends multimodal understanding to video, requiring temporal integration of visual information (and often accompanying text prompts) to answer questions about events and dynamics. It probes whether models can maintain coherent representations over time and use them for reasoning.","L1: Visual Perception, Language Comprehension, Language Production
L2: Scene Understanding & Visual Reasoning, Working Memory, Attention, Multisensory Integration
L3: Cognitive Timing & Predictive Modeling",L3
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) is a long-context evaluation where multiple similar “needle” interactions are embedded within a large “haystack,” and the model must retrieve or reproduce the correct response corresponding to a specified needle. It stresses robust long-range dependency tracking, coreference, and resistance to distractors.","L1: Language Comprehension, Language Production
L2: Working Memory, Attention, Semantic Understanding & Context Recognition, Episodic Memory, Logical Reasoning
L3: ",L2
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval evaluates well-specified professional knowledge-work tasks across many occupations by comparing model outputs to industry professionals using expert judging. Tasks often require producing realistic work artifacts (e.g., slides, spreadsheets, plans) under constraints, emphasizing quality, correctness, and usefulness.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Semantic Understanding & Context Recognition, Working Memory, Adaptive Error Correction
L3: Self-reflection, Social Reasoning & Theory of Mind",L3
SWE-Lancer,https://openai.com/index/swe-lancer/,https://arxiv.org/abs/2502.12115,"SWE-Lancer evaluates software-engineering work framed as realistic “freelance” tasks, emphasizing agentic completion of scoped engineering deliverables rather than isolated coding puzzles. It stresses understanding requirements, navigating repositories, implementing changes, and validating solutions end-to-end.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Semantic Understanding & Context Recognition, Working Memory, Adaptive Error Correction
L3: Cognitive Flexibility",L3
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks tests structured reasoning over graph-defined problems, such as following edges, tracking parent relations, or performing BFS-like traversals under context constraints. It emphasizes systematic state tracking and compositional reasoning rather than domain knowledge.","L1: 
L2: Logical Reasoning, Working Memory, Spatial Representation & Mapping, Attention, Planning, Adaptive Error Correction
L3: ",L2
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon evaluates tool-using agents on multi-step tasks that require selecting among tools, calling them correctly, and integrating results into a final answer under realistic failure modes. It targets reliability of action selection, tool semantics understanding, and robustness to partial or noisy tool outputs.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Working Memory, Semantic Understanding & Context Recognition, Adaptive Error Correction, Attention
L3: Inhibitory Control",L3
HMMT,https://matharena.ai/?view=problem&comp=hmmt--hmmt_feb_2025,https://arxiv.org/abs/2505.23281,"HMMT uses high-difficulty competition math problems (e.g., Harvard-MIT Mathematics Tournament) to probe advanced mathematical reasoning beyond standard contest sets. It emphasizes creative problem solving, long multi-step derivations, and careful constraint management.","L1: 
L2: Logical Reasoning, Planning, Working Memory, Adaptive Error Correction, Attention
L3: Cognitive Flexibility",L3
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath is an expert-level mathematics benchmark designed to measure progress on genuinely hard problems, including higher-tier questions that are difficult for current models and often require deep multi-step reasoning. It emphasizes rigorous derivations, error checking, and persistence across complex solution paths.","L1: 
L2: Logical Reasoning, Planning, Working Memory, Adaptive Error Correction, Attention
L3: Cognitive Flexibility",L3
