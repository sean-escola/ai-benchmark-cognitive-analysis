Benchmark,Website,Paper,Description,Cognitive Functions
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates automated software engineering by asking models to produce a patch that fixes a real issue in an existing GitHub repository and passes a held-out test suite. The “Verified” subset consists of tasks that have been human-validated as solvable and reliably testable, making it a widely used indicator of practical coding reliability under realistic repo constraints.","Planning, Decision-making, Logical Reasoning, Working Memory, Semantic Understanding & Context Recognition, Adaptive Error Correction"
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro is a larger, more difficult successor benchmark designed to be more contamination-resistant and more reflective of industrial software engineering. It expands beyond the original setting with more diverse tasks (including multiple languages) and a higher bar for end-to-end repo understanding, debugging, and patch correctness.","Planning, Decision-making, Logical Reasoning, Working Memory, Semantic Understanding & Context Recognition, Adaptive Error Correction, Cognitive Flexibility"
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp evaluates “deep research” style web agents: the model must search, read, and synthesize information from a controlled web corpus (or fixed index) to answer questions that require nontrivial browsing and evidence gathering. Performance depends on decomposing the task, selecting sources, tracking evidence across steps, and producing a grounded final answer.","Planning, Decision-making, Attention, Working Memory, Semantic Understanding & Context Recognition, Language Comprehension, Language Production"
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench measures interactive agent performance in customer-support-like scenarios (e.g., retail, airline, telecom) where the model must follow domain policies while using tools/APIs over multiple turns. It probes whether the agent can manage dialog state, interpret user intent, execute correct actions, and avoid policy violations or loophole exploitation.","Social Reasoning & Theory of Mind, Language Comprehension, Language Production, Planning, Decision-making, Working Memory, Inhibitory Control, Attention"
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI tests fluid, few-shot reasoning via abstract grid transformation problems: given a small set of input–output examples, the model must infer the hidden rule and apply it to a new input. The tasks are designed to reduce reliance on memorized knowledge and emphasize rapid generalization to novel structure.","Logical Reasoning, Cognitive Flexibility, Spatial Representation & Mapping, Working Memory, Attention, Scene Understanding & Visual Reasoning"
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas evaluates real-world tool use through the Model Context Protocol (MCP), where models must discover and invoke appropriate tools across multi-step workflows and then synthesize results. It emphasizes correct API selection, parameterization, error handling, and integration of tool outputs into coherent responses.","Planning, Decision-making, Working Memory, Semantic Understanding & Context Recognition, Adaptive Error Correction, Attention"
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a large, frontier-focused benchmark intended to stress broad reasoning and expert knowledge across many domains, often with multimodal inputs. It targets questions that are difficult to answer via shallow pattern matching, encouraging multi-step analysis, synthesis, and (in tool-enabled settings) careful evidence use.","Language Comprehension, Language Production, Logical Reasoning, Working Memory, Semantic Understanding & Context Recognition, Planning, Scene Understanding & Visual Reasoning"
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 consists of competition-style mathematics problems from the American Invitational Mathematics Examination, requiring precise symbolic reasoning under time-like constraints. It rewards constructing multi-step solution plans, maintaining intermediate invariants, and avoiding algebraic or combinatorial slips.","Logical Reasoning, Planning, Working Memory, Cognitive Flexibility, Adaptive Error Correction"
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a high-difficulty, graduate-level multiple-choice science benchmark designed to be “Google-proof,” with questions curated to resist superficial lookup strategies. The Diamond subset focuses on the most reliable and challenging items, emphasizing deep conceptual understanding and careful elimination reasoning.","Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Decision-making"
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends broad academic evaluation into a multilingual setting across many subjects, testing whether models can generalize knowledge and reasoning beyond English. It emphasizes robust comprehension of non-English prompts and consistent application of learned concepts across languages and domains.","Language Comprehension, Semantic Understanding & Context Recognition, Working Memory, Logical Reasoning"
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro evaluates expert-level multimodal understanding and reasoning across diverse disciplines using images paired with text questions. It stresses integrating visual evidence (figures, diagrams, tables, screenshots) with domain knowledge and multi-step reasoning to select or generate correct answers.","Visual Perception, Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Multisensory Integration, Working Memory, Logical Reasoning, Language Comprehension"
ScreenShot-Pro,https://gui-agent.github.io/grounding-leaderboard/,https://arxiv.org/abs/2504.07981,"ScreenShot-Pro measures GUI understanding and grounding from high-resolution screenshots, often requiring identification of interface elements and reasoning about spatial layout. It approximates the perceptual grounding needed for computer-use agents by testing whether models can map natural-language goals to specific on-screen targets.","Visual Perception, Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Spatial Representation & Mapping, Decision-making, Planning, Sensorimotor Coordination"
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning focuses on reasoning over figures from scientific papers (e.g., plots, charts, and diagrammatic evidence) and answering associated questions. It probes the ability to extract quantitative/relational information from visuals and combine it with textual context and scientific semantics.","Visual Perception, Scene Understanding & Visual Reasoning, Logical Reasoning, Working Memory, Semantic Understanding & Context Recognition, Language Comprehension"
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU evaluates multimodal understanding and reasoning over video, requiring models to integrate information across time rather than from a single static image. Tasks often demand tracking events, interpreting visual context, and answering questions that hinge on temporal dependencies and long-range context within a clip.","Visual Perception, Scene Understanding & Visual Reasoning, Cognitive Timing & Predictive Modeling, Working Memory, Attention, Language Comprehension, Multisensory Integration"
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) is a long-context evaluation where multiple similar “needle” interactions are embedded in a large “haystack,” and the model must retrieve the correct response associated with a specific needle. It stresses robust context tracking under distractors and accurate reference resolution over long inputs.","Working Memory, Attention, Episodic Memory, Logical Reasoning, Language Comprehension, Semantic Understanding & Context Recognition"
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval measures performance on well-specified professional knowledge-work tasks spanning many occupations, judged by expert humans via pairwise comparisons. Tasks often require producing real work artifacts (e.g., plans, analyses, documents, or structured deliverables) that reflect practical constraints, clarity, and correctness.","Planning, Decision-making, Language Production, Semantic Understanding & Context Recognition, Working Memory, Self-reflection"
SWE-Lancer,https://openai.com/index/swe-lancer/,https://arxiv.org/abs/2502.12115,"SWE-Lancer evaluates realistic software engineering work framed like freelance/contract tasks, emphasizing end-to-end delivery quality rather than isolated coding snippets. It stresses understanding ambiguous requirements, making design tradeoffs, implementing changes in context, and validating the result against expected behavior.","Planning, Decision-making, Logical Reasoning, Working Memory, Semantic Understanding & Context Recognition, Adaptive Error Correction"
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks tests algorithmic reasoning over graph-structured data expressed in text, commonly requiring multi-step traversal (e.g., BFS-style) and correct bookkeeping over long sequences. It stresses consistent step-by-step state updates and resistance to drift when many similar nodes/edges appear.","Spatial Representation & Mapping, Logical Reasoning, Working Memory, Attention, Adaptive Error Correction"
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon evaluates tool-using agents on multi-step tasks that require choosing among tools, calling them correctly, and integrating outputs into a final solution. It emphasizes orchestration reliability: selecting the right action at the right time, handling tool errors, and maintaining task state across multiple calls.","Planning, Decision-making, Working Memory, Semantic Understanding & Context Recognition, Adaptive Error Correction, Attention"
HMMT,https://matharena.ai/?view=problem&comp=hmmt--hmmt_feb_2025,https://arxiv.org/abs/2505.23281,"HMMT consists of high-difficulty competition mathematics problems (e.g., Harvard-MIT Mathematics Tournament sets) that often require creative multi-step constructions and careful proof-like reasoning. It stresses maintaining long chains of intermediate results and adapting strategy when an initial approach stalls.","Logical Reasoning, Planning, Working Memory, Cognitive Flexibility, Adaptive Error Correction"
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath targets expert-level mathematics with problems intended to be beyond routine contest patterns, emphasizing deep reasoning and formal problem-solving. Success typically requires extended planning, precise symbolic manipulation, and rigorous checking of edge cases to avoid subtle errors.","Logical Reasoning, Planning, Working Memory, Cognitive Flexibility, Adaptive Error Correction"
