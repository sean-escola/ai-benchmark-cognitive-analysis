Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates an agent’s ability to fix real issues in open-source Python repositories by generating patches that make the test suite pass. The “Verified” subset focuses on tasks that have been validated as solvable and have reliable evaluation, emphasizing end-to-end coding rather than isolated function completion.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Logical Reasoning, Working Memory, Adaptive Error Correction
L3: ",L2
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro is a harder, larger software engineering benchmark intended to be more contamination-resistant and more representative of professional development work across multiple programming languages. Models must understand a repository and issue description, implement a correct fix, and satisfy automated checks, often requiring broader refactors and stronger robustness.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Logical Reasoning, Working Memory, Adaptive Error Correction
L3: Cognitive Flexibility",L3
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp evaluates “deep research” agents on questions that require multi-step information gathering and synthesis from a curated document/web corpus. It targets the ability to search effectively, integrate evidence across sources, and produce a final grounded answer under realistic agent constraints.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Attention, Episodic Memory, Semantic Understanding & Context Recognition, Working Memory, Adaptive Error Correction
L3: ",L2
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench measures interactive tool-using agents in multi-turn customer-support simulations (e.g., retail, airline, telecom) where the agent must call APIs, follow policies, and satisfy user goals. Success depends on consistent policy adherence, accurate state tracking across turns, and effective dialogue to resolve cases.","L1: Language Comprehension, Language Production
L2: Decision-making, Planning, Working Memory, Adaptive Error Correction, Attention
L3: Social Reasoning & Theory of Mind, Inhibitory Control",L3
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI is a fluid intelligence benchmark where models infer hidden rules from a few input–output grid examples and produce the correct output grid for a new input. It is designed to emphasize abstraction, compositional generalization, and novelty over memorized knowledge.","L1: Visual Perception
L2: Logical Reasoning, Working Memory, Spatial Representation & Mapping, Scene Understanding & Visual Reasoning, Attention
L3: Cognitive Flexibility",L3
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas evaluates real-world tool use through the Model Context Protocol by requiring multi-step workflows across tool servers (discovering tools, invoking them correctly, handling errors, and composing results). It targets agent reliability in production-like API interactions rather than single-call function execution.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Working Memory, Adaptive Error Correction, Attention
L3: ",L2
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,Humanity’s Last Exam (HLE) is a large multimodal benchmark covering frontier academic and professional questions intended to be challenging even for strong models. It tests deep reasoning and synthesis (often across text and images) and is frequently evaluated both with and without external tools such as search or code execution.,"L1: Language Comprehension, Language Production, Visual Perception
L2: Logical Reasoning, Working Memory, Semantic Understanding & Context Recognition, Scene Understanding & Visual Reasoning, Planning, Decision-making
L3: ",L2
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 is a competition-math benchmark drawn from the American Invitational Mathematics Examination, featuring short-answer problems that require nontrivial derivations. It probes structured mathematical reasoning and careful multi-step computation under tight correctness constraints.","L1: 
L2: Logical Reasoning, Working Memory, Planning, Attention
L3: Cognitive Flexibility",L3
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a difficult multiple-choice science benchmark designed to be “Google-proof,” emphasizing questions that require expert-level reasoning rather than simple fact lookup. The Diamond subset is the highest-quality set, filtered to be challenging for non-experts and reliable for evaluation.","L1: Language Comprehension
L2: Logical Reasoning, Working Memory, Semantic Understanding & Context Recognition, Attention, Decision-making
L3: ",L2
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,MMMLU extends the MMLU-style academic knowledge and reasoning evaluation into multiple non-English languages across many subjects. It tests whether a model’s understanding and reasoning transfer across languages rather than relying on English-only competence.,"L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Attention
L3: ",L2
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro is a high-difficulty multimodal benchmark that tests expert-level understanding and reasoning over combined text and image inputs (e.g., diagrams, charts, scientific visuals) across many disciplines. Compared with earlier MMMU settings, it emphasizes harder questions and more reliable measurement of advanced multimodal reasoning.","L1: Visual Perception, Language Comprehension, Language Production
L2: Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Working Memory, Attention
L3: ",L2
ScreenShot-Pro,https://gui-agent.github.io/grounding-leaderboard/,https://arxiv.org/abs/2504.07981,"ScreenShot-Pro evaluates how well models can understand and reason over high-resolution screenshots of graphical user interfaces, often requiring precise identification of UI elements and their relationships. It targets visually grounded reasoning relevant to computer-use agents, including interpreting layout, text, and controls.","L1: Visual Perception, Language Comprehension
L2: Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Spatial Representation & Mapping, Logical Reasoning, Working Memory, Planning, Decision-making
L3: ",L2
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning tests models on questions that require interpreting figures, tables, and scientific diagrams from research papers, often demanding quantitative and causal reasoning grounded in the visual evidence. It is intended to capture “read-the-paper-figure” competence rather than generic image captioning.","L1: Visual Perception, Language Comprehension, Language Production
L2: Scene Understanding & Visual Reasoning, Logical Reasoning, Working Memory, Attention, Semantic Understanding & Context Recognition
L3: ",L2
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU evaluates multimodal reasoning over video, requiring models to integrate information across time (and typically accompanying text) to answer questions about events, causality, and context. It stresses temporal understanding beyond single-frame perception.","L1: Visual Perception, Language Comprehension, Language Production
L2: Scene Understanding & Visual Reasoning, Working Memory, Attention, Multisensory Integration, Logical Reasoning
L3: Cognitive Timing & Predictive Modeling",L3
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (multi-round coreference resolution) tests long-context reasoning by inserting multiple similar “needle” interactions into long “haystack” dialogues and asking the model to retrieve the correct referenced response. The 8-needle setting increases distractors and memory demands, emphasizing robust retrieval across lengthy contexts.","L1: Language Comprehension, Language Production
L2: Working Memory, Episodic Memory, Attention, Logical Reasoning
L3: ",L2
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval evaluates economically valuable, well-specified professional knowledge-work tasks across many occupations, judged by expert humans via head-to-head comparisons (wins/ties). Tasks often involve producing realistic work artifacts (e.g., spreadsheets, plans, analyses), testing end-to-end usefulness rather than academic Q&A alone.","L1: Language Production, Language Comprehension
L2: Planning, Decision-making, Semantic Understanding & Context Recognition, Working Memory, Adaptive Error Correction
L3: Self-reflection, Social Reasoning & Theory of Mind",L3
SWE-Lancer,https://openai.com/index/swe-lancer/,https://arxiv.org/abs/2502.12115,"SWE-Lancer targets agentic software engineering in settings closer to real paid work, where tasks can involve ambiguous requirements, iterative debugging, and broader repo-level changes. It emphasizes delivering correct, reviewable patches under practical constraints rather than solving isolated algorithmic problems.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Logical Reasoning, Working Memory, Adaptive Error Correction
L3: Cognitive Flexibility",L3
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks evaluates long-context graph reasoning by asking models to follow graph-structured relationships (e.g., traversals, ancestry/parent links) described in text and return the correct nodes/paths. It stresses faithful stepwise tracking of structure across many hops, where small errors compound.","L1: Language Comprehension, Language Production
L2: Spatial Representation & Mapping, Working Memory, Logical Reasoning, Attention, Planning, Adaptive Error Correction
L3: ",L2
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon is a tool-use benchmark where agents must select and chain calls to multiple tools/APIs to complete tasks, often requiring error handling, retries, and correct integration of tool outputs. It focuses on practical orchestration and reliability of multi-step tool workflows.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory, Attention
L3: ",L2
HMMT,https://matharena.ai/?view=problem&comp=hmmt--hmmt_feb_2025,https://arxiv.org/abs/2505.23281,"HMMT is a competition-math benchmark (e.g., Harvard–MIT Mathematics Tournament) featuring challenging problems that require creative constructions, proofs, or multi-step computations. It probes deeper mathematical reasoning than many short-form tests, rewarding robust solution planning and careful execution.","L1: 
L2: Logical Reasoning, Working Memory, Planning, Attention
L3: Cognitive Flexibility",L3
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath is an expert-level mathematics benchmark designed to measure progress on problems closer to the research frontier, often requiring substantial derivations and the combination of multiple techniques. It is used to evaluate whether models can reliably solve difficult, novel math problems rather than standard contest items.","L1: 
L2: Logical Reasoning, Working Memory, Planning, Attention
L3: Cognitive Flexibility",L3
