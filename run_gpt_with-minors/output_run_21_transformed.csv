Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 evaluates agentic coding and sysadmin-style tasks in real terminal environments (e.g., navigating repos, running commands, debugging, and modifying files) under a fixed harness. It emphasizes end-to-end task completion where intermediate tool outputs must be interpreted correctly and used to decide the next command.","L1: Language Comprehension
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory
L3: ",L2
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp measures deep information-seeking over the web (or a controlled web corpus), requiring models to search, read, cross-check sources, and synthesize answers. Success depends on decomposing research questions into sub-queries and integrating evidence across multiple pages and tool calls.","L1: Language Comprehension
L2: Planning, Attention, Semantic Understanding & Context Recognition, Working Memory, Decision-making
L3: ",L2
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld evaluates computer-use agents that operate graphical desktop environments to accomplish multi-step tasks across real applications. It stresses perception of UI state from screenshots, grounding actions in interface affordances, and recovering from mistakes across long trajectories.","L1: Visual Perception
L2: Visual Attention & Eye Movements, Planning, Decision-making, Sensorimotor Coordination, Adaptive Error Correction
L3: ",L2
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI tests fluid reasoning by asking models to infer hidden transformations from a few input–output grid examples and apply them to a new grid. It is designed to minimize reliance on memorized knowledge and instead probe abstraction, generalization, and robust pattern induction.","L1: Visual Perception
L2: Logical Reasoning, Working Memory, Spatial Representation & Mapping
L3: Cognitive Flexibility",L3
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon agent coherence by simulating the operation of a vending-machine business over an extended period with many sequential decisions. Models must manage inventory, negotiate with suppliers, set pricing, and adapt to market dynamics to maximize final balance.","L1: 
L2: Planning, Decision-making, Reward Mechanisms, Working Memory, Adaptive Error Correction, Semantic Understanding & Context Recognition
L3: ",L2
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity capability on real-world vulnerability tasks at scale, including identifying known vulnerabilities from descriptions and discovering new issues in codebases. It requires carefully reading code and logs, forming hypotheses about failure modes, and validating fixes or exploits with tools.","L1: Language Comprehension
L2: Logical Reasoning, Planning, Adaptive Error Correction, Working Memory
L3: ",L2
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"SpreadsheetBench measures an agent’s ability to understand and manipulate spreadsheets to solve realistic tasks (e.g., cleaning data, editing formulas, generating summaries, and producing correct outputs). It stresses structured reasoning over tabular layouts and precise execution of multi-step edits.","L1: 
L2: Logical Reasoning, Working Memory, Attention, Planning, Adaptive Error Correction
L3: ",L2
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a multimodal benchmark of frontier knowledge and reasoning, spanning difficult questions where shallow pattern matching tends to fail. It typically demands integrating domain expertise with careful reasoning, and (in tool-enabled settings) verifying with search or code.","L1: Language Comprehension, Language Production, Visual Perception
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory
L3: ",L2
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a high-quality subset of extremely challenging, graduate-level science multiple-choice questions intended to be “Google-proof.” It targets deep scientific understanding and careful multi-step reasoning rather than surface recall.","L1: Language Comprehension
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory
L3: ",L2
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro evaluates expert-level multimodal understanding and reasoning across many disciplines using images paired with questions, often in exam-like formats. It probes whether models can extract salient visual details (e.g., diagrams, charts, instruments) and combine them with domain reasoning to choose correct answers.","L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Logical Reasoning, Visual Attention & Eye Movements, Working Memory
L3: ",L2
MathArena Apex,https://matharena.ai/?view=problem&comp=apex--apex_2025,https://arxiv.org/abs/2505.23281,"MathArena Apex aggregates hard mathematical reasoning problems under a standardized evaluation interface, focusing on robust problem solving rather than short tricks. It emphasizes multi-step derivations, error checking, and consistency under challenging distributions.","L1: 
L2: Logical Reasoning, Working Memory, Planning, Adaptive Error Correction
L3: Cognitive Flexibility",L3
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning evaluates scientific figure understanding and reasoning from research-paper-style charts/figures, often requiring quantitative interpretation and multi-step inference. In tool-enabled variants, models may use code to compute values or validate hypotheses derived from the figure.","L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Logical Reasoning, Working Memory, Planning
L3: ",L2
OmniDocBench 1.5,https://github.com/opendatalab/OmniDocBench,https://arxiv.org/abs/2412.07626,"OmniDocBench 1.5 measures document understanding and OCR quality across complex layouts, including text blocks, formulas, tables, and reading order. It emphasizes faithful extraction and structural reconstruction rather than only recognizing isolated text.","L1: Visual Perception, Language Comprehension
L2: Visual Attention & Eye Movements, Semantic Understanding & Context Recognition, Working Memory
L3: ",L2
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU evaluates multimodal reasoning over videos, requiring models to track events, objects, and states across time and answer questions that depend on temporal context. It stresses integration of visual evidence across frames and maintaining coherent situation models over sequences.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Working Memory, Attention
L3: Cognitive Timing & Predictive Modeling",L3
LiveCodeBench Pro,https://livecodebenchpro.com/projects/livecodebench-pro/leaderboard,https://arxiv.org/abs/2506.11928,"LiveCodeBench Pro evaluates competitive-programming and practical coding ability using fresh, execution-verified tasks and emphasizes strong generalization. It measures whether models can design algorithms, implement correct code, and iteratively fix failures under realistic constraints.","L1: Language Comprehension
L2: Logical Reasoning, Planning, Adaptive Error Correction, Working Memory
L3: ",L2
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"FACTS Benchmark Suite systematically evaluates factuality and grounding behaviors, probing whether models avoid hallucinations and maintain consistency with provided or implied evidence. It targets reliability in long-form responses where many individual claims must remain correct.","L1: Language Production, Language Comprehension
L2: Semantic Understanding & Context Recognition, Working Memory
L3: Inhibitory Control",L3
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA evaluates physical commonsense reasoning across many languages and cultures using non-parallel items, aiming to reduce English-centric artifacts. It probes whether models can select plausible actions/outcomes in everyday physical scenarios under multilingual variation.","L1: Language Comprehension
L2: Logical Reasoning, Semantic Understanding & Context Recognition
L3: Cognitive Flexibility",L3
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) is a long-context evaluation where multiple similar “needle” turns are embedded in large “haystack” conversations, and the model must retrieve the correct response associated with a specified needle. It stresses precise multi-round coreference and robust retrieval under heavy distractors.","L1: Language Comprehension
L2: Working Memory, Attention, Episodic Memory
L3: ",L2
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval measures performance on well-specified professional knowledge-work tasks across many occupations, judged by expert humans via head-to-head comparisons. It emphasizes producing usable work artifacts (e.g., slides, spreadsheets, plans) that meet real constraints and quality standards.","L1: Language Production
L2: Planning, Decision-making, Semantic Understanding & Context Recognition, Working Memory
L3: Self-reflection",L3
SWE-Lancer,https://openai.com/index/swe-lancer/,https://arxiv.org/abs/2502.12115,"SWE-Lancer evaluates agentic software engineering on tasks resembling freelance or contract work, where the model must interpret requirements, modify codebases, and deliver correct patches. It emphasizes end-to-end delivery quality, including navigating ambiguity and validating solutions.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Adaptive Error Correction, Logical Reasoning, Working Memory
L3: ",L2
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks evaluates long-context structured reasoning over graph traversal problems (e.g., following edges, reaching targets, or answering queries about paths) presented in text form. It stresses maintaining state over many steps and avoiding drift under distractors.","L1: 
L2: Spatial Representation & Mapping, Logical Reasoning, Working Memory, Attention
L3: ",L2
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon evaluates general tool-use competence across diverse tool APIs and multi-step workflows, focusing on whether models can select, call, and chain tools correctly to solve tasks. It stresses robustness to tool errors, iterative refinement, and producing final answers grounded in tool outputs.","L1: Language Comprehension
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory
L3: ",L2
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath is a collection of very difficult mathematics problems intended to sit near the boundary of current model capability, with strong controls to reduce contamination. It emphasizes deep multi-step reasoning and (in tool-enabled settings) careful verification of derived results.","L1: 
L2: Logical Reasoning, Working Memory, Planning, Adaptive Error Correction
L3: Cognitive Flexibility",L3
