Benchmark,Website,Paper,Description,Cognitive Functions
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,SWE-bench Verified evaluates software engineering agents on real GitHub issues by asking the model to generate a code patch that makes the project’s tests pass. The “Verified” split is a curated subset where tasks are confirmed solvable and evaluation uses repository-specific test harnesses for pass/fail scoring.,"Planning, Working Memory, Logical Reasoning, Adaptive Error Correction, Semantic Understanding & Context Recognition, Decision-making"
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro is a harder software engineering benchmark with a larger and more diverse set of real-world tasks, designed to be more contamination-resistant and closer to professional development scenarios. Models must produce correct patches across multiple languages and projects under stricter, industrially relevant constraints.","Planning, Working Memory, Logical Reasoning, Adaptive Error Correction, Semantic Understanding & Context Recognition, Cognitive Flexibility, Decision-making"
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp evaluates “deep research” style agents on information-seeking questions that require searching, reading, and synthesizing evidence from multiple documents. Performance is measured by whether the agent returns the correct final answer given constrained browsing/search tools and a fixed evaluation protocol.","Planning, Attention, Working Memory, Semantic Understanding & Context Recognition, Decision-making, Self-reflection, Language Comprehension, Language Production"
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench measures an agent’s ability to complete multi-turn customer-support tasks in simulated domains (e.g., retail, airline, telecom) while using APIs and following policies. It emphasizes reliable tool invocation, consistent policy adherence, and coherent dialogue over long interactions.","Social Reasoning & Theory of Mind, Language Comprehension, Language Production, Decision-making, Inhibitory Control, Planning, Working Memory, Reward Mechanisms"
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,ARC-AGI tests fluid reasoning by presenting a few input–output grid examples and asking the model to infer the hidden transformation rule for new inputs. It is designed to emphasize novelty and systematic generalization rather than broad memorized knowledge.,"Logical Reasoning, Working Memory, Cognitive Flexibility, Spatial Representation & Mapping, Scene Understanding & Visual Reasoning, Attention, Planning"
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas evaluates real-world tool use via the Model Context Protocol (MCP), where models must discover appropriate tools, call them correctly, and compose multi-step workflows across tool servers. Tasks require handling failures, retries, and API outputs to produce correct end responses.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition, Language Comprehension, Language Production"
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam (HLE) is a frontier, often multimodal benchmark intended to probe broad expert-level reasoning and knowledge across many domains. Questions are designed to be difficult for models without careful multi-step inference and may allow tool-enabled configurations in some evaluations.","Language Comprehension, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Scene Understanding & Visual Reasoning, Planning, Language Production"
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 is a set of competition-style math problems from the American Invitational Mathematics Examination, used to evaluate mathematical reasoning and precision. Problems typically require multi-step derivations, careful constraint handling, and exact final answers.","Logical Reasoning, Working Memory, Planning, Attention, Adaptive Error Correction"
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,GPQA Diamond is a high-quality subset of GPQA consisting of graduate-level multiple-choice science questions that are difficult to answer by superficial pattern matching. It targets deep scientific reasoning and domain knowledge under “google-proof” style constraints.,"Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Attention, Decision-making"
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends MMLU to multiple languages, testing broad academic knowledge and reasoning across many subjects in non-English settings. It is commonly used to evaluate cross-lingual generalization and robustness of learned concepts.","Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Cognitive Flexibility"
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro is a challenging multimodal benchmark that requires reasoning over images (e.g., diagrams, charts, photos) together with text across many disciplines. It aims to measure expert-level multimodal understanding and compositional reasoning beyond simple recognition.","Visual Perception, Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Attention, Working Memory, Language Comprehension"
ScreenShot-Pro,https://gui-agent.github.io/grounding-leaderboard/,https://arxiv.org/abs/2504.07981,ScreenShot-Pro evaluates models on understanding high-resolution GUI screenshots and answering questions or performing grounded reasoning about interface elements. It stresses spatial grounding (where things are on screen) and correct interpretation of UI semantics under realistic visual noise.,"Visual Perception, Visual Attention & Eye Movements, Spatial Representation & Mapping, Scene Understanding & Visual Reasoning, Decision-making, Working Memory, Language Comprehension"
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,CharXiv Reasoning evaluates whether models can answer reasoning-heavy questions grounded in figures and content from scientific papers (often requiring quantitative and visual inference). It emphasizes extracting structured information from visual scientific artifacts and integrating it with textual context.,"Visual Perception, Scene Understanding & Visual Reasoning, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Attention, Language Comprehension"
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU tests multimodal understanding and reasoning over short videos, requiring models to integrate events over time and answer questions about actions, causality, and context. It emphasizes temporal scene understanding rather than single-frame recognition.","Visual Perception, Scene Understanding & Visual Reasoning, Cognitive Timing & Predictive Modeling, Working Memory, Attention, Multisensory Integration, Language Comprehension"
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 is a long-context coreference and retrieval-style evaluation where multiple similar “needle” requests are embedded in long “haystack” dialogues/documents. The model must retrieve the correct referenced response among many distractors, with accuracy measured by match quality to the target needle’s answer.","Working Memory, Attention, Episodic Memory, Language Comprehension, Semantic Understanding & Context Recognition, Inhibitory Control"
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval evaluates well-specified professional knowledge work across many occupations, where models must produce real work products (e.g., slides, spreadsheets, plans) that are judged against human professional outputs. Scoring is typically based on expert comparative judgments (wins/ties) rather than multiple-choice accuracy.","Planning, Decision-making, Language Production, Semantic Understanding & Context Recognition, Working Memory, Adaptive Error Correction, Self-reflection"
SWE-Lancer,https://openai.com/index/swe-lancer/,https://arxiv.org/abs/2502.12115,"SWE-Lancer evaluates software engineering ability on realistic, end-to-end tasks that resemble commissioned engineering work, often emphasizing implementation quality and correctness in addition to passing tests. It is intended to measure stronger “ship a fix” performance under practical constraints.","Planning, Logical Reasoning, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition, Decision-making, Cognitive Flexibility"
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks measures structured reasoning over graph problems, such as following paths, parents, or BFS-like traversals under long contexts. It targets whether models can reliably maintain and update intermediate states while executing discrete algorithmic steps.","Spatial Representation & Mapping, Working Memory, Logical Reasoning, Attention, Planning, Adaptive Error Correction"
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon evaluates tool-using agents on tasks that require selecting among tools, formatting calls correctly, handling tool outputs, and composing results across multiple steps. It is designed to stress robustness of tool orchestration and error recovery rather than pure language generation.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition, Inhibitory Control, Language Comprehension, Language Production"
HMMT,https://matharena.ai/?view=problem&comp=hmmt--hmmt_feb_2025,https://arxiv.org/abs/2505.23281,HMMT consists of high-difficulty competition math problems (from the Harvard-MIT Mathematics Tournament) used to test advanced mathematical reasoning. Problems often require creative multi-step proofs or derivations under time-competition style constraints.,"Logical Reasoning, Working Memory, Planning, Attention, Cognitive Flexibility"
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath is an expert-level mathematics benchmark designed to probe difficult, research-adjacent problem solving, often beyond standard contest difficulty. It emphasizes sustained multi-step reasoning, precision, and the ability to combine diverse mathematical ideas to reach a correct result.","Logical Reasoning, Working Memory, Planning, Attention, Cognitive Flexibility, Adaptive Error Correction"
