Benchmark,Website,Paper,Description,Cognitive Functions
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 evaluates agentic coding and system-administration skill by requiring models to complete real tasks inside a terminal environment (e.g., debugging, installing dependencies, running commands, and editing files). Success depends on choosing correct command sequences, interpreting outputs/errors, and iterating toward a working solution under realistic constraints.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Language Comprehension"
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp measures deep-research performance on questions that require searching and synthesizing information from documents (often under controlled or reproducible retrieval setups). It emphasizes finding relevant evidence, reconciling sources, and producing a grounded final answer rather than relying on memorized knowledge.","Language Comprehension, Semantic Understanding & Context Recognition, Planning, Decision-making, Attention, Working Memory, Episodic Memory"
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld evaluates multimodal “computer use” where an agent must complete tasks through a graphical desktop environment using screenshots and interactive actions. It tests end-to-end capability: perceiving UI state, planning multi-step workflows across applications, and recovering from mistakes and unexpected UI changes.","Visual Perception, Visual Attention & Eye Movements, Planning, Decision-making, Sensorimotor Coordination, Working Memory, Adaptive Error Correction"
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI is a fluid-intelligence benchmark where models infer hidden transformation rules from a few example input–output grids and apply them to a new grid. It targets generalization to novel patterns with minimal examples, emphasizing compositional abstraction rather than domain knowledge.","Visual Perception, Scene Understanding & Visual Reasoning, Logical Reasoning, Working Memory, Cognitive Flexibility"
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon agent coherence by simulating management of a vending-machine business over many decisions (pricing, inventory, supplier negotiation, etc.). The score reflects sustained strategy, adaptation to changing conditions, and avoiding compounding errors across an extended trajectory.","Planning, Decision-making, Reward Mechanisms, Working Memory, Adaptive Error Correction, Semantic Understanding & Context Recognition, Motivational Drives"
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity agents on tasks such as finding known vulnerabilities from descriptions and discovering new vulnerabilities in real open-source projects. It stresses precise technical reasoning, hypothesis testing via tools, and careful iteration based on program behavior and evidence.","Logical Reasoning, Planning, Decision-making, Working Memory, Adaptive Error Correction, Semantic Understanding & Context Recognition"
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"SpreadsheetBench tests whether models can navigate and manipulate complex spreadsheets to answer questions or produce correct transformations, often using realistic files and operations. It emphasizes structured data understanding, formula/logic consistency, and multi-step editing workflows.","Semantic Understanding & Context Recognition, Logical Reasoning, Planning, Decision-making, Working Memory, Adaptive Error Correction"
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam (HLE) is a difficult multimodal benchmark spanning frontier academic and professional knowledge, requiring reasoning rather than rote recall. Questions often demand combining domain knowledge with careful multi-step inference, and may be evaluated both with and without tool assistance depending on the setup.","Language Comprehension, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Visual Perception"
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,GPQA Diamond is a subset of extremely challenging graduate-level multiple-choice science questions designed to be resistant to shallow pattern matching. It assesses whether models can perform disciplined scientific reasoning and select the correct option under high distractor quality.,"Language Comprehension, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory"
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro is a multimodal benchmark for expert-level understanding and reasoning across many disciplines using text-plus-image questions. It probes whether models can integrate visual evidence (diagrams, plots, figures) with domain knowledge to answer complex multiple-choice problems.","Visual Perception, Scene Understanding & Visual Reasoning, Language Comprehension, Logical Reasoning, Multisensory Integration, Visual Attention & Eye Movements, Working Memory"
MathArena Apex,https://matharena.ai/?view=problem&comp=apex--apex_2025,https://arxiv.org/abs/2505.23281,"MathArena Apex evaluates advanced mathematical problem solving, typically emphasizing rigorous multi-step derivations and correctness under competition-style constraints. It is designed to separate surface-level pattern matching from deeper mathematical reasoning and verification.","Logical Reasoning, Working Memory, Planning, Adaptive Error Correction"
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning focuses on scientific figure understanding and reasoning over charts/plots from research papers, often requiring quantitative interpretation and cross-referencing visual elements with captions or text. It measures the ability to extract the right variables, relationships, and claims from dense visual scientific artifacts.","Visual Perception, Scene Understanding & Visual Reasoning, Language Comprehension, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Visual Attention & Eye Movements"
OmniDocBench 1.5,https://github.com/opendatalab/OmniDocBench,https://arxiv.org/abs/2412.07626,"OmniDocBench 1.5 benchmarks document understanding and OCR-style extraction across heterogeneous layouts, including text, formulas, and tables with reading-order requirements. It evaluates robust parsing of complex page structure and faithful reconstruction of document content.","Visual Perception, Visual Attention & Eye Movements, Language Comprehension, Semantic Understanding & Context Recognition, Spatial Representation & Mapping, Working Memory"
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU evaluates multimodal reasoning over videos, requiring models to understand events, temporal relationships, and visual details across frames to answer questions. It emphasizes integrating information over time rather than relying on single-frame cues.","Visual Perception, Visual Attention & Eye Movements, Cognitive Timing & Predictive Modeling, Working Memory, Scene Understanding & Visual Reasoning, Multisensory Integration"
LiveCodeBench Pro,https://livecodebenchpro.com/projects/livecodebench-pro/leaderboard,https://arxiv.org/abs/2506.11928,"LiveCodeBench Pro assesses real-world coding ability on fresh or continuously updated programming tasks, often emphasizing generalization beyond static training-era benchmarks. It targets writing correct code, debugging failures, and producing working solutions under constrained attempts.","Language Comprehension, Language Production, Logical Reasoning, Planning, Working Memory, Adaptive Error Correction"
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"The FACTS Benchmark Suite systematically evaluates factuality by testing whether model outputs remain consistent with evidence, avoid unsupported claims, and correctly handle uncertainty. It aims to measure hallucination-related failure modes across diverse factuality settings rather than a single QA format.","Language Comprehension, Semantic Understanding & Context Recognition, Inhibitory Control, Self-reflection, Working Memory"
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA evaluates pragmatic reasoning in everyday situations across many languages and cultural contexts, focusing on what action or interpretation is sensible given a scenario. It probes whether models can apply commonsense, context, and intent rather than only literal text matching.","Language Comprehension, Semantic Understanding & Context Recognition, Decision-making, Social Reasoning & Theory of Mind, Cognitive Flexibility"
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,MRCR v2 (8-needle) tests long-context multi-round coreference resolution by embedding multiple similar “needle” interactions inside long “haystacks” and asking the model to reproduce the correct referenced content. It stresses robust retrieval and disambiguation under high interference and long-range dependencies.,"Working Memory, Attention, Episodic Memory, Language Comprehension, Semantic Understanding & Context Recognition"
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval measures performance on well-specified professional “knowledge work” tasks across many occupations, typically judged via human comparisons to expert outputs. Tasks require producing real artifacts (e.g., plans, analyses, presentations/spreadsheets) with correctness, clarity, and practical usefulness.","Planning, Decision-making, Language Production, Semantic Understanding & Context Recognition, Working Memory, Self-reflection"
SWE-Lancer,https://openai.com/index/swe-lancer/,https://arxiv.org/abs/2502.12115,"SWE-Lancer evaluates software engineering agents on realistic repository-based tasks that resemble professional development work, often including longer-horizon debugging and feature implementation. It emphasizes choosing effective engineering actions, maintaining context across files, and converging on a correct patch.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Logical Reasoning, Language Comprehension, Language Production"
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks benchmarks graph reasoning over sequences, such as following edges, retrieving parents, or performing BFS-style traversal under memory and distractor pressure. It tests whether models can maintain and manipulate structured relational state across many steps.","Spatial Representation & Mapping, Working Memory, Logical Reasoning, Attention, Planning"
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon evaluates whether models can solve tasks by selecting, invoking, and chaining tools across heterogeneous tool ecosystems, with correctness depending on both tool choice and parameterization. It stresses robust orchestration, error handling, and integration of tool outputs into a coherent solution.","Planning, Decision-making, Working Memory, Adaptive Error Correction, Language Comprehension, Semantic Understanding & Context Recognition"
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath targets expert-level mathematics beyond standard contest sets, designed to be difficult for both humans and models and to better reflect frontier reasoning demands. It emphasizes deep multi-step derivations, careful verification, and resilience against subtle errors.","Logical Reasoning, Working Memory, Planning, Adaptive Error Correction"
