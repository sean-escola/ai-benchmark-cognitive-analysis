Benchmark,Website,Paper,Description,Cognitive Functions
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates software engineering by asking models to generate patches that resolve real issues in open-source Python repositories, validated by running the project’s test suite. The “Verified” subset consists of tasks confirmed by human annotators to be solvable and unambiguous, reducing noise from underspecified bugs.","Language Comprehension, Language Production, Planning, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition"
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro is a larger and more difficult software engineering benchmark designed to be more industrially representative and more resistant to contamination than earlier SWE-bench variants. It includes complex, multi-file changes and broader language coverage, requiring robust repository understanding and patch generation under realistic constraints.","Language Comprehension, Language Production, Planning, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition, Decision-making"
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp evaluates “deep research” agents that must browse and synthesize information from a large web corpus to answer questions, often requiring multi-step exploration and evidence gathering. It emphasizes reliable retrieval, cross-document synthesis, and staying on task under long tool-using trajectories.","Planning, Decision-making, Attention, Working Memory, Episodic Memory, Language Comprehension, Language Production, Semantic Understanding & Context Recognition"
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench evaluates interactive customer-support agents in simulated domains (e.g., retail, airline, telecom) where the agent must converse with a user and call APIs while complying with domain policies. Success depends on maintaining dialogue state, following constraints, and executing correct multi-turn workflows despite distractions and edge cases.","Social Reasoning & Theory of Mind, Language Comprehension, Language Production, Planning, Decision-making, Inhibitory Control, Working Memory, Semantic Understanding & Context Recognition"
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,ARC-AGI measures fluid reasoning via abstract grid transformation puzzles: models infer hidden rules from a few input-output examples and must generalize to a new input. It is designed to reduce reliance on memorized knowledge by emphasizing novel pattern induction and compositional rule learning.,"Logical Reasoning, Cognitive Flexibility, Working Memory, Spatial Representation & Mapping, Visual Perception, Scene Understanding & Visual Reasoning, Attention"
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas evaluates real-world tool use through the Model Context Protocol, requiring models to discover relevant tools, invoke them with correct arguments, handle errors, and integrate outputs into final answers. Tasks are multi-step and closer to production workflows, stressing reliable orchestration rather than single-call accuracy.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Language Comprehension, Language Production, Semantic Understanding & Context Recognition, Inhibitory Control"
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam (HLE) is a difficult multimodal benchmark intended to probe frontier-level knowledge and reasoning across many domains, including questions where shallow pattern matching is insufficient. Depending on the evaluation setup, models may need to combine long-form reasoning, tool use, and careful evidence-based responses.","Language Comprehension, Language Production, Logical Reasoning, Working Memory, Semantic Understanding & Context Recognition, Visual Perception, Scene Understanding & Visual Reasoning, Multisensory Integration, Planning"
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 uses problems from the American Invitational Mathematics Examination, requiring multi-step competition-style mathematical reasoning under tight constraints. It tests symbolic manipulation, careful case analysis, and error-free execution over longer solution chains.","Logical Reasoning, Working Memory, Planning, Adaptive Error Correction"
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a subset of GPQA consisting of especially challenging, graduate-level multiple-choice questions in the natural sciences designed to be difficult to answer by simple web search. It probes deep domain understanding and the ability to eliminate plausible distractors through reasoning.","Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Decision-making, Language Comprehension"
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends MMLU-style broad academic evaluation into multiple languages, testing whether models retain knowledge and reasoning competence across multilingual prompts. It highlights cross-lingual generalization, instruction following, and robustness to linguistic variation.","Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Cognitive Flexibility"
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro is a harder, “pro” variant of MMMU that evaluates multimodal understanding and expert-level reasoning across many disciplines using images paired with text questions. It stresses reading complex visuals (charts, diagrams, figures) and integrating them with domain knowledge to select correct answers.","Visual Perception, Scene Understanding & Visual Reasoning, Multisensory Integration, Language Comprehension, Logical Reasoning, Working Memory, Attention"
ScreenShot-Pro,https://gui-agent.github.io/grounding-leaderboard/,https://arxiv.org/abs/2504.07981,"ScreenShot-Pro evaluates understanding of high-resolution screenshots of software interfaces, focusing on grounding: identifying relevant UI elements, interpreting layout, and answering or acting based on what is visible. It probes fine-grained spatial and visual reasoning in realistic computer-use settings.","Visual Perception, Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Spatial Representation & Mapping, Language Comprehension, Decision-making, Working Memory"
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,CharXiv Reasoning evaluates whether models can answer questions that require interpreting and reasoning over scientific figures (and associated context) from research papers. It emphasizes extracting quantitative/structural information from plots and diagrams and combining it with textual constraints to reach correct conclusions.,"Scene Understanding & Visual Reasoning, Visual Perception, Language Comprehension, Logical Reasoning, Working Memory, Attention, Semantic Understanding & Context Recognition"
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU extends multimodal reasoning evaluation to video, requiring models to integrate information over time (events, actions, and changes) and answer questions that depend on temporal context. It tests comprehension of dynamic scenes and the ability to retain and reason over longer visual sequences.","Visual Perception, Attention, Working Memory, Cognitive Timing & Predictive Modeling, Scene Understanding & Visual Reasoning, Multisensory Integration, Language Comprehension"
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) tests long-context multi-round coreference and retrieval by embedding multiple similar “needle” interactions inside long “haystacks” and asking the model to reproduce the correct referenced response. The 8-needle variant increases distractors and ambiguity, stressing robust context tracking rather than surface matching.","Working Memory, Attention, Episodic Memory, Inhibitory Control, Language Comprehension, Semantic Understanding & Context Recognition"
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval evaluates well-specified professional “knowledge work” across many occupations, where models must produce real work artifacts (e.g., plans, analyses, slides/spreadsheets) judged against human professionals. It emphasizes end-to-end task execution quality, including reasoning about constraints, formatting, and usefulness for downstream decision-making.","Planning, Decision-making, Language Production, Language Comprehension, Semantic Understanding & Context Recognition, Working Memory, Adaptive Error Correction, Self-reflection"
SWE-Lancer,https://openai.com/index/swe-lancer/,https://arxiv.org/abs/2502.12115,"SWE-Lancer evaluates agentic software engineering in more end-to-end settings, emphasizing reliable bug fixing and implementation work that resembles real engineering tickets. Tasks often require understanding large codebases, making coordinated multi-file edits, and producing patches that pass automated checks.","Language Comprehension, Language Production, Planning, Adaptive Error Correction, Working Memory, Decision-making, Semantic Understanding & Context Recognition"
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks evaluates algorithmic reasoning over graph-structured data presented in text, requiring models to follow traversal rules (e.g., BFS-style walks) and answer queries that depend on correct path/parent relationships. It stresses systematic step-by-step state tracking and resisting shortcuts when graphs are large or distractors are present.","Spatial Representation & Mapping, Logical Reasoning, Working Memory, Attention, Inhibitory Control, Language Comprehension"
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon evaluates general tool-use competence across diverse tasks where models must select, call, and chain tools correctly, then synthesize outputs into a final solution. It emphasizes robustness to tool errors, argument formatting, and multi-step orchestration under realistic constraints.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Language Comprehension, Language Production, Inhibitory Control"
HMMT,https://matharena.ai/?view=problem&comp=hmmt--hmmt_feb_2025,https://arxiv.org/abs/2505.23281,"HMMT uses problems from the Harvard-MIT Mathematics Tournament, which are typically harder and more proof- and insight-oriented than standard competition questions. It evaluates multi-step reasoning, creative decomposition, and precision in symbolic manipulation across longer solution chains.","Logical Reasoning, Working Memory, Planning, Cognitive Flexibility, Adaptive Error Correction"
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath is a curated set of expert-level mathematics problems designed to be challenging for frontier models and informative about progress on advanced mathematical reasoning. It emphasizes deep multi-step derivations, careful handling of definitions and edge cases, and sustained coherence over long proofs or computations.","Logical Reasoning, Working Memory, Planning, Cognitive Flexibility, Adaptive Error Correction"
