Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 evaluates agentic coding and systems ability by having models operate in real terminal environments to complete practical tasks (e.g., installing dependencies, running tests, debugging, and executing command-line workflows). Success requires translating natural-language goals into correct shell actions, iteratively handling errors, and producing a working end state.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory
L3: ",L2
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp evaluates “deep research” style question answering where models must locate, synthesize, and justify answers from a controlled web/document corpus rather than relying on parametric memory alone. The benchmark emphasizes multi-step information seeking, evidence integration, and maintaining coherence across long investigative traces.","L1: Language Comprehension
L2: Planning, Decision-making, Attention, Working Memory, Semantic Understanding & Context Recognition, Logical Reasoning
L3: ",L2
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld measures computer-use autonomy in a realistic operating-system environment, requiring models to complete tasks by interacting with graphical UIs (opening apps, navigating settings, editing files, etc.). It stresses end-to-end perception–decision–action loops, where mistakes must be diagnosed and corrected through further interaction.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Planning, Decision-making, Adaptive Error Correction, Working Memory, Sensorimotor Coordination
L3: ",L2
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,ARC-AGI is a few-shot “fluid intelligence” benchmark where models infer latent rules from a handful of grid-based input–output examples and apply them to a new input. It targets novelty and abstraction: success depends on discovering the transformation program rather than recalling memorized patterns.,"L1: Visual Perception
L2: Logical Reasoning, Working Memory, Planning, Spatial Representation & Mapping
L3: Cognitive Flexibility",L3
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon agent performance in a simulated business setting where the model manages a vending machine operation over an extended period. It requires sustained strategy (pricing, inventory, supplier interactions) and the ability to adapt decisions based on outcomes over time.","L1: 
L2: Planning, Decision-making, Reward Mechanisms, Working Memory, Semantic Understanding & Context Recognition, Adaptive Error Correction
L3: ",L2
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity agent capabilities on real-world vulnerability tasks, including identifying known vulnerabilities from descriptions and, in some settings, discovering new issues. It stresses careful technical reasoning, tool-like iterative debugging, and disciplined handling of complex code and system artifacts.","L1: Language Comprehension
L2: Logical Reasoning, Planning, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition
L3: ",L2
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"SpreadsheetBench measures how well models can understand, manipulate, and produce correct outputs in complex spreadsheets, often involving formulas, tables, and multi-step transformations. It emphasizes structured reasoning over semi-structured data and robust execution of multi-stage editing/analysis workflows.","L1: 
L2: Logical Reasoning, Working Memory, Planning, Adaptive Error Correction, Semantic Understanding & Context Recognition, Decision-making
L3: ",L2
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam (HLE) is a multimodal benchmark designed to probe frontier knowledge and reasoning across difficult questions spanning many domains, often requiring synthesis rather than recall. It tests whether models can follow long, high-stakes problem statements and produce precise, well-justified answers (sometimes with tools depending on the setup).","L1: Language Comprehension, Language Production, Visual Perception
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Scene Understanding & Visual Reasoning, Planning
L3: ",L2
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a curated subset of very challenging, graduate-level multiple-choice science questions designed to be resistant to simple web search and shallow pattern matching. Performance depends on deep domain understanding and multi-step scientific reasoning under a constrained answer format.","L1: Language Comprehension
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Decision-making
L3: ",L2
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro is a strengthened multimodal understanding and reasoning benchmark derived from MMMU, featuring harder expert-level questions over images, diagrams, and text across many disciplines. It emphasizes grounding reasoning in visual evidence and combining it with domain knowledge to select or generate correct answers.","L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Logical Reasoning, Working Memory, Multisensory Integration, Decision-making
L3: ",L2
MathArena Apex,https://matharena.ai/?view=problem&comp=apex--apex_2025,https://arxiv.org/abs/2505.23281,"MathArena Apex is a competition-style mathematics benchmark that aggregates difficult problems intended to separate top frontier models under standardized evaluation. It emphasizes rigorous multi-step derivations, careful symbolic manipulation, and consistency under pressure from tricky or novel problem structures.","L1: 
L2: Logical Reasoning, Working Memory, Planning, Decision-making
L3: Cognitive Flexibility",L3
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning evaluates scientific figure understanding and reasoning, asking models to answer questions that require interpreting plots, diagrams, and figure annotations typical of research papers. It stresses extracting quantitative/relational information from visuals and integrating it with textual scientific context.","L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Logical Reasoning, Semantic Understanding & Context Recognition, Multisensory Integration, Working Memory
L3: ",L2
OmniDocBench 1.5,https://github.com/opendatalab/OmniDocBench,https://arxiv.org/abs/2412.07626,"OmniDocBench 1.5 evaluates document AI capabilities such as OCR and structured understanding across varied content types (text blocks, tables, formulas, and reading order). The benchmark targets robust parsing of complex layouts and faithful reconstruction/extraction of information from documents.","L1: Visual Perception, Language Comprehension
L2: Visual Attention & Eye Movements, Semantic Understanding & Context Recognition, Working Memory, Scene Understanding & Visual Reasoning
L3: ",L2
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU evaluates multimodal reasoning over videos, requiring models to integrate information across frames and time to answer questions about events, actions, and visual details. It stresses temporal integration and maintaining a coherent internal representation as the scene evolves.","L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Working Memory, Multisensory Integration, Attention
L3: Cognitive Timing & Predictive Modeling",L3
LiveCodeBench Pro,https://livecodebenchpro.com/projects/livecodebench-pro/leaderboard,https://arxiv.org/abs/2506.11928,"LiveCodeBench Pro evaluates coding ability on a curated set of programming tasks with strong emphasis on real execution-oriented correctness rather than stylistic code generation. It captures iterative problem solving, translating specifications into programs, and correcting mistakes to pass tests.","L1: Language Comprehension, Language Production
L2: Planning, Logical Reasoning, Adaptive Error Correction, Working Memory, Decision-making
L3: ",L2
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,The FACTS Benchmark Suite systematically evaluates factuality by measuring whether model outputs remain grounded in provided evidence and avoid unsupported claims across diverse settings. It targets reliability and calibration: strong performance reflects resisting hallucination and maintaining consistency with sources.,"L1: Language Comprehension, Language Production
L2: Semantic Understanding & Context Recognition, Working Memory
L3: Inhibitory Control, Self-reflection",L3
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA evaluates commonsense and physical-interaction reasoning across languages, focusing on whether models can select plausible actions/outcomes in everyday situations. It stresses pragmatic understanding of the world and robust generalization beyond a single cultural or linguistic context.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Decision-making
L3: Social Reasoning & Theory of Mind",L3
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) evaluates long-context, multi-round coreference resolution by embedding multiple similar “needle” interactions within a long “haystack” dialogue and asking the model to retrieve the correct referenced response. It stresses stable retention and precise retrieval under heavy distractor interference.","L1: Language Comprehension
L2: Working Memory, Episodic Memory, Attention, Semantic Understanding & Context Recognition
L3: Inhibitory Control",L3
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval evaluates economically valuable knowledge work by comparing model-produced professional artifacts (e.g., plans, analyses, spreadsheets, presentations) against human professionals via expert judging. It stresses end-to-end task execution: interpreting a goal, producing structured deliverables, and making tradeoffs under real-world constraints.","L1: Language Production, Language Comprehension
L2: Planning, Decision-making, Semantic Understanding & Context Recognition, Working Memory
L3: Self-reflection, Social Reasoning & Theory of Mind",L3
SWE-Lancer,https://openai.com/index/swe-lancer/,https://arxiv.org/abs/2502.12115,"SWE-Lancer evaluates software engineering agents on realistic repository-level tasks that require implementing changes and producing correct patches under practical constraints. It emphasizes robust engineering workflow competence, including understanding requirements, navigating codebases, and debugging failures.","L1: Language Comprehension, Language Production
L2: Planning, Logical Reasoning, Adaptive Error Correction, Working Memory, Decision-making
L3: ",L2
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks evaluates long-context structured reasoning by requiring models to follow and manipulate graph relationships (e.g., traversal, parent queries) that are presented in-context. It stresses multi-step relational inference where intermediate steps must be maintained accurately over long sequences.","L1: 
L2: Spatial Representation & Mapping, Working Memory, Logical Reasoning, Attention, Planning
L3: ",L2
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon evaluates tool-using agents in environments where success depends on selecting the right tools, composing multi-step tool chains, and recovering from tool/API errors. It emphasizes practical orchestration: mapping intentions to actions, verifying intermediate results, and adjusting plans when tools fail or return unexpected outputs.","L1: Language Comprehension
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory, Attention
L3: Inhibitory Control",L3
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath evaluates advanced mathematical reasoning on problems intended to be challenging for frontier models, often requiring deep multi-step proofs or intricate computations. It is designed to probe the limits of formal reasoning and error-free derivation under very difficult problem distributions.","L1: 
L2: Logical Reasoning, Working Memory, Planning, Decision-making
L3: Cognitive Flexibility",L3
