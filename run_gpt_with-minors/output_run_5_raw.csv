Benchmark,Website,Paper,Description,Cognitive Functions
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates software engineering agents on real GitHub issues where the model must generate a code patch that makes the repository’s tests pass. The “Verified” subset uses tasks that have been human-validated as solvable and focuses on reliable, end-to-end bug fixing under a standardized harness.","Planning, Logical Reasoning, Working Memory, Adaptive Error Correction, Semantic Understanding & Context Recognition, Decision-making"
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro is a larger, harder software engineering benchmark intended to be more realistic and contamination-resistant, spanning multiple programming languages and more complex repositories. Agents must understand issue descriptions, navigate codebases, implement fixes, and satisfy test suites under stricter evaluation conditions.","Planning, Logical Reasoning, Working Memory, Adaptive Error Correction, Semantic Understanding & Context Recognition, Decision-making, Cognitive Flexibility"
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp measures a model’s ability to do deep, grounded web research by searching and synthesizing information from an online corpus (or a controlled web index in some variants). Success requires decomposing a query, iteratively retrieving evidence, and producing a supported final answer rather than a purely parametric response.","Planning, Attention, Episodic Memory, Semantic Understanding & Context Recognition, Language Comprehension, Language Production, Decision-making"
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench evaluates interactive agent behavior in simulated customer-support domains (e.g., retail, airline, telecom) where the model must use tools/APIs and follow domain policies over multi-turn dialogues. It emphasizes consistent policy adherence, correct tool use, and user-centered problem resolution in realistic workflows.","Social Reasoning & Theory of Mind, Language Comprehension, Language Production, Decision-making, Planning, Inhibitory Control, Adaptive Error Correction, Working Memory"
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI tests “fluid” abstract reasoning using small grid-based puzzles: given a few input–output examples, the model must infer the hidden transformation rule and apply it to a new input. The benchmark is designed to reward generalization to novel patterns rather than memorization of domain knowledge.","Logical Reasoning, Working Memory, Cognitive Flexibility, Spatial Representation & Mapping, Planning, Attention"
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas evaluates real-world tool use via the Model Context Protocol (MCP), requiring models to discover tools, call them correctly, handle errors, and compose multi-step workflows across services. Tasks resemble production agent usage, emphasizing robustness, correct API interaction, and coherent final synthesis.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Language Comprehension, Language Production, Semantic Understanding & Context Recognition"
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a frontier, multimodal benchmark spanning difficult questions across many domains, intended to probe broad, expert-level knowledge and reasoning. It often rewards careful synthesis, multi-step problem solving, and (in tool-enabled settings) strategic use of search or code to verify claims.","Language Comprehension, Language Production, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Multisensory Integration, Planning"
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 evaluates competition-level mathematical problem solving on the 2025 American Invitational Mathematics Examination set. Problems require multi-step symbolic reasoning, constraint handling, and exact answer derivation under limited context.","Logical Reasoning, Working Memory, Planning, Adaptive Error Correction"
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a subset of graduate-level, “Google-proof” multiple-choice science questions designed to resist shallow retrieval and reward genuine understanding. It probes disciplined reasoning across physics, chemistry, and biology with carefully curated high-quality items.","Language Comprehension, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Decision-making"
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends broad academic knowledge testing into multiple languages, measuring whether models can understand and answer subject-matter questions beyond English. It stresses cross-lingual generalization, stable semantic representations, and consistent reasoning across languages and topics.","Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory"
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro is a professional-grade multimodal understanding benchmark with higher difficulty and stronger evaluation protocols than earlier MMMU settings. Models must combine text with images (charts, diagrams, documents, UI-like visuals) to answer domain-spanning questions requiring grounded visual reasoning.","Visual Perception, Scene Understanding & Visual Reasoning, Language Comprehension, Multisensory Integration, Logical Reasoning, Working Memory, Visual Attention & Eye Movements"
ScreenShot-Pro,https://gui-agent.github.io/grounding-leaderboard/,https://arxiv.org/abs/2504.07981,"ScreenShot-Pro evaluates GUI understanding from screenshots, often requiring the model to locate relevant interface elements and reason about actions or states in software environments. It targets visually grounded instruction following and precise interpretation of layout, icons, and text in real interfaces.","Visual Perception, Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Spatial Representation & Mapping, Language Comprehension, Decision-making, Sensorimotor Coordination"
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning evaluates scientific figure and paper understanding, asking models to answer questions that require interpreting charts/diagrams and linking them to accompanying scientific text. It stresses evidence-based reasoning over technical visuals, often benefiting from tool-assisted computation or careful cross-referencing.","Scene Understanding & Visual Reasoning, Visual Perception, Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Multisensory Integration"
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU extends multimodal reasoning to video, requiring models to integrate information across time (multiple frames/scenes) and accompanying text questions. It probes whether models can track events, recognize visual context shifts, and answer questions grounded in dynamic visual evidence.","Visual Perception, Scene Understanding & Visual Reasoning, Working Memory, Attention, Cognitive Timing & Predictive Modeling, Multisensory Integration, Language Comprehension"
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) is a long-context co-reference and retrieval evaluation in which multiple similar “needle” requests are embedded within a long “haystack” of dialogue or documents. The model must identify and reproduce the correct response associated with a specified needle, testing robustness to distractors at scale.","Working Memory, Attention, Episodic Memory, Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning"
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval measures performance on well-specified, economically relevant knowledge-work tasks across many occupations, scored via expert human comparisons (wins/ties). Tasks typically require producing real deliverables (e.g., slides, spreadsheets, plans) and following constraints with professional-level structure and judgment.","Planning, Decision-making, Language Production, Language Comprehension, Semantic Understanding & Context Recognition, Self-reflection, Adaptive Error Correction"
SWE-Lancer,https://openai.com/index/swe-lancer/,https://arxiv.org/abs/2502.12115,"SWE-Lancer evaluates agentic software engineering in a more “real work” style, emphasizing end-to-end execution on realistic tasks akin to freelance or workplace coding assignments. It typically requires interpreting requirements, modifying codebases, validating behavior, and delivering solutions that meet acceptance criteria.","Planning, Decision-making, Logical Reasoning, Working Memory, Adaptive Error Correction, Semantic Understanding & Context Recognition"
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks tests structured reasoning over graph-encoded data, such as traversals and relational queries that require tracking nodes, edges, and paths over long sequences. It is designed to measure systematic generalization on algorithmic graph operations rather than surface-pattern completion.","Logical Reasoning, Working Memory, Spatial Representation & Mapping, Attention, Planning"
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon evaluates multi-tool agent competence on heterogeneous tasks that require selecting the right tools, composing calls, and recovering from tool or intermediate-result failures. It emphasizes orchestration quality—planning, verification, and robust execution across a tool ecosystem.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Language Comprehension, Language Production"
HMMT,https://matharena.ai/?view=problem&comp=hmmt--hmmt_feb_2025,https://arxiv.org/abs/2505.23281,"HMMT evaluates advanced contest mathematics (Harvard-MIT Mathematics Tournament), typically requiring longer solution chains and higher ingenuity than standard school math. Problems probe abstraction, multi-constraint reasoning, and careful derivations under competition conditions.","Logical Reasoning, Working Memory, Planning, Cognitive Flexibility, Adaptive Error Correction"
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath targets expert-level mathematics with problems intended to be at or beyond the edge of routine training distribution, emphasizing deep reasoning over memorized templates. It often rewards sustained multi-step proof-like thinking, precise computation, and verification (sometimes with tool assistance).","Logical Reasoning, Working Memory, Planning, Cognitive Flexibility, Adaptive Error Correction"
