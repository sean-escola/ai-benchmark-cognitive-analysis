Benchmark,Website,Paper,Description,Cognitive Functions
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 evaluates agentic coding and system-operation ability in real terminal environments (e.g., using shell commands, editing files, installing dependencies, running tests, and debugging failures). Tasks emphasize end-to-end completion under realistic constraints such as tool errors, environment quirks, and iterative troubleshooting.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Language Comprehension"
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp measures deep-research performance on questions that require web browsing and evidence gathering rather than purely parametric recall. Systems must search, read multiple sources, synthesize information, and produce a final answer under time/context constraints.","Planning, Attention, Working Memory, Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning"
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld benchmarks computer-use agents operating graphical desktop applications to accomplish multi-step goals (e.g., navigating menus, filling forms, downloading/uploading files, and configuring settings). It stresses perception-to-action coupling in dynamic UIs, including recovering from mistakes and adapting strategies.","Visual Perception, Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Planning, Decision-making, Adaptive Error Correction, Sensorimotor Coordination, Working Memory"
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI tests fluid reasoning via few-shot grid-based pattern transformation puzzles, where systems infer latent rules from a small number of examples. It is designed to reduce benefits from memorization and instead probe abstraction, compositional generalization, and novel rule induction.","Logical Reasoning, Cognitive Flexibility, Spatial Representation & Mapping, Working Memory, Attention"
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon autonomous agent performance in a simulated vending-machine business over many decision steps. Agents must manage inventory, pricing, supplier interactions, and cash flow while adapting to changing conditions to maximize final profit.","Planning, Decision-making, Reward Mechanisms, Working Memory, Adaptive Error Correction, Language Production, Social Reasoning & Theory of Mind"
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity agents on tasks such as identifying known vulnerabilities from descriptions and discovering new issues in real open-source projects. Success requires understanding codebases, reproducing/locating weaknesses, and producing correct exploit or patch-oriented outputs under realistic constraints.","Logical Reasoning, Planning, Adaptive Error Correction, Working Memory, Language Comprehension, Semantic Understanding & Context Recognition"
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"SpreadsheetBench measures the ability to understand, manipulate, and construct spreadsheets to solve realistic analytical tasks (e.g., formulas, transformations, pivot-like operations, and structured outputs). It emphasizes precise execution, error checking, and maintaining consistency across many dependent cells and sheets.","Working Memory, Planning, Logical Reasoning, Adaptive Error Correction, Language Comprehension, Semantic Understanding & Context Recognition"
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a frontier-level benchmark spanning difficult questions across disciplines, often requiring multi-step reasoning and synthesis rather than straightforward recall. It may include multimodal items and is intended to stress the limits of academic knowledge, reasoning, and tool-augmented problem solving.","Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Scene Understanding & Visual Reasoning"
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,GPQA Diamond is a hard multiple-choice science QA benchmark curated so that non-experts typically fail while domain experts succeed. It targets deep scientific understanding and careful reasoning under adversarially difficult distractors.,"Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Language Comprehension"
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro evaluates expert-level multimodal understanding across many disciplines using images paired with questions that require reasoning (e.g., diagrams, plots, UI screenshots, and scientific figures). It stresses visual grounding, cross-modal integration, and selection among close answer choices.","Visual Perception, Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Working Memory, Language Comprehension"
MathArena Apex,https://matharena.ai/?view=problem&comp=apex--apex_2025,https://arxiv.org/abs/2505.23281,"MathArena Apex aggregates challenging competition-style math problems and evaluates solution accuracy under strict scoring. It probes multi-step derivations, robustness across topics, and the ability to avoid subtle algebraic or logical errors.","Logical Reasoning, Working Memory, Planning, Adaptive Error Correction, Cognitive Flexibility"
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning tests reasoning over scientific figures and related content (often from research-paper-style contexts), asking models to interpret visual evidence and derive correct answers. It emphasizes precise figure reading, quantitative/relational inference, and linking visual cues to textual questions.","Visual Perception, Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Working Memory, Language Comprehension"
OmniDocBench 1.5,https://github.com/opendatalab/OmniDocBench,https://arxiv.org/abs/2412.07626,"OmniDocBench 1.5 evaluates document understanding and OCR-style extraction across varied layouts (text blocks, formulas, tables, and reading order). It stresses faithful transcription/structure recovery and layout-aware interpretation rather than only recognizing isolated characters.","Visual Perception, Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Language Comprehension, Working Memory, Semantic Understanding & Context Recognition"
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU evaluates multimodal reasoning over videos using questions that require integrating information across frames (and often accompanying text). It stresses temporal integration, event understanding, and maintaining consistency when evidence is distributed over time.","Visual Perception, Scene Understanding & Visual Reasoning, Working Memory, Attention, Cognitive Timing & Predictive Modeling, Multisensory Integration"
LiveCodeBench Pro,https://livecodebenchpro.com/projects/livecodebench-pro/leaderboard,https://arxiv.org/abs/2506.11928,"LiveCodeBench Pro measures coding ability on fresh, competition-style programming tasks with rigorous execution-based scoring. It emphasizes algorithmic reasoning, producing correct runnable solutions, and debugging when outputs fail hidden tests.","Logical Reasoning, Planning, Adaptive Error Correction, Working Memory, Language Comprehension"
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"FACTS Benchmark Suite systematically evaluates factuality-related behaviors, including producing accurate statements, resisting hallucinations, and appropriately expressing uncertainty. It targets reliability under retrieval/no-retrieval settings and across diverse factuality stressors.","Semantic Understanding & Context Recognition, Language Comprehension, Language Production, Inhibitory Control, Self-reflection, Working Memory"
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA extends physical commonsense question answering across languages and locales, probing whether models can reason about everyday object interactions and practical actions beyond English-centric phrasing. It highlights robustness to multilingual variation and culturally diverse descriptions of physical situations.","Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning, Spatial Representation & Mapping, Working Memory"
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 is a long-context evaluation where multiple similar “needle” interactions are embedded in a large “haystack,” and the model must retrieve the correct referenced response. The 8-needle setting stresses sustained attention, interference resistance, and precise retrieval over very long inputs.","Working Memory, Attention, Episodic Memory, Language Comprehension, Inhibitory Control"
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval evaluates well-specified professional knowledge-work tasks spanning many occupations, with outputs judged against human industry professionals (often via pairwise comparisons). It emphasizes producing usable work artifacts (plans, analyses, documents) with correct constraints and strong execution quality.","Planning, Decision-making, Language Production, Language Comprehension, Semantic Understanding & Context Recognition, Social Reasoning & Theory of Mind, Working Memory"
SWE-Lancer,https://openai.com/index/swe-lancer/,https://arxiv.org/abs/2502.12115,"SWE-Lancer evaluates software-engineering capability on realistic repository-based tasks that require implementing changes, fixing bugs, and satisfying tests under constraints. It is designed to reflect real developer workflows, including reading existing code, planning edits, and iterating based on failures.","Planning, Logical Reasoning, Adaptive Error Correction, Working Memory, Language Comprehension"
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks tests long-context reasoning by embedding graph-structured data and asking models to perform traversal or relational queries (e.g., reachability and parent/neighbor reasoning). It stresses systematic stepwise inference and avoiding distraction from large volumes of irrelevant tokens.","Spatial Representation & Mapping, Logical Reasoning, Working Memory, Attention, Cognitive Flexibility"
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon evaluates tool-using agents on multi-step tasks requiring correct tool selection, parameterization, and chaining across heterogeneous APIs. It stresses robust execution under tool errors, maintaining state across steps, and producing correct final outputs from intermediate tool results.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Language Comprehension, Semantic Understanding & Context Recognition"
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath is a very challenging mathematics benchmark aimed at measuring progress on research-adjacent or expert-level problems, often requiring deep multi-step reasoning. It is intended to be difficult to solve by memorization and to better reflect genuine advances in mathematical problem solving.","Logical Reasoning, Working Memory, Planning, Cognitive Flexibility, Adaptive Error Correction"
