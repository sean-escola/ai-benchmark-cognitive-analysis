Benchmark,Website,Paper,Description,Cognitive Functions
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates an agent’s ability to fix real issues in open-source Python repositories by producing a patch that passes the project’s tests. The “Verified” subset uses problems validated by human engineers to be solvable and to have reliable evaluation, emphasizing end-to-end debugging rather than isolated coding puzzles.","Planning, Decision-making, Working Memory, Logical Reasoning, Semantic Understanding & Context Recognition, Adaptive Error Correction"
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro is a harder, larger software-engineering benchmark that expands beyond Python to multiple languages and aims to be more contamination-resistant and industrially representative. Models must understand a repository, localize the bug or missing feature, implement a correct fix, and satisfy rigorous test-based checks.","Planning, Decision-making, Working Memory, Logical Reasoning, Semantic Understanding & Context Recognition, Adaptive Error Correction, Inhibitory Control"
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp evaluates “deep research” style browsing where a model must locate and synthesize answers from a fixed document index, reducing variance from live web changes. It tests whether an agent can search effectively, read supporting documents, and provide grounded answers under tool-use constraints.","Planning, Decision-making, Attention, Working Memory, Episodic Memory, Language Comprehension, Semantic Understanding & Context Recognition"
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench measures how well an agent handles multi-turn customer-support interactions while using APIs/tools and following domain policies (e.g., retail, airline, telecom). It stresses policy compliance under conversational pressure, tool orchestration, and maintaining consistent state across turns.","Social Reasoning & Theory of Mind, Language Comprehension, Language Production, Planning, Decision-making, Working Memory, Inhibitory Control, Reward Mechanisms"
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI tests abstract, few-shot “fluid reasoning” on grid-based transformation tasks where the governing rule is novel at test time. Success requires inferring latent patterns from a handful of examples and generalizing to new inputs rather than relying on memorized templates.","Cognitive Flexibility, Logical Reasoning, Working Memory, Attention, Spatial Representation & Mapping, Scene Understanding & Visual Reasoning"
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas evaluates tool use via the Model Context Protocol (MCP), focusing on discovering appropriate tools, executing multi-step API workflows, handling errors, and synthesizing results. It targets realistic tool ecosystems where correct sequencing and robust recovery matter as much as single-call accuracy.","Planning, Decision-making, Working Memory, Semantic Understanding & Context Recognition, Adaptive Error Correction, Inhibitory Control"
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam (HLE) is a large, frontier-level benchmark spanning difficult expert questions, including multimodal items, intended to probe broad general intelligence at the edge of current models. It rewards deep reasoning, careful reading, and (in tool-enabled settings) robust evidence gathering and synthesis.","Language Comprehension, Language Production, Logical Reasoning, Working Memory, Semantic Understanding & Context Recognition, Scene Understanding & Visual Reasoning, Planning, Decision-making"
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,AIME 2025 evaluates competition-style mathematics requiring multi-step derivations and precise final numeric answers. It probes structured reasoning and error-free symbolic manipulation under time-like constraints (single-shot evaluation settings are common).,"Logical Reasoning, Working Memory, Planning, Adaptive Error Correction, Attention"
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a high-difficulty multiple-choice science QA benchmark designed to be “Google-proof,” with the Diamond subset curated for quality and difficulty. It emphasizes reasoning with specialized scientific knowledge rather than surface pattern matching.","Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Attention, Decision-making"
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends MMLU to many languages, testing broad academic knowledge and reasoning across subjects and multilingual settings. It evaluates whether models preserve competence and calibration when prompts and answer choices appear in diverse languages.","Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Attention"
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro is a professional-grade multimodal understanding benchmark with harder expert-level image+text problems across disciplines. It tests whether a model can interpret diagrams, charts, and visuals and integrate them with textual context to answer questions accurately.","Visual Perception, Scene Understanding & Visual Reasoning, Multisensory Integration, Language Comprehension, Logical Reasoning, Working Memory, Spatial Representation & Mapping"
ScreenShot-Pro,https://gui-agent.github.io/grounding-leaderboard/,https://arxiv.org/abs/2504.07981,"ScreenShot-Pro evaluates understanding of high-resolution software screenshots and GUI states, often requiring identification of UI elements, layout reasoning, and interface-specific interpretation. It targets practical “screen grounding” needed for computer-use agents (e.g., reading dashboards, forms, and settings).","Visual Perception, Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Spatial Representation & Mapping, Language Comprehension, Decision-making"
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning focuses on reasoning over figures and technical content from scientific papers, typically requiring extracting quantitative/structural information and drawing conclusions. It stresses robust figure comprehension, scientific context integration, and multi-step inference (often aided by code tools in some setups).","Visual Perception, Scene Understanding & Visual Reasoning, Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory"
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU evaluates multimodal understanding and reasoning over video, requiring models to integrate information across frames and time to answer questions. It probes temporal comprehension, event tracking, and combining visual content with text instructions or queries.","Visual Perception, Cognitive Timing & Predictive Modeling, Working Memory, Attention, Multisensory Integration, Scene Understanding & Visual Reasoning, Language Comprehension"
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 is a long-context benchmark for multi-round coreference resolution where multiple similar “needle” interactions are embedded in a long “haystack,” and the model must retrieve the correct referenced response. The 8-needle variant stresses sustained retrieval accuracy amid confusable distractors across long contexts.","Working Memory, Episodic Memory, Attention, Language Comprehension, Semantic Understanding & Context Recognition, Inhibitory Control"
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval measures performance on well-specified professional knowledge-work tasks across many occupations (e.g., producing spreadsheets, plans, analyses, and other deliverables), typically judged by experts. It emphasizes executing complex instructions, producing usable artifacts, and making practical tradeoffs under constraints.","Planning, Decision-making, Language Comprehension, Language Production, Semantic Understanding & Context Recognition, Self-reflection, Working Memory"
SWE-Lancer,https://openai.com/index/swe-lancer/,https://arxiv.org/abs/2502.12115,"SWE-Lancer evaluates agentic software engineering in a “freelance ticket” style setting, where tasks resemble real development requests and are assessed for end-to-end correctness and usefulness. It stresses scoping, implementation quality, and iterative debugging in a more product-like workflow than isolated bugfixes.","Planning, Decision-making, Language Comprehension, Language Production, Semantic Understanding & Context Recognition, Adaptive Error Correction, Working Memory"
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks tests long-context algorithmic reasoning over graph-structured data, such as following edges or parent pointers to answer queries about nodes and paths. It emphasizes maintaining and updating an internal representation of structure while performing stepwise traversal accurately.","Spatial Representation & Mapping, Working Memory, Logical Reasoning, Attention, Cognitive Timing & Predictive Modeling"
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon evaluates tool-using agents on tasks that require selecting among tools, executing correct call sequences, and synthesizing outputs into a final answer. It emphasizes robust orchestration (including handling tool failures) and disciplined adherence to task constraints during multi-step tool chains.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition, Inhibitory Control"
HMMT,https://matharena.ai/?view=problem&comp=hmmt--hmmt_feb_2025,https://arxiv.org/abs/2505.23281,"HMMT (Harvard-MIT Mathematics Tournament) benchmark sets use challenging contest math problems that often require creative insight and multi-lemma reasoning beyond routine algebra. Compared to easier competitions, solutions more frequently demand novel decomposition strategies and careful verification.","Logical Reasoning, Working Memory, Planning, Adaptive Error Correction, Attention, Cognitive Flexibility"
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath targets very hard mathematics problems intended to be at or beyond the frontier of typical model capability, often benefiting from deep multi-step reasoning and tool-assisted computation. It aims to measure progress on advanced mathematical problem solving rather than short-form arithmetic or memorized techniques.","Logical Reasoning, Working Memory, Planning, Adaptive Error Correction, Cognitive Flexibility, Attention"
