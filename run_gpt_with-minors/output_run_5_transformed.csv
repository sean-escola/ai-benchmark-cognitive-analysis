Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates software engineering agents on real GitHub issues where the model must generate a code patch that makes the repository’s tests pass. The “Verified” subset uses tasks that have been human-validated as solvable and focuses on reliable, end-to-end bug fixing under a standardized harness.","L1: 
L2: Planning, Logical Reasoning, Working Memory, Adaptive Error Correction, Semantic Understanding & Context Recognition, Decision-making
L3: ",L2
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro is a larger, harder software engineering benchmark intended to be more realistic and contamination-resistant, spanning multiple programming languages and more complex repositories. Agents must understand issue descriptions, navigate codebases, implement fixes, and satisfy test suites under stricter evaluation conditions.","L1: 
L2: Planning, Logical Reasoning, Working Memory, Adaptive Error Correction, Semantic Understanding & Context Recognition, Decision-making
L3: Cognitive Flexibility",L3
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp measures a model’s ability to do deep, grounded web research by searching and synthesizing information from an online corpus (or a controlled web index in some variants). Success requires decomposing a query, iteratively retrieving evidence, and producing a supported final answer rather than a purely parametric response.","L1: Language Comprehension, Language Production
L2: Planning, Attention, Episodic Memory, Semantic Understanding & Context Recognition, Decision-making
L3: ",L2
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench evaluates interactive agent behavior in simulated customer-support domains (e.g., retail, airline, telecom) where the model must use tools/APIs and follow domain policies over multi-turn dialogues. It emphasizes consistent policy adherence, correct tool use, and user-centered problem resolution in realistic workflows.","L1: Language Comprehension, Language Production
L2: Decision-making, Planning, Adaptive Error Correction, Working Memory
L3: Social Reasoning & Theory of Mind, Inhibitory Control",L3
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI tests “fluid” abstract reasoning using small grid-based puzzles: given a few input–output examples, the model must infer the hidden transformation rule and apply it to a new input. The benchmark is designed to reward generalization to novel patterns rather than memorization of domain knowledge.","L1: 
L2: Logical Reasoning, Working Memory, Spatial Representation & Mapping, Planning, Attention
L3: Cognitive Flexibility",L3
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas evaluates real-world tool use via the Model Context Protocol (MCP), requiring models to discover tools, call them correctly, handle errors, and compose multi-step workflows across services. Tasks resemble production agent usage, emphasizing robustness, correct API interaction, and coherent final synthesis.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition
L3: ",L2
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a frontier, multimodal benchmark spanning difficult questions across many domains, intended to probe broad, expert-level knowledge and reasoning. It often rewards careful synthesis, multi-step problem solving, and (in tool-enabled settings) strategic use of search or code to verify claims.","L1: Language Comprehension, Language Production
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Multisensory Integration, Planning
L3: ",L2
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 evaluates competition-level mathematical problem solving on the 2025 American Invitational Mathematics Examination set. Problems require multi-step symbolic reasoning, constraint handling, and exact answer derivation under limited context.","L1: 
L2: Logical Reasoning, Working Memory, Planning, Adaptive Error Correction
L3: ",L2
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a subset of graduate-level, “Google-proof” multiple-choice science questions designed to resist shallow retrieval and reward genuine understanding. It probes disciplined reasoning across physics, chemistry, and biology with carefully curated high-quality items.","L1: Language Comprehension
L2: Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Decision-making
L3: ",L2
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends broad academic knowledge testing into multiple languages, measuring whether models can understand and answer subject-matter questions beyond English. It stresses cross-lingual generalization, stable semantic representations, and consistent reasoning across languages and topics.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory
L3: ",L2
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro is a professional-grade multimodal understanding benchmark with higher difficulty and stronger evaluation protocols than earlier MMMU settings. Models must combine text with images (charts, diagrams, documents, UI-like visuals) to answer domain-spanning questions requiring grounded visual reasoning.","L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Working Memory, Visual Attention & Eye Movements
L3: ",L2
ScreenShot-Pro,https://gui-agent.github.io/grounding-leaderboard/,https://arxiv.org/abs/2504.07981,"ScreenShot-Pro evaluates GUI understanding from screenshots, often requiring the model to locate relevant interface elements and reason about actions or states in software environments. It targets visually grounded instruction following and precise interpretation of layout, icons, and text in real interfaces.","L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Spatial Representation & Mapping, Decision-making, Sensorimotor Coordination
L3: ",L2
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning evaluates scientific figure and paper understanding, asking models to answer questions that require interpreting charts/diagrams and linking them to accompanying scientific text. It stresses evidence-based reasoning over technical visuals, often benefiting from tool-assisted computation or careful cross-referencing.","L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Multisensory Integration
L3: ",L2
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU extends multimodal reasoning to video, requiring models to integrate information across time (multiple frames/scenes) and accompanying text questions. It probes whether models can track events, recognize visual context shifts, and answer questions grounded in dynamic visual evidence.","L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Working Memory, Attention, Multisensory Integration
L3: Cognitive Timing & Predictive Modeling",L3
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) is a long-context co-reference and retrieval evaluation in which multiple similar “needle” requests are embedded within a long “haystack” of dialogue or documents. The model must identify and reproduce the correct response associated with a specified needle, testing robustness to distractors at scale.","L1: Language Comprehension
L2: Working Memory, Attention, Episodic Memory, Semantic Understanding & Context Recognition, Logical Reasoning
L3: ",L2
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval measures performance on well-specified, economically relevant knowledge-work tasks across many occupations, scored via expert human comparisons (wins/ties). Tasks typically require producing real deliverables (e.g., slides, spreadsheets, plans) and following constraints with professional-level structure and judgment.","L1: Language Production, Language Comprehension
L2: Planning, Decision-making, Semantic Understanding & Context Recognition, Adaptive Error Correction
L3: Self-reflection",L3
SWE-Lancer,https://openai.com/index/swe-lancer/,https://arxiv.org/abs/2502.12115,"SWE-Lancer evaluates agentic software engineering in a more “real work” style, emphasizing end-to-end execution on realistic tasks akin to freelance or workplace coding assignments. It typically requires interpreting requirements, modifying codebases, validating behavior, and delivering solutions that meet acceptance criteria.","L1: 
L2: Planning, Decision-making, Logical Reasoning, Working Memory, Adaptive Error Correction, Semantic Understanding & Context Recognition
L3: ",L2
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks tests structured reasoning over graph-encoded data, such as traversals and relational queries that require tracking nodes, edges, and paths over long sequences. It is designed to measure systematic generalization on algorithmic graph operations rather than surface-pattern completion.","L1: 
L2: Logical Reasoning, Working Memory, Spatial Representation & Mapping, Attention, Planning
L3: ",L2
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon evaluates multi-tool agent competence on heterogeneous tasks that require selecting the right tools, composing calls, and recovering from tool or intermediate-result failures. It emphasizes orchestration quality—planning, verification, and robust execution across a tool ecosystem.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory
L3: ",L2
HMMT,https://matharena.ai/?view=problem&comp=hmmt--hmmt_feb_2025,https://arxiv.org/abs/2505.23281,"HMMT evaluates advanced contest mathematics (Harvard-MIT Mathematics Tournament), typically requiring longer solution chains and higher ingenuity than standard school math. Problems probe abstraction, multi-constraint reasoning, and careful derivations under competition conditions.","L1: 
L2: Logical Reasoning, Working Memory, Planning, Adaptive Error Correction
L3: Cognitive Flexibility",L3
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath targets expert-level mathematics with problems intended to be at or beyond the edge of routine training distribution, emphasizing deep reasoning over memorized templates. It often rewards sustained multi-step proof-like thinking, precise computation, and verification (sometimes with tool assistance).","L1: 
L2: Logical Reasoning, Working Memory, Planning, Adaptive Error Correction
L3: Cognitive Flexibility",L3
