Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates whether a model can fix real GitHub issues by producing a patch that makes a repository’s tests pass. The “Verified” subset filters to problems that have been human-validated as solvable and well-specified, emphasizing correctness under realistic repo structure and dependencies.","L1: 
L2: Planning, Logical Reasoning, Working Memory, Adaptive Error Correction, Semantic Understanding & Context Recognition, Decision-making
L3: ",L2
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro is a larger, harder software-engineering benchmark designed to be more industrially representative and more resistant to contamination than earlier SWE-bench variants. It tests end-to-end debugging and feature implementation in realistic codebases, often requiring longer-horizon reasoning across multiple files and languages.","L1: 
L2: Planning, Logical Reasoning, Working Memory, Adaptive Error Correction, Semantic Understanding & Context Recognition, Decision-making
L3: Cognitive Flexibility",L3
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp evaluates “deep research” style question answering where the model must search a document collection (or the web, depending on the setting), gather evidence, and synthesize a final answer. Performance depends on iterative query formulation, evidence selection, and maintaining consistency while navigating noisy or incomplete sources.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Attention, Working Memory, Semantic Understanding & Context Recognition
L3: ",L2
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench measures interactive agent performance in multi-turn customer-support environments (e.g., retail, airline, telecom) with tools/APIs and policy constraints. The agent must follow domain rules while helping a simulated user, handling edge cases, and keeping conversation state coherent across turns.","L1: Language Comprehension, Language Production
L2: Decision-making, Planning, Working Memory, Adaptive Error Correction
L3: Social Reasoning & Theory of Mind, Inhibitory Control",L3
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI tests few-shot, novel pattern induction on grid-based tasks where the system infers a transformation rule from a handful of examples. It is intended to probe fluid reasoning and generalization to unseen “concepts,” rather than recall of domain knowledge.","L1: Visual Perception
L2: Logical Reasoning, Working Memory, Spatial Representation & Mapping, Scene Understanding & Visual Reasoning, Attention
L3: Cognitive Flexibility",L3
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas evaluates real-world tool use via the Model Context Protocol by requiring models to discover tools, call them correctly, and compose multi-step workflows across services. It emphasizes API understanding, error handling, and robust execution in production-like tool ecosystems.","L1: Language Comprehension
L2: Planning, Decision-making, Working Memory, Adaptive Error Correction, Semantic Understanding & Context Recognition
L3: ",L2
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a large, frontier-difficulty benchmark spanning expert-level questions across many domains, often requiring multi-step reasoning and careful synthesis. It is frequently used to assess broad academic/technical competence and, in tool-enabled variants, research-and-verify behavior.","L1: Language Comprehension, Visual Perception
L2: Logical Reasoning, Working Memory, Semantic Understanding & Context Recognition, Planning, Decision-making
L3: ",L2
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,AIME 2025 consists of competition-style mathematics problems that require precise symbolic reasoning and careful multi-step derivations. It is commonly used to evaluate mathematical problem solving both with and without computational tools.,"L1: 
L2: Logical Reasoning, Working Memory, Planning, Adaptive Error Correction
L3: ",L2
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,GPQA Diamond is a subset of challenging graduate-level multiple-choice science questions curated to be difficult for non-experts and resistant to superficial pattern matching. It probes deep scientific reasoning and the ability to disambiguate close distractors under time-limited formats.,"L1: 
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Decision-making
L3: ",L2
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends broad academic knowledge testing to multiple languages, measuring whether models can answer subject-area questions beyond English. It emphasizes cross-lingual generalization of factual knowledge and reasoning across many disciplines.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory
L3: ",L2
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro evaluates expert-level multimodal understanding across many subjects where questions depend on both images and text (e.g., diagrams, charts, documents). It tests whether models can integrate visual evidence with domain knowledge and reasoning to select correct answers.","L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Attention, Working Memory
L3: ",L2
ScreenShot-Pro,https://gui-agent.github.io/grounding-leaderboard/,https://arxiv.org/abs/2504.07981,"ScreenShot-Pro measures UI/screenshot understanding for agentic interaction with software interfaces, often requiring identifying on-screen elements and reasoning about layout. It is used to assess whether models can ground instructions in high-resolution visual interfaces similar to real computer-use tasks.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Spatial Representation & Mapping, Decision-making, Attention
L3: ",L2
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning focuses on interpreting and reasoning over scientific figures (plots, tables, and composite visuals) from research papers. The benchmark targets extraction of quantitative/structural information from visuals and combining it with textual context to answer reasoning questions.","L1: Visual Perception, Language Comprehension
L2: Scene Understanding & Visual Reasoning, Logical Reasoning, Working Memory, Semantic Understanding & Context Recognition, Attention
L3: ",L2
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU extends multimodal understanding to videos, requiring temporal integration of visual events with text questions and options. It probes whether models can track entities and actions across frames and use that information for higher-level reasoning.","L1: Visual Perception, Language Comprehension
L2: Multisensory Integration, Working Memory, Attention, Scene Understanding & Visual Reasoning
L3: Cognitive Timing & Predictive Modeling",L3
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,MRCR v2 (8-needle) tests long-context multi-round coreference and retrieval by placing repeated “needle” requests inside long “haystack” conversations and asking for the correct referenced response. It stresses maintaining and accessing the right contextual binding across many similar spans.,"L1: 
L2: Working Memory, Attention, Semantic Understanding & Context Recognition, Logical Reasoning
L3: Cognitive Flexibility",L3
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval evaluates well-specified professional “knowledge work” tasks across many occupations, judged against human expert outputs (often via head-to-head comparisons). It emphasizes producing usable artifacts (e.g., plans, analyses, spreadsheets/presentations in tool-enabled settings) that meet constraints and stakeholder intent.","L1: Language Production, Language Comprehension
L2: Planning, Decision-making, Semantic Understanding & Context Recognition
L3: Self-reflection, Social Reasoning & Theory of Mind",L3
SWE-Lancer,https://openai.com/index/swe-lancer/,https://arxiv.org/abs/2502.12115,"SWE-Lancer evaluates software engineering performance on real-world tasks with an emphasis on longer-horizon, agentic coding behavior and reliability under realistic constraints. It is used to measure end-to-end capability from understanding the problem and repository to delivering correct, test-passing changes.","L1: 
L2: Planning, Logical Reasoning, Working Memory, Adaptive Error Correction, Decision-making, Semantic Understanding & Context Recognition
L3: ",L2
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks evaluates structured reasoning over graph-structured data, such as following paths, identifying reachable nodes, or tracking parent/ancestor relations under long contexts. It targets algorithmic, stepwise traversal behavior rather than open-ended language generation alone.","L1: 
L2: Spatial Representation & Mapping, Working Memory, Logical Reasoning, Attention, Planning
L3: ",L2
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon benchmarks multi-tool problem solving where models must choose among tools, sequence calls, and integrate outputs to complete tasks. It emphasizes robust orchestration, tool selection under uncertainty, and recovery from tool errors or partial results.","L1: 
L2: Planning, Decision-making, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition, Attention
L3: ",L2
HMMT,https://matharena.ai/?view=problem&comp=hmmt--hmmt_feb_2025,https://arxiv.org/abs/2505.23281,"HMMT comprises high-difficulty competition math problems that often require creative insights, careful case analysis, and long multi-step proofs or computations. It is used to test mathematical depth beyond standard contest sets, especially under strict correctness requirements.","L1: 
L2: Logical Reasoning, Working Memory, Planning, Adaptive Error Correction
L3: Cognitive Flexibility",L3
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath is designed to measure expert-level mathematics performance across tiers of difficulty, including problems intended to be challenging even for strong solvers. It evaluates sustained multi-step reasoning, precise formal manipulation, and, in tool-enabled modes, effective use of computation to support proofs and calculations.","L1: 
L2: Logical Reasoning, Working Memory, Planning, Adaptive Error Correction
L3: Cognitive Flexibility",L3
