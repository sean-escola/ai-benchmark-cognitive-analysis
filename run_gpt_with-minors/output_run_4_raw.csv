Benchmark,Website,Paper,Description,Cognitive Functions
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 evaluates agentic coding by having models complete real command-line tasks (e.g., inspecting files, installing dependencies, running tests, and fixing issues) inside a sandboxed terminal. Success depends on iteratively choosing correct shell actions, interpreting outputs, and recovering from errors under realistic constraints.","Planning, Decision-making, Logical Reasoning, Working Memory, Adaptive Error Correction, Attention"
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp measures deep-research performance where a model must answer difficult questions by searching and synthesizing evidence from a controlled web-style corpus/index. It stresses query formulation, evidence selection, and long-horizon reasoning across multiple retrieved sources rather than single-pass recall.","Planning, Decision-making, Semantic Understanding & Context Recognition, Working Memory, Attention, Language Comprehension"
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld evaluates multimodal computer-use agents that must operate a desktop-like environment to complete tasks across applications (navigation, typing, clicking, file operations, settings, etc.). It tests perception of UI state from screenshots, step-by-step planning, and robust execution with recovery from mistakes.","Visual Perception, Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Planning, Decision-making, Adaptive Error Correction, Working Memory"
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI is a few-shot abstract reasoning benchmark based on colored grid transformation puzzles where the rule must be inferred from a handful of demonstrations. It targets generalization to novel patterns, compositional rule discovery, and flexible problem solving with minimal prior task-specific training.","Logical Reasoning, Cognitive Flexibility, Working Memory, Spatial Representation & Mapping, Scene Understanding & Visual Reasoning, Attention"
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon autonomous agency by simulating a year of running a vending-machine business, requiring thousands of decisions (pricing, inventory, supplier negotiation, etc.). It emphasizes sustained coherence, goal maintenance, and adapting strategy as conditions change over time.","Planning, Decision-making, Working Memory, Reward Mechanisms, Adaptive Error Correction, Cognitive Timing & Predictive Modeling"
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity agents on tasks such as reproducing known vulnerabilities from high-level descriptions and discovering new vulnerabilities in real codebases. It stresses systematic investigation, hypothesis-driven testing, and iterative debugging in an adversarial, correctness-critical domain.","Logical Reasoning, Planning, Adaptive Error Correction, Working Memory, Attention, Semantic Understanding & Context Recognition"
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"SpreadsheetBench measures an agent’s ability to understand, edit, and compute with complex spreadsheets, often requiring multi-step transformations and formula reasoning. It evaluates whether models can maintain task context while manipulating structured data and verifying outputs programmatically.","Logical Reasoning, Planning, Working Memory, Adaptive Error Correction, Attention, Semantic Understanding & Context Recognition"
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a frontier, multimodal benchmark spanning advanced academic and professional knowledge questions designed to be difficult to solve by memorization alone. It stresses integrating domain knowledge with multi-step reasoning, and (when tools are allowed) verifying claims via external evidence.","Language Comprehension, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Visual Perception, Planning"
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a high-quality subset of graduate-level, Google-resistant multiple-choice science questions where superficial pattern-matching tends to fail. It emphasizes careful reading, scientific reasoning across physics/chemistry/biology, and selecting among close distractors.","Language Comprehension, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Attention"
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro evaluates expert-level multimodal understanding across many disciplines using images (figures, diagrams, charts) paired with text questions and structured answer choices. It stresses aligning visual evidence with textual cues and performing multi-step reasoning grounded in the image content.","Visual Perception, Scene Understanding & Visual Reasoning, Multisensory Integration, Language Comprehension, Logical Reasoning, Working Memory, Attention"
MathArena Apex,https://matharena.ai/?view=problem&comp=apex--apex_2025,https://arxiv.org/abs/2505.23281,"MathArena Apex is a competitive mathematics benchmark emphasizing difficult problem solving under standardized evaluation, often reflecting contest-style reasoning. It stresses constructing multi-step solution plans, maintaining intermediate results, and avoiding subtle algebraic or logical errors.","Logical Reasoning, Planning, Working Memory, Adaptive Error Correction, Attention"
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,CharXiv Reasoning evaluates whether models can answer questions that require interpreting scientific figures and chart-like visual artifacts commonly found in research papers. It stresses extracting quantitative/structural information from visuals and integrating it with domain text to justify conclusions.,"Visual Perception, Scene Understanding & Visual Reasoning, Multisensory Integration, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Attention"
OmniDocBench 1.5,https://github.com/opendatalab/OmniDocBench,https://arxiv.org/abs/2412.07626,"OmniDocBench 1.5 evaluates document understanding and OCR-style extraction across heterogeneous layouts including text, tables, formulas, and reading order. It measures how well models perceive structured documents and produce faithful, correctly ordered reconstructions.","Visual Perception, Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Semantic Understanding & Context Recognition, Working Memory, Attention"
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU evaluates multimodal reasoning over video by requiring models to answer questions that depend on events, temporal changes, and contextual cues across frames. It stresses integrating observations across time and performing grounded reasoning about actions and causality.","Visual Perception, Scene Understanding & Visual Reasoning, Working Memory, Attention, Cognitive Timing & Predictive Modeling, Multisensory Integration"
LiveCodeBench Pro,https://livecodebenchpro.com/projects/livecodebench-pro/leaderboard,https://arxiv.org/abs/2506.11928,"LiveCodeBench Pro measures coding ability on continuously updated programming tasks with standardized execution-based grading, aiming to reduce contamination. It stresses generating correct code, debugging failed runs, and iterating toward a passing solution under time/attempt constraints.","Language Production, Logical Reasoning, Planning, Working Memory, Adaptive Error Correction, Attention"
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"FACTS Benchmark Suite evaluates factuality and grounding by testing whether model outputs remain consistent with sources and avoid unsupported claims across varied settings. It targets reliability behaviors such as uncertainty expression, claim verification, and resisting hallucination when evidence is missing or conflicting.","Semantic Understanding & Context Recognition, Language Comprehension, Inhibitory Control, Self-reflection, Working Memory, Attention"
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA evaluates physical commonsense reasoning and everyday practicality across a globally diverse, multilingual setting. It stresses selecting plausible actions/outcomes in real-world scenarios where correct answers depend on intuitive physics and commonsense constraints.","Language Comprehension, Logical Reasoning, Semantic Understanding & Context Recognition, Cognitive Flexibility, Attention"
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) tests long-context multi-round coreference by inserting repeated, similar “needle” requests into lengthy conversational “haystacks” and asking the model to retrieve the correct associated response. It stresses maintaining and accurately retrieving specific entities/relations under heavy interference.","Working Memory, Attention, Episodic Memory, Semantic Understanding & Context Recognition"
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval measures performance on well-specified professional knowledge work across many occupations using side-by-side human judging of produced artifacts (e.g., spreadsheets, slides, plans). It stresses following constraints, producing coherent deliverables, and making practical decisions that match real workplace objectives.","Planning, Decision-making, Language Production, Semantic Understanding & Context Recognition, Working Memory, Social Reasoning & Theory of Mind"
SWE-Lancer,https://openai.com/index/swe-lancer/,https://arxiv.org/abs/2502.12115,"SWE-Lancer evaluates agentic software engineering on realistic repository-level tasks, emphasizing end-to-end patch creation and correctness under automated tests. It stresses decomposing ambiguous bug reports, navigating large codebases, and iteratively improving solutions based on tool feedback.","Planning, Decision-making, Logical Reasoning, Working Memory, Adaptive Error Correction, Attention"
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks evaluates long-context reasoning by encoding graph structures and asking models to perform operations like traversal, reachability, or parent/neighbor retrieval over large “in-context” graphs. It stresses precise multi-step symbol manipulation and resisting distraction from irrelevant nodes or edges.","Working Memory, Attention, Logical Reasoning, Spatial Representation & Mapping, Cognitive Timing & Predictive Modeling"
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon evaluates general tool-use competence across diverse APIs and multi-step workflows, focusing on choosing the right tools, calling them correctly, and integrating results. It stresses robust execution in the perceive–plan–act loop, including recovery from tool errors and ambiguous intermediate outputs.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Attention, Language Comprehension"
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath evaluates advanced mathematics at or beyond typical competition/research-adjacent difficulty, aiming to probe the frontier of formal quantitative reasoning. It stresses constructing long solution chains, maintaining invariants across steps, and verifying results (often with optional tool support).","Logical Reasoning, Planning, Working Memory, Adaptive Error Correction, Attention"
