Benchmark,Website,Paper,Description,Cognitive Functions
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates autonomous software engineering by asking a model to produce a patch that fixes real issues in open-source Python repositories given an issue description and tests. The “Verified” subset filters to tasks that are confirmed solvable and reliably graded, emphasizing end-to-end debugging rather than code-writing in isolation.","Planning, Logical Reasoning, Working Memory, Adaptive Error Correction, Semantic Understanding & Context Recognition"
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro is a larger and harder successor-style benchmark focused on real-world software engineering tasks across multiple programming languages and more contamination-resistant settings. It stresses robust repository understanding, multi-file edits, and producing changes that satisfy hidden tests under realistic constraints.","Planning, Logical Reasoning, Working Memory, Adaptive Error Correction, Cognitive Flexibility, Decision-making"
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp evaluates “deep research” behavior by requiring models to answer questions using information found by browsing a fixed web-like corpus or indexed document collection. It emphasizes iterative search, evidence gathering, and synthesis of scattered sources into a final, grounded answer.","Planning, Attention, Working Memory, Semantic Understanding & Context Recognition, Decision-making, Language Comprehension, Language Production"
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench measures agentic tool use in multi-turn, policy-constrained customer support scenarios (e.g., retail, airline, telecom) with simulated users and backend APIs. Success depends on maintaining dialogue state, following policies, and correctly calling tools to resolve cases end-to-end.","Social Reasoning & Theory of Mind, Language Comprehension, Language Production, Working Memory, Planning, Decision-making, Inhibitory Control"
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,ARC-AGI is a few-shot abstract reasoning benchmark where models infer hidden rules from a small number of grid-based input–output examples and apply them to new inputs. It targets fluid reasoning and generalization to novel tasks rather than recall of domain knowledge.,"Logical Reasoning, Cognitive Flexibility, Working Memory, Spatial Representation & Mapping, Visual Perception, Scene Understanding & Visual Reasoning"
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas evaluates real-world tool use via the Model Context Protocol, requiring models to discover tools, invoke them correctly, handle errors, and compose multi-step workflows across services. The focus is on reliable orchestration over tool catalogs and realistic API interactions.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Language Comprehension, Semantic Understanding & Context Recognition"
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam (HLE) is a large, frontier-level benchmark spanning advanced academic and professional questions, often multimodal, intended to probe capability near the limits of current models. It emphasizes difficult reasoning and broad knowledge integration under strict grading.","Language Comprehension, Language Production, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Scene Understanding & Visual Reasoning, Visual Perception"
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,AIME 2025 consists of competition-style mathematics problems designed to test multi-step quantitative reasoning under time-like constraints and minimal scaffolding. Performance depends on translating word problems into formal structure and executing correct symbolic manipulations.,"Logical Reasoning, Working Memory, Planning, Adaptive Error Correction"
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a high-difficulty, multiple-choice science question benchmark curated to be “Google-proof” and discriminative for expert reasoning. The Diamond subset emphasizes question quality and requires combining scientific knowledge with careful elimination and inference.","Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Decision-making, Language Comprehension"
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends the MMLU format to multiple languages, testing broad academic knowledge and reasoning across many subjects under multilingual prompts. It probes whether models can maintain consistent competence and calibration across languages, not just English.","Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory"
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro is a challenging multimodal benchmark where models answer expert-level questions grounded in images (e.g., charts, diagrams, screenshots) plus text. It emphasizes robust visual understanding and cross-modal reasoning with reduced shortcut opportunities compared to simpler VQA.","Visual Perception, Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Working Memory, Visual Attention & Eye Movements"
ScreenShot-Pro,https://gui-agent.github.io/grounding-leaderboard/,https://arxiv.org/abs/2504.07981,"ScreenShot-Pro evaluates UI understanding and visual grounding on high-resolution screenshots, typically requiring identifying interface elements, their relationships, and intended actions. It stresses precise spatial localization and interpreting affordances in professional software interfaces.","Visual Perception, Visual Attention & Eye Movements, Spatial Representation & Mapping, Scene Understanding & Visual Reasoning, Planning, Sensorimotor Coordination"
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning tests reasoning over scientific figures from papers, asking models to interpret plots, diagrams, and figure captions to answer questions. It emphasizes extracting quantitative/structural information from visuals and linking it with scientific context.","Visual Perception, Scene Understanding & Visual Reasoning, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Multisensory Integration"
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU extends multimodal understanding to video, requiring models to answer questions that depend on temporal events, actions, and evolving context. It probes integration of information across frames and maintaining task-relevant details over time.","Visual Perception, Attention, Working Memory, Cognitive Timing & Predictive Modeling, Scene Understanding & Visual Reasoning, Multisensory Integration"
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) evaluates long-context retrieval and multi-round coreference resolution by embedding multiple similar “needle” interactions in a long “haystack” of dialogue-like content. The model must identify and reproduce the correct referenced response, stressing robustness to interference and distractors.","Working Memory, Attention, Language Comprehension, Semantic Understanding & Context Recognition, Episodic Memory, Inhibitory Control"
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval measures performance on well-specified, economically relevant knowledge-work tasks across many occupations, judged by expert humans via head-to-head comparisons against professional outputs. Tasks often require producing structured artifacts (e.g., slides, spreadsheets, plans) and making tradeoffs under constraints.","Planning, Decision-making, Language Production, Semantic Understanding & Context Recognition, Working Memory, Self-reflection"
SWE-Lancer,https://openai.com/index/swe-lancer/,https://arxiv.org/abs/2502.12115,"SWE-Lancer evaluates agentic software engineering on freelancing-style tasks that resemble contracted work, often involving ambiguous requirements and realistic repository contexts. It emphasizes selecting an approach, implementing changes, and delivering outcomes aligned with user intent and acceptance criteria.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Cognitive Flexibility, Semantic Understanding & Context Recognition"
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks tests systematic reasoning over graph-structured data by requiring models to follow traversal rules (e.g., BFS-style walks) and answer queries that depend on correct path/parent relationships. It stresses faithful multi-step state tracking under combinatorial branching.","Spatial Representation & Mapping, Working Memory, Logical Reasoning, Attention, Cognitive Timing & Predictive Modeling"
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon evaluates how well models can use diverse tools/APIs to complete multi-step tasks, often requiring correct tool selection, argument construction, and iterative repair after failures. It targets practical agent reliability rather than purely linguistic performance.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Inhibitory Control, Semantic Understanding & Context Recognition"
HMMT,https://matharena.ai/?view=problem&comp=hmmt--hmmt_feb_2025,https://arxiv.org/abs/2505.23281,HMMT consists of high-difficulty competition mathematics problems that require creative multi-step derivations and careful handling of edge cases. It probes depth of mathematical reasoning beyond routine curriculum exercises.,"Logical Reasoning, Working Memory, Planning, Adaptive Error Correction, Cognitive Flexibility"
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath is an expert-level mathematics benchmark designed to be hard for modern models, emphasizing novel problem solving, proof-like reasoning, and long chains of inference. It aims to reduce contamination and reward genuine mathematical generalization under strict evaluation.","Logical Reasoning, Planning, Working Memory, Cognitive Flexibility, Adaptive Error Correction"
