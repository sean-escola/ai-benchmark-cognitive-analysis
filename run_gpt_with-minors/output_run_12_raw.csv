Benchmark,Website,Paper,Description,Cognitive Functions
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 evaluates agentic coding in real command-line environments, where models must execute multi-step workflows (e.g., installing dependencies, editing files, running tests, and debugging) to complete tasks. It emphasizes end-to-end reliability under tool constraints, where small mistakes can compound across steps.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Language Comprehension, Language Production"
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp evaluates deep research and information synthesis by requiring models to browse and extract evidence from a controlled document collection to answer difficult questions. It stresses grounding, cross-document integration, and producing final answers supported by retrieved sources rather than free-form recall.","Planning, Attention, Working Memory, Semantic Understanding & Context Recognition, Logical Reasoning, Language Comprehension, Language Production"
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld benchmarks multimodal “computer use” by having agents interact with a desktop-like operating system to accomplish realistic tasks (navigation, form filling, settings changes, file operations). Success requires interpreting screenshots/GUI state and choosing correct action sequences under step limits.","Visual Perception, Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Planning, Decision-making, Sensorimotor Coordination, Adaptive Error Correction, Working Memory"
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI measures abstraction and few-shot generalization using grid transformation puzzles, where the rule must be inferred from only a handful of examples. It is designed to reduce dependence on memorized knowledge and instead test flexible induction over novel patterns.","Logical Reasoning, Cognitive Flexibility, Working Memory, Spatial Representation & Mapping, Scene Understanding & Visual Reasoning, Attention"
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon autonomous agency by simulating management of a vending-machine business over extended time (inventory, pricing, supplier negotiation, budgeting). It probes whether an agent can maintain coherent goals, adapt strategy to feedback, and avoid failure modes over many decisions.","Planning, Decision-making, Reward Mechanisms, Working Memory, Adaptive Error Correction, Self-reflection"
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym tests cybersecurity capabilities by asking agents to identify known vulnerabilities in real projects and to discover new ones, typically by reading code, running tools, and validating hypotheses. It emphasizes systematic investigation, exploit reasoning, and careful iteration under realistic constraints.","Logical Reasoning, Planning, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition, Attention, Language Comprehension, Language Production"
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"SpreadsheetBench evaluates the ability to understand and manipulate complex spreadsheets, including formulas, tables, and multi-sheet dependencies. Models must execute accurate, multi-step edits and computations where mistakes can silently propagate through downstream cells.","Logical Reasoning, Working Memory, Planning, Adaptive Error Correction, Attention, Language Comprehension, Language Production"
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a difficult, frontier-knowledge benchmark spanning many academic areas and often requiring multi-step reasoning and synthesis rather than simple fact recall. It is frequently used with tool-enabled settings (e.g., search or code), highlighting the gap between raw model knowledge and grounded problem solving.","Language Comprehension, Language Production, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Multisensory Integration, Visual Perception"
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,GPQA Diamond is a high-quality subset of GPQA consisting of very challenging graduate-level science multiple-choice questions designed to be resistant to shallow web lookup. It targets deep conceptual understanding and careful discrimination between close answer choices.,"Language Comprehension, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Decision-making"
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro evaluates expert-level multimodal understanding and reasoning across many disciplines using images paired with text questions (often multiple-choice). It stresses integrating visual evidence with domain knowledge and performing nontrivial reasoning over diagrams, plots, and figures.","Visual Perception, Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Attention, Working Memory, Semantic Understanding & Context Recognition"
MathArena Apex,https://matharena.ai/?view=problem&comp=apex--apex_2025,https://arxiv.org/abs/2505.23281,"MathArena Apex aggregates difficult competition-style math problems and evaluates advanced mathematical reasoning under standardized settings. It is used to compare models’ ability to carry out multi-step derivations, maintain consistency, and avoid brittle arithmetic or logic slips.","Logical Reasoning, Working Memory, Planning, Adaptive Error Correction, Cognitive Flexibility"
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning evaluates scientific figure understanding and reasoning, typically requiring models to interpret plots/diagrams from papers and answer questions that depend on the visual evidence. It probes whether models can connect figure content to the accompanying scientific context and perform quantitative or relational reasoning.","Visual Perception, Scene Understanding & Visual Reasoning, Logical Reasoning, Attention, Working Memory, Semantic Understanding & Context Recognition, Multisensory Integration"
OmniDocBench 1.5,https://github.com/opendatalab/OmniDocBench,https://arxiv.org/abs/2412.07626,"OmniDocBench 1.5 evaluates document understanding and OCR robustness across heterogeneous layouts, including text blocks, tables, formulas, and reading order. The goal is faithful extraction and structured reconstruction rather than merely recognizing isolated text lines.","Visual Perception, Visual Attention & Eye Movements, Semantic Understanding & Context Recognition, Working Memory, Attention"
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU tests video understanding and multimodal reasoning by asking questions that require integrating information across frames and time. It stresses temporal grounding (events, causality, state changes) rather than single-image recognition.","Visual Perception, Scene Understanding & Visual Reasoning, Cognitive Timing & Predictive Modeling, Working Memory, Attention, Multisensory Integration, Logical Reasoning"
LiveCodeBench Pro,https://livecodebenchpro.com/projects/livecodebench-pro/leaderboard,https://arxiv.org/abs/2506.11928,"LiveCodeBench Pro evaluates coding ability on problems designed to reduce contamination and better reflect modern development workflows, often emphasizing correctness under execution-based grading. It probes the ability to design algorithms, implement them reliably, and debug under time/attempt constraints.","Planning, Logical Reasoning, Adaptive Error Correction, Working Memory, Language Comprehension, Language Production, Decision-making"
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"FACTS Benchmark Suite systematically evaluates factuality by testing whether model outputs remain consistent with provided evidence and whether they avoid introducing unsupported claims. It targets grounded generation behaviors relevant to real deployments, such as citation faithfulness and robustness against hallucination.","Semantic Understanding & Context Recognition, Language Comprehension, Language Production, Inhibitory Control, Self-reflection, Working Memory, Attention"
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA extends physical commonsense reasoning evaluation across many languages and cultural contexts, aiming to test whether models can reason about everyday interactions with objects and environments beyond English. It emphasizes plausible action/affordance understanding and robust multilingual comprehension.","Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning, Spatial Representation & Mapping, Decision-making"
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,MRCR v2 (8-needle) evaluates long-context multi-round coreference and retrieval by embedding multiple similar “needle” interactions inside long “haystacks” and asking for the correct referenced response. It stresses maintaining and selecting the right referent amid distractors over very long inputs.,"Working Memory, Attention, Episodic Memory, Language Comprehension, Logical Reasoning, Inhibitory Control"
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval evaluates economically valuable, well-specified knowledge-work tasks across many occupations, often requiring production of artifacts like plans, spreadsheets, presentations, and analyses. It is judged via expert comparisons, focusing on usefulness, correctness, and professional quality rather than short-form QA.","Planning, Decision-making, Language Production, Language Comprehension, Semantic Understanding & Context Recognition, Working Memory, Social Reasoning & Theory of Mind, Self-reflection"
SWE-Lancer,https://openai.com/index/swe-lancer/,https://arxiv.org/abs/2502.12115,"SWE-Lancer evaluates software engineering tasks framed around real-world impact and difficulty, emphasizing end-to-end patch generation, codebase understanding, and robust completion. It is intended to better reflect practical engineering work where planning, testing, and iterative fixes matter.","Planning, Logical Reasoning, Adaptive Error Correction, Working Memory, Decision-making, Language Comprehension, Language Production"
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks tests reasoning over graph-structured data by requiring models to follow edges, track paths, and answer questions about reachability or node relationships from long, structured descriptions. It stresses precise state tracking and resistance to distractor structure as graphs scale.","Spatial Representation & Mapping, Working Memory, Logical Reasoning, Attention, Planning"
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon evaluates general tool-use competence across diverse APIs and tasks, emphasizing correct tool selection, parameterization, and multi-step orchestration. It targets reliability in agent loops where errors in calling, interpreting, or sequencing tools can derail outcomes.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Attention, Language Comprehension, Language Production"
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath evaluates advanced mathematics at or beyond typical graduate and research levels, with tiers intended to measure progress at the frontier. It stresses rigorous multi-step reasoning, precise symbolic manipulation, and robustness to very hard problems where partial progress is common.","Logical Reasoning, Working Memory, Planning, Adaptive Error Correction, Cognitive Flexibility"
