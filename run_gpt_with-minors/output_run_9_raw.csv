Benchmark,Website,Paper,Description,Cognitive Functions
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 evaluates agentic performance on real command-line (shell/terminal) tasks, requiring models to read instructions, execute sequences of commands, inspect outputs, and iterate toward a correct end state. It emphasizes end-to-end autonomy in a constrained compute environment, including debugging failures and recovering from mistakes.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Language Comprehension, Language Production"
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp measures “deep research” ability: answering difficult questions by searching, reading, and synthesizing information from a fixed or controlled document corpus for reproducible evaluation. It stresses multi-step information foraging (querying, selecting sources, cross-checking) and producing a final justified answer rather than recalling from memory.","Planning, Decision-making, Attention, Working Memory, Episodic Memory, Semantic Understanding & Context Recognition, Language Comprehension, Language Production"
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld evaluates multimodal “computer use” agents completing tasks within real OS/desktop application environments via screenshots and UI actions. Success requires interpreting visual interfaces, navigating multi-step workflows, and handling error states (e.g., wrong windows, failed dialogs) until the goal is achieved.","Visual Perception, Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Planning, Decision-making, Sensorimotor Coordination, Working Memory, Adaptive Error Correction"
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI tests abstract pattern induction on grid-based puzzles: models infer a hidden transformation rule from a few input–output examples and apply it to a new input. It is designed to minimize benefits from memorization and emphasize flexible, novel reasoning under small-data generalization.","Logical Reasoning, Cognitive Flexibility, Spatial Representation & Mapping, Visual Perception, Working Memory, Planning"
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon autonomy by having an agent run a simulated vending-machine business over an extended timeframe, optimizing inventory, pricing, procurement, and cash flow. It probes persistence, strategic adaptation to changing conditions, and coherent decision-making across many steps.","Planning, Decision-making, Reward Mechanisms, Working Memory, Semantic Understanding & Context Recognition, Adaptive Error Correction, Social Reasoning & Theory of Mind"
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity agent capabilities on large-scale tasks such as identifying known vulnerabilities from descriptions and discovering new vulnerabilities in real open-source projects. It rewards accurate reasoning about code behavior, exploitation conditions, and systematic debugging/triage under realistic constraints.","Logical Reasoning, Planning, Decision-making, Adaptive Error Correction, Working Memory, Language Comprehension, Language Production"
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"SpreadsheetBench measures the ability to understand and manipulate complex spreadsheets, including reading tables, formulas, and structure, and producing correct edits or derived results. It emphasizes precise, multi-step operations where small mistakes cascade, reflecting real analyst workflows.","Working Memory, Planning, Decision-making, Logical Reasoning, Semantic Understanding & Context Recognition, Adaptive Error Correction"
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a difficult multimodal benchmark spanning frontier academic and professional questions, intended to probe broad, deep knowledge and reasoning beyond routine QA. Many items require synthesizing domain concepts and interpreting diagrams or figures rather than surface-level recall.","Language Comprehension, Language Production, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Visual Perception, Scene Understanding & Visual Reasoning"
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,GPQA Diamond is a high-quality subset of extremely challenging multiple-choice science questions designed to be difficult to solve via simple web search or pattern matching. It targets graduate-level scientific reasoning and careful discrimination among near-plausible distractors.,"Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Language Comprehension, Decision-making"
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro evaluates multidisciplinary multimodal understanding and reasoning, combining text with images such as charts, diagrams, screenshots, and scientific figures. It emphasizes grounded reasoning over visual evidence and integrating visual cues with domain knowledge to select or produce answers.","Visual Perception, Scene Understanding & Visual Reasoning, Multisensory Integration, Language Comprehension, Logical Reasoning, Working Memory, Attention"
MathArena Apex,https://matharena.ai/?view=problem&comp=apex--apex_2025,https://arxiv.org/abs/2505.23281,"MathArena Apex is a competitive mathematics benchmark aimed at assessing strong mathematical problem solving under standardized prompting and scoring. Problems typically require multi-step derivations, symbolic manipulation, and error-checking rather than short factual recall.","Logical Reasoning, Working Memory, Planning, Adaptive Error Correction"
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning evaluates reasoning over scientific visuals (e.g., charts/figures from papers) paired with technical questions that require interpreting plotted trends, axes, legends, and experimental context. It emphasizes grounded inference from the figure plus correct scientific/quantitative interpretation.","Visual Perception, Scene Understanding & Visual Reasoning, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Attention"
OmniDocBench 1.5,https://github.com/opendatalab/OmniDocBench,https://arxiv.org/abs/2412.07626,"OmniDocBench 1.5 evaluates document understanding and OCR across heterogeneous layouts, including text blocks, tables, formulas, and reading order. It measures whether models can faithfully extract and structure information from real documents rather than only recognizing isolated text.","Visual Perception, Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Language Comprehension, Working Memory, Semantic Understanding & Context Recognition"
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU evaluates multimodal reasoning over videos, requiring integration of events across time (and often accompanying text/audio cues) to answer questions. It stresses temporal grounding—tracking what happened when—and robust understanding of dynamic scenes.","Visual Perception, Attention, Working Memory, Cognitive Timing & Predictive Modeling, Scene Understanding & Visual Reasoning, Multisensory Integration, Auditory Processing"
LiveCodeBench Pro,https://livecodebenchpro.com/projects/livecodebench-pro/leaderboard,https://arxiv.org/abs/2506.11928,"LiveCodeBench Pro evaluates coding ability on fresh or time-separated programming problems to reduce training-data leakage, typically scored by executing generated code against tests. It probes algorithmic reasoning, implementation accuracy, and iterative debugging under realistic constraints.","Logical Reasoning, Planning, Adaptive Error Correction, Working Memory, Language Comprehension, Language Production"
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"FACTS Benchmark Suite systematically evaluates factuality-related behaviors such as making supported claims, resisting hallucinations, and staying consistent with provided evidence or context. It targets reliability under ambiguity and the ability to avoid producing confident but unsupported statements.","Semantic Understanding & Context Recognition, Language Comprehension, Language Production, Inhibitory Control, Self-reflection, Decision-making"
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA evaluates physical commonsense and procedural reasoning across many languages, emphasizing whether models can choose sensible actions or explanations in everyday scenarios. It tests robustness to linguistic variation while keeping the underlying pragmatic/physical reasoning demand similar.","Semantic Understanding & Context Recognition, Logical Reasoning, Language Comprehension, Cognitive Flexibility, Decision-making"
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) is a long-context evaluation where multiple similar “needle” interactions are embedded in long “haystacks,” and the model must retrieve the correct referenced response from the appropriate round. It stresses precise multi-round coreference resolution under heavy distraction and long-range dependencies.","Working Memory, Attention, Episodic Memory, Language Comprehension, Logical Reasoning"
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval evaluates well-specified professional knowledge-work tasks across many occupations, judged by expert humans via pairwise comparisons of produced work products (e.g., spreadsheets, presentations, plans). It emphasizes end-to-end task execution quality, instruction-following, and producing polished artifacts rather than merely answering questions.","Planning, Decision-making, Language Production, Semantic Understanding & Context Recognition, Working Memory, Self-reflection, Adaptive Error Correction"
SWE-Lancer,https://openai.com/index/swe-lancer/,https://arxiv.org/abs/2502.12115,"SWE-Lancer evaluates software engineering agents on realistic repository-based tasks that resemble contracting/“freelance” work: understanding requirements, editing code, running tests, and delivering correct patches. It probes sustained problem solving, tool-driven iteration, and robust debugging across a codebase.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Logical Reasoning, Language Comprehension, Language Production"
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks evaluates long-context reasoning by embedding large graphs and asking models to perform structured traversals (e.g., BFS/parent tracking) or answer queries that require following edges across many steps. It stresses maintaining a consistent internal representation of a complex structure while executing algorithmic reasoning.","Spatial Representation & Mapping, Working Memory, Logical Reasoning, Attention, Planning"
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon measures tool-using competence across diverse APIs/tools, emphasizing correct tool selection, argument construction, multi-step orchestration, and recovery from tool errors. It targets practical agent reliability where success depends on coordinating external actions rather than only generating text.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Language Comprehension, Language Production, Inhibitory Control"
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath evaluates advanced mathematical reasoning on problems intended to be beyond routine competition math, often requiring nontrivial insights and long derivations (sometimes with optional tool assistance like Python). It targets depth of reasoning, careful verification, and the ability to sustain correct multi-step work.","Logical Reasoning, Working Memory, Planning, Adaptive Error Correction"
