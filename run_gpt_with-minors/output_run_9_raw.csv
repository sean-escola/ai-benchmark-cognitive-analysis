Benchmark,Website,Paper,Description,Cognitive Functions
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates software engineering agents on real GitHub issues by requiring them to produce a patch that makes a project’s tests pass. The Verified subset uses tasks that have been validated as solvable and includes stronger, more reliable evaluation of correctness via tests and human verification.","Planning, Logical Reasoning, Working Memory, Adaptive Error Correction, Decision-making, Language Comprehension, Language Production, Semantic Understanding & Context Recognition"
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro is a more challenging software engineering benchmark with a larger and harder set of real-world issues, designed to be more contamination-resistant and representative of professional development work. It emphasizes end-to-end problem solving in realistic repositories, often spanning multiple files and requiring careful adherence to project conventions and tests.","Planning, Logical Reasoning, Working Memory, Adaptive Error Correction, Decision-making, Language Comprehension, Language Production, Semantic Understanding & Context Recognition"
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp measures a model’s ability to do “deep research” by finding and synthesizing information from the web (or a controlled web-like corpus) to answer difficult questions. Success depends on decomposing queries, selecting sources, cross-checking evidence, and producing a supported final response.","Planning, Decision-making, Attention, Working Memory, Episodic Memory, Semantic Understanding & Context Recognition, Language Comprehension, Language Production, Self-reflection"
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench evaluates interactive agent behavior in multi-turn customer-support scenarios (e.g., retail, airline, telecom) with simulated users and APIs, emphasizing policy compliance and task completion. It stresses whether agents can use tools correctly over long dialogues while following domain constraints and avoiding loopholes.","Social Reasoning & Theory of Mind, Inhibitory Control, Planning, Decision-making, Working Memory, Language Comprehension, Language Production, Adaptive Error Correction, Attention"
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,ARC-AGI tests fluid reasoning via abstract grid-based puzzles where the model must infer a hidden transformation rule from a few examples and apply it to a new input. It is intended to reward generalization and novel pattern induction rather than memorized knowledge.,"Logical Reasoning, Working Memory, Cognitive Flexibility, Spatial Representation & Mapping, Visual Perception, Scene Understanding & Visual Reasoning, Attention"
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas measures real-world tool-use competency through the Model Context Protocol (MCP), requiring models to discover tools, call them with correct arguments, handle errors, and synthesize outputs. Tasks are multi-step workflows resembling production API usage across heterogeneous services.","Planning, Decision-making, Working Memory, Adaptive Error Correction, Attention, Language Comprehension, Language Production, Semantic Understanding & Context Recognition"
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a difficult, frontier-knowledge benchmark spanning many domains and including multimodal questions, designed to probe capabilities near the limits of current models. It emphasizes robust reasoning and synthesis under sparse or non-obvious cues, sometimes with optional tool use depending on the evaluation setting.","Language Comprehension, Language Production, Logical Reasoning, Working Memory, Semantic Understanding & Context Recognition, Visual Perception, Scene Understanding & Visual Reasoning, Multisensory Integration, Attention"
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 evaluates competition-style mathematics on problems that typically require multi-step derivations, algebraic manipulation, and careful case analysis. It is often used to test non-trivial symbolic reasoning under time/attempt constraints (e.g., pass@1).","Logical Reasoning, Working Memory, Planning, Cognitive Flexibility, Attention"
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,GPQA Diamond is a curated subset of extremely challenging graduate-level science multiple-choice questions intended to be “Google-proof” and to require real scientific reasoning. The Diamond split focuses on high-quality items where experts reliably answer correctly while non-experts tend to fail.,"Language Comprehension, Logical Reasoning, Working Memory, Semantic Understanding & Context Recognition, Attention"
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends broad academic knowledge and reasoning evaluation into many non-English languages across dozens of subjects. It tests whether models can understand and answer subject-matter questions consistently across languages, not just in English.","Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Attention"
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro is a harder, more evaluation-robust variant of MMMU that tests expert-level multimodal understanding across many disciplines using images paired with text questions. It emphasizes reasoning over diagrams, charts, scientific figures, and other visuals that require structured interpretation.","Visual Perception, Scene Understanding & Visual Reasoning, Multisensory Integration, Visual Attention & Eye Movements, Language Comprehension, Logical Reasoning, Working Memory, Attention"
ScreenShot-Pro,https://gui-agent.github.io/grounding-leaderboard/,https://arxiv.org/abs/2504.07981,"ScreenShot-Pro evaluates screenshot understanding for software/UI contexts, requiring models to interpret high-resolution interface images and answer questions that depend on layout, controls, and visual affordances. It targets visually grounded reasoning that is critical for computer-use agents and GUI automation.","Visual Perception, Visual Attention & Eye Movements, Spatial Representation & Mapping, Scene Understanding & Visual Reasoning, Decision-making, Planning, Working Memory, Attention"
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning evaluates scientific figure understanding and reasoning over charts and visual elements from arXiv-style papers, often requiring quantitative interpretation and linking captions/text to visual evidence. It is designed to measure faithful, grounded reasoning about complex academic visuals, sometimes with optional computation tools.","Visual Perception, Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Working Memory, Language Comprehension, Semantic Understanding & Context Recognition, Attention"
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU extends multimodal reasoning to the temporal domain by asking questions that require understanding events and context across video clips (often with accompanying text). It stresses integrating information across time, tracking entities, and reasoning about changes and causality.","Visual Perception, Multisensory Integration, Working Memory, Attention, Cognitive Timing & Predictive Modeling, Scene Understanding & Visual Reasoning, Language Comprehension"
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) is a long-context retrieval and co-reference benchmark where multiple similar “needle” requests are embedded in a long “haystack” of text, and the model must recover the correct referenced answer. The 8-needle variant increases interference, testing robust context tracking rather than short-window pattern matching.","Working Memory, Attention, Episodic Memory, Language Comprehension, Semantic Understanding & Context Recognition, Inhibitory Control"
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval evaluates well-specified professional knowledge-work tasks across many occupations by having models produce real work artifacts (e.g., spreadsheets, presentations, plans) judged against human professional performance. It targets practical, end-to-end execution quality, not just question answering, under constraints typical of workplace deliverables.","Planning, Decision-making, Language Production, Language Comprehension, Working Memory, Semantic Understanding & Context Recognition, Adaptive Error Correction, Self-reflection, Social Reasoning & Theory of Mind"
SWE-Lancer,https://openai.com/index/swe-lancer/,https://arxiv.org/abs/2502.12115,"SWE-Lancer evaluates agentic software engineering in a “freelance” style setting where models must complete coding tasks that resemble paid engineering work, often involving ambiguous requirements and iterative refinement. It emphasizes realistic development behaviors like understanding requirements, implementing changes, and validating via tests or tooling.","Planning, Decision-making, Logical Reasoning, Working Memory, Adaptive Error Correction, Language Comprehension, Language Production, Semantic Understanding & Context Recognition"
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks tests long-context reasoning over graph-structured data encoded in text, requiring models to follow edges, perform traversals (e.g., BFS-like queries), and answer reachability or parent/ancestor questions. Performance depends on reliably maintaining state over many hops and resisting distractors.","Spatial Representation & Mapping, Working Memory, Logical Reasoning, Attention, Semantic Understanding & Context Recognition, Inhibitory Control"
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon evaluates tool-using agents on multi-step tasks that require selecting appropriate tools, calling them correctly, handling failures, and integrating tool outputs into final answers. It emphasizes reliability under tool friction (errors, missing fields, retries) and correct orchestration over extended interaction traces.","Planning, Decision-making, Working Memory, Adaptive Error Correction, Attention, Language Comprehension, Language Production, Semantic Understanding & Context Recognition"
HMMT,https://matharena.ai/?view=problem&comp=hmmt--hmmt_feb_2025,https://arxiv.org/abs/2505.23281,"HMMT is a high-difficulty mathematics competition benchmark that probes multi-step problem solving, creative decomposition, and careful verification under strict answer formats. Compared with many standard math sets, problems often require deeper insight and longer chains of reasoning.","Logical Reasoning, Working Memory, Planning, Cognitive Flexibility, Attention"
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath is an expert-level mathematics benchmark aimed at measuring progress on problems closer to current research and advanced olympiad-style reasoning, often requiring substantial derivations and proof-like thinking. It is designed to be difficult for models and to better reflect genuine mathematical problem-solving ability.","Logical Reasoning, Planning, Working Memory, Cognitive Flexibility, Attention, Self-reflection"
