Benchmark,Website,Paper,Description,Cognitive Functions,Max AI Tier
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates software-engineering bug-fixing by giving a model a real GitHub issue plus repository context and requiring a code patch that passes the project’s tests. The Verified split filters to problems that human annotators confirmed are solvable and correctly specified, reducing noise from underspecified tasks.","L1: Language Comprehension, Language Production
L2: Planning, Logical Reasoning, Working Memory, Adaptive Error Correction, Decision-making
L3: ",L2
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro is a harder, larger-scale successor to SWE-bench aimed at more contamination-resistant and industrially realistic software engineering. It includes more complex repositories and tasks (including multiple languages) that require deeper debugging, refactoring, and integration to satisfy tests and requirements.","L1: Language Comprehension, Language Production
L2: Planning, Logical Reasoning, Working Memory, Adaptive Error Correction, Decision-making
L3: ",L2
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp measures an agent’s ability to answer difficult questions by searching, reading, and synthesizing information from documents, emphasizing reproducible evaluation of browsing-style research. It targets multi-step information retrieval where the agent must decide what to look for, verify evidence, and compose a supported final answer.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Attention, Working Memory, Semantic Understanding & Context Recognition, Adaptive Error Correction
L3: ",L2
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench evaluates interactive, tool-using customer-support agents in simulated domains (e.g., retail, airline, telecom) with policies and APIs. The agent must follow rules, maintain conversational state over many turns, and execute correct tool calls to resolve user requests.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Working Memory, Reward Mechanisms
L3: Social Reasoning & Theory of Mind, Inhibitory Control",L3
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,ARC-AGI tests few-shot abstract reasoning via grid-based puzzles where the model infers a hidden transformation rule from a handful of examples and applies it to a new input. It emphasizes novelty and compositional generalization rather than memorized knowledge.,"L1: Visual Perception
L2: Logical Reasoning, Working Memory, Attention, Spatial Representation & Mapping, Scene Understanding & Visual Reasoning
L3: Cognitive Flexibility",L3
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas assesses real-world tool use via the Model Context Protocol (MCP), where models must discover available tools, call them with correct arguments, handle errors, and integrate outputs into a final response. Tasks are multi-step workflows over authentic, production-like tool servers and APIs.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Working Memory, Adaptive Error Correction
L3: Inhibitory Control",L3
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam (HLE) is a difficult, frontier-knowledge benchmark spanning many disciplines (often multimodal) designed to stress deep reasoning and synthesis beyond routine exam questions. It commonly requires chaining multiple ideas, resisting shallow pattern matching, and producing well-justified answers.","L1: Language Comprehension, Language Production
L2: Logical Reasoning, Working Memory, Attention, Semantic Understanding & Context Recognition, Scene Understanding & Visual Reasoning
L3: ",L2
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 consists of competition mathematics problems (integer answers) intended for high-achieving students, requiring nontrivial algebra, number theory, geometry, and combinatorics. It measures correctness under time-like pressure and the ability to execute multi-step derivations reliably.","L1: 
L2: Logical Reasoning, Working Memory, Planning, Adaptive Error Correction, Attention
L3: ",L2
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a highly challenging multiple-choice science benchmark (physics, chemistry, biology) curated to be difficult for non-experts and resistant to simple web lookup. The “Diamond” subset emphasizes question quality and discriminative power, stressing precise scientific reasoning.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Attention, Decision-making
L3: ",L2
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends MMLU to multiple languages, testing broad academic knowledge and reasoning across many subjects with multilingual prompts. It probes whether a model’s knowledge and reasoning transfer across linguistic contexts, not just English.","L1: Language Comprehension
L2: Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Attention
L3: ",L2
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro is a more challenging variant of MMMU that evaluates expert-level multimodal understanding and reasoning across disciplines using images (e.g., diagrams, charts, scenes) paired with text questions. It emphasizes grounded reasoning from visual evidence and domain knowledge rather than caption-like description.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Multisensory Integration, Logical Reasoning, Working Memory, Semantic Understanding & Context Recognition
L3: ",L2
ScreenShot-Pro,https://gui-agent.github.io/grounding-leaderboard/,https://arxiv.org/abs/2504.07981,"ScreenShot-Pro evaluates screenshot understanding for GUI/desktop-style images, where models must correctly interpret interface elements, layout, and sometimes implied user goals. It targets visually grounded reasoning about UI structure and the mapping from what is seen to an intended action or answer.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Visual Attention & Eye Movements, Spatial Representation & Mapping, Decision-making, Working Memory, Semantic Understanding & Context Recognition
L3: ",L2
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning evaluates reasoning over scientific documents and figures drawn from arXiv-style papers, requiring models to interpret plots, tables, and notation and answer questions that depend on that evidence. It stresses faithful grounding in visual/structured content and scientific-style inference.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Logical Reasoning, Working Memory, Attention, Semantic Understanding & Context Recognition, Multisensory Integration
L3: ",L2
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU extends multimodal understanding to video, requiring models to integrate information across frames and answer questions about events, causality, and temporal relationships. It stresses temporal integration, tracking entities over time, and robust grounded reasoning from dynamic visual input.","L1: Visual Perception
L2: Scene Understanding & Visual Reasoning, Working Memory, Attention, Semantic Understanding & Context Recognition
L3: Cognitive Timing & Predictive Modeling",L3
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (8-needle) is a long-context, multi-round co-reference and retrieval benchmark where many similar “needle” interactions are embedded in a large “haystack,” and the model must reproduce the correct response associated with a specified needle. The 8-needle setting increases distractors and tests whether models can maintain and retrieve the right binding across long contexts.","L1: 
L2: Working Memory, Attention, Semantic Understanding & Context Recognition, Episodic Memory, Logical Reasoning
L3: ",L2
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval evaluates economically valuable, well-specified professional knowledge work across many occupations (e.g., producing spreadsheets, presentations, plans), typically judged by experts via pairwise comparisons. It emphasizes end-to-end artifact quality, instruction following, and practical decision-making under realistic constraints.","L1: Language Production, Language Comprehension
L2: Planning, Decision-making, Semantic Understanding & Context Recognition, Working Memory
L3: Social Reasoning & Theory of Mind",L3
SWE-Lancer,https://openai.com/index/swe-lancer/,https://arxiv.org/abs/2502.12115,"SWE-Lancer evaluates agentic software engineering on longer, more realistic “freelance-style” tasks that require scoping the work, navigating a codebase, and delivering a working change. It emphasizes sustained problem solving, iterative debugging, and integrating changes safely within existing systems.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Working Memory, Adaptive Error Correction, Logical Reasoning
L3: Self-reflection",L3
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks tests whether models can perform multi-hop reasoning over graph-structured data expressed in text, such as following parent pointers or executing BFS-like traversals. It is designed to stress systematic traversal, state tracking, and robustness to distractors in long contexts.","L1: 
L2: Spatial Representation & Mapping, Working Memory, Attention, Logical Reasoning, Planning, Adaptive Error Correction
L3: ",L2
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon benchmarks tool-using agents on multi-step tasks requiring selecting appropriate tools, invoking them correctly, and composing results into a coherent solution. It emphasizes reliability under workflow complexity, including recovery from tool errors and maintaining task state across many steps.","L1: Language Comprehension, Language Production
L2: Planning, Decision-making, Working Memory, Adaptive Error Correction
L3: Inhibitory Control",L3
HMMT,https://matharena.ai/?view=problem&comp=hmmt--hmmt_feb_2025,https://arxiv.org/abs/2505.23281,"HMMT problems (as used in modern model evals) draw from high-difficulty competition mathematics that require creative insights and multi-step derivations. Compared with standard exams, they more strongly stress strategy selection and proof-like reasoning under ambiguity.","L1: 
L2: Logical Reasoning, Working Memory, Planning, Attention
L3: Cognitive Flexibility",L3
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath is a contamination-resistant benchmark of advanced mathematics, tiered by difficulty, targeting problems that require substantial original reasoning rather than recall. It measures the ability to sustain complex chains of inference, manage subgoals, and avoid subtle algebraic or logical errors.","L1: 
L2: Logical Reasoning, Working Memory, Planning, Adaptive Error Correction, Attention
L3: Cognitive Flexibility",L3
