Benchmark,Website,Paper,Description,Cognitive Functions
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates an agent’s ability to fix real issues in open-source Python repositories by generating patches that make the test suite pass. The “Verified” subset focuses on tasks that have been validated as solvable and have reliable evaluation, emphasizing end-to-end coding rather than isolated function completion.","Planning, Decision-making, Logical Reasoning, Working Memory, Adaptive Error Correction, Language Comprehension, Language Production"
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro is a harder, larger software engineering benchmark intended to be more contamination-resistant and more representative of professional development work across multiple programming languages. Models must understand a repository and issue description, implement a correct fix, and satisfy automated checks, often requiring broader refactors and stronger robustness.","Planning, Decision-making, Logical Reasoning, Working Memory, Adaptive Error Correction, Cognitive Flexibility, Language Comprehension, Language Production"
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp evaluates “deep research” agents on questions that require multi-step information gathering and synthesis from a curated document/web corpus. It targets the ability to search effectively, integrate evidence across sources, and produce a final grounded answer under realistic agent constraints.","Planning, Decision-making, Attention, Episodic Memory, Semantic Understanding & Context Recognition, Language Comprehension, Language Production, Working Memory, Adaptive Error Correction"
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench measures interactive tool-using agents in multi-turn customer-support simulations (e.g., retail, airline, telecom) where the agent must call APIs, follow policies, and satisfy user goals. Success depends on consistent policy adherence, accurate state tracking across turns, and effective dialogue to resolve cases.","Social Reasoning & Theory of Mind, Decision-making, Planning, Inhibitory Control, Working Memory, Language Comprehension, Language Production, Adaptive Error Correction, Attention"
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI is a fluid intelligence benchmark where models infer hidden rules from a few input–output grid examples and produce the correct output grid for a new input. It is designed to emphasize abstraction, compositional generalization, and novelty over memorized knowledge.","Logical Reasoning, Working Memory, Cognitive Flexibility, Spatial Representation & Mapping, Visual Perception, Scene Understanding & Visual Reasoning, Attention"
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas evaluates real-world tool use through the Model Context Protocol by requiring multi-step workflows across tool servers (discovering tools, invoking them correctly, handling errors, and composing results). It targets agent reliability in production-like API interactions rather than single-call function execution.","Planning, Decision-making, Working Memory, Adaptive Error Correction, Language Comprehension, Language Production, Attention"
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,Humanity’s Last Exam (HLE) is a large multimodal benchmark covering frontier academic and professional questions intended to be challenging even for strong models. It tests deep reasoning and synthesis (often across text and images) and is frequently evaluated both with and without external tools such as search or code execution.,"Language Comprehension, Language Production, Logical Reasoning, Working Memory, Semantic Understanding & Context Recognition, Visual Perception, Scene Understanding & Visual Reasoning, Planning, Decision-making"
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 is a competition-math benchmark drawn from the American Invitational Mathematics Examination, featuring short-answer problems that require nontrivial derivations. It probes structured mathematical reasoning and careful multi-step computation under tight correctness constraints.","Logical Reasoning, Working Memory, Planning, Cognitive Flexibility, Attention"
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a difficult multiple-choice science benchmark designed to be “Google-proof,” emphasizing questions that require expert-level reasoning rather than simple fact lookup. The Diamond subset is the highest-quality set, filtered to be challenging for non-experts and reliable for evaluation.","Language Comprehension, Logical Reasoning, Working Memory, Semantic Understanding & Context Recognition, Attention, Decision-making"
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,MMMLU extends the MMLU-style academic knowledge and reasoning evaluation into multiple non-English languages across many subjects. It tests whether a model’s understanding and reasoning transfer across languages rather than relying on English-only competence.,"Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Attention"
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro is a high-difficulty multimodal benchmark that tests expert-level understanding and reasoning over combined text and image inputs (e.g., diagrams, charts, scientific visuals) across many disciplines. Compared with earlier MMMU settings, it emphasizes harder questions and more reliable measurement of advanced multimodal reasoning.","Visual Perception, Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Working Memory, Attention, Language Comprehension, Language Production"
ScreenShot-Pro,https://gui-agent.github.io/grounding-leaderboard/,https://arxiv.org/abs/2504.07981,"ScreenShot-Pro evaluates how well models can understand and reason over high-resolution screenshots of graphical user interfaces, often requiring precise identification of UI elements and their relationships. It targets visually grounded reasoning relevant to computer-use agents, including interpreting layout, text, and controls.","Visual Perception, Visual Attention & Eye Movements, Scene Understanding & Visual Reasoning, Spatial Representation & Mapping, Language Comprehension, Logical Reasoning, Working Memory, Planning, Decision-making"
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning tests models on questions that require interpreting figures, tables, and scientific diagrams from research papers, often demanding quantitative and causal reasoning grounded in the visual evidence. It is intended to capture “read-the-paper-figure” competence rather than generic image captioning.","Visual Perception, Scene Understanding & Visual Reasoning, Logical Reasoning, Working Memory, Attention, Semantic Understanding & Context Recognition, Language Comprehension, Language Production"
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU evaluates multimodal reasoning over video, requiring models to integrate information across time (and typically accompanying text) to answer questions about events, causality, and context. It stresses temporal understanding beyond single-frame perception.","Visual Perception, Scene Understanding & Visual Reasoning, Cognitive Timing & Predictive Modeling, Working Memory, Attention, Multisensory Integration, Logical Reasoning, Language Comprehension, Language Production"
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,"MRCR v2 (multi-round coreference resolution) tests long-context reasoning by inserting multiple similar “needle” interactions into long “haystack” dialogues and asking the model to retrieve the correct referenced response. The 8-needle setting increases distractors and memory demands, emphasizing robust retrieval across lengthy contexts.","Working Memory, Episodic Memory, Attention, Language Comprehension, Language Production, Logical Reasoning"
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval evaluates economically valuable, well-specified professional knowledge-work tasks across many occupations, judged by expert humans via head-to-head comparisons (wins/ties). Tasks often involve producing realistic work artifacts (e.g., spreadsheets, plans, analyses), testing end-to-end usefulness rather than academic Q&A alone.","Planning, Decision-making, Language Production, Language Comprehension, Semantic Understanding & Context Recognition, Working Memory, Adaptive Error Correction, Self-reflection, Social Reasoning & Theory of Mind"
SWE-Lancer,https://openai.com/index/swe-lancer/,https://arxiv.org/abs/2502.12115,"SWE-Lancer targets agentic software engineering in settings closer to real paid work, where tasks can involve ambiguous requirements, iterative debugging, and broader repo-level changes. It emphasizes delivering correct, reviewable patches under practical constraints rather than solving isolated algorithmic problems.","Planning, Decision-making, Logical Reasoning, Working Memory, Adaptive Error Correction, Cognitive Flexibility, Language Comprehension, Language Production"
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks evaluates long-context graph reasoning by asking models to follow graph-structured relationships (e.g., traversals, ancestry/parent links) described in text and return the correct nodes/paths. It stresses faithful stepwise tracking of structure across many hops, where small errors compound.","Spatial Representation & Mapping, Working Memory, Logical Reasoning, Attention, Planning, Adaptive Error Correction, Language Comprehension, Language Production"
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon is a tool-use benchmark where agents must select and chain calls to multiple tools/APIs to complete tasks, often requiring error handling, retries, and correct integration of tool outputs. It focuses on practical orchestration and reliability of multi-step tool workflows.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Attention, Language Comprehension, Language Production"
HMMT,https://matharena.ai/?view=problem&comp=hmmt--hmmt_feb_2025,https://arxiv.org/abs/2505.23281,"HMMT is a competition-math benchmark (e.g., Harvard–MIT Mathematics Tournament) featuring challenging problems that require creative constructions, proofs, or multi-step computations. It probes deeper mathematical reasoning than many short-form tests, rewarding robust solution planning and careful execution.","Logical Reasoning, Working Memory, Planning, Cognitive Flexibility, Attention"
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath is an expert-level mathematics benchmark designed to measure progress on problems closer to the research frontier, often requiring substantial derivations and the combination of multiple techniques. It is used to evaluate whether models can reliably solve difficult, novel math problems rather than standard contest items.","Logical Reasoning, Working Memory, Planning, Cognitive Flexibility, Attention"
