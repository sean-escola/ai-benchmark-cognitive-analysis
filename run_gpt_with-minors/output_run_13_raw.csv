Benchmark,Website,Paper,Description,Cognitive Functions
Terminal-Bench 2.0,https://www.tbench.ai/,,"Terminal-Bench 2.0 evaluates autonomous agents in real command-line environments, requiring them to navigate filesystems, run shell commands, install/use tools, and complete multi-step tasks end-to-end. It emphasizes robust execution under noisy real-world conditions (e.g., dependency issues, runtime errors) rather than purely writing code snippets.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition"
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp evaluates deep-research style web browsing where models must search, gather evidence, and synthesize answers across multiple documents. The benchmark focuses on reliable information seeking and multi-step reasoning grounded in retrieved sources, often with constraints to reduce reliance on memorization.","Planning, Attention, Working Memory, Semantic Understanding & Context Recognition, Logical Reasoning, Language Comprehension"
OSWorld,https://os-world.github.io/,https://arxiv.org/abs/2404.07972,"OSWorld benchmarks “computer use” in realistic desktop-like operating system environments, where agents must complete tasks by interacting with GUI applications across multiple steps. It tests whether a model can perceive interface state and execute correct action sequences under partial observability and changing UI conditions.","Visual Perception, Scene Understanding & Visual Reasoning, Planning, Decision-making, Sensorimotor Coordination, Working Memory, Attention"
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI measures abstract pattern discovery and few-shot generalization using small grid-based puzzles, where the system must infer a hidden transformation rule from a few examples. It is designed to emphasize fluid reasoning and compositional generalization over training-set memorization.","Logical Reasoning, Cognitive Flexibility, Working Memory, Planning, Attention"
Vending-Bench 2,https://andonlabs.com/evals/vending-bench-2,https://arxiv.org/abs/2502.15840,"Vending-Bench 2 evaluates long-horizon agent coherence and strategy by simulating the management of a vending-machine business over an extended period. Agents must make repeated decisions (pricing, inventory, supplier interactions, budgeting) and adapt to market dynamics to maximize outcomes.","Planning, Decision-making, Reward Mechanisms, Working Memory, Adaptive Error Correction, Semantic Understanding & Context Recognition"
CyberGym,https://www.cybergym.io/,https://arxiv.org/abs/2506.02548,"CyberGym evaluates cybersecurity capability on real-world software vulnerability tasks, including identifying known vulnerabilities from descriptions and sometimes discovering new issues. It emphasizes practical reasoning about codebases, exploitation conditions, and remediation under realistic constraints.","Logical Reasoning, Planning, Adaptive Error Correction, Working Memory, Semantic Understanding & Context Recognition, Language Comprehension"
SpreadsheetBench,https://spreadsheetbench.github.io/,https://arxiv.org/abs/2406.14991,"SpreadsheetBench tests the ability to understand, edit, and generate complex spreadsheets using realistic artifacts and tasks (e.g., formulas, tables, formatting, and multi-sheet logic). It targets reliable structured manipulation where small mistakes can cascade into incorrect outputs.","Logical Reasoning, Working Memory, Attention, Planning, Adaptive Error Correction, Semantic Understanding & Context Recognition"
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a broad, frontier-difficulty benchmark spanning many academic and professional domains, including multimodal questions. It is intended to probe advanced reasoning and knowledge integration, often benefiting from careful tool use and cross-source synthesis when tools are allowed.","Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Planning, Multisensory Integration, Visual Perception"
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a high-quality subset of challenging, graduate-level science multiple-choice questions designed to be difficult for non-experts and resistant to superficial pattern matching. It probes deep conceptual understanding and careful reasoning under tight answer constraints.","Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Language Comprehension, Attention"
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro evaluates expert-level multimodal understanding across diverse disciplines, requiring models to answer questions grounded in images, diagrams, and accompanying text. It stresses fine-grained visual interpretation plus cross-modal reasoning rather than text-only recall.","Visual Perception, Scene Understanding & Visual Reasoning, Multisensory Integration, Language Comprehension, Logical Reasoning, Attention, Working Memory"
MathArena Apex,https://matharena.ai/?view=problem&comp=apex--apex_2025,https://arxiv.org/abs/2505.23281,"MathArena Apex aggregates difficult competition-style mathematics problems and evaluates high-end mathematical reasoning performance. It emphasizes multi-step derivations, precision, and robustness across varied problem types under standardized evaluation.","Logical Reasoning, Planning, Working Memory, Adaptive Error Correction, Attention"
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning evaluates reasoning over scientific figures and paper-style visual content, often requiring interpretation of plots, annotations, and visual structure. It targets grounded analysis where correct answers depend on extracting and integrating visual evidence with technical context.","Visual Perception, Scene Understanding & Visual Reasoning, Language Comprehension, Logical Reasoning, Attention, Working Memory, Multisensory Integration"
OmniDocBench 1.5,https://github.com/opendatalab/OmniDocBench,https://arxiv.org/abs/2412.07626,"OmniDocBench 1.5 evaluates document understanding and OCR-style extraction across heterogeneous document elements such as text blocks, formulas, tables, and reading order. It emphasizes accurate perception and structured reconstruction of complex page layouts.","Visual Perception, Visual Attention & Eye Movements, Attention, Language Comprehension, Semantic Understanding & Context Recognition, Working Memory"
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU evaluates multimodal understanding and reasoning over videos, requiring models to integrate information across time (events, actions, and context) alongside text prompts. It probes temporal grounding and cross-frame consistency rather than single-image recognition.","Visual Perception, Multisensory Integration, Working Memory, Attention, Cognitive Timing & Predictive Modeling, Scene Understanding & Visual Reasoning"
LiveCodeBench Pro,https://livecodebenchpro.com/projects/livecodebench-pro/leaderboard,https://arxiv.org/abs/2506.11928,"LiveCodeBench Pro benchmarks coding in a setting closer to interactive development, emphasizing correct program behavior and iterative problem solving rather than one-shot code generation. It is designed to reflect practical software engineering competence under time-evolving tasks.","Planning, Logical Reasoning, Adaptive Error Correction, Working Memory, Language Production, Language Comprehension, Decision-making"
FACTS Benchmark Suite,https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/,https://arxiv.org/abs/2512.10791,"FACTS Benchmark Suite systematically evaluates factuality and grounding, testing whether models produce accurate claims, appropriately express uncertainty, and avoid unsupported statements. It targets reliability across diverse factuality failure modes, including subtle distortions and overconfident errors.","Semantic Understanding & Context Recognition, Logical Reasoning, Inhibitory Control, Self-reflection, Attention, Language Comprehension"
Global PIQA,https://huggingface.co/datasets/mrlbenchmarks/global-piqa-nonparallel,https://arxiv.org/abs/2510.24081,"Global PIQA evaluates practical reasoning about everyday physical and social situations across many languages and locales, aiming to reduce English-centric bias. It probes whether models can select plausible actions/solutions using common-sense constraints rather than niche factual recall.","Semantic Understanding & Context Recognition, Logical Reasoning, Social Reasoning & Theory of Mind, Language Comprehension, Attention"
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,MRCR v2 (8-needle) tests long-context retrieval and multi-round coreference by embedding multiple similar “needle” interactions within long “haystack” conversations and asking the model to reproduce the correct referenced content. It emphasizes accurate tracking amid distractors and extended contexts.,"Working Memory, Attention, Language Comprehension, Episodic Memory, Semantic Understanding & Context Recognition"
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval evaluates well-specified, economically meaningful knowledge-work tasks across many occupations, often judged by expert humans via head-to-head comparisons. Tasks emphasize producing usable work artifacts (e.g., plans, analyses, slides/spreadsheets) with real-world constraints and quality standards.","Planning, Decision-making, Language Production, Semantic Understanding & Context Recognition, Self-reflection, Working Memory, Social Reasoning & Theory of Mind"
SWE-Lancer,https://openai.com/index/swe-lancer/,https://arxiv.org/abs/2502.12115,"SWE-Lancer evaluates software engineering performance in more agentic, end-to-end scenarios that resemble contracting or “task completion for a client,” typically involving navigating a repo, implementing changes, and validating results. It emphasizes reliability, iteration, and producing shippable patches under realistic constraints.","Planning, Logical Reasoning, Adaptive Error Correction, Working Memory, Decision-making, Language Comprehension, Language Production"
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks evaluates reasoning over graph-structured data embedded in text, such as following paths, tracking parent/child relations, or performing constrained traversals. It targets systematic multi-step relational reasoning and resistance to distraction over longer sequences.","Spatial Representation & Mapping, Working Memory, Attention, Logical Reasoning, Planning"
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon evaluates tool-use competence across a variety of tasks where models must decide when and how to call tools, interpret results, and recover from failures. It emphasizes robust orchestration, correct parameterization, and multi-step execution rather than pure language generation.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Attention, Semantic Understanding & Context Recognition"
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath evaluates expert-level mathematics, emphasizing difficult problems that require long multi-step reasoning and careful verification, often with optional tool assistance (e.g., Python). It is intended to measure progress on advanced mathematical problem solving near the frontier of model capability.","Logical Reasoning, Planning, Working Memory, Adaptive Error Correction, Attention"
