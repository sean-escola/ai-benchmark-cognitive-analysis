Benchmark,Website,Paper,Description,Cognitive Functions
SWE-bench Verified,https://openai.com/index/introducing-swe-bench-verified/,https://arxiv.org/abs/2310.06770,"SWE-bench Verified evaluates software engineering by requiring a model/agent to generate a correct code patch for real GitHub issues in Python repositories, validated by running the project’s tests. The “Verified” subset emphasizes issues that have been human-checked as solvable and reliably testable, reducing noise from ambiguous tasks.","Planning, Logical Reasoning, Working Memory, Adaptive Error Correction, Language Comprehension, Language Production"
SWE-bench Pro,https://scale.com/research/swe_bench_pro,https://arxiv.org/abs/2509.16941,"SWE-bench Pro is a harder, larger software engineering benchmark that extends beyond the Verified setting to more diverse and contamination-resistant tasks, spanning multiple programming languages and more complex repos. Models must understand the issue, modify code correctly, and satisfy automated evaluation checks (typically test suites) under realistic constraints.","Planning, Logical Reasoning, Working Memory, Adaptive Error Correction, Decision-making, Language Comprehension, Language Production"
BrowseComp,https://openai.com/index/browsecomp/,https://arxiv.org/abs/2504.12516,"BrowseComp measures deep web-research performance: models must answer hard questions by searching and synthesizing information from a controlled or specified document index, emphasizing reproducibility. It stresses long-horizon information gathering, source integration, and precise final answers rather than pure memorized knowledge.","Planning, Attention, Working Memory, Semantic Understanding & Context Recognition, Language Comprehension, Language Production, Decision-making"
τ2-bench,https://sierra.ai/uk/blog/benchmarking-ai-agents,https://arxiv.org/abs/2406.12045,"τ2-bench evaluates interactive customer-support agents in simulated domains (e.g., retail, airline, telecom) that must use tools/APIs and follow domain policies over multi-turn dialogues. Performance depends on correctly interpreting user intent, executing procedural steps, and adhering to constraints that resemble real organizational rules.","Social Reasoning & Theory of Mind, Inhibitory Control, Decision-making, Planning, Working Memory, Language Comprehension, Language Production, Reward Mechanisms"
ARC-AGI,https://arcprize.org/arc-agi,https://arxiv.org/abs/1911.01547,"ARC-AGI tests “fluid” abstraction and generalization by asking systems to infer transformation rules from a few input–output grid examples and apply them to new grids. The tasks are designed to be novel and to reward compact, compositional reasoning rather than broad factual recall.","Cognitive Flexibility, Spatial Representation & Mapping, Scene Understanding & Visual Reasoning, Working Memory, Logical Reasoning, Attention"
MCP-Atlas,https://scale.com/leaderboard/mcp_atlas,,"MCP-Atlas benchmarks tool use in realistic, multi-step workflows using the Model Context Protocol, where models must discover relevant tools, call them with correct schemas, handle errors, and compose results. It targets agentic reliability in heterogeneous tool ecosystems rather than single-shot QA.","Planning, Decision-making, Working Memory, Adaptive Error Correction, Semantic Understanding & Context Recognition, Language Comprehension, Language Production"
Humanity’s Last Exam,https://agi.safe.ai/,https://arxiv.org/abs/2501.14249,"Humanity’s Last Exam is a large, frontier-focused benchmark intended to probe difficult reasoning and academic knowledge, often across multimodal questions. It emphasizes solving novel or high-complexity problems (sometimes with optional tools) and producing verifiably correct responses under exam-like conditions.","Language Comprehension, Language Production, Logical Reasoning, Semantic Understanding & Context Recognition, Working Memory, Scene Understanding & Visual Reasoning, Visual Perception"
AIME 2025,https://matharena.ai/?view=problem&comp=aime--aime_2025,https://arxiv.org/abs/2505.23281,"AIME 2025 consists of competition mathematics problems drawn from the American Invitational Mathematics Examination, typically requiring multi-step derivations and careful algebra/number theory/combinatorics. It primarily evaluates mathematical reasoning accuracy under time- and attention-like constraints (single-shot answers).","Logical Reasoning, Working Memory, Attention, Planning"
GPQA Diamond,https://artificialanalysis.ai/evaluations/gpqa-diamond,https://arxiv.org/abs/2311.12022,"GPQA Diamond is a subset of challenging graduate-level science multiple-choice questions curated to be difficult for non-experts and resistant to shallow pattern-matching. Success requires precise comprehension, scientific domain reasoning, and selecting the best-supported option among distractors.","Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Decision-making"
MMMLU,https://huggingface.co/datasets/openai/MMMLU,https://arxiv.org/abs/2009.03300,"MMMLU extends broad academic knowledge testing into multiple languages across many subjects, evaluating whether a model’s competence transfers beyond English. It assesses multilingual understanding, subject-matter recall, and reasoning needed to choose correct answers across diverse domains.","Language Comprehension, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory"
MMMU-Pro,https://mmmu-benchmark.github.io/,https://arxiv.org/abs/2409.02813,"MMMU-Pro is a multimodal benchmark emphasizing expert-level understanding and reasoning over text-plus-image problems across many disciplines. Compared with earlier MMMU settings, it pushes higher difficulty and more robust evaluation of visual evidence use and cross-modal reasoning.","Visual Perception, Scene Understanding & Visual Reasoning, Multisensory Integration, Logical Reasoning, Working Memory, Language Comprehension, Language Production"
ScreenShot-Pro,https://gui-agent.github.io/grounding-leaderboard/,https://arxiv.org/abs/2504.07981,"ScreenShot-Pro evaluates GUI understanding from screenshots, typically requiring a model/agent to interpret interface layout and answer questions or ground actions to specific UI elements. It measures whether systems can reliably map visual structures (menus, buttons, forms) to intended operations in software contexts.","Visual Perception, Visual Attention & Eye Movements, Spatial Representation & Mapping, Scene Understanding & Visual Reasoning, Sensorimotor Coordination, Decision-making, Planning"
CharXiv Reasoning,https://charxiv.github.io/,https://arxiv.org/abs/2406.18521,"CharXiv Reasoning focuses on reasoning over scientific figures (and associated paper context), requiring models to extract information from plots/diagrams and answer higher-level questions. It stresses faithful visual evidence use, cross-referencing textual context, and multi-step scientific reasoning.","Visual Perception, Scene Understanding & Visual Reasoning, Semantic Understanding & Context Recognition, Logical Reasoning, Working Memory, Language Comprehension, Language Production"
Video-MMMU,https://videommmu.github.io/,https://arxiv.org/abs/2501.13826,"Video-MMMU evaluates multimodal understanding over videos paired with questions that require temporal integration and reasoning. It probes whether models can track events across time, relate visual changes to language questions, and synthesize a coherent answer from dynamic content.","Visual Perception, Cognitive Timing & Predictive Modeling, Working Memory, Attention, Multisensory Integration, Language Comprehension, Logical Reasoning"
MRCR v2 (8-needle),https://huggingface.co/datasets/openai/mrcr,https://arxiv.org/abs/2409.12640,MRCR v2 (8-needle) tests long-context multi-round coreference and retrieval by embedding multiple similar “needle” requests in long “haystacks” of text and asking the model to reproduce the correct target response. It primarily stresses sustained context maintenance and precise reference tracking under heavy interference.,"Working Memory, Attention, Episodic Memory, Language Comprehension, Language Production, Semantic Understanding & Context Recognition"
GDPval,https://openai.com/index/gdpval/,https://arxiv.org/abs/2510.04374,"GDPval measures performance on well-specified professional knowledge-work tasks spanning many occupations, scored by expert human judges via pairwise comparisons. Tasks often require producing real work artifacts (e.g., plans, analyses, documents), emphasizing end-to-end execution quality and usefulness.","Planning, Decision-making, Working Memory, Semantic Understanding & Context Recognition, Language Production, Language Comprehension, Self-reflection"
SWE-Lancer,https://openai.com/index/swe-lancer/,https://arxiv.org/abs/2502.12115,"SWE-Lancer evaluates software engineering in a more “real-world job” style, where systems tackle practical coding tasks resembling freelance or contract work (e.g., implementing features, fixing bugs, making improvements) under realistic constraints. It emphasizes reliable end-to-end delivery, not just isolated snippets or toy problems.","Planning, Decision-making, Adaptive Error Correction, Working Memory, Logical Reasoning, Language Comprehension, Language Production"
Graphwalks,https://huggingface.co/datasets/openai/graphwalks,,"Graphwalks tests structured long-context reasoning over graphs described in text, where models must perform traversal-like operations (e.g., follow edges, compute reachable nodes, track parent relationships). It targets robustness to distraction and the ability to maintain and manipulate an internal structured representation over many steps.","Spatial Representation & Mapping, Working Memory, Attention, Logical Reasoning, Cognitive Timing & Predictive Modeling"
Toolathon,https://toolathlon.xyz,https://arxiv.org/abs/2510.25726,"Toolathon evaluates agentic tool use across varied tasks that require selecting tools, issuing correct calls, integrating tool outputs, and recovering from tool failures. It emphasizes practical orchestration—choosing the right sequence of actions—rather than single-model, single-turn generation.","Planning, Decision-making, Working Memory, Adaptive Error Correction, Semantic Understanding & Context Recognition, Language Comprehension, Language Production"
HMMT,https://matharena.ai/?view=problem&comp=hmmt--hmmt_feb_2025,https://arxiv.org/abs/2505.23281,"HMMT consists of high-difficulty contest mathematics problems (e.g., from Harvard-MIT Mathematics Tournament settings) that demand creative multi-step reasoning and careful symbolic manipulation. It probes reliability under complex constraints and the ability to avoid subtle logical or arithmetic errors.","Logical Reasoning, Working Memory, Attention, Planning"
FrontierMath,https://epoch.ai/frontiermath,https://arxiv.org/abs/2411.04872,"FrontierMath targets expert-level mathematics beyond standard contests, with problems designed to be difficult for current systems and to reflect research-grade or near-research-grade reasoning. It evaluates sustained formal problem solving, error-avoidant derivation, and the ability to combine multiple advanced techniques.","Logical Reasoning, Working Memory, Planning, Adaptive Error Correction, Attention"
